"""
ì„¤ë¬¸ì¡°ì‚¬ ë°ì´í„° ì²˜ë¦¬ ë° OpenSearch ìƒ‰ì¸ ëª¨ë“ˆ
"""

import pandas as pd
from opensearchpy import OpenSearch
from opensearchpy.helpers import streaming_bulk
import datetime
import json
import logging
from typing import Tuple, Dict, Any, Generator, List

logger = logging.getLogger(__name__)


def validate_response_data(df: pd.DataFrame) -> bool:
    """
    ì‘ë‹µ ë°ì´í„°í”„ë ˆì„ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.

    Args:
        df: ì‘ë‹µ ë°ì´í„°í”„ë ˆì„

    Returns:
        ê²€ì¦ ì„±ê³µ ì—¬ë¶€
    """
    required_columns = ['mb_sn']

    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    if not all(col in df.columns for col in required_columns):
        missing = [col for col in required_columns if col not in df.columns]
        logger.error(f"ğŸš¨ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}")
        return False

    # mb_sn null í™•ì¸
    null_count = df['mb_sn'].isnull().sum()
    if null_count > 0:
        logger.warning(f"âš ï¸ mb_snì´ nullì¸ í–‰ì´ {null_count}ê°œ ìˆìŠµë‹ˆë‹¤. í•´ë‹¹ í–‰ì€ ê±´ë„ˆëœë‹ˆë‹¤.")

    return True


def generate_user_documents(
    df_chunk: pd.DataFrame,
    questions_meta: Dict[str, Dict[str, Any]],
    index_name: str
) -> Generator[dict, None, None]:
    """
    ë°ì´í„°í”„ë ˆì„ ì²­í¬ë¡œë¶€í„° OpenSearch ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    âœ… MULTI íƒ€ì…: ê° ì„ íƒì§€ë¥¼ ë³„ë„ì˜ qa_pairs í•­ëª©ìœ¼ë¡œ ì €ì¥
    âœ… SINGLE/Numeric/String íƒ€ì…: ë‹¨ì¼ qa_pairs í•­ëª©ìœ¼ë¡œ ì €ì¥
    âœ… ëª¨ë“  ë‹µë³€ì— ëŒ€í•´ embedding_text ìƒì„±
    âœ… answer_vectorëŠ” ë‚˜ì¤‘ì— ë°°ì¹˜ ì„ë² ë”©ìœ¼ë¡œ ìƒì„±

    Args:
        df_chunk: ì‘ë‹µ ë°ì´í„° ì²­í¬
        questions_meta: ì§ˆë¬¸ ë©”íƒ€ë°ì´í„°
        index_name: ì¸ë±ìŠ¤ ì´ë¦„

    Yields:
        OpenSearch bulk APIìš© ì•¡ì…˜ ë”•ì…”ë„ˆë¦¬
    """
    for _, row in df_chunk.iterrows():
        user_id = row.get("mb_sn")

        # user_id ê²€ì¦
        if not user_id or pd.isna(user_id):
            continue

        all_qa_pairs_for_user = []

        for q_code, raw_answer in row.items():
            # mb_sn ê±´ë„ˆë›°ê¸°
            if q_code == "mb_sn":
                continue

            # nullì´ë‚˜ ë¹ˆ ê°’ ê±´ë„ˆë›°ê¸°
            if raw_answer is None or pd.isna(raw_answer):
                continue

            # ì§ˆë¬¸ ë©”íƒ€ë°ì´í„° í™•ì¸
            q_info = questions_meta.get(q_code)
            if not q_info:
                logger.debug(f"  âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ ì½”ë“œ: {q_code}")
                continue

            q_text = q_info["text"]
            q_type = q_info["type"]

            # ë‹µë³€ íƒ€ì…ë³„ ì²˜ë¦¬
            if q_type == "MULTI":
                # âœ… MULTI: ê° ì„ íƒì§€ë¥¼ ë³„ë„ qa_pairë¡œ ì €ì¥
                answer_codes = str(raw_answer).split(",")

                for code in answer_codes:
                    code = code.strip()
                    if code and code != '':
                        answer_text = q_info["options"].get(code, f"ì•Œ ìˆ˜ ì—†ëŠ” ì½”ë“œ: {code}")
                        if answer_text and answer_text != f"ì•Œ ìˆ˜ ì—†ëŠ” ì½”ë“œ: {code}":
                            # embedding_text ìƒì„±
                            embedding_text = f"{q_text} ì§ˆë¬¸ì— '{answer_text}'ë¼ê³  ë‹µë³€"

                            qa_pair_doc = {
                                "q_code": q_code,
                                "q_type": q_type,
                                "q_text": q_text,
                                "answer_text": answer_text,
                                "embedding_text": embedding_text,
                                "answer_vector": None  # ë‚˜ì¤‘ì— ë°°ì¹˜ ì„ë² ë”©
                            }
                            all_qa_pairs_for_user.append(qa_pair_doc)

            elif q_type == "SINGLE":
                # âœ… SINGLE: ë‹¨ì¼ qa_pairë¡œ ì €ì¥
                code = str(raw_answer).strip()
                if code and code != '':
                    answer_text = q_info["options"].get(code, raw_answer)
                    if answer_text:
                        # embedding_text ìƒì„±
                        embedding_text = f"{q_text} ì§ˆë¬¸ì— '{answer_text}'ë¼ê³  ë‹µë³€"

                        qa_pair_doc = {
                            "q_code": q_code,
                            "q_type": q_type,
                            "q_text": q_text,
                            "answer_text": answer_text,
                            "embedding_text": embedding_text,
                            "answer_vector": None  # ë‚˜ì¤‘ì— ë°°ì¹˜ ì„ë² ë”©
                        }
                        all_qa_pairs_for_user.append(qa_pair_doc)

            else:
                # âœ… Numeric, String: ë‹¨ì¼ qa_pairë¡œ ì €ì¥
                answer_text = str(raw_answer).strip()
                if answer_text and answer_text != '':
                    # Numeric íƒ€ì…: ì •ìˆ˜ì¸ ê²½ìš° .0 ì œê±° (2.0 â†’ 2)
                    if q_type == "Numeric":
                        try:
                            # floatë¡œ ë³€í™˜ í›„ ì •ìˆ˜ì¸ì§€ í™•ì¸
                            num_val = float(answer_text)
                            if num_val.is_integer():
                                answer_text = str(int(num_val))
                        except:
                            pass  # ë³€í™˜ ì‹¤íŒ¨ì‹œ ì›ë³¸ ìœ ì§€

                    # embedding_text ìƒì„±
                    embedding_text = f"{q_text} ì§ˆë¬¸ì— '{answer_text}'ë¼ê³  ë‹µë³€"

                    qa_pair_doc = {
                        "q_code": q_code,
                        "q_type": q_type,
                        "q_text": q_text,
                        "answer_text": answer_text,
                        "embedding_text": embedding_text,
                        "answer_vector": None  # ë‚˜ì¤‘ì— ë°°ì¹˜ ì„ë² ë”©
                    }
                    all_qa_pairs_for_user.append(qa_pair_doc)

        if all_qa_pairs_for_user:
            final_user_document = {
                "user_id": str(user_id),
                "timestamp": datetime.datetime.now().isoformat(),
                "qa_pairs": all_qa_pairs_for_user,
            }

            yield {
                "_index": index_name,
                "_id": str(user_id),
                "_source": final_user_document
            }


def process_and_bulk_index(
    os_client: OpenSearch,
    questions_meta: Dict[str, Dict[str, Any]],
    response_file: str,
    index_name: str,
    embedding_model = None,
    chunk_size: int = 1000,
    bulk_chunk_size: int = 500
) -> Tuple[int, int]:
    """
    ì‘ë‹µ CSVë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ì½ê³ , ë³€í™˜í•˜ë©°, OpenSearchì— ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ìƒ‰ì¸í•©ë‹ˆë‹¤.

    Args:
        os_client: OpenSearch í´ë¼ì´ì–¸íŠ¸
        questions_meta: ì§ˆë¬¸ ë©”íƒ€ë°ì´í„°
        response_file: ì‘ë‹µ CSV íŒŒì¼ ê²½ë¡œ
        index_name: ì¸ë±ìŠ¤ ì´ë¦„
        embedding_model: ì„ë² ë”© ëª¨ë¸ (KURE-v1 ë“±)
        chunk_size: CSV ì½ê¸° ì²­í¬ í¬ê¸° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        bulk_chunk_size: bulk API ì²­í¬ í¬ê¸° (ë„¤íŠ¸ì›Œí¬ íš¨ìœ¨ì„±)

    Returns:
        (ì„±ê³µ ê±´ìˆ˜, ì‹¤íŒ¨ ê±´ìˆ˜) íŠœí”Œ
    """

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    try:
        # ë¨¼ì € ì „ì²´ í–‰ ìˆ˜ í™•ì¸ (ì§„í–‰ë¥  í‘œì‹œìš©)
        total_rows = sum(1 for _ in open(response_file, encoding="utf-8-sig")) - 1  # í—¤ë” ì œì™¸
        logger.info(f"ğŸ“Š ì²˜ë¦¬í•  ì´ ì‘ë‹µ ìˆ˜: {total_rows:,}ê°œ")
    except FileNotFoundError:
        logger.error(f"ğŸš¨ ì‘ë‹µ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {response_file}")
        raise
    except Exception as e:
        logger.error(f"ğŸš¨ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        raise

    total_success = 0
    total_failed = 0
    processed_count = 0
    failed_docs = []

    logger.info("â³ ë°ì´í„° ì²˜ë¦¬ ë° ìƒ‰ì¸ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    try:
        # ì²­í¬ ë‹¨ìœ„ë¡œ CSV ì½ê¸°
        chunk_iterator = pd.read_csv(
            response_file,
            encoding="utf-8-sig",
            chunksize=chunk_size,
            dtype=str  # ëª¨ë“  ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ ì½ìŒ
        )

        for chunk_num, df_chunk in enumerate(chunk_iterator, 1):
            # NaNì„ Noneìœ¼ë¡œ ë³€í™˜
            df_chunk = df_chunk.where(pd.notnull(df_chunk), None)

            # ë°ì´í„° ê²€ì¦ (ì²« ì²­í¬ë§Œ)
            if chunk_num == 1:
                if not validate_response_data(df_chunk):
                    raise ValueError("ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨")

            # 1. ë¬¸ì„œ ìƒì„± (ì„ë² ë”© ì—†ì´)
            actions = list(generate_user_documents(df_chunk, questions_meta, index_name))

            if not actions:
                continue

            # 2. ë°°ì¹˜ ì„ë² ë”© ìƒì„±
            if embedding_model:
                _generate_batch_embeddings_for_actions(actions, embedding_model, batch_size=64)

            # ì²« ë²ˆì§¸ ì²­í¬ì˜ ìƒ˜í”Œ ì¶œë ¥
            if chunk_num == 1:
                logger.info("\n--- ğŸ“„ ì²« ë²ˆì§¸ ì‚¬ìš©ì ë¬¸ì„œ ìƒ˜í”Œ (ì„ë² ë”© í¬í•¨) ---")
                if actions:
                    logger.info(json.dumps(actions[0]["_source"], indent=2, ensure_ascii=False))
                logger.info("--------------------------------\n")

            # 3. bulk index
            for ok, response in streaming_bulk(
                os_client,
                actions,
                chunk_size=bulk_chunk_size,
                raise_on_error=False,
                raise_on_exception=False,
                request_timeout=60
            ):
                processed_count += 1

                if ok:
                    total_success += 1
                else:
                    total_failed += 1
                    # ì‹¤íŒ¨í•œ ë¬¸ì„œ ì •ë³´ ê¸°ë¡ (ìµœëŒ€ 100ê°œê¹Œì§€ë§Œ)
                    if len(failed_docs) < 100:
                        failed_docs.append(response)

                    # ì²« 10ê°œ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ë§Œ ìƒì„¸ ë¡œê·¸
                    if total_failed <= 10:
                        logger.error(f"âŒ ë¬¸ì„œ ìƒ‰ì¸ ì‹¤íŒ¨: {response}")

            # ì§„í–‰ë¥  í‘œì‹œ (ì²­í¬ ë‹¨ìœ„)
            progress = (processed_count / total_rows * 100) if total_rows > 0 else 0
            logger.info(f"ì²­í¬ {chunk_num} ì²˜ë¦¬ ì™„ë£Œ... {processed_count:,}/{total_rows:,} ({progress:.1f}%) "
                  f"| ì„±ê³µ: {total_success:,} | ì‹¤íŒ¨: {total_failed:,}")

        print("\n")  # ì¤„ë°”ê¿ˆ

        # ìµœì¢… ê²°ê³¼
        logger.info("=" * 60)
        logger.info(f"ğŸ‰ ìƒ‰ì¸ ì‘ì—… ì™„ë£Œ!")
        logger.info(f"   âœ… ì„±ê³µ: {total_success:,}ê°œ")
        logger.info(f"   âŒ ì‹¤íŒ¨: {total_failed:,}ê°œ")
        logger.info(f"   ğŸ“Š ì´ ì²˜ë¦¬: {processed_count:,}ê°œ")

        if total_failed > 0:
            logger.warning(f"\nâš ï¸ {total_failed}ê°œ ë¬¸ì„œ ìƒ‰ì¸ ì‹¤íŒ¨")
            if failed_docs:
                logger.warning("ì‹¤íŒ¨í•œ ë¬¸ì„œ ìƒ˜í”Œ (ìµœëŒ€ 5ê°œ):")
                for i, doc in enumerate(failed_docs[:5], 1):
                    logger.warning(f"  {i}. {doc}")

        logger.info("=" * 60)

        # ì¸ë±ìŠ¤ refresh (ê²€ìƒ‰ ê°€ëŠ¥í•˜ë„ë¡)
        logger.info("ğŸ”„ ì¸ë±ìŠ¤ refresh ì¤‘...")
        os_client.indices.refresh(index=index_name)
        logger.info("âœ… refresh ì™„ë£Œ")

        return total_success, total_failed

    except Exception as e:
        logger.error(f"ğŸš¨ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}", exc_info=True)
        raise


def _generate_batch_embeddings_for_actions(
    actions: List[Dict],
    embedding_model,
    batch_size: int = 64
) -> None:
    """
    ì•¡ì…˜ ë¦¬ìŠ¤íŠ¸ì˜ ëª¨ë“  qa_pairsì— ëŒ€í•´ ë°°ì¹˜ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        actions: OpenSearch bulk APIìš© ì•¡ì…˜ ë¦¬ìŠ¤íŠ¸
        embedding_model: ì„ë² ë”© ëª¨ë¸ (KURE-v1 ë“±)
        batch_size: ë°°ì¹˜ í¬ê¸°
    """
    # 1. ëª¨ë“  embedding_text ìˆ˜ì§‘
    all_texts = []
    text_indices = []  # (action_idx, qa_pair_idx)

    for action_idx, action in enumerate(actions):
        qa_pairs = action["_source"].get("qa_pairs", [])
        for qa_idx, qa_pair in enumerate(qa_pairs):
            embedding_text = qa_pair.get("embedding_text")
            if embedding_text:
                all_texts.append(embedding_text)
                text_indices.append((action_idx, qa_idx))

    if not all_texts:
        logger.debug("ğŸ“Š ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ì—†ìŒ")
        return

    logger.info(f"ğŸ“Š {len(all_texts):,}ê°œ í…ìŠ¤íŠ¸ ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì¤‘... (ë°°ì¹˜ í¬ê¸°: {batch_size})")

    try:
        # ë°°ì¹˜ ì¸ì½”ë”©
        vectors = embedding_model.encode(
            all_texts,
            batch_size=batch_size,
            show_progress_bar=False,
            convert_to_tensor=False
        )

        # numpy arrayë¥¼ listë¡œ ë³€í™˜
        if hasattr(vectors, 'tolist'):
            vectors = [v.tolist() if hasattr(v, 'tolist') else v for v in vectors]
        elif not isinstance(vectors, list):
            vectors = vectors.tolist()

        # ë²¡í„° í• ë‹¹
        for (action_idx, qa_idx), vector in zip(text_indices, vectors):
            actions[action_idx]["_source"]["qa_pairs"][qa_idx]["answer_vector"] = vector

        logger.info(f"âœ… ë°°ì¹˜ ì„ë² ë”© ì™„ë£Œ: {len(vectors):,}ê°œ")

    except Exception as e:
        logger.error(f"âŒ ë°°ì¹˜ ì„ë² ë”© ì‹¤íŒ¨: {e}", exc_info=True)
        # Fallback: ì„ë² ë”© ì—†ì´ ì§„í–‰ (answer_vectorëŠ” Noneìœ¼ë¡œ ìœ ì§€)
        logger.warning("âš ï¸ ì„ë² ë”© ì—†ì´ ìƒ‰ì¸ì„ ì§„í–‰í•©ë‹ˆë‹¤")


def verify_indexed_data(
    os_client: OpenSearch,
    index_name: str,
    sample_user_id: str = None
) -> None:
    """
    ìƒ‰ì¸ëœ ë°ì´í„°ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.

    Args:
        os_client: OpenSearch í´ë¼ì´ì–¸íŠ¸
        index_name: ì¸ë±ìŠ¤ ì´ë¦„
        sample_user_id: ìƒ˜í”Œë¡œ ì¡°íšŒí•  ì‚¬ìš©ì ID (ì„ íƒ)
    """
    try:
        # ì „ì²´ ë¬¸ì„œ ìˆ˜ í™•ì¸
        count = os_client.count(index=index_name)
        logger.info(f"ğŸ“Š '{index_name}' ì¸ë±ìŠ¤ì˜ ì´ ë¬¸ì„œ ìˆ˜: {count['count']:,}ê°œ")

        # ìƒ˜í”Œ ë¬¸ì„œ ì¡°íšŒ
        if sample_user_id:
            doc = os_client.get(index=index_name, id=sample_user_id)
            logger.info(f"\n--- ìƒ˜í”Œ ë¬¸ì„œ (user_id: {sample_user_id}) ---")
            logger.info(json.dumps(doc['_source'], indent=2, ensure_ascii=False))
        else:
            # ëœë¤ ìƒ˜í”Œ ì¡°íšŒ
            result = os_client.search(
                index=index_name,
                body={
                    "size": 1,
                    "query": {"match_all": {}}
                }
            )
            if result['hits']['hits']:
                doc = result['hits']['hits'][0]
                logger.info(f"\n--- ìƒ˜í”Œ ë¬¸ì„œ (user_id: {doc['_id']}) ---")
                logger.info(json.dumps(doc['_source'], indent=2, ensure_ascii=False))

    except Exception as e:
        logger.error(f"âš ï¸ ë°ì´í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
