"""
ë°ì´í„° ì¸ë±ì„œ FastAPI ë¼ìš°í„°
"""

import logging
import os
from typing import Optional
from fastapi import APIRouter, HTTPException, Depends, Query
from opensearchpy import OpenSearch
from pydantic import BaseModel, Field

from .parser import parse_question_metadata, validate_metadata
from .opensearch import (
    create_survey_index,
    update_index_refresh_interval,
    get_index_stats,
    force_merge_index
)
from .core import process_and_bulk_index, verify_indexed_data

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

router = APIRouter(
    prefix="/indexer",
    tags=["Data Indexer"]
)


class IndexConfig(BaseModel):
    """ì¸ë±ì‹± ì„¤ì •"""
    index_name: str = Field(default="s_welcome_2nd", description="ì¸ë±ìŠ¤ ì´ë¦„")
    question_file: str = Field(default="./data/question_list.csv", description="ì§ˆë¬¸ ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ")
    response_file: str = Field(default="./data/response_list.csv", description="ì‘ë‹µ ë°ì´í„° íŒŒì¼ ê²½ë¡œ")
    force_recreate: bool = Field(default=False, description="ê¸°ì¡´ ì¸ë±ìŠ¤ ê°•ì œ ì¬ìƒì„± ì—¬ë¶€")
    chunk_size: int = Field(default=1000, ge=100, le=10000, description="CSV ì½ê¸° ì²­í¬ í¬ê¸°")
    bulk_chunk_size: int = Field(default=500, ge=100, le=2000, description="Bulk API ì²­í¬ í¬ê¸°")
    number_of_shards: int = Field(default=3, ge=1, le=10, description="ìƒ¤ë“œ ê°œìˆ˜")
    number_of_replicas: int = Field(default=1, ge=0, le=3, description="ë³µì œë³¸ ê°œìˆ˜")
    optimize_after_indexing: bool = Field(default=True, description="ìƒ‰ì¸ í›„ ìµœì í™”(force merge) ìˆ˜í–‰ ì—¬ë¶€")


class IndexResponse(BaseModel):
    """ì¸ë±ì‹± ì‘ë‹µ"""
    message: str
    index_name: str
    success_count: int
    failed_count: int
    total_docs: Optional[int] = None
    index_size_mb: Optional[float] = None


@router.get("/")
def read_root():
    """API ìƒíƒœ í™•ì¸"""
    return {
        "message": "Survey Data Indexer API ì‹¤í–‰ ì¤‘",
        "version": "2.0",
        "endpoints": [
            "/indexer/index-survey-data",
            "/indexer/index/{index_name}"
        ]
    }


@router.post("/index-survey-data", response_model=IndexResponse)
def index_survey_data_by_user(
    config: IndexConfig,
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """
    ì„¤ë¬¸ ë°ì´í„° ì „ì²´ë¥¼ ìƒ‰ì¸í•˜ëŠ” ë©”ì¸ ë¡œì§ ì‹¤í–‰.

    - ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ/ìœ ì§€ ì˜µì…˜
    - ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
    - ìŠ¤íŠ¸ë¦¬ë° bulkë¡œ ì•ˆì •ì ì¸ ìƒ‰ì¸
    - ë°°ì¹˜ ì„ë² ë”© ìƒì„±
    - ìƒ‰ì¸ í›„ ìë™ ìµœì í™”
    """
    try:
        # OpenSearch ì—°ê²° í™•ì¸
        if not os_client or not os_client.ping():
            raise HTTPException(
                status_code=503,
                detail="OpenSearch ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”."
            )

        # ì„ë² ë”© ëª¨ë¸ í™•ì¸
        embedding_model = getattr(router, 'embedding_model', None)
        if not embedding_model:
            logger.warning("âš ï¸ ì„ë² ë”© ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ì„ë² ë”© ì—†ì´ ìƒ‰ì¸ì„ ì§„í–‰í•©ë‹ˆë‹¤.")

        logger.info("\n" + "=" * 60)
        logger.info("ğŸš€ ì„¤ë¬¸ ë°ì´í„° ìƒ‰ì¸ ì‘ì—… ì‹œì‘")
        logger.info("=" * 60)
        logger.info(f"   ì¸ë±ìŠ¤: {config.index_name}")
        logger.info(f"   ì§ˆë¬¸ íŒŒì¼: {config.question_file}")
        logger.info(f"   ì‘ë‹µ íŒŒì¼: {config.response_file}")
        logger.info(f"   ê°•ì œ ì¬ìƒì„±: {config.force_recreate}")
        logger.info(f"   ì²­í¬ í¬ê¸°: {config.chunk_size}")
        logger.info(f"   Bulk ì²­í¬: {config.bulk_chunk_size}")
        logger.info(f"   ì„ë² ë”© ëª¨ë¸: {'ì‚¬ìš©' if embedding_model else 'ë¯¸ì‚¬ìš©'}")
        logger.info("=" * 60 + "\n")

        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(config.question_file):
            raise HTTPException(
                status_code=404,
                detail=f"ì§ˆë¬¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config.question_file}"
            )
        if not os.path.exists(config.response_file):
            raise HTTPException(
                status_code=404,
                detail=f"ì‘ë‹µ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config.response_file}"
            )

        # 1ë‹¨ê³„: ì¸ë±ìŠ¤ ìƒì„±
        logger.info("\n[1/4] ğŸ“ ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        create_survey_index(
            os_client=os_client,
            index_name=config.index_name,
            force_recreate=config.force_recreate,
            number_of_shards=config.number_of_shards,
            number_of_replicas=config.number_of_replicas
        )

        # 2ë‹¨ê³„: ì§ˆë¬¸ ë©”íƒ€ë°ì´í„° íŒŒì‹±
        logger.info("\n[2/4] ğŸ“– ì§ˆë¬¸ ë©”íƒ€ë°ì´í„° íŒŒì‹± ì¤‘...")
        questions_meta = parse_question_metadata(config.question_file)

        # ë©”íƒ€ë°ì´í„° ê²€ì¦
        if not validate_metadata(questions_meta):
            raise HTTPException(
                status_code=400,
                detail="ì§ˆë¬¸ ë©”íƒ€ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨"
            )

        # 3ë‹¨ê³„: ë°ì´í„° ì²˜ë¦¬ ë° ìƒ‰ì¸
        logger.info("\n[3/4] ğŸ”„ ë°ì´í„° ì²˜ë¦¬ ë° ìƒ‰ì¸ ì¤‘...")
        success_count, failed_count = process_and_bulk_index(
            os_client=os_client,
            questions_meta=questions_meta,
            response_file=config.response_file,
            index_name=config.index_name,
            embedding_model=embedding_model,
            chunk_size=config.chunk_size,
            bulk_chunk_size=config.bulk_chunk_size
        )

        # 4ë‹¨ê³„: ìƒ‰ì¸ í›„ ìµœì í™”
        logger.info("\n[4/4] âš™ï¸ ìƒ‰ì¸ í›„ ìµœì í™” ì¤‘...")

        # refresh_intervalì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ë³µêµ¬
        update_index_refresh_interval(os_client, config.index_name, "1s")

        # force merge (ì„ íƒì‚¬í•­)
        if config.optimize_after_indexing:
            force_merge_index(os_client, config.index_name)

        # ì¸ë±ìŠ¤ í†µê³„ ì¡°íšŒ
        stats = get_index_stats(os_client, config.index_name)

        logger.info("\n" + "=" * 60)
        logger.info("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        logger.info("=" * 60 + "\n")

        return IndexResponse(
            message="ë°ì´í„° ìƒ‰ì¸ ì‘ì—… ì™„ë£Œ",
            index_name=config.index_name,
            success_count=success_count,
            failed_count=failed_count,
            total_docs=stats['doc_count'] if stats else None,
            index_size_mb=stats['size_mb'] if stats else None
        )

    except HTTPException:
        raise
    except FileNotFoundError as e:
        logger.error(f"ğŸš¨ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        raise HTTPException(status_code=404, detail=str(e))
    except ValueError as e:
        logger.error(f"ğŸš¨ ë°ì´í„° ê²€ì¦ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"ğŸš¨ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )


@router.delete("/index/{index_name}")
def delete_index(
    index_name: str,
    confirm: bool = Query(False, description="ì‚­ì œ í™•ì¸"),
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """
    ì¸ë±ìŠ¤ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤. (ì£¼ì˜: ë³µêµ¬ ë¶ˆê°€ëŠ¥)
    """
    if not confirm:
        raise HTTPException(
            status_code=400,
            detail="ì¸ë±ìŠ¤ ì‚­ì œë¥¼ í™•ì¸í•˜ë ¤ë©´ confirm=trueë¥¼ ì„¤ì •í•˜ì„¸ìš”."
        )

    try:
        if not os_client.indices.exists(index=index_name):
            raise HTTPException(
                status_code=404,
                detail=f"ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {index_name}"
            )

        os_client.indices.delete(index=index_name)
        logger.info(f"ğŸ—‘ï¸ '{index_name}' ì¸ë±ìŠ¤ë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")

        return {
            "message": f"'{index_name}' ì¸ë±ìŠ¤ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "index_name": index_name
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ğŸš¨ ì¸ë±ìŠ¤ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))
