"""KURE-v1 ì„ë² ë”© ìœ í‹¸ë¦¬í‹°"""
import logging
import torch
from typing import List, Union
from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)


class KUREv1EmbeddingModel:
    """
    KURE-v1 í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë˜í¼

    íŠ¹ì§•:
    - 1024ì°¨ì› dense vector
    - ìµœëŒ€ 512 í† í° ê¸¸ì´
    - í•œêµ­ì–´ ìµœì í™”
    - GPU ê°€ì† ì§€ì›
    """

    def __init__(
        self,
        model_name: str = "nlpai-lab/KURE-v1",
        device: str = None,
        batch_size: int = 32
    ):
        """
        Args:
            model_name: ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸: KURE-v1)
            device: ì¥ì¹˜ ('cuda', 'cpu', None=ìë™)
            batch_size: ë°°ì¹˜ í¬ê¸°
        """
        self.model_name = model_name
        self.batch_size = batch_size

        # ì¥ì¹˜ ìë™ ì„ íƒ
        if device is None:
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        else:
            self.device = device

        # ëª¨ë¸ ë¡œë“œ
        logger.info(f"ğŸ”„ KURE-v1 ëª¨ë¸ ë¡œë”© ì¤‘... (ì¥ì¹˜: {self.device})")

        self.model = SentenceTransformer(
            model_name,
            device=self.device
        )

        # KURE-v1 ì„¤ì •
        self.model.max_seq_length = 512  # ìµœëŒ€ í† í° ê¸¸ì´
        self.embedding_dim = 1024

        logger.info(f"âœ… KURE-v1 ë¡œë“œ ì™„ë£Œ (ì°¨ì›: {self.embedding_dim})")

        # ëª¨ë¸ ì›Œë°ì—…
        self._warmup()

    def _warmup(self):
        """ëª¨ë¸ ì›Œë°ì—… (ì²« ì‹¤í–‰ ì§€ì—° ì œê±°)"""
        try:
            dummy_text = "ëª¨ë¸ ì›Œë°ì—… í…ŒìŠ¤íŠ¸"
            _ = self.model.encode(dummy_text, convert_to_numpy=True)
            logger.info("ğŸ”¥ ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ ì›Œë°ì—… ì‹¤íŒ¨ (ë¬´ì‹œ ê°€ëŠ¥): {e}")

    def encode(
        self,
        texts: Union[str, List[str]],
        batch_size: int = None,
        show_progress: bool = False,
        normalize_embeddings: bool = True
    ) -> Union[List[float], List[List[float]]]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜

        Args:
            texts: ë‹¨ì¼ í…ìŠ¤íŠ¸ ë˜ëŠ” í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸° (Noneì´ë©´ ê¸°ë³¸ê°’)
            show_progress: ì§„í–‰ìƒí™© í‘œì‹œ
            normalize_embeddings: L2 ì •ê·œí™” ì—¬ë¶€

        Returns:
            ì„ë² ë”© ë²¡í„° (ë‹¨ì¼ or ë¦¬ìŠ¤íŠ¸)
        """
        if batch_size is None:
            batch_size = self.batch_size

        # ë‹¨ì¼ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        if isinstance(texts, str):
            vector = self.model.encode(
                texts,
                convert_to_numpy=False,
                normalize_embeddings=normalize_embeddings,
                show_progress_bar=False
            )
            return vector.tolist()

        # ë°°ì¹˜ ì²˜ë¦¬
        if len(texts) == 0:
            return []

        logger.debug(f"ğŸ“Š ë°°ì¹˜ ì„ë² ë”©: {len(texts)}ê°œ í…ìŠ¤íŠ¸ (ë°°ì¹˜: {batch_size})")

        vectors = self.model.encode(
            texts,
            batch_size=batch_size,
            convert_to_numpy=False,
            normalize_embeddings=normalize_embeddings,
            show_progress_bar=show_progress
        )

        return [v.tolist() for v in vectors]

    def encode_batch_with_metadata(
        self,
        texts: List[str],
        metadata: List[dict] = None,
        batch_size: int = None
    ) -> List[dict]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•˜ê³  ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ë°˜í™˜

        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            metadata: ê° í…ìŠ¤íŠ¸ì— ëŒ€ì‘í•˜ëŠ” ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            [{"text": ..., "vector": [...], "metadata": {...}}, ...]
        """
        vectors = self.encode(texts, batch_size=batch_size)

        results = []
        for i, (text, vector) in enumerate(zip(texts, vectors)):
            result = {
                "text": text,
                "vector": vector,
                "metadata": metadata[i] if metadata else {}
            }
            results.append(result)

        return results

    def get_similarity(
        self,
        text1: str,
        text2: str,
        metric: str = "cosine"
    ) -> float:
        """
        ë‘ í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ ê³„ì‚°

        Args:
            text1: ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸
            text2: ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸
            metric: ìœ ì‚¬ë„ ë©”íŠ¸ë¦­ ('cosine', 'euclidean')

        Returns:
            ìœ ì‚¬ë„ ì ìˆ˜
        """
        v1 = torch.tensor(self.encode(text1))
        v2 = torch.tensor(self.encode(text2))

        if metric == "cosine":
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            similarity = torch.nn.functional.cosine_similarity(
                v1.unsqueeze(0),
                v2.unsqueeze(0)
            ).item()
        elif metric == "euclidean":
            # ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ (ì‘ì„ìˆ˜ë¡ ìœ ì‚¬)
            distance = torch.nn.functional.pairwise_distance(
                v1.unsqueeze(0),
                v2.unsqueeze(0)
            ).item()
            # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
            similarity = 1 / (1 + distance)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë©”íŠ¸ë¦­: {metric}")

        return similarity

    def get_info(self) -> dict:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_name": self.model_name,
            "embedding_dim": self.embedding_dim,
            "max_seq_length": self.model.max_seq_length,
            "device": self.device,
            "batch_size": self.batch_size,
            "cuda_available": torch.cuda.is_available()
        }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_embedding_model_instance = None


def get_embedding_model(
    model_name: str = "nlpai-lab/KURE-v1",
    device: str = None,
    batch_size: int = 32
) -> KUREv1EmbeddingModel:
    """ì„ë² ë”© ëª¨ë¸ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _embedding_model_instance

    if _embedding_model_instance is None:
        _embedding_model_instance = KUREv1EmbeddingModel(
            model_name=model_name,
            device=device,
            batch_size=batch_size
        )

    return _embedding_model_instance
