"""OpenSearch ìœ í‹¸ë¦¬í‹° ë° ì¸ë±ìŠ¤ ê´€ë¦¬"""
import logging
from typing import List, Dict, Any
from opensearchpy import OpenSearch
from opensearchpy.helpers import bulk
from ..config import Config

logger = logging.getLogger(__name__)


def create_opensearch_client(config: Config) -> OpenSearch:
    """OpenSearch í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    client = OpenSearch(
        hosts=[{
            "host": config.OPENSEARCH_HOST,
            "port": config.OPENSEARCH_PORT
        }],
        http_auth=(config.OPENSEARCH_USER, config.OPENSEARCH_PASSWORD),
        use_ssl=config.OPENSEARCH_USE_SSL,
        verify_certs=False,
        ssl_show_warn=False,
        timeout=30,
        max_retries=3,
        retry_on_timeout=True
    )

    # ì—°ê²° í…ŒìŠ¤íŠ¸
    try:
        info = client.info()
        logger.info(f"âœ… OpenSearch ì—°ê²° ì„±ê³µ: v{info['version']['number']}")
        return client
    except Exception as e:
        logger.error(f"âŒ OpenSearch ì—°ê²° ì‹¤íŒ¨: {e}")
        raise


def create_hybrid_index(client: OpenSearch, index_name: str, config: Config):
    """í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì¡°ì˜ OpenSearch ì¸ë±ìŠ¤ ìƒì„±"""

    # kNN í”ŒëŸ¬ê·¸ì¸ ì„¤ì •
    settings = {
        "index": {
            "knn": True,  # kNN í™œì„±í™”
            "knn.algo_param.ef_search": 512,  # ê²€ìƒ‰ ì‹œ íƒìƒ‰ ë²”ìœ„
            "number_of_shards": 2,
            "number_of_replicas": 1,
            "refresh_interval": "5s"
        },
        "analysis": {
            "analyzer": {
                "korean_analyzer": {
                    "type": "custom",
                    "tokenizer": "nori_tokenizer",
                    "filter": ["nori_part_of_speech", "lowercase", "nori_readingform"]
                }
            }
        }
    }

    # ë§¤í•‘ ì •ì˜
    mappings = {
        "properties": {
            "user_id": {
                "type": "keyword"
            },

            # ì¸êµ¬í†µê³„ í•„í„° (ì •í™•í•œ term ë§¤ì¹­)
            "demographics": {
                "properties": {
                    "age_group": {"type": "keyword"},
                    "gender": {"type": "keyword"},
                    "region": {"type": "keyword"},
                    "occupation": {"type": "keyword"},
                    "income": {"type": "keyword"},
                    "education": {"type": "keyword"},
                    "marital_status": {"type": "keyword"},
                    "household": {"type": "keyword"}
                }
            },

            # ê¸°íƒ€ ê°ê´€ì‹ (ì¸êµ¬í†µê³„ ì•„ë‹˜)
            "other_objectives": {
                "type": "object",
                "enabled": True
            },

            # ì£¼ê´€ì‹ ì‘ë‹µ (nested + kNN vector)
            "subjective_responses": {
                "type": "nested",
                "properties": {
                    "q_text": {
                        "type": "text",
                        "analyzer": "korean_analyzer",
                        "fields": {
                            "keyword": {"type": "keyword"}
                        }
                    },
                    "q_code": {"type": "keyword"},
                    "q_category": {"type": "keyword"},
                    "answer_text": {
                        "type": "text",
                        "analyzer": "korean_analyzer",
                        "term_vector": "with_positions_offsets"  # í•˜ì´ë¼ì´íŒ…ìš©
                    },
                    "answer_vector": {
                        "type": "knn_vector",
                        "dimension": config.EMBEDDING_DIM,
                        "method": {
                            "name": "hnsw",
                            "space_type": "cosinesimil",
                            "engine": config.VECTOR_ENGINE,
                            "parameters": {
                                "ef_construction": config.HNSW_EF_CONSTRUCTION,
                                "m": config.HNSW_M
                            }
                        }
                    },
                    "answer_length": {"type": "integer"}
                }
            },

            # ì „ì²´ ì£¼ê´€ì‹ í†µí•© í…ìŠ¤íŠ¸ (í‚¤ì›Œë“œ ê²€ìƒ‰ìš©)
            "all_subjective_text": {
                "type": "text",
                "analyzer": "korean_analyzer"
            },

            # ë©”íƒ€ë°ì´í„°
            "metadata": {
                "properties": {
                    "timestamp": {"type": "date"},
                    "total_questions": {"type": "integer"},
                    "demographic_count": {"type": "integer"},
                    "objective_count": {"type": "integer"},
                    "subjective_count": {"type": "integer"},
                    "avg_answer_length": {"type": "float"}
                }
            }
        }
    }

    # ì¸ë±ìŠ¤ ìƒì„±
    if client.indices.exists(index=index_name):
        logger.info(f"ğŸ‘ ì¸ë±ìŠ¤ '{index_name}' ì´ë¯¸ ì¡´ì¬")
        return

    try:
        client.indices.create(
            index=index_name,
            body={"settings": settings, "mappings": mappings}
        )
        logger.info(f"âœ… OpenSearch í•˜ì´ë¸Œë¦¬ë“œ ì¸ë±ìŠ¤ '{index_name}' ìƒì„± ì™„ë£Œ")

    except Exception as e:
        logger.error(f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        raise


def bulk_index_documents(client: OpenSearch, actions: List[Dict[str, Any]]) -> tuple:
    """ëŒ€ëŸ‰ ë¬¸ì„œ ìƒ‰ì¸"""
    if not actions:
        logger.warning("âš ï¸ ìƒ‰ì¸í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
        return 0, []

    try:
        success, failed = bulk(
            client,
            actions,
            raise_on_error=False,
            refresh=True,
            request_timeout=60
        )

        if failed:
            logger.warning(f"âš ï¸ {len(failed)}ê°œ ë¬¸ì„œ ìƒ‰ì¸ ì‹¤íŒ¨")
            for fail in failed[:5]:  # ì²˜ìŒ 5ê°œë§Œ ë¡œê¹…
                logger.warning(f"   ì‹¤íŒ¨ ì˜ˆì‹œ: {fail}")

        logger.info(f"âœ… ë²Œí¬ ìƒ‰ì¸ ì™„ë£Œ: ì„±ê³µ {success}ê°œ, ì‹¤íŒ¨ {len(failed)}ê°œ")
        return success, failed

    except Exception as e:
        logger.error(f"âŒ ë²Œí¬ ìƒ‰ì¸ ì˜¤ë¥˜: {e}")
        raise


def warmup_knn_index(client: OpenSearch, index_name: str, dimension: int = 1024):
    """kNN ì¸ë±ìŠ¤ ì›Œë°ì—… (ì„±ëŠ¥ í–¥ìƒ)"""
    try:
        client.indices.refresh(index=index_name)

        # ë”ë¯¸ ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ìºì‹œ ì›Œë°
        dummy_vector = [0.0] * dimension

        client.search(
            index=index_name,
            body={
                "size": 1,
                "query": {
                    "nested": {
                        "path": "subjective_responses",
                        "query": {
                            "knn": {
                                "subjective_responses.answer_vector": {
                                    "vector": dummy_vector,
                                    "k": 1
                                }
                            }
                        }
                    }
                }
            }
        )

        logger.info(f"âœ… kNN ì¸ë±ìŠ¤ ì›Œë°ì—… ì™„ë£Œ: {index_name}")

    except Exception as e:
        logger.warning(f"âš ï¸ kNN ì›Œë°ì—… ì‹¤íŒ¨ (ë¬´ì‹œ ê°€ëŠ¥): {e}")


def delete_index(client: OpenSearch, index_name: str):
    """ì¸ë±ìŠ¤ ì‚­ì œ (ì¬ìƒì„± ì‹œ ì‚¬ìš©)"""
    if client.indices.exists(index=index_name):
        client.indices.delete(index=index_name)
        logger.info(f"ğŸ—‘ï¸ ì¸ë±ìŠ¤ '{index_name}' ì‚­ì œ ì™„ë£Œ")
    else:
        logger.info(f"ğŸ‘ ì¸ë±ìŠ¤ '{index_name}' ì¡´ì¬í•˜ì§€ ì•ŠìŒ")


def get_index_stats(client: OpenSearch, index_name: str) -> Dict[str, Any]:
    """ì¸ë±ìŠ¤ í†µê³„ ì¡°íšŒ"""
    try:
        stats = client.indices.stats(index=index_name)
        count = client.count(index=index_name)

        return {
            "index_name": index_name,
            "doc_count": count['count'],
            "store_size": stats['_all']['total']['store']['size_in_bytes'],
            "store_size_mb": round(stats['_all']['total']['store']['size_in_bytes'] / 1024 / 1024, 2),
            "search_total": stats['_all']['total']['search']['query_total'],
            "search_time_ms": stats['_all']['total']['search']['query_time_in_millis']
        }

    except Exception as e:
        logger.error(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {}
