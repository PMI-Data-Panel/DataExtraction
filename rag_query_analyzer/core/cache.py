import re
import hashlib
import logging
from collections import OrderedDict
from typing import Optional, Dict, Set, List
from ..models.query import QueryAnalysis
from ..config import Config

logger = logging.getLogger(__name__)


class LRUCachedAnalyzer:
    """LRU ìºì‹œë¥¼ í™œìš©í•œ ê³ ì† ë¶„ì„ê¸°
    
    ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¿¼ë¦¬ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ìºì‹±í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
    """
    
    def __init__(self, config: Config = None):
        """ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ê°ì²´
        """
        self.config = config or Config()
        self.cache_size = self.config.CACHE_SIZE
        self.similarity_threshold = self.config.CACHE_SIMILARITY_THRESHOLD
        
        # LRU ìºì‹œ (OrderedDict ì‚¬ìš©)
        self.cache: OrderedDict[str, QueryAnalysis] = OrderedDict()
        
        # í†µê³„
        self.hit_count = 0
        self.miss_count = 0
        
        # ìºì‹œ ì¸ë±ìŠ¤ (ë¹ ë¥¸ ìœ ì‚¬ ê²€ìƒ‰ìš©)
        self.cache_index: Dict[str, Set[str]] = {}  # í‚¤ì›Œë“œ -> ìºì‹œí‚¤ ë§¤í•‘
        
        logger.info(f"LRUCachedAnalyzer ì´ˆê¸°í™” (í¬ê¸°: {self.cache_size})")
    
    def _get_cache_key(self, query: str, use_claude: Optional[bool]) -> str:
        """ì¿¼ë¦¬ì˜ ìºì‹œ í‚¤ ìƒì„±
        
        Args:
            query: ì›ë³¸ ì¿¼ë¦¬
            
        Returns:
            MD5 í•´ì‹œ ê¸°ë°˜ ìºì‹œ í‚¤
        """
        # ì •ê·œí™”: ê³µë°± ì •ë¦¬, ì†Œë¬¸ì ë³€í™˜
        normalized = re.sub(r'\s+', ' ', query.lower().strip())
        prefix = "1" if use_claude else "0"
        digest = hashlib.md5(normalized.encode()).hexdigest()
        return f"{prefix}:{digest}"
    
    def get_cached(self, query: str, use_claude: Optional[bool] = None) -> Optional[QueryAnalysis]:
        """ìºì‹œì—ì„œ ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            query: ì¡°íšŒí•  ì¿¼ë¦¬
            
        Returns:
            ìºì‹œëœ ë¶„ì„ ê²°ê³¼ ë˜ëŠ” None
        """
        key = self._get_cache_key(query, use_claude)
        
        # ì •í™•í•œ ë§¤ì¹­
        if key in self.cache:
            # LRU: ìµœê·¼ ì‚¬ìš© í•­ëª©ì„ ëìœ¼ë¡œ ì´ë™
            self.cache.move_to_end(key)
            self.hit_count += 1
            
            result = self.cache[key]
            result.cache_hit = True
            
            logger.debug(f"ğŸ’¾ ìºì‹œ íˆíŠ¸! (íˆíŠ¸ìœ¨: {self.get_hit_rate():.1%})")
            return result
        
        # ìœ ì‚¬ ì¿¼ë¦¬ ê²€ìƒ‰
        similar_result = self._find_similar_cached(query, use_claude)
        if similar_result:
            self.hit_count += 1
            similar_result.cache_hit = True
            logger.debug(f"ğŸ’¾ ìœ ì‚¬ ì¿¼ë¦¬ ìºì‹œ íˆíŠ¸!")
            return similar_result
        
        self.miss_count += 1
        return None
    
    def _find_similar_cached(self, query: str, use_claude: Optional[bool]) -> Optional[QueryAnalysis]:
        """ìœ ì‚¬í•œ ìºì‹œëœ ì¿¼ë¦¬ ì°¾ê¸°
        
        Args:
            query: ì°¾ì„ ì¿¼ë¦¬
            
        Returns:
            ìœ ì‚¬í•œ ìºì‹œëœ ë¶„ì„ ê²°ê³¼ ë˜ëŠ” None
        """
        query_words = set(query.lower().split())
        
        if not query_words:
            return None
        
        best_match = None
        best_similarity = 0
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ í›„ë³´ ì°¾ê¸°
        candidate_keys = set()
        for word in query_words:
            if word in self.cache_index:
                candidate_keys.update(self.cache_index[word])
        
        # í›„ë³´ë“¤ê³¼ ìœ ì‚¬ë„ ê³„ì‚°
        for cache_key in candidate_keys:
            if cache_key not in self.cache:
                continue
            if use_claude is not None:
                desired_flag = bool(use_claude)
                candidate_flag = cache_key.startswith("1:")
                if candidate_flag != desired_flag:
                    continue
                continue
            
            cached_analysis = self.cache[cache_key]
            
            # ìºì‹œëœ ì¿¼ë¦¬ì˜ í‚¤ì›Œë“œ ì¶”ì¶œ
            cached_words = set(cached_analysis.must_terms + cached_analysis.should_terms)
            
            if not cached_words:
                continue
            
            # Jaccard ìœ ì‚¬ë„ ê³„ì‚°
            similarity = len(query_words & cached_words) / len(query_words | cached_words)
            
            if similarity > best_similarity and similarity >= self.similarity_threshold:
                best_similarity = similarity
                best_match = cached_analysis
        
        if best_match:
            logger.debug(f"ìœ ì‚¬ë„ {best_similarity:.2f}ë¡œ ìºì‹œ ë§¤ì¹­")
        
        return best_match
    
    def set_cached(self, query: str, analysis: QueryAnalysis, use_claude: Optional[bool] = None):
        """ë¶„ì„ ê²°ê³¼ ìºì‹±
        
        Args:
            query: ì›ë³¸ ì¿¼ë¦¬
            analysis: ë¶„ì„ ê²°ê³¼
        """
        key = self._get_cache_key(query, use_claude)
        
        # ì´ë¯¸ ìºì‹œì— ìˆìœ¼ë©´ ëìœ¼ë¡œ ì´ë™
        if key in self.cache:
            self.cache.move_to_end(key)
        else:
            # ìºì‹œ í¬ê¸° ì œí•œ ì²´í¬
            if len(self.cache) >= self.cache_size:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                oldest_key = next(iter(self.cache))
                self._remove_from_index(oldest_key)
                self.cache.pop(oldest_key)
                logger.debug(f"ìºì‹œ ê³µê°„ í™•ë³´: ì˜¤ë˜ëœ í•­ëª© ì œê±°")
        
        # ìºì‹œì— ì €ì¥
        self.cache[key] = analysis
        
        # ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        self._add_to_index(key, analysis)
        
        logger.debug(f"ìºì‹œ ì €ì¥: {key[:8]}...")
    
    def _add_to_index(self, cache_key: str, analysis: QueryAnalysis):
        """ìºì‹œ ì¸ë±ìŠ¤ì— ì¶”ê°€
        
        Args:
            cache_key: ìºì‹œ í‚¤
            analysis: ë¶„ì„ ê²°ê³¼
        """
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = analysis.must_terms + analysis.should_terms
        
        for keyword in keywords:
            keyword_lower = keyword.lower()
            if keyword_lower not in self.cache_index:
                self.cache_index[keyword_lower] = set()
            self.cache_index[keyword_lower].add(cache_key)
    
    def _remove_from_index(self, cache_key: str):
        """ìºì‹œ ì¸ë±ìŠ¤ì—ì„œ ì œê±°
        
        Args:
            cache_key: ì œê±°í•  ìºì‹œ í‚¤
        """
        # í•´ë‹¹ í‚¤ë¥¼ ê°€ì§„ ëª¨ë“  ì¸ë±ìŠ¤ ì—”íŠ¸ë¦¬ ì •ë¦¬
        for keyword, keys in list(self.cache_index.items()):
            if cache_key in keys:
                keys.remove(cache_key)
                if not keys:
                    del self.cache_index[keyword]
    
    def get_hit_rate(self) -> float:
        """ìºì‹œ íˆíŠ¸ìœ¨ ë°˜í™˜
        
        Returns:
            íˆíŠ¸ìœ¨ (0-1)
        """
        total = self.hit_count + self.miss_count
        return self.hit_count / total if total > 0 else 0.0
    
    def get_statistics(self) -> Dict:
        """ìºì‹œ í†µê³„ ë°˜í™˜
        
        Returns:
            í†µê³„ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        return {
            "cache_size": len(self.cache),
            "max_size": self.cache_size,
            "hit_count": self.hit_count,
            "miss_count": self.miss_count,
            "hit_rate": self.get_hit_rate(),
            "index_size": len(self.cache_index)
        }
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.cache.clear()
        self.cache_index.clear()
        self.hit_count = 0
        self.miss_count = 0
        logger.info("ğŸ’¾ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def warm_up(self, common_queries: List[str]):
        """ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¿¼ë¦¬ë¡œ ìºì‹œ ì˜ˆì—´
        
        Args:
            common_queries: ì˜ˆì—´í•  ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ìºì‹œ ì˜ˆì—´ ì‹œì‘: {len(common_queries)}ê°œ ì¿¼ë¦¬")
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê° ì¿¼ë¦¬ì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼ë¥¼ ë¯¸ë¦¬ ìƒì„±í•˜ì—¬ ìºì‹±
        pass
