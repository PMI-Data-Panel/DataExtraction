import re
import logging
from typing import List, Dict, Tuple, Set
from .base import BaseAnalyzer
from ..models.query import QueryAnalysis

logger = logging.getLogger(__name__)


class RuleBasedAnalyzer(BaseAnalyzer):
    """ê°•í™”ëœ ê·œì¹™ ê¸°ë°˜ ì¿¼ë¦¬ ë¶„ì„ê¸°

    ì •ê·œ í‘œí˜„ì‹ê³¼ íŒ¨í„´ ë§¤ì¹­ì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    ë‹¨ìˆœí•œ ì¿¼ë¦¬ëŠ” ì´ê²ƒë§Œìœ¼ë¡œë„ ì¶©ë¶„íˆ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.patterns = self._init_patterns()
        self.keyword_expansions = self._init_keyword_expansions()
        self.meta_keywords = self._init_meta_keywords()
        self.demographic_keywords = self._init_demographic_keywords()
        self.behavior_keywords = self._init_behavior_keywords()
        logger.info("RuleBasedAnalyzer ì´ˆê¸°í™” ì™„ë£Œ (ê°•í™”ë¨)")
    
    def get_name(self) -> str:
        """ë¶„ì„ê¸° ì´ë¦„ ë°˜í™˜"""
        return "RuleBasedAnalyzer"
    
    def _init_patterns(self) -> Dict:
        """íŒ¨í„´ ì •ì˜"""
        return {
            "age": {
                "pattern": r'(\d+ëŒ€|\d+ì„¸|[ì´ì‚¼ì‚¬ì˜¤ìœ¡ì¹ íŒ”êµ¬]ì‹­ëŒ€)',
                "type": "demographic"
            },
            "gender": {
                "pattern": r'(ë‚¨ì„±|ì—¬ì„±|ë‚¨ì|ì—¬ì|ë‚¨|ì—¬)',
                "type": "demographic"
            },
            "region": {
                "pattern": r'(ì„œìš¸|ë¶€ì‚°|ëŒ€êµ¬|ì¸ì²œ|ê´‘ì£¼|ëŒ€ì „|ìš¸ì‚°|ê²½ê¸°|ê°•ì›|ì¶©ë¶|ì¶©ë‚¨|ì „ë¶|ì „ë‚¨|ê²½ë¶|ê²½ë‚¨|ì œì£¼|ì„¸ì¢…)',
                "type": "demographic"
            },
            "job": {
                "pattern": r'(í•™ìƒ|ì§ì¥ì¸|ì£¼ë¶€|ìì˜ì—…|ì „ë¬¸ì§|ì‚¬ë¬´ì§|ì„œë¹„ìŠ¤ì§|ìƒì‚°ì§|ë¬´ì§|í”„ë¦¬ëœì„œ)',
                "type": "demographic"
            },
            "marital": {
                "pattern": r'(ë¯¸í˜¼|ê¸°í˜¼|ì‹±ê¸€|ê²°í˜¼)',
                "type": "demographic"
            },
            "emotion": {
                "pattern": r'(ë§Œì¡±|ë¶ˆë§Œ|í–‰ë³µ|ìŠ¤íŠ¸ë ˆìŠ¤|ê¸ì •|ë¶€ì •|ì¢‹|ì‹«|í¸ì•ˆ|ë¶ˆí¸)',
                "type": "sentiment"
            },
            "frequency": {
                "pattern": r'(ìì£¼|ê°€ë”|ë§¤ì¼|ë§¤ì£¼|ë§¤ì›”|í•­ìƒ|ì „í˜€|ê±°ì˜)',
                "type": "behavioral"
            },
            "comparison": {
                "pattern": r'(ë¹„êµ|ì°¨ì´|ëŒ€ë¹„|versus|vs|ë³´ë‹¤|ë”)',
                "type": "comparison"
            }
        }

    def _init_keyword_expansions(self) -> Dict[str, List[str]]:
        """í‚¤ì›Œë“œ í™•ì¥ ê·œì¹™"""
        return {
            # ë‚˜ì´
            "20ëŒ€": ["20-29", "ì´ì‹­ëŒ€", "twenties"],
            "30ëŒ€": ["30-39", "ì‚¼ì‹­ëŒ€", "thirties"],
            "40ëŒ€": ["40-49", "ì‚¬ì‹­ëŒ€", "forties"],
            "50ëŒ€": ["50-59", "ì˜¤ì‹­ëŒ€", "fifties"],

            # ì„±ë³„
            "ë‚¨ì„±": ["ë‚¨ì", "ë‚¨"],
            "ì—¬ì„±": ["ì—¬ì", "ì—¬"],

            # ë§Œì¡±ë„
            "ë§Œì¡±": ["ë§Œì¡±í•¨", "ë§Œì¡±ìŠ¤ëŸ¬ì›€", "satisfied"],
            "ë¶ˆë§Œì¡±": ["ë¶ˆë§Œ", "ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ì›€", "dissatisfied"],

            # ë¹ˆë„
            "ìì£¼": ["ë¹ˆë²ˆíˆ", "ë§ì´", "often"],
            "ê°€ë”": ["ë•Œë•Œë¡œ", "ì¢…ì¢…", "sometimes"]
        }
    
    def _init_meta_keywords(self) -> set:
        """ë©”íƒ€ í‚¤ì›Œë“œ (ê²€ìƒ‰ ì¡°ê±´ì—ì„œ ì œì™¸)"""
        return {
            'ì„¤ë¬¸ì¡°ì‚¬', 'ì„¤ë¬¸', 'ë°ì´í„°', 'ìë£Œ', 'ì •ë³´',
            'ë³´ì—¬ì¤˜', 'ë³´ì—¬ì£¼ì„¸ìš”', 'ì•Œë ¤ì¤˜', 'ì•Œë ¤ì£¼ì„¸ìš”',
            'ê²€ìƒ‰', 'ì°¾ì•„ì¤˜', 'ì°¾ì•„ì£¼ì„¸ìš”', 'ì¡°íšŒ',
            'ì„', 'ë¥¼', 'ì´', 'ê°€', 'ì˜', 'ì—', 'ì—ì„œ',
            'ì™€', 'ê³¼', 'ì—ê²Œ', 'í•œí…Œ', 'ëª…', 'ê°œ', 'ê±´',
            'ì‚¬ëŒ', 'ì¸', 'ë¶„', 'ì¤‘', 'ì¤‘ì—', 'ì¤‘ì—ì„œ',
            'ì‘ë‹µì', 'ì°¸ì—¬ì', 'ì°¸ì—¬ìë“¤', 'ì„¤ë¬¸ì‘ë‹µì', 'ì„¤ë¬¸ì°¸ì—¬ì'
        }
    
    def _init_demographic_keywords(self) -> set:
        """Demographics í‚¤ì›Œë“œ (must ì¡°ê±´ìœ¼ë¡œë§Œ ê°€ì•¼ í•¨)"""
        return {
            # ì—°ë ¹
            '10ëŒ€', '20ëŒ€', '30ëŒ€', '40ëŒ€', '50ëŒ€', '60ëŒ€', '70ëŒ€',
            '10-19', '20-29', '30-39', '40-49', '50-59', '60-69', '70-79',
            'ì‹­ëŒ€', 'ì´ì‹­ëŒ€', 'ì‚¼ì‹­ëŒ€', 'ì‚¬ì‹­ëŒ€', 'ì˜¤ì‹­ëŒ€', 'ìœ¡ì‹­ëŒ€', 'ì¹ ì‹­ëŒ€',
            # ì„±ë³„
            'ë‚¨ì„±', 'ì—¬ì„±', 'ë‚¨ì', 'ì—¬ì', 'ë‚¨', 'ì—¬',
            # ì§ì—…
            'ì‚¬ë¬´ì§', 'ì „ë¬¸ì§', 'ì„œë¹„ìŠ¤ì§', 'í•™ìƒ', 'ì£¼ë¶€', 'ìì˜ì—…',
            'ì§ì¥ì¸', 'ìƒì‚°ì§', 'ë¬´ì§', 'í”„ë¦¬ëœì„œ', 'ì‚¬ë¬´ì›', 'í™”ì´íŠ¸ì¹¼ë¼',
            'ëŒ€í•™ìƒ', 'ê³ ë“±í•™ìƒ', 'ì†Œìƒê³µì¸', 'ì˜ì‚¬', 'ë³€í˜¸ì‚¬', 'íšŒê³„ì‚¬',
            'ì„œë¹„ìŠ¤ì—…', 'íŒë§¤ì§', 'ì˜ì—…', 'ë¸”ë£¨ì¹¼ë¼',
            'ê³µë¬´ì›', 'ê³µì§ì', 'ê³µë¬´', 'ê³µì§', 'ê°€ì •ì£¼ë¶€', 'ì‹¤ì§', 'ë¯¸ì·¨ì—…', 'ììœ ì§',
            'ì¤‘/ê³ ë“±í•™ìƒ', 'ëŒ€í•™ìƒ/ëŒ€í•™ì›ìƒ', 'ê°„í˜¸ì‚¬', 'ì—”ì§€ë‹ˆì–´', 'í”„ë¡œê·¸ë˜ë¨¸',
            'ìƒì‚°/ë…¸ë¬´ì§', 'ê²½ì˜/ê´€ë¦¬ì§', 'ê²½ì˜ê´€ë¦¬ì§', 'êµì§', 'êµì‚¬', 'êµìˆ˜', 'ê°•ì‚¬',
            # ì§€ì—­
            'ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°',
            'ê²½ê¸°', 'ê°•ì›', 'ì¶©ë¶', 'ì¶©ë‚¨', 'ì „ë¶', 'ì „ë‚¨', 'ê²½ë¶', 'ê²½ë‚¨', 'ì œì£¼', 'ì„¸ì¢…',
            # ê²°í˜¼ì—¬ë¶€
            'ë¯¸í˜¼', 'ê¸°í˜¼', 'ì‹±ê¸€', 'ê²°í˜¼', 'ì´í˜¼', 'ì‚¬ë³„', 'ë³„ê±°',
            # í•™ë ¥
            'ê³ ì¡¸', 'ëŒ€ì¡¸', 'ëŒ€í•™ì›', 'ì„ì‚¬', 'ë°•ì‚¬', 'ëŒ€í•™ ì¬í•™', 'ëŒ€í•™êµ ì¬í•™',
            # ì†Œë“
            '100ë§Œì›', '200ë§Œì›', '300ë§Œì›', '400ë§Œì›', '500ë§Œì›',
            'ì†Œë“', 'ê¸‰ì—¬', 'ì—°ë´‰',
            # ê°€ì¡±ìˆ˜, ìë…€ìˆ˜
            '1ëª…', '2ëª…', '3ëª…', '4ëª…', '5ëª…', 'ê°€ì¡±ìˆ˜', 'ìë…€ìˆ˜', 'í˜¼ì', 'ë…ê±°',
            # ì§ë¬´
            'IT', 'ì•„ì´í‹°', 'ê°œë°œ', 'í”„ë¡œê·¸ë˜ë°', 'ì½”ë”©',
            'ê²½ì˜', 'ì¸ì‚¬', 'ì´ë¬´', 'ì‚¬ë¬´',
            'ìƒì‚°', 'ì •ë¹„', 'ê¸°ëŠ¥', 'ë…¸ë¬´',
            'ì„œë¹„ìŠ¤', 'ì—¬í–‰', 'ìˆ™ë°•', 'ìŒì‹', 'ë¯¸ìš©', 'ë³´ì•ˆ',
            'ì˜ë£Œ', 'ê°„í˜¸', 'ë³´ê±´', 'ë³µì§€',
            'ê±´ì„¤', 'ê±´ì¶•', 'í† ëª©', 'í™˜ê²½',
            'êµìœ¡', 'êµì‚¬', 'ê°•ì‚¬', 'êµì§ì›',
            'ìœ í†µ', 'ë¬¼ë¥˜', 'ìš´ì†¡', 'ìš´ì „',
            'ë¬´ì—­', 'ì˜ì—…', 'íŒë§¤', 'ë§¤ì¥ê´€ë¦¬',
            'ì „ì', 'ê¸°ê³„', 'ê¸°ìˆ ', 'í™”í•™', 'ì—°êµ¬ê°œë°œ', 'R&D',
            'ì¬ë¬´', 'íšŒê³„', 'ê²½ë¦¬',
            'ë§ˆì¼€íŒ…', 'ê´‘ê³ ', 'í™ë³´', 'ì¡°ì‚¬',
            'ê¸ˆìœµ', 'ë³´í—˜', 'ì¦ê¶Œ',
            'ê³ ê°ìƒë‹´', 'TM', 'í…”ë ˆë§ˆì¼€íŒ…',
            'ë²•ë¥ ', 'ì¸ë¬¸ì‚¬íšŒ',
            'ë””ìì¸',
            'ë¬¸í™”', 'ìŠ¤í¬ì¸ ',
            'ì¸í„°ë„·', 'í†µì‹ ',
            'ë°©ì†¡', 'ì–¸ë¡ ',
            'ê²Œì„'
        }

    def _init_behavior_keywords(self) -> set:
        """í–‰ë™/ìŠµê´€ ê´€ë ¨ í‚¤ì›Œë“œ"""
        return {
            'í¡ì—°', 'í¡ì—°ì', 'í¡ì—°í•˜ëŠ”', 'í¡ì—°ëŸ‰', 'í¡ì—°ë¥ ', 'í¡ì—°ìœ¨',
            'ë¹„í¡ì—°', 'ê¸ˆì—°', 'ê¸ˆì—°ì', 'ë‹´ë°°', 'ë‹´ë°°í”¼ëŠ”', 'ë‹´ë°°í”¼ì›€', 'ë‹´ë°°í”¼ìš°ëŠ”',
            'ë‹´ë°°í”¼ê³ ', 'ë‹´ë°°í”¼ë©°', 'ë‹´ë°°í”¼ë©´ì„œ', 'ë‹´ë°°í”¼ê±°ë‚˜',  # âœ… ì¶”ê°€: ë‹´ë°° í”¼ê³  ë“± ì—°ê²°í˜•
            'ì°¨ëŸ‰', 'ìë™ì°¨', 'ì°¨ëŸ‰ì—¬ë¶€', 'ë³´ìœ ì°¨ëŸ‰', 'ì°¨', 'ì°¨ëŸ‰ë³´ìœ ', 'ì°¨ëŸ‰ ë³´ìœ ',
            'ì†Œìœ ', 'ì†Œìœ í•˜ëŠ”', 'ì†Œìœ í•œ', 'ê°€ì§„', 'ë³´ìœ ', 'ë³´ìœ í•œ', 'ë³´ìœ í•˜ëŠ”',
            'ë§¥ì£¼', 'ì™€ì¸', 'ì†Œì£¼', 'ìˆ ', 'ìŒì£¼', 'ë¹„ìŒì£¼', 'ê¸ˆì£¼', 'ìŒìš©', 'ìŒìš©ê²½í—˜'
        }
    
    def analyze(self, query: str, context: str = "") -> QueryAnalysis:
        """ê°•í™”ëœ ê·œì¹™ ê¸°ë°˜ ì¿¼ë¦¬ ë¶„ì„

        Args:
            query: ë¶„ì„í•  ì¿¼ë¦¬
            context: ì¶”ê°€ ë§¥ë½

        Returns:
            ë¶„ì„ ê²°ê³¼
        """
        if not self.validate_query(query):
            return self._create_empty_analysis()

        query = self.preprocess_query(query)

        # í‚¤ì›Œë“œ ì¶”ì¶œ
        must_terms = []
        should_terms = []
        intent_hints = []
        expanded_keywords = {}

        for pattern_name, pattern_info in self.patterns.items():
            matches = re.findall(pattern_info["pattern"], query)
            if matches:
                for match in matches:
                    # ì¤‘ë³µ ì œê±°
                    if match not in must_terms:
                        must_terms.append(match)

                        # í‚¤ì›Œë“œ í™•ì¥
                        if match in self.keyword_expansions:
                            expanded_keywords[match] = self.keyword_expansions[match]
                            should_terms.extend(self.keyword_expansions[match])

                intent_hints.append(pattern_info["type"])

        # Demographics í‚¤ì›Œë“œ ë¶„ë¦¬ (must_termsì—ì„œ ì œê±°, í•„í„°ë¡œë§Œ ì²˜ë¦¬)
        demographic_terms = [t for t in must_terms if t in self.demographic_keywords]
        must_terms = [t for t in must_terms if t not in self.demographic_keywords]

        if demographic_terms:
            logger.info(f"ğŸ” [RuleAnalyzer] Demographics í‚¤ì›Œë“œ ë¶„ë¦¬: {demographic_terms}")

        # ì¶”ê°€ í‚¤ì›Œë“œ ì¶”ì¶œ (íŒ¨í„´ ì™¸, Demographics ì œì™¸)
        additional_terms = self._extract_additional_keywords(query, must_terms + demographic_terms)

        # í–‰ë™ ì¡°ê±´ ì¶”ì¶œ (ì˜ˆ: í¡ì—°ì, ë¹„í¡ì—°ì ë“±)
        behavioral_conditions, behavior_tokens = self._extract_behavioral_conditions(query)
        if behavior_tokens:
            behavior_keywords_lower = {kw.lower() for kw in self.behavior_keywords}
            behavior_tokens_lower = {token.lower() for token in behavior_tokens}

            def is_behavior_term(term: str) -> bool:
                term_lower = term.lower()
                if term_lower in behavior_keywords_lower:
                    return True
                for token_lower in behavior_tokens_lower:
                    if token_lower and token_lower in term_lower:
                        return True
                for kw_lower in behavior_keywords_lower:
                    if kw_lower and kw_lower in term_lower:
                        return True
                return False

            removed_behavior = [t for t in must_terms + should_terms + demographic_terms + additional_terms if is_behavior_term(t)]
            must_terms = [t for t in must_terms if not is_behavior_term(t)]
            should_terms = [t for t in should_terms if not is_behavior_term(t)]
            demographic_terms = [t for t in demographic_terms if not is_behavior_term(t)]
            additional_terms = [t for t in additional_terms if not is_behavior_term(t)]

            if removed_behavior:
                logger.info(f"ğŸ” [RuleAnalyzer] í–‰ë™ í‚¤ì›Œë“œ ì œê±°: {list(set(removed_behavior))}")
            if behavioral_conditions:
                logger.info(f"   âœ… í–‰ë™ ì¡°ê±´ ì¶”ì¶œ: {behavioral_conditions}")

        must_terms.extend(additional_terms)

        # ìµœì¢… ìš©ì–´ ì •ë¦¬: í–‰ë™/ë©”íƒ€ í‚¤ì›Œë“œ ì œê±°
        def _is_behavior_term(term: str) -> bool:
            term_lower = term.lower()
            if term_lower in {kw.lower() for kw in self.behavior_keywords}:
                return True
            for keyword in self.behavior_keywords:
                if keyword.lower() in term_lower:
                    return True
            return False

        def _is_meta_term(term: str) -> bool:
            return term in self.meta_keywords or term.lower() in {kw.lower() for kw in self.meta_keywords}

        original_must_count = len(must_terms)
        original_should_count = len(should_terms)
        removed_meta_must = [t for t in must_terms if _is_meta_term(t)]
        removed_meta_should = [t for t in should_terms if _is_meta_term(t)]

        must_terms = [t for t in must_terms if t and not _is_behavior_term(t) and not _is_meta_term(t)]
        should_terms = [t for t in should_terms if t and not _is_behavior_term(t) and not _is_meta_term(t)]

        if removed_meta_must or removed_meta_should:
            logger.info(f"ğŸ” [RuleAnalyzer] ë©”íƒ€ í‚¤ì›Œë“œ ì œê±°: must={removed_meta_must}, should={removed_meta_should}")

        # ì˜ë„ ê²°ì •
        intent = self._determine_intent(intent_hints)

        # Alpha ê°’ ê²°ì •
        alpha = self._calculate_alpha(intent)

        # ì‹ ë¢°ë„ ê³„ì‚° (í‚¤ì›Œë“œê°€ ë§ì„ìˆ˜ë¡ ë†’ìŒ)
        confidence = min(0.7, 0.3 + len(must_terms) * 0.1)

        return QueryAnalysis(
            intent=intent,
            must_terms=list(set(must_terms)),
            should_terms=list(set(should_terms)),
            must_not_terms=[],
            alpha=alpha,
            expanded_keywords=expanded_keywords,
            confidence=confidence,
            explanation=f"ê·œì¹™ ê¸°ë°˜ ë¶„ì„ - {len(must_terms)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ",
            reasoning_steps=[
                "íŒ¨í„´ ë§¤ì¹­ ìˆ˜í–‰",
                f"ì¶”ì¶œëœ í‚¤ì›Œë“œ: {', '.join(must_terms[:5])}",
                f"ì˜ë„: {intent}"
            ],
            analyzer_used=self.get_name(),
            behavioral_conditions=behavioral_conditions,
        )

    def _extract_additional_keywords(self, query: str, existing_terms: List[str]) -> List[str]:
        """íŒ¨í„´ ì™¸ ì¶”ê°€ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Demographicsì™€ ë©”íƒ€ í‚¤ì›Œë“œ ì œì™¸
        """
        additional = []
        
        # í† í°í™” (í•œê¸€ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬)
        tokens = re.findall(r'\w+', query)
        
        for token in tokens:
            # ì œì™¸ ì¡°ê±´
            if (token in existing_terms or 
                token in self.meta_keywords or 
                token in self.demographic_keywords or
                token in self.behavior_keywords or
                len(token) <= 1):
                continue
            
            additional.append(token)
        
        logger.info(f"ğŸ”‘ ì‹¤ì œ ê²€ìƒ‰ í‚¤ì›Œë“œ (Demographics ì œì™¸): {additional}")
        return additional
    
    def _determine_intent(self, hints: List[str]) -> str:
        """íŒíŠ¸ ê¸°ë°˜ ì˜ë„ ê²°ì •"""
        if not hints:
            return "hybrid"
        
        hint_counts = {}
        for hint in hints:
            hint_counts[hint] = hint_counts.get(hint, 0) + 1
        
        # ê°€ì¥ ë§ì€ íŒíŠ¸ íƒ€ì…
        dominant_hint = max(hint_counts, key=hint_counts.get)
        
        # íŒíŠ¸ë¥¼ ì˜ë„ë¡œ ë§¤í•‘
        intent_map = {
            "demographic": "exact_match",
            "sentiment": "semantic_search",
            "behavioral": "hybrid",
            "comparison": "hybrid"
        }
        
        return intent_map.get(dominant_hint, "hybrid")
    
    def _calculate_alpha(self, intent: str) -> float:
        """ì˜ë„ì— ë”°ë¥¸ alpha ê°’ ê³„ì‚°"""
        alpha_map = {
            "exact_match": 0.2,
            "semantic_search": 0.8,
            "hybrid": 0.5
        }
        return alpha_map.get(intent, 0.5)
    
    def _create_empty_analysis(self) -> QueryAnalysis:
        """ë¹ˆ ë¶„ì„ ê²°ê³¼ ìƒì„±"""
        return QueryAnalysis(
            intent="hybrid",
            must_terms=[],
            should_terms=[],
            must_not_terms=[],
            alpha=0.5,
            expanded_keywords={},
            confidence=0.0,
            explanation="ìœ íš¨í•˜ì§€ ì•Šì€ ì¿¼ë¦¬",
            analyzer_used=self.get_name(),
            behavioral_conditions={},
        )

    # ---------------------------------------------------------
    # í–‰ë™ ì¡°ê±´ ì¶”ì¶œ
    # ---------------------------------------------------------
    def _extract_behavioral_conditions(self, query: str) -> Tuple[Dict[str, bool], Set[str]]:
        """ì¿¼ë¦¬ì—ì„œ í–‰ë™ ì¡°ê±´(ì˜ˆ: í¡ì—° ì—¬ë¶€, ì°¨ëŸ‰ ë³´ìœ )ì„ ì¶”ì¶œ"""
        lowered = query.lower()
        conditions: Dict[str, bool] = {}
        tokens_to_remove: Set[str] = set()
        tokens = re.findall(r'\w+', query)

        def mark_tokens(keyword_list: Tuple[str, ...]) -> None:
            for token in tokens:
                token_lower = token.lower()
                for keyword in keyword_list:
                    if keyword in token_lower:
                        tokens_to_remove.add(token)
                        break

        specs = {
            "smoker": {
                "negative": [
                    r'ë¹„\s*í¡ì—°ì?',
                    r'í¡ì—°\s*(ì•ˆ|ì•Š|í•˜ì§€|ì•ˆí•¨|ì•ŠìŒ)',
                    r'ë‹´ë°°\s*(ì•ˆ|ì•Š)\s*í”¼',
                    r'ë‹´ë°°ë¥¼\s*í”¼ì›Œë³¸\s*ì \s*ì´\s*ì—†',
                    r'ê¸ˆì—°ì?',
                    r'non[-\s]?smoker',
                    r'ë‹´ë°°\s*ì•ˆ\s*í”¼',
                ],
                "positive": [
                    r'í¡ì—°\s*ì',
                    r'í¡ì—°\s*ì¤‘',
                    r'í¡ì—°\s*í•˜ëŠ”',
                    r'í¡ì—°\s*í•˜ê³ ',      # "smoking and" - conjunction form
                    r'í¡ì—°\s*í•˜ë©°',      # "while smoking"
                    r'í¡ì—°\s*í•˜ë©´ì„œ',    # "while smoking"
                    r'í¡ì—°\s*í•˜ê±°ë‚˜',    # "smoking or"
                    r'ë‹´ë°°\s*(í”¼ìš°|í”¼ëŠ”|í”¼ìš´|í”¼ì›€|í”¼ê³ |í”¼ë©°|í”¼ë©´ì„œ|í”¼ê±°ë‚˜)',  # âœ… ì¶”ê°€: í”¼ê³ , í”¼ë©°, í”¼ë©´ì„œ, í”¼ê±°ë‚˜
                    r'smoker',
                ],
                "token_keywords": ("í¡ì—°", "ë‹´ë°°"),
            },
            "has_vehicle": {
                "negative": [
                    r'ì°¨ëŸ‰\s*(ì—†|ë¯¸ë³´ìœ |ë¯¸ì†Œìœ )',
                    r'ìë™ì°¨\s*(ì—†|ë¯¸ë³´ìœ |ë¯¸ì†Œìœ )',
                    r'ì°¨\s*ì—†',
                    r'ì°¨ê°€\s*ì—†',
                    r'ë¬´\s*ì°¨ëŸ‰',
                    r'ì°¨ëŸ‰\s*ë³´ìœ \s*(ì•ˆ|ì•Š)',
                ],
                "positive": [
                    r'ì°¨ëŸ‰\s*(ìˆ|ë³´ìœ |ì†Œìœ )',
                    r'ìë™ì°¨\s*(ìˆ|ë³´ìœ |ì†Œìœ )',
                    r'ì°¨\s*ìˆ',
                    r'ì°¨ê°€\s*ìˆ',
                    r'ì°¨ëŸ‰\s*ë³´ìœ ',
                    r'ì°¨ëŸ‰\s*ì†Œìœ ',
                    r'ì°¨ë¥¼\s*ì†Œìœ í•˜ëŠ”',   # "owning a car" - descriptive form
                    r'ì°¨ë¥¼\s*ì†Œìœ í•˜ê³ ',   # "owning a car and"
                    r'ì°¨ë¥¼\s*ê°€ì§„',       # "having a car"
                    r'ì°¨ë¥¼\s*ë³´ìœ í•œ',     # "possessing a car"
                    r'ì°¨ëŸ‰ì„\s*ì†Œìœ í•˜ëŠ”',
                    r'ì°¨ëŸ‰ì„\s*ì†Œìœ í•˜ê³ ',
                    r'ì°¨ëŸ‰ì„\s*ê°€ì§„',
                    r'ì°¨ëŸ‰ì„\s*ë³´ìœ í•œ',
                ],
                "token_keywords": ("ì°¨ëŸ‰", "ìë™ì°¨", "ì°¨", "ë³´ìœ ì°¨ëŸ‰", "ì°¨ëŸ‰ì—¬ë¶€"),
            },
            "drinks_beer": {
                "positive": [
                    r'ë§¥ì£¼\s*(ë§ˆì‹œ|ìŒìš©|ì„ í˜¸|ì¢‹ì•„|ì¦ê¸°)',
                    r'ë§¥ì£¼',
                    r'beer',
                ],
                "token_keywords": ("ë§¥ì£¼", "beer"),
            },
            "drinks_wine": {
                "positive": [
                    r'ì™€ì¸\s*(ë§ˆì‹œ|ìŒìš©|ì„ í˜¸|ì¢‹ì•„|ì¦ê¸°)',
                    r'ì™€ì¸',
                    r'wine',
                ],
                "token_keywords": ("ì™€ì¸", "wine"),
            },
            "drinks_soju": {
                "positive": [
                    r'ì†Œì£¼\s*(ë§ˆì‹œ|ìŒìš©|ì„ í˜¸|ì¢‹ì•„|ì¦ê¸°)',
                    r'ì†Œì£¼',
                    r'soju',
                ],
                "token_keywords": ("ì†Œì£¼", "soju"),
            },
            "non_drinker": {
                "positive": [
                    r'ìˆ \s*(ì•ˆ|ì•Š)\s*ë§ˆ',
                    r'ìˆ \s*ëª»\s*ë§ˆ',
                    r'ë¹„ìŒì£¼',
                    r'ê¸ˆì£¼',
                    r'ë…¼\s*ë“œë§ì»¤',
                    r'non[-\s]?drinker',
                    r'ìˆ ì„\s*ë§ˆì‹œì§€\s*ì•Š',
                ],
                "token_keywords": ("ìˆ ", "ìŒì£¼", "ë¹„ìŒì£¼", "ê¸ˆì£¼"),
            },
        }

        for key, spec in specs.items():
            found = False
            # negative íŒ¨í„´ ì²´í¬ (ìˆëŠ” ê²½ìš°ë§Œ)
            for pattern in spec.get("negative", []):
                if re.search(pattern, lowered):
                    conditions[key] = False
                    mark_tokens(spec["token_keywords"])
                    found = True
                    break
            if found:
                continue
            # positive íŒ¨í„´ ì²´í¬
            for pattern in spec.get("positive", []):
                if re.search(pattern, lowered):
                    conditions[key] = True
                    mark_tokens(spec["token_keywords"])
                    break

        return conditions, tokens_to_remove