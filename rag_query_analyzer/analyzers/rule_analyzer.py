import re
import logging
from typing import List, Dict
from .base import BaseAnalyzer
from ..models.query import QueryAnalysis

logger = logging.getLogger(__name__)


class RuleBasedAnalyzer(BaseAnalyzer):
    """ê°•í™”ëœ ê·œì¹™ ê¸°ë°˜ ì¿¼ë¦¬ ë¶„ì„ê¸°

    ì •ê·œ í‘œí˜„ì‹ê³¼ íŒ¨í„´ ë§¤ì¹­ì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    ë‹¨ìˆœí•œ ì¿¼ë¦¬ëŠ” ì´ê²ƒë§Œìœ¼ë¡œë„ ì¶©ë¶„íˆ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.patterns = self._init_patterns()
        self.keyword_expansions = self._init_keyword_expansions()
        self.meta_keywords = self._init_meta_keywords()
        self.demographic_keywords = self._init_demographic_keywords()
        logger.info("RuleBasedAnalyzer ì´ˆê¸°í™” ì™„ë£Œ (ê°•í™”ë¨)")
    
    def get_name(self) -> str:
        """ë¶„ì„ê¸° ì´ë¦„ ë°˜í™˜"""
        return "RuleBasedAnalyzer"
    
    def _init_patterns(self) -> Dict:
        """íŒ¨í„´ ì •ì˜"""
        return {
            "age": {
                "pattern": r'(\d+ëŒ€|\d+ì„¸|[ì´ì‚¼ì‚¬ì˜¤ìœ¡ì¹ íŒ”êµ¬]ì‹­ëŒ€)',
                "type": "demographic"
            },
            "gender": {
                "pattern": r'(ë‚¨ì„±|ì—¬ì„±|ë‚¨ì|ì—¬ì|ë‚¨|ì—¬)',
                "type": "demographic"
            },
            "region": {
                "pattern": r'(ì„œìš¸|ë¶€ì‚°|ëŒ€êµ¬|ì¸ì²œ|ê´‘ì£¼|ëŒ€ì „|ìš¸ì‚°|ê²½ê¸°|ê°•ì›|ì¶©ë¶|ì¶©ë‚¨|ì „ë¶|ì „ë‚¨|ê²½ë¶|ê²½ë‚¨|ì œì£¼|ì„¸ì¢…)',
                "type": "demographic"
            },
            "job": {
                "pattern": r'(í•™ìƒ|ì§ì¥ì¸|ì£¼ë¶€|ìì˜ì—…|ì „ë¬¸ì§|ì‚¬ë¬´ì§|ì„œë¹„ìŠ¤ì§|ìƒì‚°ì§|ë¬´ì§|í”„ë¦¬ëœì„œ)',
                "type": "demographic"
            },
            "marital": {
                "pattern": r'(ë¯¸í˜¼|ê¸°í˜¼|ì‹±ê¸€|ê²°í˜¼)',
                "type": "demographic"
            },
            "emotion": {
                "pattern": r'(ë§Œì¡±|ë¶ˆë§Œ|í–‰ë³µ|ìŠ¤íŠ¸ë ˆìŠ¤|ê¸ì •|ë¶€ì •|ì¢‹|ì‹«|í¸ì•ˆ|ë¶ˆí¸)',
                "type": "sentiment"
            },
            "frequency": {
                "pattern": r'(ìì£¼|ê°€ë”|ë§¤ì¼|ë§¤ì£¼|ë§¤ì›”|í•­ìƒ|ì „í˜€|ê±°ì˜)',
                "type": "behavioral"
            },
            "comparison": {
                "pattern": r'(ë¹„êµ|ì°¨ì´|ëŒ€ë¹„|versus|vs|ë³´ë‹¤|ë”)',
                "type": "comparison"
            }
        }

    def _init_keyword_expansions(self) -> Dict[str, List[str]]:
        """í‚¤ì›Œë“œ í™•ì¥ ê·œì¹™"""
        return {
            # ë‚˜ì´
            "20ëŒ€": ["20-29", "ì´ì‹­ëŒ€", "twenties"],
            "30ëŒ€": ["30-39", "ì‚¼ì‹­ëŒ€", "thirties"],
            "40ëŒ€": ["40-49", "ì‚¬ì‹­ëŒ€", "forties"],
            "50ëŒ€": ["50-59", "ì˜¤ì‹­ëŒ€", "fifties"],

            # ì„±ë³„
            "ë‚¨ì„±": ["ë‚¨ì", "ë‚¨"],
            "ì—¬ì„±": ["ì—¬ì", "ì—¬"],

            # ë§Œì¡±ë„
            "ë§Œì¡±": ["ë§Œì¡±í•¨", "ë§Œì¡±ìŠ¤ëŸ¬ì›€", "satisfied"],
            "ë¶ˆë§Œì¡±": ["ë¶ˆë§Œ", "ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ì›€", "dissatisfied"],

            # ë¹ˆë„
            "ìì£¼": ["ë¹ˆë²ˆíˆ", "ë§ì´", "often"],
            "ê°€ë”": ["ë•Œë•Œë¡œ", "ì¢…ì¢…", "sometimes"]
        }
    
    def _init_meta_keywords(self) -> set:
        """ë©”íƒ€ í‚¤ì›Œë“œ (ê²€ìƒ‰ ì¡°ê±´ì—ì„œ ì œì™¸)"""
        return {
            'ì„¤ë¬¸ì¡°ì‚¬', 'ì„¤ë¬¸', 'ë°ì´í„°', 'ìë£Œ', 'ì •ë³´',
            'ë³´ì—¬ì¤˜', 'ë³´ì—¬ì£¼ì„¸ìš”', 'ì•Œë ¤ì¤˜', 'ì•Œë ¤ì£¼ì„¸ìš”',
            'ê²€ìƒ‰', 'ì°¾ì•„ì¤˜', 'ì°¾ì•„ì£¼ì„¸ìš”', 'ì¡°íšŒ',
            'ì„', 'ë¥¼', 'ì´', 'ê°€', 'ì˜', 'ì—', 'ì—ì„œ',
            'ì™€', 'ê³¼', 'ì—ê²Œ', 'í•œí…Œ', 'ëª…', 'ê°œ', 'ê±´',
            'ì‚¬ëŒ', 'ì¸', 'ë¶„', 'ì¤‘', 'ì¤‘ì—', 'ì¤‘ì—ì„œ'
        }
    
    def _init_demographic_keywords(self) -> set:
        """Demographics í‚¤ì›Œë“œ (must ì¡°ê±´ìœ¼ë¡œë§Œ ê°€ì•¼ í•¨)"""
        return {
            # ì—°ë ¹
            '10ëŒ€', '20ëŒ€', '30ëŒ€', '40ëŒ€', '50ëŒ€', '60ëŒ€', '70ëŒ€',
            '10-19', '20-29', '30-39', '40-49', '50-59', '60-69', '70-79',
            'ì‹­ëŒ€', 'ì´ì‹­ëŒ€', 'ì‚¼ì‹­ëŒ€', 'ì‚¬ì‹­ëŒ€', 'ì˜¤ì‹­ëŒ€', 'ìœ¡ì‹­ëŒ€', 'ì¹ ì‹­ëŒ€',
            # ì„±ë³„
            'ë‚¨ì„±', 'ì—¬ì„±', 'ë‚¨ì', 'ì—¬ì', 'ë‚¨', 'ì—¬',
            # ì§ì—…
            'ì‚¬ë¬´ì§', 'ì „ë¬¸ì§', 'ì„œë¹„ìŠ¤ì§', 'í•™ìƒ', 'ì£¼ë¶€', 'ìì˜ì—…',
            'ì§ì¥ì¸', 'ìƒì‚°ì§', 'ë¬´ì§', 'í”„ë¦¬ëœì„œ', 'ì‚¬ë¬´ì›', 'í™”ì´íŠ¸ì¹¼ë¼',
            'ëŒ€í•™ìƒ', 'ê³ ë“±í•™ìƒ', 'ì†Œìƒê³µì¸', 'ì˜ì‚¬', 'ë³€í˜¸ì‚¬', 'íšŒê³„ì‚¬',
            'ì„œë¹„ìŠ¤ì—…', 'íŒë§¤ì§', 'ì˜ì—…', 'ë¸”ë£¨ì¹¼ë¼',
            # ì§€ì—­
            'ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°',
            'ê²½ê¸°', 'ê°•ì›', 'ì¶©ë¶', 'ì¶©ë‚¨', 'ì „ë¶', 'ì „ë‚¨', 'ê²½ë¶', 'ê²½ë‚¨', 'ì œì£¼', 'ì„¸ì¢…',
            # ê¸°íƒ€
            'ë¯¸í˜¼', 'ê¸°í˜¼', 'ì‹±ê¸€', 'ê²°í˜¼'
        }
    
    def analyze(self, query: str, context: str = "") -> QueryAnalysis:
        """ê°•í™”ëœ ê·œì¹™ ê¸°ë°˜ ì¿¼ë¦¬ ë¶„ì„

        Args:
            query: ë¶„ì„í•  ì¿¼ë¦¬
            context: ì¶”ê°€ ë§¥ë½

        Returns:
            ë¶„ì„ ê²°ê³¼
        """
        if not self.validate_query(query):
            return self._create_empty_analysis()

        query = self.preprocess_query(query)

        # í‚¤ì›Œë“œ ì¶”ì¶œ
        must_terms = []
        should_terms = []
        intent_hints = []
        expanded_keywords = {}

        for pattern_name, pattern_info in self.patterns.items():
            matches = re.findall(pattern_info["pattern"], query)
            if matches:
                for match in matches:
                    # ì¤‘ë³µ ì œê±°
                    if match not in must_terms:
                        must_terms.append(match)

                        # í‚¤ì›Œë“œ í™•ì¥
                        if match in self.keyword_expansions:
                            expanded_keywords[match] = self.keyword_expansions[match]
                            should_terms.extend(self.keyword_expansions[match])

                intent_hints.append(pattern_info["type"])

        # Demographics í‚¤ì›Œë“œ ë¶„ë¦¬ (must_termsì—ì„œ ì œê±°, í•„í„°ë¡œë§Œ ì²˜ë¦¬)
        demographic_terms = [t for t in must_terms if t in self.demographic_keywords]
        must_terms = [t for t in must_terms if t not in self.demographic_keywords]
        
        # ì¶”ê°€ í‚¤ì›Œë“œ ì¶”ì¶œ (íŒ¨í„´ ì™¸, Demographics ì œì™¸)
        additional_terms = self._extract_additional_keywords(query, must_terms + demographic_terms)
        must_terms.extend(additional_terms)

        # ì˜ë„ ê²°ì •
        intent = self._determine_intent(intent_hints)

        # Alpha ê°’ ê²°ì •
        alpha = self._calculate_alpha(intent)

        # ì‹ ë¢°ë„ ê³„ì‚° (í‚¤ì›Œë“œê°€ ë§ì„ìˆ˜ë¡ ë†’ìŒ)
        confidence = min(0.7, 0.3 + len(must_terms) * 0.1)

        return QueryAnalysis(
            intent=intent,
            must_terms=list(set(must_terms)),
            should_terms=list(set(should_terms)),
            must_not_terms=[],
            alpha=alpha,
            expanded_keywords=expanded_keywords,
            confidence=confidence,
            explanation=f"ê·œì¹™ ê¸°ë°˜ ë¶„ì„ - {len(must_terms)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ",
            reasoning_steps=[
                "íŒ¨í„´ ë§¤ì¹­ ìˆ˜í–‰",
                f"ì¶”ì¶œëœ í‚¤ì›Œë“œ: {', '.join(must_terms[:5])}",
                f"ì˜ë„: {intent}"
            ],
            analyzer_used=self.get_name()
        )

    def _extract_additional_keywords(self, query: str, existing_terms: List[str]) -> List[str]:
        """íŒ¨í„´ ì™¸ ì¶”ê°€ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Demographicsì™€ ë©”íƒ€ í‚¤ì›Œë“œ ì œì™¸
        """
        additional = []
        
        # í† í°í™” (í•œê¸€ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬)
        tokens = re.findall(r'\w+', query)
        
        for token in tokens:
            # ì œì™¸ ì¡°ê±´
            if (token in existing_terms or 
                token in self.meta_keywords or 
                token in self.demographic_keywords or
                len(token) <= 1):
                continue
            
            additional.append(token)
        
        logger.info(f"ğŸ”‘ ì‹¤ì œ ê²€ìƒ‰ í‚¤ì›Œë“œ (Demographics ì œì™¸): {additional}")
        return additional
    
    def _determine_intent(self, hints: List[str]) -> str:
        """íŒíŠ¸ ê¸°ë°˜ ì˜ë„ ê²°ì •"""
        if not hints:
            return "hybrid"
        
        hint_counts = {}
        for hint in hints:
            hint_counts[hint] = hint_counts.get(hint, 0) + 1
        
        # ê°€ì¥ ë§ì€ íŒíŠ¸ íƒ€ì…
        dominant_hint = max(hint_counts, key=hint_counts.get)
        
        # íŒíŠ¸ë¥¼ ì˜ë„ë¡œ ë§¤í•‘
        intent_map = {
            "demographic": "exact_match",
            "sentiment": "semantic_search",
            "behavioral": "hybrid",
            "comparison": "hybrid"
        }
        
        return intent_map.get(dominant_hint, "hybrid")
    
    def _calculate_alpha(self, intent: str) -> float:
        """ì˜ë„ì— ë”°ë¥¸ alpha ê°’ ê³„ì‚°"""
        alpha_map = {
            "exact_match": 0.2,
            "semantic_search": 0.8,
            "hybrid": 0.5
        }
        return alpha_map.get(intent, 0.5)
    
    def _create_empty_analysis(self) -> QueryAnalysis:
        """ë¹ˆ ë¶„ì„ ê²°ê³¼ ìƒì„±"""
        return QueryAnalysis(
            intent="hybrid",
            must_terms=[],
            should_terms=[],
            must_not_terms=[],
            alpha=0.5,
            expanded_keywords={},
            confidence=0.0,
            explanation="ìœ íš¨í•˜ì§€ ì•Šì€ ì¿¼ë¦¬",
            analyzer_used=self.get_name()
        )