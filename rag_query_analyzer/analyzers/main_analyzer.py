import time
import logging
import asyncio
from typing import List, Tuple, Optional, Dict
from concurrent.futures import ThreadPoolExecutor

from .base import BaseAnalyzer
from .claude_analyzer import ClaudeAnalyzer
from .semantic_analyzer import SemanticAnalyzer
from .rule_analyzer import RuleBasedAnalyzer

from ..models.query import QueryAnalysis, SearchResult
from ..core import (
    SemanticModel,
    MultiStepQueryRewriter,
    QueryOptimizer,
    LRUCachedAnalyzer
)
from ..utils import Reranker
from connectors.hybrid_searcher import OpenSearchHybridQueryBuilder
from ..config import Config

logger = logging.getLogger(__name__)


class AdvancedRAGQueryAnalyzer:
    """Í≥†Í∏â RAG ÏøºÎ¶¨ Î∂ÑÏÑùÍ∏∞
    
    Ïó¨Îü¨ Î∂ÑÏÑù Ï†ÑÎûµÏùÑ ÌÜµÌï©ÌïòÍ≥† ÏµúÏ†ÅÌôîÎêú Í≤ÄÏÉâÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.
    """
    
    def __init__(self, config: Config = None):
        """Ï¥àÍ∏∞Ìôî
        
        Args:
            config: ÏÑ§Ï†ï Í∞ùÏ≤¥
        """
        self.config = config or Config()
        self.config.validate()
        
        # ÌïµÏã¨ Ïª¥Ìè¨ÎÑåÌä∏ Ï¥àÍ∏∞Ìôî
        self._init_components()
        
        # Î∂ÑÏÑùÍ∏∞ Ï≤¥Ïù∏ Ï¥àÍ∏∞Ìôî
        self._init_analyzers()
        
        # ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨ ÏÑ§Ï†ï
        if self.config.ENABLE_ASYNC:
            self.executor = ThreadPoolExecutor(max_workers=self.config.MAX_WORKERS)
        else:
            self.executor = None
        
        logger.info("AdvancedRAGQueryAnalyzer Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
    
    def _init_components(self):
        """Ïª¥Ìè¨ÎÑåÌä∏ Ï¥àÍ∏∞Ìôî"""
        self.semantic_model = SemanticModel()
        self.query_rewriter = MultiStepQueryRewriter(self.config)
        self.query_optimizer = QueryOptimizer(self.config)
        # ‚ö†Ô∏è QueryExpander Ï†úÍ±∞: ÌïòÎìúÏΩîÎî©Îêú ÎèôÏùòÏñ¥ ÎåÄÏã† HybridSynonymExpander ÏÇ¨Ïö©
        self.es_query_builder = OpenSearchHybridQueryBuilder(self.config)
        
        # Ï∫êÏãú
        if self.config.ENABLE_CACHE:
            self.cache = LRUCachedAnalyzer(self.config)
        else:
            self.cache = None
        
        # Î¶¨Îû≠Ïª§
        if self.config.ENABLE_RERANKING:
            self.reranker = Reranker(self.config)
        else:
            self.reranker = None
    
    def _init_analyzers(self):
        """Î∂ÑÏÑùÍ∏∞ Ï≤¥Ïù∏ Ï¥àÍ∏∞Ìôî"""
        self.analyzers = [
            ("Claude", ClaudeAnalyzer(self.config)),
            ("Semantic", SemanticAnalyzer(self.config)),
            ("Rule", RuleBasedAnalyzer())
        ]
    
    def analyze_query(self, 
                     query: str, 
                     context: str = "",
                     metadata: Dict = None) -> QueryAnalysis:
        """ÏøºÎ¶¨ Î∂ÑÏÑù (Î©îÏù∏ ÏóîÌä∏Î¶¨ Ìè¨Ïù∏Ìä∏)
        
        Args:
            query: Î∂ÑÏÑùÌï† ÏøºÎ¶¨
            context: ÏÑ§Î¨∏ Îß•ÎùΩ
            metadata: Ï∂îÍ∞Ä Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
            
        Returns:
            Î∂ÑÏÑù Í≤∞Í≥º
        """
        start_time = time.time()
        
        # Ï∫êÏãú ÌôïÏù∏
        if self.cache:
            cached = self.cache.get_cached(query)
            if cached:
                cached.execution_time = time.time() - start_time
                return cached
        
        # ÏøºÎ¶¨ ÌôïÏû• (Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÌôúÏö©)
        # ‚ö†Ô∏è QueryExpander.expand_with_context Ï†úÍ±∞: ÌïÑÏöîÏãú Î≥ÑÎèÑ Íµ¨ÌòÑ
        # if metadata:
        #     query = self.query_expander.expand_with_context(query, metadata)
        
        # Ìè¥Î∞± Ï≤¥Ïù∏ÏúºÎ°ú Î∂ÑÏÑù
        analysis = self._analyze_with_fallback(query, context)
        
        # Í≥ºÍ±∞ ÏÑ±Îä• Îç∞Ïù¥ÌÑ∞ ÌôúÏö©
        optimal_params = self.query_optimizer.find_optimal_params(query)
        if optimal_params and optimal_params["confidence"] > 0.7:
            analysis.alpha = optimal_params["optimal_alpha"]
            logger.info(f"Í≥ºÍ±∞ Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò alpha Ï°∞Ï†ï: {analysis.alpha:.2f}")
        
        # Ïã§Ìñâ ÏãúÍ∞Ñ Í∏∞Î°ù
        analysis.execution_time = time.time() - start_time
        
        # Ï∫êÏãú Ï†ÄÏû•
        if self.cache:
            self.cache.set_cached(query, analysis)
        
        return analysis
    
    def _analyze_with_fallback(self, query: str, context: str) -> QueryAnalysis:
        """Ìè¥Î∞± Ï≤¥Ïù∏ÏùÑ ÌÜµÌïú Î∂ÑÏÑù
        
        Args:
            query: Î∂ÑÏÑùÌï† ÏøºÎ¶¨
            context: Îß•ÎùΩ
            
        Returns:
            Î∂ÑÏÑù Í≤∞Í≥º
        """
        for name, analyzer in self.analyzers:
            try:
                logger.info(f"üîç {name} Î∂ÑÏÑùÍ∏∞ ÏãúÎèÑ Ï§ë...")
                analysis = analyzer.analyze(query, context)
                
                # ÏÑ±Í≥µÏ†ÅÏù∏ Î∂ÑÏÑùÏù∏ÏßÄ ÌôïÏù∏
                if analysis.confidence >= 0.3 and analysis.must_terms:
                    logger.info(f"‚úÖ {name} Î∂ÑÏÑùÍ∏∞ ÏÑ±Í≥µ")
                    return analysis
                    
            except Exception as e:
                logger.warning(f"{name} Î∂ÑÏÑùÍ∏∞ Ïã§Ìå®: {e}")
                continue
        
        # Î™®Îì† Î∂ÑÏÑùÍ∏∞ Ïã§Ìå®Ïãú Í∏∞Î≥∏Í∞í
        logger.warning("Î™®Îì† Î∂ÑÏÑùÍ∏∞ Ïã§Ìå®, Í∏∞Î≥∏Í∞í Î∞òÌôò")
        return self._create_default_analysis(query)
    
    def analyze_with_rewriting(self, 
                              query: str, 
                              context: str = "",
                              metadata: Dict = None) -> Tuple[QueryAnalysis, List[str]]:
        """ÏøºÎ¶¨ Ïû¨ÏûëÏÑ±ÏùÑ Ìè¨Ìï®Ìïú Ï¢ÖÌï© Î∂ÑÏÑù
        
        Args:
            query: ÏõêÎ≥∏ ÏøºÎ¶¨
            context: Îß•ÎùΩ
            metadata: Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
            
        Returns:
            (Î∂ÑÏÑù Í≤∞Í≥º, Ïû¨ÏûëÏÑ±Îêú ÏøºÎ¶¨Îì§)
        """
        # 1. ÏøºÎ¶¨ Ïû¨ÏûëÏÑ±
        rewrites = self.query_rewriter.rewrite_query(query, context)
        
        # 2. ÏõêÎ≥∏ ÏøºÎ¶¨ Î∂ÑÏÑù
        main_analysis = self.analyze_query(query, context, metadata)
        
        # 3. Ïû¨ÏûëÏÑ±Îêú ÏøºÎ¶¨Îì§ÎèÑ Î∂ÑÏÑùÌïòÏó¨ ÌÜµÌï©
        if rewrites:
            for rw_type, rw_query in rewrites[:2]:  # ÏÉÅÏúÑ 2Í∞úÎßå
                try:
                    sub_analysis = self.analyze_query(rw_query, context, metadata)
                    main_analysis.merge_with(sub_analysis)
                except Exception as e:
                    logger.warning(f"Ïû¨ÏûëÏÑ± ÏøºÎ¶¨ Î∂ÑÏÑù Ïã§Ìå® ({rw_type}): {e}")
        
        # Ïû¨ÏûëÏÑ± ÏøºÎ¶¨ Ï†ÄÏû•
        main_analysis.rewritten_queries = [q for _, q in rewrites]
        
        return main_analysis, main_analysis.rewritten_queries
    
    def rerank_results(self, 
                      query: str, 
                      results: List[SearchResult],
                      top_k: Optional[int] = None) -> List[SearchResult]:
        """Í≤ÄÏÉâ Í≤∞Í≥º Î¶¨Îû≠ÌÇπ
        
        Args:
            query: ÏõêÎ≥∏ ÏøºÎ¶¨
            results: Í≤ÄÏÉâ Í≤∞Í≥º
            top_k: ÏÉÅÏúÑ kÍ∞ú Î∞òÌôò
            
        Returns:
            Î¶¨Îû≠ÌÇπÎêú Í≤∞Í≥º
        """
        if not self.reranker:
            return results[:top_k] if top_k else results
        
        return self.reranker.rerank(query, results, top_k)
    
    def build_search_query(self, 
                         analysis: QueryAnalysis,
                         query_vector: Optional[List[float]] = None,
                         size: Optional[int] = None,
                         filters: List[Dict] = None) -> Dict:
        """Elasticsearch Í≤ÄÏÉâ ÏøºÎ¶¨ Íµ¨ÏÑ±
        
        Args:
            analysis: ÏøºÎ¶¨ Î∂ÑÏÑù Í≤∞Í≥º
            query_vector: ÏûÑÎ≤†Îî© Î≤°ÌÑ∞
            size: ÏöîÏ≤≠Ìï† Î¨∏ÏÑú Í∞úÏàò
            filters: ÌïÑÌÑ∞ Ï°∞Í±¥
            
        Returns:
            Elasticsearch ÏøºÎ¶¨
        """
        if size is None:
            size = self.config.INITIAL_SEARCH_SIZE

        return self.es_query_builder.build_complete_request(
            analysis=analysis,
            query_vector=query_vector,
            size=size,
            filters=filters
        )
    
    async def analyze_batch_async(self, 
                                 queries: List[str],
                                 context: str = "") -> List[QueryAnalysis]:
        """Î∞∞Ïπò ÏøºÎ¶¨ ÎπÑÎèôÍ∏∞ Î∂ÑÏÑù
        
        Args:
            queries: ÏøºÎ¶¨ Î¶¨Ïä§Ìä∏
            context: Îß•ÎùΩ
            
        Returns:
            Î∂ÑÏÑù Í≤∞Í≥º Î¶¨Ïä§Ìä∏
        """
        if not self.executor:
            # ÎèôÍ∏∞ Ï≤òÎ¶¨
            return [self.analyze_query(q, context) for q in queries]
        
        loop = asyncio.get_event_loop()
        tasks = []
        
        for query in queries:
            task = loop.run_in_executor(
                self.executor,
                self.analyze_query,
                query,
                context
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        logger.info(f"‚úÖ {len(results)}Í∞ú ÏøºÎ¶¨ Î∞∞Ïπò Ï≤òÎ¶¨ ÏôÑÎ£å")
        
        return results
    
    def log_performance(self, 
                       query: str,
                       analysis: QueryAnalysis,
                       results: List[SearchResult],
                       user_feedback: Optional[float] = None):
        """ÏÑ±Îä• Î°úÍπÖ
        
        Args:
            query: ÏõêÎ≥∏ ÏøºÎ¶¨
            analysis: Î∂ÑÏÑù Í≤∞Í≥º
            results: Í≤ÄÏÉâ Í≤∞Í≥º
            user_feedback: ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞±
        """
        self.query_optimizer.log_performance(
            query, analysis, results, user_feedback
        )
    
    def explain_analysis(self, analysis: QueryAnalysis) -> str:
        """Î∂ÑÏÑù Í≤∞Í≥ºÎ•º ÏÇ¨Ïö©Ïûê ÏπúÌôîÏ†ÅÏúºÎ°ú ÏÑ§Î™Ö
        
        Args:
            analysis: Î∂ÑÏÑù Í≤∞Í≥º
            
        Returns:
            ÏÑ§Î™Ö Î¨∏ÏûêÏó¥
        """
        lines = []
        lines.append("üìä ÏøºÎ¶¨ Î∂ÑÏÑù Í≤∞Í≥º")
        lines.append("=" * 50)
        
        # Í≤ÄÏÉâ Ï†ÑÎûµ
        intent_map = {
            "exact_match": "Ï†ïÌôïÌïú Ï°∞Í±¥ Îß§Ïπ≠",
            "semantic_search": "ÏùòÎØ∏Ï†Å Ïú†ÏÇ¨ÏÑ± Í≤ÄÏÉâ",
            "hybrid": "Î≥µÌï© Í≤ÄÏÉâ (Ï°∞Í±¥ + ÏùòÎØ∏)"
        }
        lines.append(f"Í≤ÄÏÉâ Ï†ÑÎûµ: {intent_map.get(analysis.intent, analysis.intent)}")
        
        # Ïã†Î¢∞ÎèÑ
        if analysis.confidence >= 0.7:
            conf_level = "ÎÜíÏùå ‚úÖ"
        elif analysis.confidence >= 0.4:
            conf_level = "Î≥¥ÌÜµ ‚ö†Ô∏è"
        else:
            conf_level = "ÎÇÆÏùå ‚ùå"
        lines.append(f"Ïã†Î¢∞ÎèÑ: {analysis.confidence:.0%} ({conf_level})")
        
        # ÌÇ§ÏõåÎìú
        if analysis.must_terms:
            lines.append(f"\nÌïÑÏàò Ï°∞Í±¥: {', '.join(analysis.must_terms)}")
        if analysis.should_terms:
            lines.append(f"ÏÑ†ÌÉù Ï°∞Í±¥: {', '.join(analysis.should_terms)}")
        if analysis.must_not_terms:
            lines.append(f"Ï†úÏô∏ Ï°∞Í±¥: {', '.join(analysis.must_not_terms)}")
        
        # ÌôïÏû• ÌÇ§ÏõåÎìú
        if analysis.expanded_keywords:
            lines.append("\nÌôïÏû•Îêú ÌÇ§ÏõåÎìú:")
            for key, values in list(analysis.expanded_keywords.items())[:3]:
                lines.append(f"  ‚Ä¢ {key} ‚Üí {', '.join(values[:3])}")
        
        # Ï∂îÎ°† Í≥ºÏ†ï
        if analysis.reasoning_steps:
            lines.append("\nÎ∂ÑÏÑù Í≥ºÏ†ï:")
            for step in analysis.reasoning_steps[:3]:
                lines.append(f"  ‚Ä¢ {step}")
        
        # ÏÑ±Îä• Ï†ïÎ≥¥
        if analysis.execution_time > 0:
            lines.append(f"\nÏã§Ìñâ ÏãúÍ∞Ñ: {analysis.execution_time:.3f}Ï¥à")
        
        return "\n".join(lines)
    
    def get_statistics(self) -> Dict:
        """ÏãúÏä§ÌÖú ÌÜµÍ≥Ñ Î∞òÌôò
        
        Returns:
            ÌÜµÍ≥Ñ Ï†ïÎ≥¥
        """
        stats = {
            "cache_enabled": self.config.ENABLE_CACHE,
            "reranking_enabled": self.config.ENABLE_RERANKING,
            "async_enabled": self.config.ENABLE_ASYNC
        }
        
        if self.cache:
            stats.update(self.cache.get_statistics())
        
        stats["performance_logs"] = len(self.query_optimizer.performance_logs)
        
        return stats
    
    def _create_default_analysis(self, query: str) -> QueryAnalysis:
        """Í∏∞Î≥∏ Î∂ÑÏÑù Í≤∞Í≥º ÏÉùÏÑ±
        
        Args:
            query: ÏõêÎ≥∏ ÏøºÎ¶¨
            
        Returns:
            Í∏∞Î≥∏ Î∂ÑÏÑù Í≤∞Í≥º
        """
        return QueryAnalysis(
            intent="hybrid",
            must_terms=[query],
            should_terms=[],
            must_not_terms=[],
            alpha=0.5,
            expanded_keywords={},
            confidence=0.1,
            explanation="Í∏∞Î≥∏ Î∂ÑÏÑù (Ìè¥Î∞±)",
            analyzer_used="default",
            fallback_used=True
        )

