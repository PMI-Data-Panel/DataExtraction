import time
import logging
import asyncio
from typing import List, Tuple, Optional, Dict, Set
from concurrent.futures import ThreadPoolExecutor

from .base import BaseAnalyzer
from .claude_analyzer import ClaudeAnalyzer
from .demographic_extractor import DemographicExtractor
from .semantic_analyzer import SemanticAnalyzer
from .rule_analyzer import RuleBasedAnalyzer

from ..models.query import QueryAnalysis, SearchResult
from ..core import (
    SemanticModel,
    MultiStepQueryRewriter,
    QueryOptimizer,
    LRUCachedAnalyzer
)
from ..utils import Reranker
from connectors.hybrid_searcher import OpenSearchHybridQueryBuilder
from ..config import Config

logger = logging.getLogger(__name__)


class AdvancedRAGQueryAnalyzer:
    """Í≥†Í∏â RAG ÏøºÎ¶¨ Î∂ÑÏÑùÍ∏∞
    
    Ïó¨Îü¨ Î∂ÑÏÑù Ï†ÑÎûµÏùÑ ÌÜµÌï©ÌïòÍ≥† ÏµúÏ†ÅÌôîÎêú Í≤ÄÏÉâÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.
    """
    
    def __init__(self, config: Config = None):
        """Ï¥àÍ∏∞Ìôî
        
        Args:
            config: ÏÑ§Ï†ï Í∞ùÏ≤¥
        """
        self.config = config or Config()
        self.config.validate()
        
        # ÌïµÏã¨ Ïª¥Ìè¨ÎÑåÌä∏ Ï¥àÍ∏∞Ìôî
        self._init_components()
        
        # Î∂ÑÏÑùÍ∏∞ Ï≤¥Ïù∏ Ï¥àÍ∏∞Ìôî
        self._init_analyzers()
        
        # ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨ ÏÑ§Ï†ï
        if self.config.ENABLE_ASYNC:
            self.executor = ThreadPoolExecutor(max_workers=self.config.MAX_WORKERS)
        else:
            self.executor = None
        
        logger.info("AdvancedRAGQueryAnalyzer Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
    
    def _init_components(self):
        """Ïª¥Ìè¨ÎÑåÌä∏ Ï¥àÍ∏∞Ìôî"""
        self.semantic_model = SemanticModel()
        self.query_rewriter = MultiStepQueryRewriter(self.config)
        self.query_optimizer = QueryOptimizer(self.config)
        # ‚ö†Ô∏è QueryExpander Ï†úÍ±∞: ÌïòÎìúÏΩîÎî©Îêú ÎèôÏùòÏñ¥ ÎåÄÏã† HybridSynonymExpander ÏÇ¨Ïö©
        self.es_query_builder = OpenSearchHybridQueryBuilder(self.config)
        
        # Ï∫êÏãú
        if self.config.ENABLE_CACHE:
            self.cache = LRUCachedAnalyzer(self.config)
        else:
            self.cache = None
        
        # Î¶¨Îû≠Ïª§
        if self.config.ENABLE_RERANKING:
            self.reranker = Reranker(self.config)
        else:
            self.reranker = None
    
    def _init_analyzers(self):
        """Î∂ÑÏÑùÍ∏∞ Ï≤¥Ïù∏ Ï¥àÍ∏∞Ìôî"""
        self.rule_analyzer = RuleBasedAnalyzer()
        self.semantic_analyzer = SemanticAnalyzer(self.config)
        self.claude_analyzer = None
        self.analyzers = [
            ("Semantic", self.semantic_analyzer),
            ("Rule", self.rule_analyzer),
        ]
        if self.config.ENABLE_CLAUDE_ANALYZER:
            self.claude_analyzer = ClaudeAnalyzer(self.config)
            self.analyzers.insert(0, ("Claude", self.claude_analyzer))
    
    def analyze_query(self, 
                     query: str, 
                     context: str = "",
                     metadata: Dict = None,
                     use_claude: Optional[bool] = None) -> QueryAnalysis:
        """ÏøºÎ¶¨ Î∂ÑÏÑù (Î©îÏù∏ ÏóîÌä∏Î¶¨ Ìè¨Ïù∏Ìä∏)
        
        Args:
            query: Î∂ÑÏÑùÌï† ÏøºÎ¶¨
            context: ÏÑ§Î¨∏ Îß•ÎùΩ
            metadata: Ï∂îÍ∞Ä Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
            
        Returns:
            Î∂ÑÏÑù Í≤∞Í≥º
        """
        start_time = time.time()
        
        # Ï∫êÏãú ÌôïÏù∏
        if use_claude is None:
            use_claude = self.config.ENABLE_CLAUDE_ANALYZER

        if self.cache:
            cached = self.cache.get_cached(query, use_claude=use_claude)
            if cached:
                cached.execution_time = time.time() - start_time
                return cached
        
        # Ìè¥Î∞± Ï≤¥Ïù∏ÏúºÎ°ú Î∂ÑÏÑù
        analysis = self._analyze_with_fallback(query, context, use_claude=use_claude)
        analysis = self._normalize_analysis(analysis, query, context)
        
        # Í≥ºÍ±∞ ÏÑ±Îä• Îç∞Ïù¥ÌÑ∞ ÌôúÏö©
        optimal_params = self.query_optimizer.find_optimal_params(query)
        if optimal_params and optimal_params["confidence"] > 0.7:
            analysis.alpha = optimal_params["optimal_alpha"]
            logger.info(f"Í≥ºÍ±∞ Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò alpha Ï°∞Ï†ï: {analysis.alpha:.2f}")
        
        # Ïã§Ìñâ ÏãúÍ∞Ñ Í∏∞Î°ù
        analysis.execution_time = time.time() - start_time
        
        # Ï∫êÏãú Ï†ÄÏû•
        if self.cache:
            self.cache.set_cached(query, analysis, use_claude=use_claude)
        
        return analysis
    
    def _analyze_with_fallback(self, query: str, context: str, use_claude: Optional[bool]) -> QueryAnalysis:
        """Ìè¥Î∞± Ï≤¥Ïù∏ÏùÑ ÌÜµÌïú Î∂ÑÏÑù
        
        Args:
            query: Î∂ÑÏÑùÌï† ÏøºÎ¶¨
            context: Îß•ÎùΩ
            
        Returns:
            Î∂ÑÏÑù Í≤∞Í≥º
        """
        pipeline: List[Tuple[str, BaseAnalyzer]] = []
        if use_claude:
            if self.claude_analyzer is None:
                if not self.config.CLAUDE_API_KEY:
                    logger.warning("Claude Î∂ÑÏÑùÍ∏∞Í∞Ä ÏöîÏ≤≠ÎêòÏóàÏßÄÎßå CLAUDE_API_KEYÍ∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. Claude Îã®Í≥ÑÎ•º Í±¥ÎÑàÎúÅÎãàÎã§.")
                else:
                    try:
                        self.claude_analyzer = ClaudeAnalyzer(self.config)
                    except Exception as exc:
                        logger.warning(f"Claude Î∂ÑÏÑùÍ∏∞ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {exc}")
            if self.claude_analyzer is not None:
                pipeline.append(("Claude", self.claude_analyzer))

        pipeline.extend([
            ("Semantic", self.semantic_analyzer),
            ("Rule", self.rule_analyzer),
        ])

        for name, analyzer in pipeline:
            try:
                logger.info(f"üîç {name} Î∂ÑÏÑùÍ∏∞ ÏãúÎèÑ Ï§ë...")
                analysis = analyzer.analyze(query, context)

                # ÏÑ±Í≥µÏ†ÅÏù∏ Î∂ÑÏÑùÏù∏ÏßÄ ÌôïÏù∏
                # ‚≠ê must_termsÍ∞Ä ÎπÑÏñ¥ÏûàÏñ¥ÎèÑ demographicsÎÇò behavioral_conditionsÍ∞Ä ÏûàÏúºÎ©¥ ÏÑ±Í≥µ
                has_useful_content = (
                    analysis.must_terms or
                    (hasattr(analysis, 'demographic_entities') and analysis.demographic_entities) or
                    (hasattr(analysis, 'behavioral_conditions') and analysis.behavioral_conditions)
                )
                if analysis.confidence >= 0.3 and has_useful_content:
                    logger.info(f"‚úÖ {name} Î∂ÑÏÑùÍ∏∞ ÏÑ±Í≥µ (must_terms={len(analysis.must_terms) if analysis.must_terms else 0}, demographics={len(analysis.demographic_entities) if hasattr(analysis, 'demographic_entities') and analysis.demographic_entities else 0}, behavioral={bool(getattr(analysis, 'behavioral_conditions', {}))})")
                    return analysis
                else:
                    logger.info(f"‚ö†Ô∏è {name} Î∂ÑÏÑùÍ∏∞ Í≤∞Í≥º Î∂ÄÏ°± (confidence={analysis.confidence:.2f}, has_content={has_useful_content})")

            except Exception as e:
                logger.warning(f"{name} Î∂ÑÏÑùÍ∏∞ Ïã§Ìå®: {e}")
                continue
        
        # Î™®Îì† Î∂ÑÏÑùÍ∏∞ Ïã§Ìå®Ïãú Í∏∞Î≥∏Í∞í
        logger.warning("Î™®Îì† Î∂ÑÏÑùÍ∏∞ Ïã§Ìå®, Í∏∞Î≥∏Í∞í Î∞òÌôò")
        return self._create_default_analysis(query)
    
    def analyze_with_rewriting(self, 
                              query: str, 
                              context: str = "",
                              metadata: Dict = None) -> Tuple[QueryAnalysis, List[str]]:
        """ÏøºÎ¶¨ Ïû¨ÏûëÏÑ±ÏùÑ Ìè¨Ìï®Ìïú Ï¢ÖÌï© Î∂ÑÏÑù
        
        Args:
            query: ÏõêÎ≥∏ ÏøºÎ¶¨
            context: Îß•ÎùΩ
            metadata: Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
            
        Returns:
            (Î∂ÑÏÑù Í≤∞Í≥º, Ïû¨ÏûëÏÑ±Îêú ÏøºÎ¶¨Îì§)
        """
        # 1. ÏøºÎ¶¨ Ïû¨ÏûëÏÑ±
        rewrites = self.query_rewriter.rewrite_query(query, context)
        
        # 2. ÏõêÎ≥∏ ÏøºÎ¶¨ Î∂ÑÏÑù
        main_analysis = self.analyze_query(query, context, metadata)
        
        # 3. Ïû¨ÏûëÏÑ±Îêú ÏøºÎ¶¨Îì§ÎèÑ Î∂ÑÏÑùÌïòÏó¨ ÌÜµÌï©
        if rewrites:
            for rw_type, rw_query in rewrites[:2]:  # ÏÉÅÏúÑ 2Í∞úÎßå
                try:
                    sub_analysis = self.analyze_query(rw_query, context, metadata)
                    main_analysis.merge_with(sub_analysis)
                except Exception as e:
                    logger.warning(f"Ïû¨ÏûëÏÑ± ÏøºÎ¶¨ Î∂ÑÏÑù Ïã§Ìå® ({rw_type}): {e}")
        
        # Ïû¨ÏûëÏÑ± ÏøºÎ¶¨ Ï†ÄÏû•
        main_analysis.rewritten_queries = [q for _, q in rewrites]
        
        return main_analysis, main_analysis.rewritten_queries
    
    def rerank_results(self, 
                      query: str, 
                      results: List[SearchResult],
                      top_k: Optional[int] = None) -> List[SearchResult]:
        """Í≤ÄÏÉâ Í≤∞Í≥º Î¶¨Îû≠ÌÇπ
        
        Args:
            query: ÏõêÎ≥∏ ÏøºÎ¶¨
            results: Í≤ÄÏÉâ Í≤∞Í≥º
            top_k: ÏÉÅÏúÑ kÍ∞ú Î∞òÌôò
            
        Returns:
            Î¶¨Îû≠ÌÇπÎêú Í≤∞Í≥º
        """
        if not self.reranker:
            return results[:top_k] if top_k else results
        
        return self.reranker.rerank(query, results, top_k)
    
    def build_search_query(self, 
                         analysis: QueryAnalysis,
                         query_vector: Optional[List[float]] = None,
                         size: Optional[int] = None,
                         filters: List[Dict] = None) -> Dict:
        """Elasticsearch Í≤ÄÏÉâ ÏøºÎ¶¨ Íµ¨ÏÑ±
        
        Args:
            analysis: ÏøºÎ¶¨ Î∂ÑÏÑù Í≤∞Í≥º
            query_vector: ÏûÑÎ≤†Îî© Î≤°ÌÑ∞
            size: ÏöîÏ≤≠Ìï† Î¨∏ÏÑú Í∞úÏàò
            filters: ÌïÑÌÑ∞ Ï°∞Í±¥
            
        Returns:
            Elasticsearch ÏøºÎ¶¨
        """
        if size is None:
            size = self.config.INITIAL_SEARCH_SIZE

        return self.es_query_builder.build_complete_request(
            analysis=analysis,
            query_vector=query_vector,
            size=size,
            filters=filters
        )
    
    async def analyze_batch_async(self, 
                                 queries: List[str],
                                 context: str = "") -> List[QueryAnalysis]:
        """Î∞∞Ïπò ÏøºÎ¶¨ ÎπÑÎèôÍ∏∞ Î∂ÑÏÑù
        
        Args:
            queries: ÏøºÎ¶¨ Î¶¨Ïä§Ìä∏
            context: Îß•ÎùΩ
            
        Returns:
            Î∂ÑÏÑù Í≤∞Í≥º Î¶¨Ïä§Ìä∏
        """
        if not self.executor:
            # ÎèôÍ∏∞ Ï≤òÎ¶¨
            return [self.analyze_query(q, context) for q in queries]
        
        loop = asyncio.get_event_loop()
        tasks = []
        
        for query in queries:
            task = loop.run_in_executor(
                self.executor,
                self.analyze_query,
                query,
                context
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        logger.info(f"‚úÖ {len(results)}Í∞ú ÏøºÎ¶¨ Î∞∞Ïπò Ï≤òÎ¶¨ ÏôÑÎ£å")
        
        return results
    
    def log_performance(self, 
                       query: str,
                       analysis: QueryAnalysis,
                       results: List[SearchResult],
                       user_feedback: Optional[float] = None):
        """ÏÑ±Îä• Î°úÍπÖ
        
        Args:
            query: ÏõêÎ≥∏ ÏøºÎ¶¨
            analysis: Î∂ÑÏÑù Í≤∞Í≥º
            results: Í≤ÄÏÉâ Í≤∞Í≥º
            user_feedback: ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞±
        """
        self.query_optimizer.log_performance(
            query, analysis, results, user_feedback
        )
    
    def explain_analysis(self, analysis: QueryAnalysis) -> str:
        """Î∂ÑÏÑù Í≤∞Í≥ºÎ•º ÏÇ¨Ïö©Ïûê ÏπúÌôîÏ†ÅÏúºÎ°ú ÏÑ§Î™Ö
        
        Args:
            analysis: Î∂ÑÏÑù Í≤∞Í≥º
            
        Returns:
            ÏÑ§Î™Ö Î¨∏ÏûêÏó¥
        """
        lines = []
        lines.append("üìä ÏøºÎ¶¨ Î∂ÑÏÑù Í≤∞Í≥º")
        lines.append("=" * 50)
        
        # Í≤ÄÏÉâ Ï†ÑÎûµ
        intent_map = {
            "exact_match": "Ï†ïÌôïÌïú Ï°∞Í±¥ Îß§Ïπ≠",
            "semantic_search": "ÏùòÎØ∏Ï†Å Ïú†ÏÇ¨ÏÑ± Í≤ÄÏÉâ",
            "hybrid": "Î≥µÌï© Í≤ÄÏÉâ (Ï°∞Í±¥ + ÏùòÎØ∏)"
        }
        lines.append(f"Í≤ÄÏÉâ Ï†ÑÎûµ: {intent_map.get(analysis.intent, analysis.intent)}")
        
        # Ïã†Î¢∞ÎèÑ
        if analysis.confidence >= 0.7:
            conf_level = "ÎÜíÏùå ‚úÖ"
        elif analysis.confidence >= 0.4:
            conf_level = "Î≥¥ÌÜµ ‚ö†Ô∏è"
        else:
            conf_level = "ÎÇÆÏùå ‚ùå"
        lines.append(f"Ïã†Î¢∞ÎèÑ: {analysis.confidence:.0%} ({conf_level})")
        
        # ÌÇ§ÏõåÎìú
        if analysis.must_terms:
            lines.append(f"\nÌïÑÏàò Ï°∞Í±¥: {', '.join(analysis.must_terms)}")
        if analysis.should_terms:
            lines.append(f"ÏÑ†ÌÉù Ï°∞Í±¥: {', '.join(analysis.should_terms)}")
        if analysis.must_not_terms:
            lines.append(f"Ï†úÏô∏ Ï°∞Í±¥: {', '.join(analysis.must_not_terms)}")
        
        # ÌôïÏû• ÌÇ§ÏõåÎìú
        if analysis.expanded_keywords:
            lines.append("\nÌôïÏû•Îêú ÌÇ§ÏõåÎìú:")
            for key, values in list(analysis.expanded_keywords.items())[:3]:
                lines.append(f"  ‚Ä¢ {key} ‚Üí {', '.join(values[:3])}")
        
        # Ï∂îÎ°† Í≥ºÏ†ï
        if analysis.reasoning_steps:
            lines.append("\nÎ∂ÑÏÑù Í≥ºÏ†ï:")
            for step in analysis.reasoning_steps[:3]:
                lines.append(f"  ‚Ä¢ {step}")
        
        # ÏÑ±Îä• Ï†ïÎ≥¥
        if analysis.execution_time > 0:
            lines.append(f"\nÏã§Ìñâ ÏãúÍ∞Ñ: {analysis.execution_time:.3f}Ï¥à")
        
        return "\n".join(lines)
    
    def get_statistics(self) -> Dict:
        """ÏãúÏä§ÌÖú ÌÜµÍ≥Ñ Î∞òÌôò
        
        Returns:
            ÌÜµÍ≥Ñ Ï†ïÎ≥¥
        """
        stats = {
            "cache_enabled": self.config.ENABLE_CACHE,
            "reranking_enabled": self.config.ENABLE_RERANKING,
            "async_enabled": self.config.ENABLE_ASYNC
        }
        
        if self.cache:
            stats.update(self.cache.get_statistics())
        
        stats["performance_logs"] = len(self.query_optimizer.performance_logs)
        
        return stats
    
    def _normalize_analysis(self, analysis: QueryAnalysis, query: str, context: str) -> QueryAnalysis:
        """Î∂ÑÏÑù Í≤∞Í≥º Ï†ïÍ∑úÌôî (ÌñâÎèô Ï°∞Í±¥/Î∂àÏö©Ïñ¥ Î≥¥Í∞ï)"""
        if not analysis:
            return analysis

        # ‚≠ê ÎîîÎ≤ÑÍπÖ: Claude Î∂ÑÏÑù Í≤∞Í≥º ÌôïÏù∏
        logger.warning(f"[Ï†ïÍ∑úÌôî Ï†Ñ] must_terms={analysis.must_terms}")
        logger.warning(f"[Ï†ïÍ∑úÌôî Ï†Ñ] demographic_entities={len(analysis.demographic_entities) if analysis.demographic_entities else 0}Í∞ú")
        logger.warning(f"[Ï†ïÍ∑úÌôî Ï†Ñ] behavioral_conditions={analysis.behavioral_conditions}")
        if analysis.demographic_entities:
            for entity in analysis.demographic_entities:
                logger.warning(f"  - {entity.demographic_type.value}: {entity.value}")

        # Rule Í∏∞Î∞ò Î∂àÏö©Ïñ¥/ÌñâÎèô ÌÇ§ÏõåÎìú
        rule_analyzer = getattr(self, "rule_analyzer", None) or RuleBasedAnalyzer()
        meta_lower = {kw.lower() for kw in rule_analyzer.meta_keywords}
        behavior_lower = {kw.lower() for kw in rule_analyzer.behavior_keywords}
        demographic_extractor = DemographicExtractor()

        def _is_meta(term: str) -> bool:
            lowered = term.lower()
            if lowered in meta_lower:
                return True
            return any(kw in lowered for kw in meta_lower)

        def _is_behavior(term: str) -> bool:
            lowered = term.lower()
            if lowered in behavior_lower:
                return True
            return any(kw in lowered for kw in behavior_lower)

        # must_terms Ï†ïÎ¶¨
        sanitized_must: List[str] = []
        removed_behavior_terms: List[str] = []
        removed_demographic_terms: List[str] = []
        for term in analysis.must_terms:
            if not term:
                continue
            if _is_meta(term):
                continue
            if _is_behavior(term):
                removed_behavior_terms.append(term)
                continue
            sanitized_must.append(term)

        # should_terms Ï†ïÎ¶¨
        sanitized_should: List[str] = []
        for term in analysis.should_terms:
            if not term or _is_meta(term):
                continue
            if _is_behavior(term):
                removed_behavior_terms.append(term)
                continue
            sanitized_should.append(term)

        # Demographics Ï∂îÏ∂ú Î∞è Ï†úÍ±∞
        # ‚≠ê ClaudeÍ∞Ä Ïù¥ÎØ∏ Ï∂îÏ∂úÌïú demographic_entitiesÍ∞Ä ÏûàÏúºÎ©¥ ÏÇ¨Ïö©, ÏóÜÏúºÎ©¥ DemographicExtractor ÏÇ¨Ïö©
        if analysis.demographic_entities:
            # ClaudeÍ∞Ä Ï∂îÏ∂úÌïú demographics ÏÇ¨Ïö©
            demographics_list = analysis.demographic_entities
            logger.info(f"‚úÖ ClaudeÍ∞Ä Ï∂îÏ∂úÌïú demographics ÏÇ¨Ïö©: {len(demographics_list)}Í∞ú")
        else:
            # Ìè¥Î∞±: DemographicExtractor ÏÇ¨Ïö©
            demographics = demographic_extractor.extract(query)
            demographics_list = demographics.demographics
            logger.info(f"‚ö†Ô∏è Claude demographics ÏóÜÏùå ‚Üí DemographicExtractor ÏÇ¨Ïö©: {len(demographics_list)}Í∞ú")

        demographic_tokens: Set[str] = set()
        for entity in demographics_list:
            demographic_tokens.add(entity.raw_value.lower())
            demographic_tokens.add(entity.value.lower())
            for syn in entity.synonyms:
                demographic_tokens.add(str(syn).lower())

        if demographic_tokens:
            sanitized_must = [
                term for term in sanitized_must
                if term.lower() not in demographic_tokens
            ]
            sanitized_should = [
                term for term in sanitized_should
                if term.lower() not in demographic_tokens
            ]
            removed_demographic_terms = [
                term for term in analysis.must_terms + analysis.should_terms
                if term and term.lower() in demographic_tokens
            ]

        # ÌñâÎèô Ï°∞Í±¥ Î≥¥Í∞ï (Rule Î∂ÑÏÑù Í≤∞Í≥ºÏôÄ Î≥ëÌï©) - must/should Ìï†Îãπ Ï†ÑÏóê Î®ºÏ†Ä ÏàòÌñâ
        if not analysis.behavioral_conditions:
            try:
                rule_analysis = rule_analyzer.analyze(query, context)
                if rule_analysis.behavioral_conditions:
                    analysis.behavioral_conditions = dict(rule_analysis.behavioral_conditions)
            except Exception as exc:
                logger.debug(f"Rule Î∂ÑÏÑù Î≥¥Í∞ï Ïã§Ìå®: {exc}")

        # ÌñâÎèô ÌÇ§ÏõåÎìú Ï≤òÎ¶¨:
        # - behavioral_conditionsÍ∞Ä ÏûàÏúºÎ©¥: ÏôÑÏ†Ñ Ï†úÍ±∞ (OpenSearch ÌïÑÌÑ∞Î°ú Ï≤òÎ¶¨Îê®)
        # - behavioral_conditionsÍ∞Ä ÏóÜÏúºÎ©¥: should_termsÎ°ú ÏôÑÌôî (ÏùòÎØ∏ Í≤ÄÏÉâ)
        if removed_behavior_terms:
            if not analysis.behavioral_conditions:
                # behavioral_conditionsÍ∞Ä ÏóÜÏúºÎ©¥ should_termsÎ°ú ÏôÑÌôî
                existing_should_lower = {term.lower() for term in sanitized_should}
                for term in removed_behavior_terms:
                    lowered = term.lower()
                    if lowered in existing_should_lower:
                        continue
                    sanitized_should.append(term)
                    existing_should_lower.add(lowered)
                logger.info(f"‚ö†Ô∏è Behavioral conditions ÏóÜÏùå ‚Üí ÌñâÎèô ÌÇ§ÏõåÎìúÎ•º should_termsÎ°ú ÏôÑÌôî: {removed_behavior_terms}")
            else:
                # behavioral_conditionsÍ∞Ä ÏûàÏúºÎ©¥ ÏôÑÏ†Ñ Ï†úÍ±∞ (ÌïÑÌÑ∞Î°ú Ï≤òÎ¶¨Îê®)
                logger.info(f"‚úÖ Behavioral conditions ÏûàÏùå ‚Üí ÌñâÎèô ÌÇ§ÏõåÎìú Ï†úÍ±∞: {removed_behavior_terms}")

        # Ï†ïÎ¶¨Îêú Î¶¨Ïä§Ìä∏ Î∞òÏòÅ (ÏûÖÎ†• ÏàúÏÑú Ïú†ÏßÄ)
        def _dedupe(items: List[str]) -> List[str]:
            seen = set()
            ordered: List[str] = []
            for item in items:
                lowered = item.lower()
                if lowered in seen:
                    continue
                seen.add(lowered)
                ordered.append(item)
            return ordered

        analysis.must_terms = _dedupe(sanitized_must)
        analysis.should_terms = _dedupe(sanitized_should)

        # Demographics Ï†ïÎ≥¥ Ï†ÄÏû• (Claude Í≤∞Í≥ºÎ•º Ïú†ÏßÄÌïòÍ±∞ÎÇò DemographicExtractor Í≤∞Í≥º Ï†ÄÏû•)
        # ‚≠ê ClaudeÍ∞Ä Ïù¥ÎØ∏ Ï∂îÏ∂úÌïú Í≤ΩÏö∞ Ïú†ÏßÄ, ÏïÑÎãàÎ©¥ DemographicExtractor Í≤∞Í≥º Ï†ÄÏû•
        if not analysis.demographic_entities:
            analysis.demographic_entities = demographics_list
        analysis.removed_demographic_terms = removed_demographic_terms

        logger.info(f"üîç Demographics ÏµúÏ¢Ö Ï†ÄÏû•: {len(analysis.demographic_entities)}Í∞ú")
        if removed_demographic_terms:
            logger.info(f"   ‚ùå Ï†úÍ±∞Îêú Demographics ÌÇ§ÏõåÎìú: {removed_demographic_terms}")

        return analysis

    def _create_default_analysis(self, query: str) -> QueryAnalysis:
        """Í∏∞Î≥∏ Î∂ÑÏÑù Í≤∞Í≥º ÏÉùÏÑ±
        
        Args:
            query: ÏõêÎ≥∏ ÏøºÎ¶¨
            
        Returns:
            Í∏∞Î≥∏ Î∂ÑÏÑù Í≤∞Í≥º
        """
        return QueryAnalysis(
            intent="hybrid",
            must_terms=[query],
            should_terms=[],
            must_not_terms=[],
            alpha=0.5,
            expanded_keywords={},
            confidence=0.1,
            explanation="Í∏∞Î≥∏ Î∂ÑÏÑù (Ìè¥Î∞±)",
            analyzer_used="default",
            fallback_used=True,
            behavioral_conditions={},
        )

