import pandas as pd
import logging
import datetime
from typing import Dict, List, Any, Tuple, Optional
from pathlib import Path

logger = logging.getLogger(__name__)


class QuestionClassifier:
    """ì§ˆë¬¸ì„ ê°ê´€ì‹/ì£¼ê´€ì‹ìœ¼ë¡œ ë¶„ë¥˜í•˜ê³  ë©”íƒ€ë°ì´í„° ê´€ë¦¬"""

    def __init__(self, question_list_path: str = "./data/question_list.csv"):
        """
        question_list.csv ì˜ˆìƒ í˜•ì‹:
        q_code,q_text,q_type,category,field_name
        Q1,ë‚˜ì´ëŒ€ëŠ”?,ê°ê´€ì‹,ì¸êµ¬í†µê³„,age_group
        Q2,ì„±ë³„ì€?,ê°ê´€ì‹,ì¸êµ¬í†µê³„,gender
        Q3,ì„œë¹„ìŠ¤ ë§Œì¡±ë„ëŠ”?,ì£¼ê´€ì‹,ë§Œì¡±ë„,satisfaction
        """
        self.question_map = {}
        self.demographic_fields = set()
        self.objective_questions = set()
        self.subjective_questions = set()

        if Path(question_list_path).exists():
            self._load_from_csv(question_list_path)
        else:
            logger.warning(f"âš ï¸ {question_list_path} íŒŒì¼ ì—†ìŒ. íœ´ë¦¬ìŠ¤í‹± ë¶„ë¥˜ ì‚¬ìš©")
            self.use_heuristic = True

    def _load_from_csv(self, path: str):
        """CSVì—ì„œ ì§ˆë¬¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        try:
            df = pd.read_csv(path, encoding="utf-8-sig")

            for _, row in df.iterrows():
                q_text = row['q_text'].strip()
                q_type = row['q_type'].strip()

                self.question_map[q_text] = {
                    'q_code': row.get('q_code', ''),
                    'q_type': q_type,
                    'category': row.get('category', 'unknown'),
                    'field_name': row.get('field_name', '')
                }

                if q_type == 'ê°ê´€ì‹':
                    self.objective_questions.add(q_text)
                    if row.get('category') == 'ì¸êµ¬í†µê³„':
                        self.demographic_fields.add(q_text)
                else:
                    self.subjective_questions.add(q_text)

            logger.info(f"âœ… ì§ˆë¬¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ: ê°ê´€ì‹ {len(self.objective_questions)}ê°œ, ì£¼ê´€ì‹ {len(self.subjective_questions)}ê°œ")
            self.use_heuristic = False

        except Exception as e:
            logger.error(f"âŒ question_list.csv ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.use_heuristic = True

    def classify(self, q_text: str, answer_text: str) -> str:
        """ì§ˆë¬¸ì„ ê°ê´€ì‹/ì£¼ê´€ì‹ìœ¼ë¡œ ë¶„ë¥˜"""
        # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ë¶„ë¥˜
        if not self.use_heuristic and q_text in self.question_map:
            return self.question_map[q_text]['q_type']

        # íœ´ë¦¬ìŠ¤í‹±: ë‹µë³€ ê¸¸ì´ì™€ íŒ¨í„´ìœ¼ë¡œ íŒë‹¨
        answer_str = str(answer_text).strip()

        # ëª…í™•í•œ ê°ê´€ì‹ íŒ¨í„´
        objective_patterns = [
            'ë‚¨ì„±', 'ì—¬ì„±', 'ë‚¨ì', 'ì—¬ì',
            '10ëŒ€', '20ëŒ€', '30ëŒ€', '40ëŒ€', '50ëŒ€', '60ëŒ€', '70ëŒ€',
            'ë§¤ìš° ê·¸ë ‡ë‹¤', 'ê·¸ë ‡ë‹¤', 'ë³´í†µ', 'ì•„ë‹ˆë‹¤', 'ë§¤ìš° ì•„ë‹ˆë‹¤',
            'ì˜ˆ', 'ì•„ë‹ˆì˜¤', 'Y', 'N'
        ]

        if answer_str in objective_patterns or len(answer_str) <= 15:
            return 'ê°ê´€ì‹'

        # ê¸´ ë‹µë³€ì€ ì£¼ê´€ì‹
        if len(answer_str) > 50:
            return 'ì£¼ê´€ì‹'

        # ì‰¼í‘œë‚˜ ë§ˆì¹¨í‘œê°€ ìˆìœ¼ë©´ ì£¼ê´€ì‹
        if ',' in answer_str or '.' in answer_str or 'ã€‚' in answer_str:
            return 'ì£¼ê´€ì‹'

        # ê¸°ë³¸ê°’: ì• ë§¤í•˜ë©´ ì£¼ê´€ì‹ìœ¼ë¡œ (ì•ˆì „)
        return 'ì£¼ê´€ì‹' if len(answer_str) > 20 else 'ê°ê´€ì‹'

    def get_demographic_field(self, q_text: str) -> Optional[str]:
        """ì§ˆë¬¸ì„ demographic í•„ë“œëª…ìœ¼ë¡œ ë§¤í•‘"""
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ë¨¼ì € ì°¾ê¸°
        if q_text in self.question_map:
            field_name = self.question_map[q_text].get('field_name')
            if field_name:
                return field_name

        # íœ´ë¦¬ìŠ¤í‹± ë§¤í•‘
        field_mapping = {
            'ë‚˜ì´': 'age_group',
            'ë‚˜ì´ëŒ€': 'age_group',
            'ì—°ë ¹': 'age_group',
            'ì„±ë³„': 'gender',
            'ì§€ì—­': 'region',
            'ê±°ì£¼': 'region',
            'ì§ì—…': 'occupation',
            'ì†Œë“': 'income',
            'í•™ë ¥': 'education',
            'ê²°í˜¼': 'marital_status',
            'ê°€êµ¬': 'household'
        }

        q_lower = q_text.lower()
        for keyword, field in field_mapping.items():
            if keyword in q_lower:
                return field

        return None

    def is_demographic(self, q_text: str) -> bool:
        """ì¸êµ¬í†µê³„ ì§ˆë¬¸ì¸ì§€ í™•ì¸"""
        if not self.use_heuristic:
            return q_text in self.demographic_fields

        demo_keywords = ['ë‚˜ì´', 'ì—°ë ¹', 'ì„±ë³„', 'ì§€ì—­', 'ê±°ì£¼', 'ì§ì—…', 'ì†Œë“', 'í•™ë ¥']
        return any(kw in q_text for kw in demo_keywords)


def process_survey_data_hybrid(
    df_responses: pd.DataFrame,
    embedding_model,
    index_name: str,
    classifier: QuestionClassifier = None
) -> List[Dict[str, Any]]:
    """í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì¡°ë¡œ ì„¤ë¬¸ ë°ì´í„° ë³€í™˜

    êµ¬ì¡°:
    - demographics: ê°ê´€ì‹ ì¸êµ¬í†µê³„ ë°ì´í„° (ì •í™•í•œ í•„í„°ìš©)
    - subjective_responses: ì£¼ê´€ì‹ ë‹µë³€ë§Œ (nested + ë²¡í„°)
    - all_subjective_text: ì£¼ê´€ì‹ í†µí•© í…ìŠ¤íŠ¸ (í‚¤ì›Œë“œ ê²€ìƒ‰ìš©)
    """

    if classifier is None:
        classifier = QuestionClassifier()

    actions = []
    total_users = len(df_responses)

    logger.info(f"ğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì¡°ë¡œ {total_users}ëª… ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")

    stats = {
        'total_users': 0,
        'total_demographics': 0,
        'total_subjectives': 0,
        'skipped_users': 0
    }

    for user_count, (_, row) in enumerate(df_responses.iterrows(), 1):
        if user_count % 100 == 0 or user_count == 1 or user_count == total_users:
            logger.info(f"ğŸ”„ ì²˜ë¦¬ ì¤‘... ({user_count}/{total_users})")

        user_id = row.get("mb_sn")
        if not user_id or pd.isna(user_id):
            user_id = f"user_{user_count}"

        # 1. ë°ì´í„° ë¶„ë¥˜
        demographics = {}
        other_objectives = {}  # ì¸êµ¬í†µê³„ê°€ ì•„ë‹Œ ê°ê´€ì‹
        subjective_responses = []
        all_subjective_texts = []

        for q_text, answer_text in row.items():
            if q_text == "mb_sn" or pd.isna(answer_text):
                continue

            answer_text = str(answer_text).strip()
            if not answer_text or answer_text.lower() in ['nan', 'none', '']:
                continue

            # ì§ˆë¬¸ ë¶„ë¥˜
            q_type = classifier.classify(q_text, answer_text)

            if q_type == 'ê°ê´€ì‹':
                # ì¸êµ¬í†µê³„ì¸ì§€ í™•ì¸
                if classifier.is_demographic(q_text):
                    field_name = classifier.get_demographic_field(q_text)
                    if field_name:
                        demographics[field_name] = answer_text
                        stats['total_demographics'] += 1
                else:
                    # ì¸êµ¬í†µê³„ê°€ ì•„ë‹Œ ê°ê´€ì‹ (ì˜ˆ: ë§Œì¡±ë„ ë“±ê¸‰)
                    other_objectives[q_text] = answer_text

            else:  # ì£¼ê´€ì‹
                # ì„ë² ë”©ì€ ë‚˜ì¤‘ì— ë°°ì¹˜ë¡œ ì²˜ë¦¬
                q_info = classifier.question_map.get(q_text, {})

                subjective_responses.append({
                    "q_text": q_text,
                    "q_code": q_info.get('q_code', q_text[:20]),
                    "q_category": q_info.get('category', 'unknown'),
                    "answer_text": answer_text,
                    "answer_vector": None,  # ë‚˜ì¤‘ì— ë°°ì¹˜ë¡œ ìƒì„±
                    "answer_length": len(answer_text)
                })

                all_subjective_texts.append(answer_text)

        # 2. ë¬¸ì„œ êµ¬ì„±
        if not demographics and not subjective_responses:
            stats['skipped_users'] += 1
            continue

        final_document = {
            "user_id": str(user_id),
            "demographics": demographics,
            "other_objectives": other_objectives,  # ì„ íƒì 
            "subjective_responses": subjective_responses,
            "all_subjective_text": " ".join(all_subjective_texts),
            "metadata": {
                "timestamp": datetime.datetime.now().isoformat(),
                "total_questions": len(row) - 1,
                "demographic_count": len(demographics),
                "objective_count": len(other_objectives),
                "subjective_count": len(subjective_responses),
                "avg_answer_length": (
                    sum(r['answer_length'] for r in subjective_responses) / len(subjective_responses)
                    if subjective_responses else 0
                )
            }
        }

        actions.append({
            "_index": index_name,
            "_id": str(user_id),
            "_source": final_document
        })

        stats['total_users'] += 1

    # ë°°ì¹˜ ì„ë² ë”© ìƒì„± (KURE-v1 ìµœì í™”)
    logger.info(f"ğŸ”„ ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì¤‘... ({stats['total_subjectives']}ê°œ)")
    _generate_batch_embeddings(actions, embedding_model)

    # ìµœì¢… í†µê³„
    logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ë³€í™˜ ì™„ë£Œ:")
    logger.info(f"   - ì²˜ë¦¬ëœ ì‚¬ìš©ì: {stats['total_users']}ëª…")
    logger.info(f"   - ì¸êµ¬í†µê³„ í•„ë“œ: {stats['total_demographics']}ê°œ")
    logger.info(f"   - ì£¼ê´€ì‹ ë‹µë³€: {stats['total_subjectives']}ê°œ")
    logger.info(f"   - ìŠ¤í‚µëœ ì‚¬ìš©ì: {stats['skipped_users']}ëª…")

    return actions


def _generate_batch_embeddings(actions: List[Dict], embedding_model, batch_size: int = 64):
    """
    ëª¨ë“  ì£¼ê´€ì‹ ë‹µë³€ì˜ ì„ë² ë”©ì„ ë°°ì¹˜ë¡œ ìƒì„± (KURE-v1 ìµœì í™”)

    ì¥ì :
    - GPU í™œìš© ê·¹ëŒ€í™”
    - ì†ë„ 10-50ë°° í–¥ìƒ
    - ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
    """
    # 1. ëª¨ë“  í…ìŠ¤íŠ¸ ìˆ˜ì§‘
    all_texts = []
    text_indices = []  # (action_idx, response_idx)

    for action_idx, action in enumerate(actions):
        subjective_responses = action["_source"].get("subjective_responses", [])
        for response_idx, response in enumerate(subjective_responses):
            if response["answer_vector"] is None:
                all_texts.append(response["answer_text"])
                text_indices.append((action_idx, response_idx))

    if not all_texts:
        logger.info("ğŸ“Š ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ì—†ìŒ")
        return

    # 2. ë°°ì¹˜ ì„ë² ë”© ìƒì„±
    logger.info(f"ğŸ“Š {len(all_texts)}ê°œ í…ìŠ¤íŠ¸ ë°°ì¹˜ ì„ë² ë”© ìƒì„± (ë°°ì¹˜ í¬ê¸°: {batch_size})")

    try:
        # hasattrë¡œ encode ë©”ì„œë“œ í™•ì¸
        if hasattr(embedding_model, 'encode'):
            # ë°°ì¹˜ ì¸ì½”ë”©
            vectors = embedding_model.encode(
                all_texts,
                batch_size=batch_size,
                show_progress_bar=True,
                convert_to_tensor=False
            )

            # numpy arrayë¥¼ listë¡œ ë³€í™˜
            if hasattr(vectors, 'tolist'):
                vectors = [v.tolist() for v in vectors]
            elif isinstance(vectors, list):
                pass
            else:
                vectors = vectors.tolist()

        else:
            raise AttributeError("embedding_modelì— encode ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤")

    except Exception as e:
        logger.error(f"âŒ ë°°ì¹˜ ì„ë² ë”© ì‹¤íŒ¨: {e}")
        # Fallback: ê°œë³„ ì¸ì½”ë”©
        logger.warning("âš ï¸ Fallback: ê°œë³„ ì¸ì½”ë”©ìœ¼ë¡œ ì „í™˜")
        vectors = []
        for text in all_texts:
            try:
                vec = embedding_model.encode(text)
                if hasattr(vec, 'tolist'):
                    vec = vec.tolist()
                vectors.append(vec)
            except Exception as e2:
                logger.error(f"âŒ ê°œë³„ ì¸ì½”ë”©ë„ ì‹¤íŒ¨: {e2}")
                # ì œë¡œ ë²¡í„°ë¡œ ëŒ€ì²´
                vectors.append([0.0] * 1024)

    # 3. ë²¡í„° í• ë‹¹
    for (action_idx, response_idx), vector in zip(text_indices, vectors):
        actions[action_idx]["_source"]["subjective_responses"][response_idx]["answer_vector"] = vector

    logger.info(f"âœ… ë°°ì¹˜ ì„ë² ë”© ì™„ë£Œ: {len(vectors)}ê°œ")


def analyze_survey_structure(df_responses: pd.DataFrame) -> Dict[str, Any]:
    """ì„¤ë¬¸ ë°ì´í„° êµ¬ì¡° ë¶„ì„ (ë””ë²„ê¹…/ëª¨ë‹ˆí„°ë§ìš©)"""

    analysis = {
        'total_rows': len(df_responses),
        'total_columns': len(df_responses.columns),
        'columns': list(df_responses.columns),
        'missing_values': {},
        'unique_values': {},
        'value_distributions': {}
    }

    for col in df_responses.columns:
        if col == 'mb_sn':
            continue

        # ê²°ì¸¡ê°’
        missing = df_responses[col].isna().sum()
        analysis['missing_values'][col] = missing

        # ê³ ìœ ê°’ ê°œìˆ˜
        unique = df_responses[col].nunique()
        analysis['unique_values'][col] = unique

        # ê°ê´€ì‹ ì¶”ì • (ê³ ìœ ê°’ì´ 20ê°œ ë¯¸ë§Œ)
        if unique < 20:
            value_counts = df_responses[col].value_counts().head(10)
            analysis['value_distributions'][col] = value_counts.to_dict()

    return analysis
