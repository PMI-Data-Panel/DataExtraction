# redis_celery/tasks/search_tasks.py
"""
ðŸš€ ê°œì„ ëœ Celery ë³‘ë ¬ ê²€ìƒ‰ íƒœìŠ¤í¬

ì£¼ìš” ê°œì„  ì‚¬í•­:
1. ì¸ë±ìŠ¤ë³„ ê°œë³„ Celery Task ìƒì„± â†’ Worker ê°„ ë³‘ë ¬ ì²˜ë¦¬
2. Redis ìºì‹± ê°•í™” (ì¿¼ë¦¬ í•´ì‹œ ê¸°ë°˜)
3. Task Chordë¥¼ í™œìš©í•œ RRF ê²°í•©
4. ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”
"""
import asyncio
import json
import logging
import hashlib
import os

from typing import Dict, Any, List, Tuple, Optional
from time import perf_counter
from celery import group, chord, signals

from redis_celery.celery_app import celery_app
from opensearchpy import OpenSearch
from qdrant_client import QdrantClient
from connectors.data_fetcher import DataFetcher
from connectors.hybrid_searcher import calculate_rrf_score
from rag_query_analyzer.config import get_config
from sentence_transformers import SentenceTransformer
from redis import ConnectionPool, StrictRedis

logger = logging.getLogger(__name__)
_os_client = None
_qdrant_client = None
_redis_pool = None
_embedding_model = None
_config = None

# ==========================================
# ðŸ“Œ 1. ì¿¼ë¦¬ ìºì‹± ìœ í‹¸ë¦¬í‹°
# ==========================================
@signals.worker_process_init.connect
def setup_worker_environment(**kwargs):
    """
    Worker í”„ë¡œì„¸ìŠ¤ ì‹œìž‘ ì‹œ í´ë¼ì´ì–¸íŠ¸ì™€ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ìºì‹œí•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” Task ì‹¤í–‰ ë¹„ìš©ì„ íšê¸°ì ìœ¼ë¡œ ì¤„ì—¬ì¤ë‹ˆë‹¤.
    """
    global _os_client, _qdrant_client, _redis_pool, _embedding_model, _config

    try:
        # 1. ì„¤ì • ë¡œë“œ (Task ì‹¤í–‰ ë¹„ìš©ì´ ì•„ë‹˜)
        os.environ['SENTENCE_TRANSFORMERS_HOME'] = '/app/.cache/models'
        _config = get_config()
        
        # 2. í´ë¼ì´ì–¸íŠ¸ ì—°ê²° í’€ ìƒì„±
        # Docker Compose í™˜ê²½ì—ì„œëŠ” 'redis' ì„œë¹„ìŠ¤ ì´ë¦„ ì‚¬ìš©
        _redis_pool = ConnectionPool(
            host=os.getenv('REDIS_HOST', 'redis'), 
            port=int(os.getenv('REDIS_PORT', '6379')),
            db=int(os.getenv('CACHE_DB', '2')),
            decode_responses=True, max_connections=20,
            socket_connect_timeout=5, socket_timeout=5
        )
        _redis_client = StrictRedis(connection_pool=_redis_pool) # ì´ ì¸ìŠ¤í„´ìŠ¤ëŠ” Taskì—ì„œ ì‚¬ìš©

        # 3. OpenSearch í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ì¸ë¼ì¸ í†µí•©)
        _os_client = OpenSearch(
            hosts=[{
                'host': os.getenv('OPENSEARCH_HOST', 'redis'), # Docker service name fix
                'port': int(os.getenv('OPENSEARCH_PORT', '9200'))
            }],
            http_auth=(os.getenv('OPENSEARCH_USER', 'admin'), os.getenv('OPENSEARCH_PASSWORD', 'admin')),
            use_ssl=False, verify_certs=False, timeout=30,
        )
        
        # 4. Qdrant í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ì¸ë¼ì¸ í†µí•©)
        _qdrant_client = QdrantClient(
            host=os.getenv('QDRANT_HOST', 'redis'), 
            port=int(os.getenv('QDRANT_PORT', '6333')), 
            timeout=30
        )

        logger.info("[OK] External Clients (OS/Qdrant) initialized.")
        load_model_flag = os.getenv('LOAD_EMBEDDING_MODEL', 'False').lower() == 'true'
        
        if load_model_flag:
            model_name = _config.EMBEDDING_MODEL
            _embedding_model = SentenceTransformer(model_name)
            logger.info(f"âœ… Worker ì‹œìž‘: ìž„ë² ë”© ëª¨ë¸ '{model_name}' ë¡œë“œ ì™„ë£Œ")
        else:
            logger.info("â„¹ï¸ Worker ì‹œìž‘: ìž„ë² ë”© ëª¨ë¸ ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤ (LOAD_EMBEDDING_MODEL=False)")
        
        logger.info(f"âœ… Worker ì‹œìž‘: ìž„ë² ë”© ëª¨ë¸ '{model_name}' ë¡œë“œ ì™„ë£Œ")

    except Exception as e:
        logger.critical(f"âŒ Worker ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise # Workerê°€ ì´ˆê¸°í™” ì‹¤íŒ¨í•˜ë©´ ì£½ë„ë¡ ê°•ì œ (Healthcheck ì‹¤íŒ¨ ìœ ë„)


def get_redis_client() -> StrictRedis:
    """Taskì—ì„œ ìºì‹œëœ Redis ì—°ê²° í’€ì„ ì‚¬ìš©í•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
    global _redis_pool
    if _redis_pool is None:
        raise RuntimeError("Redis Pool not initialized.")
    return StrictRedis(connection_pool=_redis_pool)


def get_cache_key(query: str, index_name: str, filters: List[Dict] = None) -> str:
    """ìºì‹œ í‚¤ ìƒì„± (ì¿¼ë¦¬ í•´ì‹œ)"""
    cache_data = {
        'query': query,
        'index_name': index_name,
        'filters': filters or []
    }
    cache_str = json.dumps(cache_data, sort_keys=True, ensure_ascii=False)
    return f"search:{hashlib.md5(cache_str.encode()).hexdigest()}"


def get_cached_results(redis_client: StrictRedis, cache_key: str) -> Optional[Dict]:
    """Redisì—ì„œ ìºì‹œëœ ê²°ê³¼ ì¡°íšŒ"""
    try:
        cached = redis_client.get(cache_key)
        if cached:
            logger.info(f"âœ… ìºì‹œ HIT: {cache_key}")
            return json.loads(cached)
    except Exception as e:
        logger.warning(f"ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    return None


def cache_results(redis_client: StrictRedis, cache_key: str, results: Dict, ttl: int = 3600):
    """Redisì— ê²°ê³¼ ìºì‹±"""
    try:
        redis_client.setex(
            cache_key,
            ttl,
            json.dumps(results, ensure_ascii=False)
        )
        logger.info(f"âœ… ìºì‹œ ì €ìž¥: {cache_key}")
    except Exception as e:
        logger.warning(f"ìºì‹œ ì €ìž¥ ì‹¤íŒ¨: {e}")

# ==========================================
# ðŸ“Œ 2. ë‹¨ì¼ ì¸ë±ìŠ¤ ê²€ìƒ‰ Task (Worker ë¶„ì‚° ì²˜ë¦¬)
# ==========================================
@celery_app.task(
    name='tasks.search_single_index',
    bind=True,
    max_retries=2,
    default_retry_delay=3,
)
def search_single_index_task(
    self,
    query: str,
    index_name: str,
    query_vector: List[float],
    filters: List[Dict[str, Any]],
    search_size: int,
    qdrant_limit: int,
) -> Dict[str, Any]:
    """ë‹¨ì¼ ì¸ë±ìŠ¤ ê²€ìƒ‰"""
    task_id = self.request.id
    start_time = perf_counter()
    
    try:
        logger.info(f"ðŸ” [{task_id}] {index_name} ê²€ìƒ‰ ì‹œìž‘")
        
        # Redis ìºì‹œ í™•ì¸
        redis_client = get_redis_client()
        os_client, qdrant_client = get_search_clients() # ìºì‹œëœ í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜

        cache_key = get_cache_key(query, index_name, filters)
        cached = get_cached_results(redis_client, cache_key)
        if cached:
            return cached
        
        # âœ… EventLoop ìƒì„± ë° ì‹¤í–‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        loop_start = perf_counter()
        keyword_results, vector_results = loop.run_until_complete(
            execute_hybrid_search_async(
                os_client=os_client,
                qdrant_client=qdrant_client,
                index_name=index_name,
                query_vector=query_vector,
                filters=filters,
                search_size=search_size,
                qdrant_limit=qdrant_limit,
            )
        )
        
        # RRF ê²°í•©
        rrf_results = calculate_rrf_score(
            keyword_results, vector_results, k=60
        ) if vector_results else keyword_results
        
        for doc in rrf_results:
            doc['_index'] = index_name
        
        took_ms = (perf_counter() - start_time) * 1000
        
        result = {
            'index_name': index_name,
            'keyword_results': keyword_results,
            'vector_results': vector_results,
            'rrf_results': rrf_results,
            'took_ms': took_ms
        }
        
        # ìºì‹±
        cache_results(redis_client, cache_key, result)
        
        logger.info(f"âœ… [{task_id}] {index_name} ì™„ë£Œ: {len(rrf_results)}ê±´ ({took_ms:.2f}ms)")
        return result
        
    except Exception as exc:
        logger.error(f"âŒ [{task_id}] {index_name} ì‹¤íŒ¨: {exc}", exc_info=True)
        raise self.retry(exc=exc, countdown=3)
    
    finally:
        # âœ… EventLoop ì•ˆì „í•œ ì •ë¦¬
        if loop is not None:
            try:
                # ë‚¨ì€ Task ì·¨ì†Œ
                pending = asyncio.all_tasks(loop)
                for task in pending:
                    task.cancel()
                
                # ì •ë¦¬ ì‹¤í–‰
                if not loop.is_closed():
                    loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))
                    loop.close()
                    
            except Exception as e:
                logger.warning(f"âš ï¸ EventLoop ì •ë¦¬ ì‹¤íŒ¨: {e}")
            finally:
                # ì´ì „ ì´ë²¤íŠ¸ ë£¨í”„ ì´ˆê¸°í™”
                asyncio.set_event_loop(None)


async def execute_hybrid_search_async(
    os_client: OpenSearch,
    qdrant_client: QdrantClient,
    index_name: str,
    query_vector: List[float],
    filters: List[Dict[str, Any]],
    search_size: int,
    qdrant_limit: int,
) -> Tuple[List[Dict], List[Dict]]:
    """OpenSearch + Qdrant ë¹„ë™ê¸° ë³‘ë ¬ ì‹¤í–‰"""
    
    # ì¿¼ë¦¬ êµ¬ì„±
    os_query = {
        'query': {'match_all': {}},
        'size': search_size,
        '_source': ['user_id', 'metadata', 'qa_pairs', 'timestamp']
    }
    
    if filters:
        cleaned_filters = [remove_inner_hits(f) for f in filters]
        os_query['query'] = {'bool': {'must': cleaned_filters}}
    
    # ðŸ”¥ OpenSearch + Qdrant ë™ì‹œ ì‹¤í–‰
    opensearch_task = asyncio.to_thread(
        search_opensearch_sync,
        os_client=os_client,
        index_name=index_name,
        query=os_query,
        search_size=search_size,
    )
    
    qdrant_task = asyncio.to_thread(
        search_qdrant_sync,
        qdrant_client=qdrant_client,
        collection_name=index_name,
        query_vector=query_vector,
        limit=qdrant_limit,
    )
    
    os_response, qdrant_results = await asyncio.gather(opensearch_task, qdrant_task)
    
    keyword_results = os_response['hits']['hits'] if os_response else []
    
    return keyword_results, qdrant_results


def search_opensearch_sync(
    os_client: OpenSearch,
    index_name: str,
    query: Dict[str, Any],
    search_size: int,
) -> Dict[str, Any]:
    """OpenSearch ê²€ìƒ‰ (ë™ê¸°)"""
    try:
        return os_client.search(
            index=index_name,
            body=query,
            size=search_size,
            request_timeout=10,
            ignore=[404]
        )
    except Exception as e:
        logger.warning(f"âš ï¸ OpenSearch {index_name} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return {'hits': {'hits': []}}


def search_qdrant_sync(
    qdrant_client: QdrantClient,
    collection_name: str,
    query_vector: List[float],
    limit: int,
) -> List[Dict[str, Any]]:
    """Qdrant ê²€ìƒ‰ (ë™ê¸°)"""
    if not query_vector:
        return []
    
    try:
        results = qdrant_client.search(
            collection_name=collection_name,
            query_vector=query_vector,
            limit=limit,
            score_threshold=0.3,
        )
        
        return [
            {
                '_id': str(r.id),
                '_score': r.score,
                '_source': r.payload
            }
            for r in results
        ]
    except Exception as e:
        logger.debug(f"âš ï¸ Qdrant {collection_name} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []


# ==========================================
# ðŸ“Œ 3. ì „ì²´ ì¸ë±ìŠ¤ ë³‘ë ¬ ê²€ìƒ‰ Orchestrator
# ==========================================
def get_cached_config_model():
    """ìºì‹œëœ Config ë° SentenceTransformer ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    # ðŸš¨ ì´ í•¨ìˆ˜ëŠ” íŒŒì¼ ìƒë‹¨ì˜ ì „ì—­ ë³€ìˆ˜ _configì™€ _embedding_modelì„ ì°¸ì¡°í•©ë‹ˆë‹¤.
    global _config, _embedding_model
    if _config is None or _embedding_model is None:
        raise RuntimeError("Worker environment failed to initialize model/config.")
    return _config, _embedding_model

@celery_app.task(
    name='tasks.parallel_hybrid_search_orchestrator',
    bind=True,
)
def parallel_hybrid_search_orchestrator(
    self,
    query: str,
    index_name: str = "*",
    size: int = 10,
    use_vector_search: bool = True
):
    """
    ðŸš€ ì „ì²´ ì¸ë±ìŠ¤ ë³‘ë ¬ ê²€ìƒ‰ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
    
    - Celery Chordë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸ë±ìŠ¤ë³„ Task ë³‘ë ¬ ì‹¤í–‰
    - ëª¨ë“  Task ì™„ë£Œ í›„ RRF ìž¬ê²°í•©
    """
    task_id = self.request.id
    logger.info(f"ðŸš€ [{task_id}] ì „ì²´ ì¸ë±ìŠ¤ ë³‘ë ¬ ê²€ìƒ‰ ì‹œìž‘: query='{query}'")
    
    try:
        # 1. ì„¤ì • ë° ì¿¼ë¦¬ ë¶„ì„
        config, embedding_model = get_cached_config_model()
        
        from rag_query_analyzer.analyzers.main_analyzer import AdvancedRAGQueryAnalyzer
        from rag_query_analyzer.analyzers.demographic_extractor import DemographicExtractor
        
        analyzer = AdvancedRAGQueryAnalyzer(config)
        analysis = analyzer.analyze_query(query)
        
        extractor = DemographicExtractor()
        extracted_entities, requested_size = extractor.extract_with_size(query)
        actual_size = max(1, min(requested_size, 100))
        
        # 2. ìž„ë² ë”© ë²¡í„° ìƒì„±
        query_vector = None
        if use_vector_search:
            query_vector = embedding_model.encode(query).tolist()
        
        # 3. í•„í„° êµ¬ì„±
        filters = build_filters(extracted_entities)
        age_gender_filters = [f for f in filters if is_age_or_gender_filter(f)]
        occupation_filters = [f for f in filters if is_occupation_filter(f)]
        
        has_filters = bool(filters)
        
        if has_filters:
            qdrant_limit = min(500, max(300, actual_size * 10))
            search_size = max(1000, min(actual_size * 20, 5000))
        else:
            qdrant_limit = min(200, max(100, actual_size * 2))
            search_size = actual_size * 2
        
        # 4. ì¸ë±ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        os_client, _ = get_search_clients()
        all_indices = get_all_survey_indices(os_client)
        
        logger.info(f"ðŸ“‹ [{task_id}] ê²€ìƒ‰í•  ì¸ë±ìŠ¤: {len(all_indices)}ê°œ")
        
        # 5. ðŸ”¥ ì¸ë±ìŠ¤ë³„ Task ìƒì„± (Celery Worker Poolì— ë¶„ì‚°)
        search_tasks = []
        
        # welcome_1st
        search_tasks.append(
            search_single_index_task.s(
                query=query,
                index_name="s_welcome_1st",
                query_vector=query_vector,
                filters=age_gender_filters,
                search_size=search_size,
                qdrant_limit=qdrant_limit,
            )
        )
        
        # welcome_2nd
        search_tasks.append(
            search_single_index_task.s(
                query=query,
                index_name="s_welcome_2nd",
                query_vector=query_vector,
                filters=occupation_filters,
                search_size=search_size,
                qdrant_limit=qdrant_limit,
            )
        )
        
        # survey_25_* ì¸ë±ìŠ¤ë“¤
        survey_indices = [idx for idx in all_indices if idx.startswith('survey_25')]
        for survey_index in survey_indices:
            search_tasks.append(
                search_single_index_task.s(
                    query=query,
                    index_name=survey_index,
                    query_vector=query_vector,
                    filters=filters,
                    search_size=search_size,
                    qdrant_limit=qdrant_limit,
                )
            )
        
        # 6. ðŸ”¥ Celery Chord: ë³‘ë ¬ ì‹¤í–‰ + ì½œë°±
        # group()ìœ¼ë¡œ ëª¨ë“  Task ë³‘ë ¬ ì‹¤í–‰ â†’ combine_results_task()ë¡œ ê²°í•©
        job = chord(
            group(search_tasks),
            on_error=handle_chord_error.s(task_id=task_id)  # âœ… ì—ëŸ¬ í•¸ë“¤ëŸ¬
        )(
            combine_results_task.s(
                query=query,
                size=actual_size,
                task_id=task_id,
            )
        )
        final_result = job.get(timeout=120, propagate=True)
        logger.info(f"âœ… [{task_id}] {len(search_tasks)}ê°œ Task ë³‘ë ¬ ì‹¤í–‰ ì‹œìž‘")
        
        return final_result
        
    except Exception as exc:
        logger.error(f"âŒ [{task_id}] ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì‹¤íŒ¨: {exc}", exc_info=True)
        return {
            'status': 'failed',
            'error': str(exc),
            'task_id': task_id,
        }

# âœ… Chord ì—ëŸ¬ í•¸ë“¤ëŸ¬
@celery_app.task(name='tasks.handle_chord_error')
def handle_chord_error(request, exc, traceback, task_id: str):
    """Chord Task ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬"""
    logger.error(f"âŒ Chord Task ì‹¤íŒ¨: task_id={task_id}, error={exc}")
    logger.error(f"Traceback: {traceback}")
    
    # Redisì— ì‹¤íŒ¨ ìƒíƒœ ê¸°ë¡
    redis_client = get_redis_client()
    redis_client.setex(
        f"task:{task_id}:error",
        3600,
        json.dumps({
            'error': str(exc),
            'traceback': str(traceback),
            'timestamp': perf_counter()
        })
    )
    
# ==========================================
# ðŸ“Œ 4. RRF ìž¬ê²°í•© Task (Callback)
# ==========================================

@celery_app.task(
    name='tasks.combine_results',
    bind=True,
)
def combine_results_task(
    self,
    index_results: List[Dict],
    query: str,
    size: int,
    task_id: str,
):
    """
    ðŸ”¥ ëª¨ë“  ì¸ë±ìŠ¤ ê²€ìƒ‰ ì™„ë£Œ í›„ RRF ìž¬ê²°í•©
    
    - Celery Chordì˜ ì½œë°±ìœ¼ë¡œ ì‹¤í–‰
    - user_id ê¸°ì¤€ ê²°í•© + Redis ìºì‹±
    """
    try:
        logger.info(f"ðŸ”„ [{task_id}] RRF ìž¬ê²°í•© ì‹œìž‘: {len(index_results)}ê°œ ì¸ë±ìŠ¤")
        
        # 1. ê° ì¸ë±ìŠ¤ì˜ RRF ê²°ê³¼ ìˆ˜ì§‘
        all_rrf_results = []
        for result in index_results:
            if isinstance(result, dict) and 'rrf_results' in result:
                rrf_results = result['rrf_results']
                index_name = result['index_name']
                
                # ì¸ë±ìŠ¤ ì •ë³´ ì¶”ê°€
                for doc in rrf_results:
                    doc['_index'] = index_name
                
                all_rrf_results.append(rrf_results)
                logger.info(f"  âœ… {index_name}: {len(rrf_results)}ê±´")
        
        # 2. user_id ê¸°ì¤€ ê²°í•©
        combined_results = combine_by_user_id(all_rrf_results)
        
        logger.info(f"  âœ… RRF ìž¬ê²°í•© ì™„ë£Œ: {len(combined_results)}ê±´")
        
        # 3. Redis ìºì‹±
        redis_client = get_redis_client()
        
        cache_results_to_redis(
            redis_client=redis_client,
            task_id=task_id,
            results=combined_results,
            ttl=3600
        )
        
        # 4. ìµœì¢… ê²°ê³¼
        final_hits = combined_results[:size]
        results = format_search_results(final_hits)
        
        return {
            'status': 'completed',
            'task_id': task_id,
            'query': query,
            'total_hits': len(combined_results),
            'max_score': final_hits[0].get('_score', 0.0) if final_hits else 0.0,
            'results': results,
        }
        
    except Exception as exc:
        logger.error(f"âŒ [{task_id}] RRF ìž¬ê²°í•© ì‹¤íŒ¨: {exc}", exc_info=True)
        raise


# ==========================================
# ðŸ“Œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==========================================
def get_search_clients() -> Tuple[OpenSearch, QdrantClient]:
    """OpenSearch & Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (Worker ìºì‹œ ë°˜í™˜ìœ¼ë¡œ ëŒ€ì²´)"""
    # ðŸ’¡ Worker ì‹œìž‘ ì‹œ _os_client, _qdrant_clientê°€ ìƒì„±ë˜ì—ˆë‹¤ê³  ê°€ì •
    global _os_client, _qdrant_client
    
    if _os_client is None:
        # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œì¼œ Taskê°€ ì •ìƒ ì¢…ë£Œë˜ë„ë¡ í•¨
        logger.error("Search clients not initialized. Worker setup failed.")
        # ðŸš¨ ì—¬ê¸°ì„œ RuntimeErrorë¥¼ ë°œìƒì‹œí‚¤ì§€ ì•Šìœ¼ë©´ Task ì‹¤íŒ¨ ë¡œê·¸ê°€ ëª…í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìžˆìŒ
        raise RuntimeError("Search clients not initialized. Worker setup failed.")
    
    return _os_client, _qdrant_client


def get_all_survey_indices(os_client: OpenSearch) -> List[str]:
    """OpenSearch ì¸ë±ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
    try:
        indices_response = os_client.cat.indices(format='json')
        survey_indices = [
            idx['index'] for idx in indices_response
            if idx['index'].startswith('s_welcome') or idx['index'].startswith('survey_25')
        ]
        survey_indices.sort()
        return survey_indices
    except Exception as e:
        logger.error(f"âŒ ì¸ë±ìŠ¤ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return ['s_welcome_1st', 's_welcome_2nd']


def combine_by_user_id(rrf_results_list: List[List[Dict]]) -> List[Dict]:
    """user_id ê¸°ì¤€ RRF ì ìˆ˜ í•©ì‚°"""
    user_map = {}
    
    for rrf_results in rrf_results_list:
        for doc in rrf_results:
            user_id = extract_user_id(doc)
            if user_id:
                if user_id not in user_map:
                    user_map[user_id] = []
                user_map[user_id].append(doc)
    
    # RRF ì ìˆ˜ í•©ì‚°
    final_results = []
    for user_id, docs in user_map.items():
        if len(docs) == 1:
            final_results.append(docs[0])
        else:
            total_score = sum(d.get('_score', 0.0) for d in docs)
            best_doc = max(docs, key=lambda d: d.get('_score', 0.0))
            best_doc['_score'] = total_score
            best_doc['_rrf_details'] = {
                'combined_score': total_score,
                'source_count': len(docs),
                'sources': [d.get('_index', 'unknown') for d in docs]
            }
            final_results.append(best_doc)
    
    final_results.sort(key=lambda d: d.get('_score', 0.0), reverse=True)
    return final_results


def cache_results_to_redis(
    redis_client: StrictRedis,
    task_id: str,
    results: List[Dict],
    ttl: int = 3600
):
    """Redis íŽ˜ì´ì§• ìºì‹œ"""
    id_list_key = f"task:{task_id}:ids"
    user_ids = [extract_user_id(doc) for doc in results if extract_user_id(doc)]
    
    if user_ids:
        redis_client.delete(id_list_key)
        redis_client.rpush(id_list_key, *user_ids)
        redis_client.expire(id_list_key, ttl)
    
    for doc in results:
        user_id = extract_user_id(doc)
        if user_id:
            data_key = f"task:{task_id}:data:{user_id}"
            result_data = {
                'user_id': user_id,
                'score': doc.get('_score', 0.0),
                'timestamp': doc.get('_source', {}).get('timestamp'),
                'qa_pairs': doc.get('_source', {}).get('qa_pairs', [])[:5],
                'index': doc.get('_index', 'unknown'),
            }
            redis_client.setex(
                data_key,
                ttl,
                json.dumps(result_data, ensure_ascii=False)
            )


def extract_user_id(doc: Dict) -> str:
    """ë¬¸ì„œì—ì„œ user_id ì¶”ì¶œ"""
    source = doc.get('_source', {})
    if not source and 'doc' in doc:
        source = doc.get('doc', {}).get('_source', {})
    
    return source.get('user_id') or doc.get('_id', '')


def format_search_results(hits: List[Dict]) -> List[Dict]:
    """ê²€ìƒ‰ ê²°ê³¼ í¬ë§·"""
    results = []
    for doc in hits:
        source = doc.get('_source', {})
        results.append({
            'user_id': extract_user_id(doc),
            'score': doc.get('_score', 0.0),
            'timestamp': source.get('timestamp'),
            'qa_pairs': source.get('qa_pairs', [])[:5],
            'index': doc.get('_index', 'unknown'),
        })
    return results


def build_filters(extracted_entities: Any) -> List[Dict]:
    """í•„í„° êµ¬ì„±"""
    from rag_query_analyzer.models.entities import DemographicType
    
    filters = []
    for demo in extracted_entities.demographics:
        metadata_only = demo.demographic_type in {DemographicType.AGE, DemographicType.GENDER}
        filter_clause = demo.to_opensearch_filter(
            metadata_only=metadata_only,
            include_qa_fallback=True,
        )
        if filter_clause and filter_clause != {"match_all": {}}:
            filters.append(filter_clause)
    
    return filters


def is_age_or_gender_filter(filter_dict: Dict[str, Any]) -> bool:
    """ì—°ë ¹/ì„±ë³„ í•„í„° í™•ì¸"""
    filter_str = json.dumps(filter_dict, ensure_ascii=False)
    age_gender_keywords = [
        "metadata.age_group", "metadata.gender", "birth_year", "ì—°ë ¹", "ë‚˜ì´", "ì„±ë³„"
    ]
    return any(keyword in filter_str for keyword in age_gender_keywords)


def is_occupation_filter(filter_dict: Dict[str, Any]) -> bool:
    """ì§ì—… í•„í„° í™•ì¸"""
    filter_str = json.dumps(filter_dict, ensure_ascii=False)
    occupation_keywords = [
        "metadata.occupation", "occupation", "ì§ì—…", "ì§ë¬´"
    ]
    return any(keyword in filter_str for keyword in occupation_keywords)


def remove_inner_hits(query_dict: Dict[str, Any]) -> Dict[str, Any]:
    """inner_hits ì œê±° (ìž¬ê·€)"""
    import copy
    cleaned = copy.deepcopy(query_dict)
    
    if isinstance(cleaned, dict):
        if 'nested' in cleaned:
            if 'inner_hits' in cleaned['nested']:
                del cleaned['nested']['inner_hits']
            if 'query' in cleaned['nested']:
                cleaned['nested']['query'] = remove_inner_hits(cleaned['nested']['query'])
        
        if 'bool' in cleaned:
            for key in ['must', 'should', 'must_not', 'filter']:
                if key in cleaned['bool']:
                    if isinstance(cleaned['bool'][key], list):
                        cleaned['bool'][key] = [remove_inner_hits(item) for item in cleaned['bool'][key]]
                    else:
                        cleaned['bool'][key] = remove_inner_hits(cleaned['bool'][key])
    
    return cleaned