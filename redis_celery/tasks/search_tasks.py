# redis_celery/tasks/search_tasks.py
"""
Celery ë³‘ë ¬ ê²€ìƒ‰ íƒœìŠ¤í¬ (ì „ì²´ ì¸ë±ìŠ¤ ë³‘ë ¬ ì²˜ë¦¬)
"""
import asyncio
import json
import logging
from typing import Dict, Any, List, Tuple
from time import perf_counter

from redis_celery.celery_app import celery_app
from opensearchpy import OpenSearch
from qdrant_client import QdrantClient
from connectors.data_fetcher import DataFetcher
from connectors.hybrid_searcher import calculate_rrf_score
from rag_query_analyzer.config import get_config
from sentence_transformers import SentenceTransformer
import redis
import os

logger = logging.getLogger(__name__)

# ì „ì—­ ìºì‹œ
_embedding_model = None
_config = None


def get_clients():
    """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    os_client = OpenSearch(
        hosts=[{
            'host': os.getenv('OPENSEARCH_HOST', 'localhost'),
            'port': int(os.getenv('OPENSEARCH_PORT', '9200'))
        }],
        http_auth=(
            os.getenv('OPENSEARCH_USER', 'admin'),
            os.getenv('OPENSEARCH_PASSWORD', 'admin')
        ),
        use_ssl=False,
        verify_certs=False,
        timeout=30,
    )
    
    qdrant_client = QdrantClient(
        host=os.getenv('QDRANT_HOST', 'localhost'),
        port=int(os.getenv('QDRANT_PORT', '6333')),
        timeout=30
    )
    
    redis_client = redis.StrictRedis(
        host=os.getenv('REDIS_HOST', 'localhost'),
        port=int(os.getenv('REDIS_PORT', '6379')),
        db=int(os.getenv('CACHE_DB', '2')),
        decode_responses=True
    )
    
    return os_client, qdrant_client, redis_client


def get_embedding_model():
    """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
    global _embedding_model, _config
    
    if _embedding_model is None:
        _config = get_config()
        _embedding_model = SentenceTransformer(_config.EMBEDDING_MODEL)
        logger.info("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    return _embedding_model, _config


@celery_app.task(
    name='tasks.search_tasks.parallel_hybrid_search_all',
    bind=True,
    max_retries=3,
    default_retry_delay=5,
)
def parallel_hybrid_search_all(
    self,
    query: str,
    index_name: str = "*",
    size: int = 10,
    use_vector_search: bool = True
):
    """
    ğŸš€ ì „ì²´ ì¸ë±ìŠ¤ ë³‘ë ¬ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    
    - welcome_1st, welcome_2nd, survey_25_* (30ê°œ) ë™ì‹œ ê²€ìƒ‰
    - ê° ì¸ë±ìŠ¤ë§ˆë‹¤ OpenSearch + Qdrant ë³‘ë ¬ ì‹¤í–‰
    - RRF ê²°í•© í›„ Redis ìºì‹±
    """
    task_id = self.request.id
    logger.info(f"ğŸš€ [Task {task_id}] ì „ì²´ ì¸ë±ìŠ¤ ë³‘ë ¬ ê²€ìƒ‰ ì‹œì‘: query='{query}'")
    
    try:
        os_client, qdrant_client, redis_client = get_clients()
        embedding_model, config = get_embedding_model()
        
        data_fetcher = DataFetcher(
            opensearch_client=os_client,
            qdrant_client=qdrant_client,
            async_opensearch_client=None
        )
        
        # ë¹„ë™ê¸° ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            result = loop.run_until_complete(
                execute_all_indices_parallel_search(
                    query=query,
                    index_name=index_name,
                    size=size,
                    use_vector_search=use_vector_search,
                    data_fetcher=data_fetcher,
                    embedding_model=embedding_model,
                    config=config,
                    redis_client=redis_client,
                    task_id=task_id,
                )
            )
        finally:
            loop.close()
        
        logger.info(f"âœ… [Task {task_id}] ì „ì²´ ì¸ë±ìŠ¤ ë³‘ë ¬ ê²€ìƒ‰ ì™„ë£Œ: {result['total_hits']}ê±´")
        return result
        
    except Exception as exc:
        logger.error(f"âŒ [Task {task_id}] ê²€ìƒ‰ ì‹¤íŒ¨: {exc}", exc_info=True)
        
        try:
            raise self.retry(exc=exc, countdown=5)
        except self.MaxRetriesExceededError:
            return {
                'status': 'failed',
                'error': str(exc),
                'error_type': type(exc).__name__,
                'query': query,
                'task_id': task_id,
            }


async def execute_all_indices_parallel_search(
    query: str,
    index_name: str,
    size: int,
    use_vector_search: bool,
    data_fetcher: DataFetcher,
    embedding_model: SentenceTransformer,
    config: Any,
    redis_client: redis.StrictRedis,
    task_id: str,
) -> Dict[str, Any]:
    """
    ì „ì²´ ì¸ë±ìŠ¤ ë³‘ë ¬ ê²€ìƒ‰ í•µì‹¬ ë¡œì§
    """
    from rag_query_analyzer.analyzers.main_analyzer import AdvancedRAGQueryAnalyzer
    from rag_query_analyzer.analyzers.demographic_extractor import DemographicExtractor
    
    timings = {}
    overall_start = perf_counter()
    
    # 1. ì¿¼ë¦¬ ë¶„ì„
    analyzer = AdvancedRAGQueryAnalyzer(config)
    analysis = analyzer.analyze_query(query)
    
    extractor = DemographicExtractor()
    extracted_entities, requested_size = extractor.extract_with_size(query)
    
    actual_size = max(1, min(requested_size, 100))
    
    # 2. ì„ë² ë”© ë²¡í„° ìƒì„±
    query_vector = None
    if use_vector_search:
        query_vector = embedding_model.encode(query).tolist()
    
    # 3. í•„í„° êµ¬ì„±
    filters = build_filters(extracted_entities)
    age_gender_filters = [f for f in filters if is_age_or_gender_filter(f)]
    occupation_filters = [f for f in filters if is_occupation_filter(f)]
    
    has_filters = bool(filters)
    
    if has_filters:
        qdrant_limit = min(500, max(300, actual_size * 10))
        search_size = max(1000, min(actual_size * 20, 5000))
    else:
        qdrant_limit = min(200, max(100, actual_size * 2))
        search_size = actual_size * 2
    
    logger.info(f"ğŸ” [Task {task_id}] ë³‘ë ¬ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°: size={search_size}, qdrant_limit={qdrant_limit}")
    
    # ğŸš€ 4. ì „ì²´ ì¸ë±ìŠ¤ ë³‘ë ¬ ê²€ìƒ‰
    parallel_start = perf_counter()
    
    # 4-1. ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    all_indices = get_all_survey_indices(data_fetcher.os_client)
    logger.info(f"ğŸ“‹ [Task {task_id}] ê²€ìƒ‰í•  ì¸ë±ìŠ¤: {len(all_indices)}ê°œ")
    
    # 4-2. ì¸ë±ìŠ¤ë³„ ê²€ìƒ‰ íƒœìŠ¤í¬ ìƒì„±
    search_tasks = []
    
    # welcome_1st (ì—°ë ¹/ì„±ë³„ í•„í„°)
    search_tasks.append(
        search_index_parallel(
            data_fetcher=data_fetcher,
            index_name="s_welcome_1st",
            search_size=search_size,
            qdrant_limit=qdrant_limit,
            query_vector=query_vector,
            filters=age_gender_filters,
            task_id=task_id,
        )
    )
    
    # welcome_2nd (ì§ì—… í•„í„°)
    search_tasks.append(
        search_index_parallel(
            data_fetcher=data_fetcher,
            index_name="s_welcome_2nd",
            search_size=search_size,
            qdrant_limit=qdrant_limit,
            query_vector=query_vector,
            filters=occupation_filters,
            task_id=task_id,
        )
    )
    
    # survey_25_* ì¸ë±ìŠ¤ë“¤ (30ê°œ)
    survey_indices = [idx for idx in all_indices if idx.startswith('survey_25')]
    for survey_index in survey_indices:
        search_tasks.append(
            search_index_parallel(
                data_fetcher=data_fetcher,
                index_name=survey_index,
                search_size=search_size,
                qdrant_limit=qdrant_limit,
                query_vector=query_vector,
                filters=filters,  # ì „ì²´ í•„í„° ì ìš©
                task_id=task_id,
            )
        )
    
    # ğŸ”¥ 4-3. ëª¨ë“  ì¸ë±ìŠ¤ ë™ì‹œ ê²€ìƒ‰ (32ê°œ ë³‘ë ¬ ì‹¤í–‰)
    all_results = await asyncio.gather(*search_tasks, return_exceptions=True)
    
    timings['parallel_search_ms'] = (perf_counter() - parallel_start) * 1000
    logger.info(f"âš¡ [Task {task_id}] ì „ì²´ ì¸ë±ìŠ¤ ë³‘ë ¬ ê²€ìƒ‰ ì™„ë£Œ: {timings['parallel_search_ms']:.2f}ms")
    
    # 5. ê²°ê³¼ ìˆ˜ì§‘ ë° RRF ê²°í•©
    rrf_start = perf_counter()
    
    all_rrf_results = []
    for i, result in enumerate(all_results):
        if isinstance(result, Exception):
            logger.warning(f"âš ï¸ ì¸ë±ìŠ¤ {i} ê²€ìƒ‰ ì‹¤íŒ¨: {result}")
            continue
        
        keyword_results, vector_results, index_name = result
        
        # ì¸ë±ìŠ¤ë³„ RRF
        if vector_results:
            index_rrf = calculate_rrf_score(keyword_results, vector_results, k=60)
        else:
            index_rrf = keyword_results
        
        # ì¸ë±ìŠ¤ ì •ë³´ ì¶”ê°€
        for doc in index_rrf:
            doc['_index'] = index_name
        
        all_rrf_results.append(index_rrf)
        logger.info(f"  âœ… {index_name}: {len(index_rrf)}ê±´")
    
    # user_id ê¸°ì¤€ ê²°í•©
    combined_results = combine_by_user_id(all_rrf_results)
    
    timings['rrf_combination_ms'] = (perf_counter() - rrf_start) * 1000
    logger.info(f"  âœ… RRF ê²°í•© ì™„ë£Œ: {len(combined_results)}ê±´")
    
    # 6. Redis ìºì‹±
    cache_start = perf_counter()
    cache_results_to_redis(
        redis_client=redis_client,
        task_id=task_id,
        results=combined_results,
        ttl=3600
    )
    timings['redis_cache_ms'] = (perf_counter() - cache_start) * 1000
    
    # 7. ìµœì¢… ê²°ê³¼
    final_hits = combined_results[:actual_size]
    results = format_search_results(final_hits)
    
    total_duration_ms = (perf_counter() - overall_start) * 1000
    timings['total_ms'] = total_duration_ms
    
    return {
        'status': 'completed',
        'task_id': task_id,
        'query': query,
        'total_hits': len(combined_results),
        'max_score': final_hits[0].get('_score', 0.0) if final_hits else 0.0,
        'results': results,
        'query_analysis': {
            'intent': analysis.intent,
            'must_terms': analysis.must_terms,
            'should_terms': analysis.should_terms,
            'extracted_entities': extracted_entities.to_dict(),
            'filters': filters,
        },
        'search_summary': {
            'total_indices_searched': len(all_indices),
            'welcome_1st': len([r for r in combined_results if r.get('_index') == 's_welcome_1st']),
            'welcome_2nd': len([r for r in combined_results if r.get('_index') == 's_welcome_2nd']),
            'survey_indices': len([r for r in combined_results if r.get('_index', '').startswith('survey_25')]),
        },
        'timings_ms': timings,
        'took_ms': int(total_duration_ms),
    }


def get_all_survey_indices(os_client: OpenSearch) -> List[str]:
    """
    OpenSearchì—ì„œ ëª¨ë“  survey ì¸ë±ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    
    Returns:
        ['s_welcome_1st', 's_welcome_2nd', 'survey_25_01', 'survey_25_02', ...]
    """
    try:
        # cat.indices APIë¡œ ëª¨ë“  ì¸ë±ìŠ¤ ì¡°íšŒ
        indices_response = os_client.cat.indices(format='json')
        
        # survey ê´€ë ¨ ì¸ë±ìŠ¤ë§Œ í•„í„°ë§
        survey_indices = [
            idx['index'] for idx in indices_response
            if idx['index'].startswith('s_welcome') or idx['index'].startswith('survey_25')
        ]
        
        survey_indices.sort()  # ì •ë ¬
        logger.info(f"ğŸ“‹ ë°œê²¬ëœ ì¸ë±ìŠ¤: {survey_indices}")
        
        return survey_indices
        
    except Exception as e:
        logger.error(f"âŒ ì¸ë±ìŠ¤ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì¸ë±ìŠ¤ë§Œ ë°˜í™˜
        return ['s_welcome_1st', 's_welcome_2nd']


async def search_index_parallel(
    data_fetcher: DataFetcher,
    index_name: str,
    search_size: int,
    qdrant_limit: int,
    query_vector: List[float],
    filters: List[Dict[str, Any]],
    task_id: str,
) -> Tuple[List[Dict], List[Dict], str]:
    """
    ë‹¨ì¼ ì¸ë±ìŠ¤ ë³‘ë ¬ ê²€ìƒ‰ (OpenSearch + Qdrant ë™ì‹œ)
    
    Returns:
        (keyword_results, vector_results, index_name)
    """
    # ì¿¼ë¦¬ êµ¬ì„±
    query = {
        'query': {'match_all': {}},
        'size': search_size,
        '_source': ['user_id', 'metadata', 'qa_pairs', 'timestamp']
    }
    
    if filters:
        # inner_hits ì œê±° (ì„±ëŠ¥ ìµœì í™”)
        cleaned_filters = [remove_inner_hits(f) for f in filters]
        query['query'] = {'bool': {'must': cleaned_filters}}
    
    # ğŸ”¥ OpenSearch + Qdrant ë™ì‹œ ì‹¤í–‰
    opensearch_task = asyncio.to_thread(
        search_opensearch_sync,
        os_client=data_fetcher.os_client,
        index_name=index_name,
        query=query,
        search_size=search_size,
    )
    
    qdrant_task = asyncio.to_thread(
        search_qdrant_sync,
        qdrant_client=data_fetcher.qdrant_client,
        collection_name=index_name,
        query_vector=query_vector,
        limit=qdrant_limit,
    )
    
    try:
        os_response, qdrant_results = await asyncio.gather(opensearch_task, qdrant_task)
        
        keyword_results = os_response['hits']['hits'] if os_response else []
        
        return keyword_results, qdrant_results, index_name
        
    except Exception as e:
        logger.warning(f"âš ï¸ {index_name} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return [], [], index_name


def search_opensearch_sync(
    os_client: OpenSearch,
    index_name: str,
    query: Dict[str, Any],
    search_size: int,
) -> Dict[str, Any]:
    """OpenSearch ê²€ìƒ‰ (ë™ê¸°)"""
    try:
        return os_client.search(
            index=index_name,
            body=query,
            size=search_size,
            request_timeout=10,
            ignore=[404]  # ì¸ë±ìŠ¤ ì—†ìœ¼ë©´ ë¬´ì‹œ
        )
    except Exception as e:
        logger.warning(f"âš ï¸ OpenSearch {index_name} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return {'hits': {'hits': []}}


def search_qdrant_sync(
    qdrant_client: QdrantClient,
    collection_name: str,
    query_vector: List[float],
    limit: int,
) -> List[Dict[str, Any]]:
    """Qdrant ê²€ìƒ‰ (ë™ê¸°)"""
    if not query_vector:
        return []
    
    try:
        results = qdrant_client.search(
            collection_name=collection_name,
            query_vector=query_vector,
            limit=limit,
            score_threshold=0.3,
        )
        
        return [
            {
                '_id': str(r.id),
                '_score': r.score,
                '_source': r.payload
            }
            for r in results
        ]
    except Exception as e:
        logger.debug(f"âš ï¸ Qdrant {collection_name} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []


def remove_inner_hits(query_dict: Dict[str, Any]) -> Dict[str, Any]:
    """ì¬ê·€ì ìœ¼ë¡œ inner_hits ì œê±° (ì„±ëŠ¥ ìµœì í™”)"""
    import copy
    cleaned = copy.deepcopy(query_dict)
    
    if isinstance(cleaned, dict):
        if 'nested' in cleaned:
            if 'inner_hits' in cleaned['nested']:
                del cleaned['nested']['inner_hits']
            if 'query' in cleaned['nested']:
                cleaned['nested']['query'] = remove_inner_hits(cleaned['nested']['query'])
        
        if 'bool' in cleaned:
            for key in ['must', 'should', 'must_not', 'filter']:
                if key in cleaned['bool']:
                    if isinstance(cleaned['bool'][key], list):
                        cleaned['bool'][key] = [remove_inner_hits(item) for item in cleaned['bool'][key]]
                    else:
                        cleaned['bool'][key] = remove_inner_hits(cleaned['bool'][key])
    
    return cleaned


def combine_by_user_id(rrf_results_list: List[List[Dict]]) -> List[Dict]:
    """user_id ê¸°ì¤€ìœ¼ë¡œ ê²°ê³¼ ê²°í•©"""
    user_map = {}
    
    for rrf_results in rrf_results_list:
        for doc in rrf_results:
            user_id = extract_user_id(doc)
            if user_id:
                if user_id not in user_map:
                    user_map[user_id] = []
                user_map[user_id].append(doc)
    
    # RRF ì ìˆ˜ í•©ì‚°
    final_results = []
    for user_id, docs in user_map.items():
        if len(docs) == 1:
            final_results.append(docs[0])
        else:
            total_score = sum(d.get('_score', 0.0) for d in docs)
            best_doc = max(docs, key=lambda d: d.get('_score', 0.0))
            best_doc['_score'] = total_score
            best_doc['_rrf_details'] = {
                'combined_score': total_score,
                'source_count': len(docs),
                'sources': [d.get('_index', 'unknown') for d in docs]
            }
            final_results.append(best_doc)
    
    final_results.sort(key=lambda d: d.get('_score', 0.0), reverse=True)
    return final_results


def cache_results_to_redis(
    redis_client: redis.StrictRedis,
    task_id: str,
    results: List[Dict],
    ttl: int = 3600
):
    """Redisì— ê²€ìƒ‰ ê²°ê³¼ ìºì‹±"""
    # 1. user_id ë¦¬ìŠ¤íŠ¸ ì €ì¥
    id_list_key = f"task:{task_id}:ids"
    user_ids = [extract_user_id(doc) for doc in results if extract_user_id(doc)]
    
    if user_ids:
        redis_client.delete(id_list_key)
        redis_client.rpush(id_list_key, *user_ids)
        redis_client.expire(id_list_key, ttl)
    
    # 2. ê° ë¬¸ì„œ ìƒì„¸ ì •ë³´ ì €ì¥
    for doc in results:
        user_id = extract_user_id(doc)
        if user_id:
            data_key = f"task:{task_id}:data:{user_id}"
            result_data = {
                'user_id': user_id,
                'score': doc.get('_score', 0.0),
                'timestamp': doc.get('_source', {}).get('timestamp'),
                'qa_pairs': doc.get('_source', {}).get('qa_pairs', [])[:5],
                'index': doc.get('_index', 'unknown'),
            }
            redis_client.setex(
                data_key,
                ttl,
                json.dumps(result_data, ensure_ascii=False)
            )
    
    logger.info(f"âœ… Redis ìºì‹± ì™„ë£Œ: {len(user_ids)}ê±´")


def extract_user_id(doc: Dict) -> str:
    """ë¬¸ì„œì—ì„œ user_id ì¶”ì¶œ"""
    source = doc.get('_source', {})
    if not source and 'doc' in doc:
        source = doc.get('doc', {}).get('_source', {})
    
    return source.get('user_id') or doc.get('_id', '')


def format_search_results(hits: List[Dict]) -> List[Dict]:
    """ê²€ìƒ‰ ê²°ê³¼ í¬ë§·"""
    results = []
    for doc in hits:
        source = doc.get('_source', {})
        results.append({
            'user_id': extract_user_id(doc),
            'score': doc.get('_score', 0.0),
            'timestamp': source.get('timestamp'),
            'qa_pairs': source.get('qa_pairs', [])[:5],
            'index': doc.get('_index', 'unknown'),
        })
    return results


def build_filters(extracted_entities: Any) -> List[Dict]:
    """í•„í„° êµ¬ì„±"""
    from rag_query_analyzer.models.entities import DemographicType
    
    filters = []
    for demo in extracted_entities.demographics:
        metadata_only = demo.demographic_type in {DemographicType.AGE, DemographicType.GENDER}
        filter_clause = demo.to_opensearch_filter(
            metadata_only=metadata_only,
            include_qa_fallback=True,
        )
        if filter_clause and filter_clause != {"match_all": {}}:
            filters.append(filter_clause)
    
    return filters


def is_age_or_gender_filter(filter_dict: Dict[str, Any]) -> bool:
    """ì—°ë ¹/ì„±ë³„ í•„í„° ì—¬ë¶€ í™•ì¸"""
    import json
    try:
        filter_str = json.dumps(filter_dict, ensure_ascii=False)
    except Exception:
        filter_str = str(filter_dict)
    
    age_gender_keywords = [
        "metadata.age_group", "metadata.gender", "birth_year", "ì—°ë ¹", "ë‚˜ì´", "ì„±ë³„"
    ]
    return any(keyword in filter_str for keyword in age_gender_keywords)


def is_occupation_filter(filter_dict: Dict[str, Any]) -> bool:
    """ì§ì—… í•„í„° ì—¬ë¶€ í™•ì¸"""
    import json
    try:
        filter_str = json.dumps(filter_dict, ensure_ascii=False)
    except Exception:
        filter_str = str(filter_dict)
    
    occupation_keywords = [
        "metadata.occupation", "occupation", "ì§ì—…", "ì§ë¬´"
    ]
    return any(keyword in filter_str for keyword in occupation_keywords)