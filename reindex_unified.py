#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""s_welcome_1stì™€ s_welcome_2ndë¥¼ user_id ê¸°ì¤€ìœ¼ë¡œ í†µí•©í•˜ì—¬ ìƒˆ ì¸ë±ìŠ¤ ìƒì„±"""
import json
import sys
from collections import defaultdict
from opensearchpy import OpenSearch
from opensearchpy.helpers import bulk
from rag_query_analyzer.config import get_config

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

config = get_config()

client = OpenSearch(
    hosts=[{'host': config.OPENSEARCH_HOST, 'port': config.OPENSEARCH_PORT}],
    http_auth=(config.OPENSEARCH_USER, config.OPENSEARCH_PASSWORD),
    use_ssl=config.OPENSEARCH_USE_SSL,
    verify_certs=config.OPENSEARCH_VERIFY_CERTS,
    ssl_assert_hostname=config.OPENSEARCH_SSL_ASSERT_HOSTNAME,
    ssl_show_warn=False,
    timeout=60
)

NEW_INDEX = "welcome_unified"

print("=" * 80)
print("s_welcome_1st + s_welcome_2nd í†µí•© ì¬ìƒ‰ì¸")
print("=" * 80)

# 1. ìƒˆ ì¸ë±ìŠ¤ ìƒì„± (ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ì‚­ì œ)
print(f"\n[1] ìƒˆ ì¸ë±ìŠ¤ ìƒì„±: {NEW_INDEX}")
print("-" * 80)

if client.indices.exists(index=NEW_INDEX):
    print(f"ê¸°ì¡´ ì¸ë±ìŠ¤ {NEW_INDEX} ì‚­ì œ...")
    client.indices.delete(index=NEW_INDEX)

# s_welcome_2ndì˜ ë§¤í•‘ ê°€ì ¸ì˜¤ê¸°
mapping_2nd = client.indices.get_mapping(index="s_welcome_2nd")
mapping = mapping_2nd["s_welcome_2nd"]["mappings"]

client.indices.create(index=NEW_INDEX, body={"mappings": mapping})
print(f"âœ… ì¸ë±ìŠ¤ {NEW_INDEX} ìƒì„± ì™„ë£Œ")

# 2. s_welcome_1st ë°ì´í„° ë¡œë“œ (metadata ì†ŒìŠ¤)
print(f"\n[2] s_welcome_1st ë°ì´í„° ë¡œë“œ (metadata ì†ŒìŠ¤)")
print("-" * 80)

user_metadata = {}
scroll_size = 1000

response = client.search(
    index="s_welcome_1st",
    scroll="5m",
    size=scroll_size,
    body={
        "query": {"match_all": {}},
        "_source": ["user_id", "metadata", "qa_pairs"]
    }
)

scroll_id = response['_scroll_id']
total_1st = response['hits']['total']['value']
processed_1st = 0

print(f"ì´ {total_1st}ê±´ ë¡œë“œ ì¤‘...")

while True:
    hits = response['hits']['hits']
    if not hits:
        break

    for hit in hits:
        source = hit['_source']
        user_id = source.get('user_id')
        if user_id:
            user_metadata[user_id] = {
                'metadata': source.get('metadata', {}),
                'qa_pairs_1st': source.get('qa_pairs', [])
            }

    processed_1st += len(hits)
    print(f"  ì§„í–‰: {processed_1st}/{total_1st} ({processed_1st * 100 // total_1st}%)")

    response = client.scroll(scroll_id=scroll_id, scroll="5m")

client.clear_scroll(scroll_id=scroll_id)
print(f"âœ… s_welcome_1st ë¡œë“œ ì™„ë£Œ: {len(user_metadata)}ëª…")

# 3. s_welcome_2ndì™€ í†µí•©í•˜ì—¬ ìƒˆ ì¸ë±ìŠ¤ì— ì €ì¥
print(f"\n[3] s_welcome_2ndì™€ í†µí•©í•˜ì—¬ {NEW_INDEX}ì— ì €ì¥")
print("-" * 80)

response = client.search(
    index="s_welcome_2nd",
    scroll="5m",
    size=scroll_size,
    body={
        "query": {"match_all": {}},
        "_source": True
    }
)

scroll_id = response['_scroll_id']
total_2nd = response['hits']['total']['value']
processed_2nd = 0
unified_docs = []
batch_size = 500

print(f"ì´ {total_2nd}ê±´ í†µí•© ì¤‘...")

matched_count = 0
unmatched_count = 0

while True:
    hits = response['hits']['hits']
    if not hits:
        break

    for hit in hits:
        source = hit['_source']
        user_id = source.get('user_id')

        if user_id and user_id in user_metadata:
            # ë§¤ì¹­ë¨: metadataì™€ qa_pairs í†µí•©
            matched_count += 1
            unified_doc = {
                '_index': NEW_INDEX,
                '_id': hit['_id'],
                '_source': {
                    **source,
                    'metadata': user_metadata[user_id]['metadata'],  # 1stì˜ metadata ì‚¬ìš©
                    'qa_pairs': user_metadata[user_id]['qa_pairs_1st'] + source.get('qa_pairs', [])  # í†µí•©
                }
            }
        else:
            # ë§¤ì¹­ ì•ˆë¨: 2nd ë°ì´í„°ë§Œ ì‚¬ìš©
            unmatched_count += 1
            unified_doc = {
                '_index': NEW_INDEX,
                '_id': hit['_id'],
                '_source': source
            }

        unified_docs.append(unified_doc)

        # ë°°ì¹˜ ì €ì¥
        if len(unified_docs) >= batch_size:
            success, errors = bulk(client, unified_docs, raise_on_error=False)
            if errors:
                print(f"  âš ï¸ ë°°ì¹˜ ì €ì¥ ì¤‘ {len(errors)}ê°œ ì—ëŸ¬")
            unified_docs = []

    processed_2nd += len(hits)
    print(f"  ì§„í–‰: {processed_2nd}/{total_2nd} ({processed_2nd * 100 // total_2nd}%) - ë§¤ì¹­: {matched_count}, ë¯¸ë§¤ì¹­: {unmatched_count}")

    response = client.scroll(scroll_id=scroll_id, scroll="5m")

# ë‚¨ì€ ë¬¸ì„œ ì €ì¥
if unified_docs:
    success, errors = bulk(client, unified_docs, raise_on_error=False)
    if errors:
        print(f"  âš ï¸ ë§ˆì§€ë§‰ ë°°ì¹˜ ì €ì¥ ì¤‘ {len(errors)}ê°œ ì—ëŸ¬")

client.clear_scroll(scroll_id=scroll_id)
print(f"\nâœ… í†µí•© ì™„ë£Œ!")
print(f"  - ë§¤ì¹­ëœ ì‚¬ìš©ì: {matched_count}ëª…")
print(f"  - ë¯¸ë§¤ì¹­ ì‚¬ìš©ì: {unmatched_count}ëª…")

# 4. ê²€ì¦: 30ëŒ€ ë‚¨ì„± + ì°¨ëŸ‰ í…ŒìŠ¤íŠ¸
print(f"\n[4] ê²€ì¦: 30ëŒ€ ë‚¨ì„± + ì°¨ëŸ‰='ìˆë‹¤' í…ŒìŠ¤íŠ¸")
print("-" * 80)

test_query = {
    "size": 5,
    "query": {
        "bool": {
            "filter": [
                {
                    "bool": {
                        "should": [
                            {"term": {"metadata.gender.keyword": "M"}},
                            {"match": {"metadata.gender": "ë‚¨ì„±"}}
                        ],
                        "minimum_should_match": 1
                    }
                },
                {
                    "bool": {
                        "should": [
                            {"term": {"metadata.age_group.keyword": "30s"}},
                            {"match": {"metadata.age_group": "30ëŒ€"}}
                        ],
                        "minimum_should_match": 1
                    }
                },
                {
                    "nested": {
                        "path": "qa_pairs",
                        "query": {
                            "bool": {
                                "must": [
                                    {"match": {"qa_pairs.q_text": "ë³´ìœ ì°¨ëŸ‰ì—¬ë¶€"}},
                                    {"match": {"qa_pairs.answer": "ìˆë‹¤"}}
                                ]
                            }
                        }
                    }
                }
            ]
        }
    },
    "_source": ["user_id", "metadata.gender", "metadata.age_group"]
}

response = client.search(index=NEW_INDEX, body=test_query)
test_count = response['hits']['total']['value']

print(f"ê²°ê³¼: {test_count}ê±´")

if test_count > 0:
    print(f"\nğŸ‰ ì„±ê³µ! í†µí•© ì¸ë±ìŠ¤ê°€ ì œëŒ€ë¡œ ì‘ë™í•©ë‹ˆë‹¤!")
    print(f"\nìƒ˜í”Œ user_id:")
    for hit in response['hits']['hits'][:5]:
        user_id = hit['_source'].get('user_id')
        gender = hit['_source'].get('metadata', {}).get('gender')
        age_group = hit['_source'].get('metadata', {}).get('age_group')
        print(f"  - {user_id} ({gender}, {age_group})")

    print(f"\në‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì •ì„ ë³€ê²½í•˜ì„¸ìš”:")
    print(f"  WELCOME_INDEX='{NEW_INDEX}' (í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” .env íŒŒì¼)")
else:
    print(f"âŒ ì—¬ì „íˆ 0ê±´ì…ë‹ˆë‹¤.")
    print(f"   â†’ 1stì™€ 2ndì˜ user_idê°€ ê²¹ì¹˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

print("\n" + "=" * 80)
print("ì¬ìƒ‰ì¸ ì™„ë£Œ!")
print("=" * 80)
