"""í´ë¼ìš°ë“œ ë°ì´í„° í˜ì²˜ - OpenSearch ë° Qdrantì—ì„œ ë°ì´í„° ì¡°íšŒ"""
import asyncio
import logging
from typing import List, Dict, Any, Optional
from opensearchpy import OpenSearch, AsyncOpenSearch

logger = logging.getLogger(__name__)


class DataFetcher:
    """
    í†µí•© ë°ì´í„° í˜ì²˜

    OpenSearch, Qdrant ë“± ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ëŠ” í†µí•© ì¸í„°í˜ì´ìŠ¤
    """

    def __init__(
        self,
        opensearch_client: OpenSearch = None,
        qdrant_client=None,
        async_opensearch_client: Optional[AsyncOpenSearch] = None,
    ):
        """
        Args:
            opensearch_client: OpenSearch í´ë¼ì´ì–¸íŠ¸
            qdrant_client: Qdrant í´ë¼ì´ì–¸íŠ¸ (ì„ íƒ)
            async_opensearch_client: ë¹„ë™ê¸° OpenSearch í´ë¼ì´ì–¸íŠ¸ (ì„ íƒ)
        """
        self.os_client = opensearch_client
        self.os_async_client = async_opensearch_client
        self.qdrant_client = qdrant_client

    def search_opensearch(
        self,
        index_name: str,
        query: Dict[str, Any],
        size: int = 10,
        source_filter: Optional[Dict[str, Any]] = None,
        request_timeout: Optional[int] = None,
    ) -> Dict[str, Any]:
        """
        OpenSearchì—ì„œ ê²€ìƒ‰

        Args:
            index_name: ì¸ë±ìŠ¤ ì´ë¦„
            query: OpenSearch ì¿¼ë¦¬ DSL
            size: ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜
            source_filter: _source í•„í„°ë§ (ì˜ˆ: {"includes": ["user_id", "metadata"], "excludes": ["qa_pairs"]})

        Returns:
            ê²€ìƒ‰ ê²°ê³¼
        """
        if not self.os_client:
            raise ValueError("OpenSearch í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        try:
            # ì¿¼ë¦¬ ë””ë²„ê¹… (DEBUG ë ˆë²¨ë¡œ ì¶•ì†Œ)
            import json
            logger.debug(f"ğŸ” OpenSearch ì¿¼ë¦¬:\n{json.dumps(query, indent=2, ensure_ascii=False)}")

            # _source í•„í„°ë§ ì¶”ê°€
            search_body = query.copy()
            if source_filter:
                search_body["_source"] = source_filter
                logger.debug(f"  ğŸ“‹ _source í•„í„°ë§ ì ìš©: {source_filter}")

            response = self.os_client.search(
                index=index_name,
                body=search_body,
                size=size,
                request_timeout=request_timeout
            )

            logger.info(f"âœ… OpenSearch ê²€ìƒ‰ ì™„ë£Œ: {response['hits']['total']['value']}ê±´")
            return response

        except Exception as e:
            logger.error(f"âŒ OpenSearch ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            raise

    async def search_opensearch_async(
        self,
        index_name: str,
        query: Dict[str, Any],
        size: int = 10,
        source_filter: Optional[Dict[str, Any]] = None,
        request_timeout: Optional[int] = None,
    ) -> Dict[str, Any]:
        """OpenSearch ë¹„ë™ê¸° ê²€ìƒ‰"""
        if not self.os_async_client:
            raise ValueError("Async OpenSearch í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        try:
            import json
            logger.debug(f"ğŸ” [async] OpenSearch ì¿¼ë¦¬:\n{json.dumps(query, indent=2, ensure_ascii=False)}")

            search_body = query.copy()
            if source_filter:
                search_body["_source"] = source_filter
                logger.debug(f"  ğŸ“‹ _source í•„í„°ë§ ì ìš© (async): {source_filter}")

            response = await self.os_async_client.search(
                index=index_name,
                body=search_body,
                size=size,
                request_timeout=request_timeout
            )

            hits_total = response.get('hits', {}).get('total', {}).get('value', 0)
            logger.info(f"âœ… [async] OpenSearch ê²€ìƒ‰ ì™„ë£Œ: {hits_total}ê±´")
            return response

        except Exception as e:
            logger.error(f"âŒ [async] OpenSearch ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            raise

    async def get_document_by_id_async(
        self,
        index_name: str,
        doc_id: str,
        **kwargs
    ) -> Optional[Dict[str, Any]]:
        """IDë¡œ ë¬¸ì„œ ë¹„ë™ê¸° ì¡°íšŒ"""
        if not self.os_async_client:
            raise ValueError("Async OpenSearch í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        try:
            response = await self.os_async_client.get(
                index=index_name,
                id=doc_id,
                **kwargs
            )
            if response.get('found'):
                return response.get('_source')
            return None
        except Exception as e:
            logger.warning(f"âš ï¸ [async] ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨ (ID: {doc_id}): {e}")
            return None

    async def multi_get_documents_async(
        self,
        index_name: str,
        doc_ids: List[str],
        batch_size: Optional[int] = None,
        request_timeout: int = 180,  # ëŒ€ëŸ‰ ë°ì´í„° ì¡°íšŒ ëŒ€ì‘ (ì „ì²´ ë°ì´í„° ì•½ 35000ê°œ)
        source_fields: Optional[List[str]] = None,
    ) -> List[Dict[str, Any]]:
        """ë¹„ë™ê¸° ë¬¸ì„œ ì¼ê´„ ì¡°íšŒ (ë°°ì¹˜) -> raw docs ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        if not doc_ids:
            return []

        if batch_size is None:
            if len(doc_ids) <= 100:
                batch_size = 50
            elif len(doc_ids) <= 500:
                batch_size = 100
            else:
                batch_size = 200

        batches = [
            doc_ids[i:i + batch_size]
            for i in range(0, len(doc_ids), batch_size)
        ]

        logger.info(f"ğŸ“¦ [async] {index_name} ë°°ì¹˜ ì¡°íšŒ: {len(doc_ids)}ê±´ â†’ {len(batches)}ê°œ ë°°ì¹˜ (í¬ê¸°: {batch_size})")

        async def fetch_batch(batch_ids: List[str], batch_num: int) -> List[Dict[str, Any]]:
            if not batch_ids:
                return []
            mget_body = [{"_index": index_name, "_id": uid} for uid in batch_ids]
            try:
                if self.os_async_client:
                    response = await self.os_async_client.mget(
                        body={"docs": mget_body},
                        ignore=[404],
                        request_timeout=request_timeout,
                        _source=source_fields
                    )
                else:
                    loop = asyncio.get_event_loop()
                    response = await loop.run_in_executor(
                        None,
                        lambda: self.os_client.mget(
                            body={"docs": mget_body},
                            ignore=[404],
                            request_timeout=request_timeout,
                            _source=source_fields
                        )
                    )
                docs = response.get('docs', [])
                found = sum(1 for item in docs if item.get('found'))
                logger.debug(f"  âœ… [async] {index_name} ë°°ì¹˜ {batch_num}: {found}/{len(batch_ids)}ê±´")
                return docs
            except Exception as e:
                logger.warning(f"  âš ï¸ [async] {index_name} ë°°ì¹˜ {batch_num} ì‹¤íŒ¨: {e}")
                return []

        results: List[Dict[str, Any]] = []
        max_concurrent = 3
        for i in range(0, len(batches), max_concurrent):
            batch_group = batches[i:i + max_concurrent]
            tasks = [
                fetch_batch(batch, i + j + 1)
                for j, batch in enumerate(batch_group)
            ]
            batch_results = await asyncio.gather(*tasks)
            for docs in batch_results:
                results.extend(docs)

        logger.info(f"  âœ… [async] {index_name} ë°°ì¹˜ ì¡°íšŒ ì™„ë£Œ: {len(results)}/{len(doc_ids)}ê±´ (raw docs)")
        return results

    @staticmethod
    def docs_to_user_map(docs: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """mget ê²°ê³¼ë¥¼ user_id -> source dictë¡œ ë³€í™˜"""
        result = {}
        for doc in docs or []:
            if doc.get('found'):
                result[doc['_id']] = doc.get('_source', {})
        return result

    def get_document_by_id(
        self,
        index_name: str,
        doc_id: str
    ) -> Optional[Dict[str, Any]]:
        """
        IDë¡œ ë¬¸ì„œ ì¡°íšŒ

        Args:
            index_name: ì¸ë±ìŠ¤ ì´ë¦„
            doc_id: ë¬¸ì„œ ID

        Returns:
            ë¬¸ì„œ ë°ì´í„° ë˜ëŠ” None
        """
        if not self.os_client:
            raise ValueError("OpenSearch í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        try:
            response = self.os_client.get(
                index=index_name,
                id=doc_id
            )
            return response['_source']

        except Exception as e:
            logger.warning(f"âš ï¸ ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨ (ID: {doc_id}): {e}")
            return None

    def multi_get_documents(
        self,
        index_name: str,
        doc_ids: List[str]
    ) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ë¬¸ì„œ ì¼ê´„ ì¡°íšŒ

        Args:
            index_name: ì¸ë±ìŠ¤ ì´ë¦„
            doc_ids: ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸

        Returns:
            ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.os_client:
            raise ValueError("OpenSearch í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        try:
            response = self.os_client.mget(
                index=index_name,
                body={"ids": doc_ids}
            )

            documents = []
            for doc in response['docs']:
                if doc.get('found'):
                    documents.append(doc['_source'])

            logger.info(f"âœ… ë¬¸ì„œ ì¼ê´„ ì¡°íšŒ ì™„ë£Œ: {len(documents)}/{len(doc_ids)}ê±´")
            return documents

        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì¼ê´„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise

    def scroll_search(
        self,
        index_name: str,
        query: Dict[str, Any],
        batch_size: int = 100,
        scroll_time: str = "2m"
    ):
        """
        ëŒ€ëŸ‰ ë°ì´í„° ìŠ¤í¬ë¡¤ ê²€ìƒ‰ (ì œë„ˆë ˆì´í„°)

        Args:
            index_name: ì¸ë±ìŠ¤ ì´ë¦„
            query: OpenSearch ì¿¼ë¦¬ DSL
            batch_size: ë°°ì¹˜ í¬ê¸°
            scroll_time: ìŠ¤í¬ë¡¤ ìœ ì§€ ì‹œê°„

        Yields:
            ë¬¸ì„œ ë°°ì¹˜
        """
        if not self.os_client:
            raise ValueError("OpenSearch í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        try:
            # ì´ˆê¸° ê²€ìƒ‰
            response = self.os_client.search(
                index=index_name,
                body=query,
                scroll=scroll_time,
                size=batch_size
            )

            scroll_id = response['_scroll_id']
            hits = response['hits']['hits']

            while hits:
                yield [hit['_source'] for hit in hits]

                # ë‹¤ìŒ ë°°ì¹˜
                response = self.os_client.scroll(
                    scroll_id=scroll_id,
                    scroll=scroll_time
                )
                scroll_id = response['_scroll_id']
                hits = response['hits']['hits']

            # ìŠ¤í¬ë¡¤ ì •ë¦¬
            self.os_client.clear_scroll(scroll_id=scroll_id)

        except Exception as e:
            logger.error(f"âŒ ìŠ¤í¬ë¡¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            raise

    async def scroll_search_async(
        self,
        index_name: str,
        query: Dict[str, Any],
        batch_size: int = 1000,
        scroll_time: str = "5m",
        num_slices: int = 4,
        source_filter: Optional[Dict[str, Any]] = None,
        request_timeout: int = 300
    ) -> List[Dict[str, Any]]:
        """
        â­ ë¹„ë™ê¸° ë³‘ë ¬ Scroll API (Sliced Scroll)

        35000ê±´ ì „ì²´ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ì¡°íšŒí•˜ê¸° ìœ„í•´ ë³‘ë ¬ ìŠ¬ë¼ì´ìŠ¤ ì‚¬ìš©

        Args:
            index_name: ì¸ë±ìŠ¤ ì´ë¦„
            query: OpenSearch ì¿¼ë¦¬ DSL
            batch_size: ê° ìŠ¬ë¼ì´ìŠ¤ì˜ ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ 1000)
            scroll_time: ìŠ¤í¬ë¡¤ ìœ ì§€ ì‹œê°„ (ê¸°ë³¸ 5ë¶„)
            num_slices: ë³‘ë ¬ ìŠ¬ë¼ì´ìŠ¤ ê°œìˆ˜ (ê¸°ë³¸ 4)
            source_filter: _source í•„í„°ë§
            request_timeout: ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)

        Returns:
            ì „ì²´ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.os_async_client:
            raise ValueError("Async OpenSearch í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        logger.info(f"ğŸ”„ Scroll API ì‹œì‘: {num_slices}ê°œ ìŠ¬ë¼ì´ìŠ¤ ë³‘ë ¬ ì²˜ë¦¬")

        async def fetch_slice(slice_id: int) -> List[Dict[str, Any]]:
            """ë‹¨ì¼ ìŠ¬ë¼ì´ìŠ¤ ì²˜ë¦¬"""
            slice_results = []
            scroll_id = None

            try:
                # ìŠ¬ë¼ì´ìŠ¤ ì¿¼ë¦¬ ìƒì„±
                slice_query = query.copy()
                if source_filter:
                    slice_query["_source"] = source_filter

                slice_query["slice"] = {
                    "id": slice_id,
                    "max": num_slices
                }

                # ì´ˆê¸° ê²€ìƒ‰
                response = await self.os_async_client.search(
                    index=index_name,
                    body=slice_query,
                    scroll=scroll_time,
                    size=batch_size,
                    request_timeout=request_timeout
                )

                scroll_id = response.get('_scroll_id')
                hits = response.get('hits', {}).get('hits', [])
                total_in_slice = response.get('hits', {}).get('total', {}).get('value', 0)

                logger.debug(f"  ğŸ“‹ Slice {slice_id}/{num_slices}: ì´ {total_in_slice}ê±´ ì˜ˆìƒ")

                # ì²« ë°°ì¹˜ ì¶”ê°€
                slice_results.extend(hits)

                # ë‚˜ë¨¸ì§€ ë°°ì¹˜ ìŠ¤í¬ë¡¤
                batch_count = 1
                while hits:
                    response = await self.os_async_client.scroll(
                        scroll_id=scroll_id,
                        scroll=scroll_time,
                        request_timeout=request_timeout
                    )

                    scroll_id = response.get('_scroll_id')
                    hits = response.get('hits', {}).get('hits', [])

                    if hits:
                        slice_results.extend(hits)
                        batch_count += 1

                logger.info(f"  âœ… Slice {slice_id}/{num_slices}: {len(slice_results)}ê±´ ìˆ˜ì§‘ ({batch_count}ê°œ ë°°ì¹˜)")

                # ìŠ¤í¬ë¡¤ ì •ë¦¬
                if scroll_id:
                    try:
                        await self.os_async_client.clear_scroll(scroll_id=scroll_id)
                    except Exception as e:
                        logger.warning(f"  âš ï¸ Slice {slice_id} ìŠ¤í¬ë¡¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

                return slice_results

            except Exception as e:
                logger.error(f"  âŒ Slice {slice_id}/{num_slices} ì‹¤íŒ¨: {e}")
                # ìŠ¤í¬ë¡¤ ì •ë¦¬ ì‹œë„
                if scroll_id:
                    try:
                        await self.os_async_client.clear_scroll(scroll_id=scroll_id)
                    except:
                        pass
                raise

        try:
            # ëª¨ë“  ìŠ¬ë¼ì´ìŠ¤ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
            tasks = [fetch_slice(i) for i in range(num_slices)]
            slice_results = await asyncio.gather(*tasks)

            # ê²°ê³¼ í•©ì¹˜ê¸°
            all_hits = []
            for slice_hits in slice_results:
                all_hits.extend(slice_hits)

            logger.info(f"âœ… Scroll API ì™„ë£Œ: ì´ {len(all_hits)}ê±´ ìˆ˜ì§‘ ({num_slices}ê°œ ìŠ¬ë¼ì´ìŠ¤)")

            return all_hits

        except Exception as e:
            logger.error(f"âŒ Scroll API ì‹¤íŒ¨: {e}")
            raise

    def aggregate_data(
        self,
        index_name: str,
        query: Dict[str, Any],
        aggregations: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ì§‘ê³„ ì¿¼ë¦¬ ì‹¤í–‰

        Args:
            index_name: ì¸ë±ìŠ¤ ì´ë¦„
            query: í•„í„° ì¿¼ë¦¬
            aggregations: ì§‘ê³„ ì •ì˜

        Returns:
            ì§‘ê³„ ê²°ê³¼
        """
        if not self.os_client:
            raise ValueError("OpenSearch í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        try:
            query_body = query.copy()
            query_body["aggs"] = aggregations
            query_body["size"] = 0  # ë¬¸ì„œëŠ” ë°˜í™˜í•˜ì§€ ì•ŠìŒ

            response = self.os_client.search(
                index=index_name,
                body=query_body
            )

            logger.info(f"âœ… ì§‘ê³„ ì¿¼ë¦¬ ì™„ë£Œ")
            return response.get('aggregations', {})

        except Exception as e:
            logger.error(f"âŒ ì§‘ê³„ ì¿¼ë¦¬ ì‹¤íŒ¨: {e}")
            raise

    # Qdrant ë©”ì„œë“œ (í–¥í›„ í™•ì¥)
    def search_qdrant(self, collection_name: str, vector: List[float], limit: int = 10):
        """
        Qdrantì—ì„œ ë²¡í„° ê²€ìƒ‰ (í”Œë ˆì´ìŠ¤í™€ë”)

        Args:
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            vector: ì¿¼ë¦¬ ë²¡í„°
            limit: ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜

        Returns:
            ê²€ìƒ‰ ê²°ê³¼
        """
        if not self.qdrant_client:
            raise NotImplementedError("Qdrant í´ë¼ì´ì–¸íŠ¸ê°€ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        # TODO: Qdrant ê²€ìƒ‰ êµ¬í˜„
        logger.warning("âš ï¸ Qdrant ê²€ìƒ‰ì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        return []
