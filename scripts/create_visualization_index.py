"""
OpenSearch ìŠ¤í¬ë¡¤ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸/ë‹µë³€ ê²½í–¥ ë¶„ì„ìš© ì¸ë±ìŠ¤ ìƒì„±

ê¸°ì¡´ survey_responses_merged ì¸ë±ìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ì„œ
ì‹œê°í™”ì— ìµœì í™”ëœ ìƒˆë¡œìš´ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

from __future__ import annotations

import argparse
import json
import sys
import re
import os
from collections import defaultdict
from typing import Dict, List, Any, Optional
from datetime import datetime

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

from opensearchpy import OpenSearch
from opensearchpy.helpers import bulk


DEFAULT_HOST = "159.223.47.188"
DEFAULT_PORT = 9200
DEFAULT_USER = "admin"
DEFAULT_PASSWORD = "AVNS_1unywEqMDAepzpSW6vU"
SOURCE_INDEX = "survey_responses_merged"
TARGET_INDEX = "survey_qa_analysis"


def build_client(host: str, port: int, username: str, password: str) -> OpenSearch:
    """OpenSearch í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    return OpenSearch(
        hosts=[{"host": host, "port": port}],
        http_auth=(username, password),
        use_ssl=True,
        verify_certs=False,
        ssl_assert_hostname=False,
        ssl_show_warn=False,
        timeout=60,
        max_retries=3,
        retry_on_timeout=True,
    )


def normalize_answer(answer: Any) -> Any:
    """ë‹µë³€ì„ ì •ê·œí™” (ë¦¬ìŠ¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€, ë‹¨ì¼ ê°’ì€ ë¬¸ìì—´)"""
    if isinstance(answer, list):
        # ë¹ˆ ê°’ ì œê±°
        filtered = [str(item).strip() for item in answer if item and str(item).strip()]
        return filtered if filtered else None
    if answer is None:
        return None
    result = str(answer).strip()
    return result if result else None


def text_to_field_name(text: str) -> str:
    """ì§ˆë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¬¸ í•„ë“œëª…ìœ¼ë¡œ ë³€í™˜"""
    # ê¸°ë³¸ ë§¤í•‘ (ì¼ë°˜ì ì¸ ì§ˆë¬¸ë“¤)
    mapping = {
        # ì¸êµ¬í†µê³„ ì§ˆë¬¸
        "ê·€í•˜ì˜ ì„±ë³„ì€": "gender",
        "ê·€í•˜ì˜ ì¶œìƒë…„ë„ëŠ” ì–´ë–»ê²Œ ë˜ì‹­ë‹ˆê¹Œ?": "birth_year",
        "íšŒì›ë‹˜ê»˜ì„œ í˜„ì¬ ì‚´ê³  ê³„ì‹  ì§€ì—­ì€ ì–´ë””ì¸ê°€ìš”?": "region",
        # ì§€ì—­ë³„ ì„¸ë¶€ ì§€ì—­ ì§ˆë¬¸ (ì£¼ìš” ì§€ì—­ë“¤)
        "ê·¸ë ‡ë‹¤ë©´, ì„œìš¸ì˜ ì–´ëŠ êµ¬ì— ì‚´ê³  ê³„ì‹ ê°€ìš”?": "sub_region",
        "ê·¸ë ‡ë‹¤ë©´, ë¶€ì‚°ì˜ ì–´ëŠ êµ¬ì— ì‚´ê³  ê³„ì‹ ê°€ìš”?": "sub_region",
        "ê·¸ë ‡ë‹¤ë©´, ê²½ê¸°ì˜ ì–´ëŠ êµ¬ì— ì‚´ê³  ê³„ì‹ ê°€ìš”?": "sub_region",
        "ê·¸ë ‡ë‹¤ë©´, ì¸ì²œì˜ ì–´ëŠ êµ¬ì— ì‚´ê³  ê³„ì‹ ê°€ìš”?": "sub_region",
        "ê·¸ë ‡ë‹¤ë©´, ëŒ€êµ¬ì˜ ì–´ëŠ êµ¬ì— ì‚´ê³  ê³„ì‹ ê°€ìš”?": "sub_region",
        "ê·¸ë ‡ë‹¤ë©´, ëŒ€ì „ì˜ ì–´ëŠ êµ¬ì— ì‚´ê³  ê³„ì‹ ê°€ìš”?": "sub_region",
        "ê·¸ë ‡ë‹¤ë©´, ê´‘ì£¼ì˜ ì–´ëŠ êµ¬ì— ì‚´ê³  ê³„ì‹ ê°€ìš”?": "sub_region",
        "ê·¸ë ‡ë‹¤ë©´, ìš¸ì‚°ì˜ ì–´ëŠ êµ¬ì— ì‚´ê³  ê³„ì‹ ê°€ìš”?": "sub_region",
        "ê·¸ë ‡ë‹¤ë©´, ê°•ì›ì˜ ì–´ëŠ êµ¬ì— ì‚´ê³  ê³„ì‹ ê°€ìš”?": "sub_region",
        "ê·¸ë ‡ë‹¤ë©´, ì¶©ë¶ì˜ ì–´ëŠ êµ¬ì— ì‚´ê³  ê³„ì‹ ê°€ìš”?": "sub_region",
        "ê·¸ë ‡ë‹¤ë©´, ì¶©ë‚¨ì˜ ì–´ëŠ êµ¬ì— ì‚´ê³  ê³„ì‹ ê°€ìš”?": "sub_region",
        "ê·¸ë ‡ë‹¤ë©´, ì „ë¶ì˜ ì–´ëŠ êµ¬ì— ì‚´ê³  ê³„ì‹ ê°€ìš”?": "sub_region",
        "ê·¸ë ‡ë‹¤ë©´, ì „ë‚¨ì˜ ì–´ëŠ êµ¬ì— ì‚´ê³  ê³„ì‹ ê°€ìš”?": "sub_region",
        "ê·¸ë ‡ë‹¤ë©´, ê²½ë¶ì˜ ì–´ëŠ êµ¬ì— ì‚´ê³  ê³„ì‹ ê°€ìš”?": "sub_region",
        "ê·¸ë ‡ë‹¤ë©´, ê²½ë‚¨ì˜ ì–´ëŠ êµ¬ì— ì‚´ê³  ê³„ì‹ ê°€ìš”?": "sub_region",
        "ê·¸ë ‡ë‹¤ë©´, ì œì£¼ì˜ ì–´ëŠ êµ¬ì— ì‚´ê³  ê³„ì‹ ê°€ìš”?": "sub_region",
        "ê·¸ë ‡ë‹¤ë©´, ê¸°íƒ€ / í•´ì™¸ì˜ ì–´ëŠ êµ¬ì— ì‚´ê³  ê³„ì‹ ê°€ìš”?": "sub_region",
        
        # ê°€ì¡±/ì¸ì ì‚¬í•­
        "ê²°í˜¼ì—¬ë¶€": "marriage",
        "ìë…€ìˆ˜": "children_count",
        "ê°€ì¡±ìˆ˜": "family_count",
        
        # êµìœ¡/ì§ì—…
        "ìµœì¢…í•™ë ¥": "education",
        "ì§ì—…": "job",
        "ì§ë¬´": "job_role",
        
        # ì†Œë“
        "ì›”í‰ê·  ê°œì¸ì†Œë“": "personal_income",
        "ì›”í‰ê·  ê°€êµ¬ì†Œë“": "household_income",
        
        # ê°€ì „ì œí’ˆ
        "ë³´ìœ ê°€ì „ì œí’ˆ": "appliances",
        
        # íœ´ëŒ€í°
        "ë³´ìœ  íœ´ëŒ€í° ë‹¨ë§ê¸° ë¸Œëœë“œ": "phone_brand",
        "ë³´ìœ  íœ´ëŒ€í° ëª¨ë¸ëª…": "phone_model",
        
        # ìë™ì°¨
        "ë³´ìœ ì°¨ëŸ‰ì—¬ë¶€": "car_owned",
        "ìë™ì°¨ ì œì¡°ì‚¬": "car_brand",
        "ìë™ì°¨ ëª¨ë¸": "car_model",
        
        # í¡ì—°
        "í¡ì—°ê²½í—˜": "smoke_type",
        "í¡ì—°ê²½í—˜ ë‹´ë°°ë¸Œëœë“œ": "smoke_brand",
        
        # ìŒì£¼
        "ìŒìš©ê²½í—˜ ìˆ ": "drink_type",
    }
    
    # ë§¤í•‘ì— ìˆìœ¼ë©´ ì‚¬ìš©
    if text in mapping:
        return mapping[text]
    
    # ìë™ ë³€í™˜: í•œê¸€ì„ ì˜ë¬¸ìœ¼ë¡œ ë³€í™˜
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë³€í™˜
    text_lower = text.lower()
    
    # ì¸êµ¬í†µê³„ ì§ˆë¬¸
    if "ì„±ë³„" in text:
        return "gender"
    elif "ì¶œìƒë…„ë„" in text or "ì¶œìƒ" in text:
        return "birth_year"
    elif "ì§€ì—­" in text and "êµ¬" not in text:
        return "region"
    elif "êµ¬ì— ì‚´ê³ " in text or "ì–´ëŠ êµ¬" in text or ("ê·¸ë ‡ë‹¤ë©´" in text and "ì–´ëŠ" in text and "ì‚´ê³ " in text):
        # ëª¨ë“  ì§€ì—­ì˜ ì„¸ë¶€ ì§€ì—­ì€ sub_regionìœ¼ë¡œ í†µì¼
        return "sub_region"
    
    # ê°€ì¡±/ì¸ì ì‚¬í•­
    elif "ê²°í˜¼" in text:
        return "marriage"
    elif "ìë…€" in text:
        return "children_count"
    elif "ê°€ì¡±" in text:
        return "family_count"
    
    # êµìœ¡/ì§ì—…
    elif "í•™ë ¥" in text or "í•™ì›" in text:
        return "education"
    elif "ì§ì—…" in text and "ì§ë¬´" not in text:
        return "job"
    elif "ì§ë¬´" in text:
        return "job_role"
    
    # ì†Œë“
    elif "ì†Œë“" in text:
        if "ê°œì¸" in text:
            return "personal_income"
        elif "ê°€êµ¬" in text:
            return "household_income"
        return "income"
    
    # ê°€ì „ì œí’ˆ
    elif "ê°€ì „" in text or ("ì œí’ˆ" in text and "íœ´ëŒ€í°" not in text and "ìë™ì°¨" not in text):
        return "appliances"
    
    # íœ´ëŒ€í°
    elif "íœ´ëŒ€í°" in text or "ìŠ¤ë§ˆíŠ¸í°" in text:
        if "ë¸Œëœë“œ" in text or "ì œì¡°ì‚¬" in text:
            return "phone_brand"
        elif "ëª¨ë¸" in text:
            return "phone_model"
        return "phone"
    
    # ìë™ì°¨
    elif "ì°¨ëŸ‰" in text or "ìë™ì°¨" in text:
        if "ì—¬ë¶€" in text:
            return "car_owned"
        elif "ì œì¡°ì‚¬" in text or "ë¸Œëœë“œ" in text:
            return "car_brand"
        elif "ëª¨ë¸" in text:
            return "car_model"
        return "car"
    
    # í¡ì—°
    elif "í¡ì—°" in text or "ë‹´ë°°" in text:
        if "ë¸Œëœë“œ" in text:
            return "smoke_brand"
        return "smoke_type"
    
    # ìŒì£¼
    elif "ìŒìš©" in text or ("ìˆ " in text and "ì£¼ë¥˜" not in text):
        return "drink_type"
    
    # ê¸°ë³¸ ë³€í™˜: í•œê¸€ ì œê±°í•˜ê³  ì˜ë¬¸ í‚¤ì›Œë“œ ì¶”ì¶œ ë˜ëŠ” í•´ì‹œ ì‚¬ìš©
    # ì§ˆë¬¸ í…ìŠ¤íŠ¸ì˜ í•´ì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ìœ í•œ í•„ë“œëª… ìƒì„±
    field_hash = str(abs(hash(text)))[:8]
    return f"q_{field_hash}"


def create_analysis_index(client: OpenSearch, index_name: str, force_recreate: bool = False):
    """ì‹œê°í™” ë¶„ì„ìš© ì¸ë±ìŠ¤ ìƒì„±
    
    âš ï¸ ì£¼ì˜: ê¸°ì¡´ ì¸ë±ìŠ¤ëŠ” ì ˆëŒ€ ì‚­ì œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    force_recreate=Trueì¼ ë•Œë§Œ ëŒ€ìƒ ì¸ë±ìŠ¤(target_index)ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    ì›ë³¸ ì¸ë±ìŠ¤(source_index)ëŠ” ì ˆëŒ€ ê±´ë“œë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    
    if client.indices.exists(index=index_name):
        if force_recreate:
            print(f"\n{'='*60}")
            print(f"âš ï¸  ê²½ê³ : ê¸°ì¡´ ì¸ë±ìŠ¤ '{index_name}'ë¥¼ ì‚­ì œí•˜ë ¤ê³  í•©ë‹ˆë‹¤!")
            print(f"{'='*60}")
            print(f"ì´ ì‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"ê³„ì†í•˜ë ¤ë©´ 10ì´ˆ ì´ë‚´ì— Ctrl+Cë¥¼ ëˆŒëŸ¬ ì·¨ì†Œí•˜ì„¸ìš”...")
            import time
            for i in range(10, 0, -1):
                print(f"  ì‚­ì œê¹Œì§€ {i}ì´ˆ...", end='\r')
                time.sleep(1)
            print(f"\n{'='*60}")
            print(f"âš ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ '{index_name}' ì‚­ì œ ì¤‘...")
            client.indices.delete(index=index_name)
            print(f"âœ… ì¸ë±ìŠ¤ '{index_name}' ì‚­ì œ ì™„ë£Œ")
        else:
            print(f"â„¹ï¸ ì¸ë±ìŠ¤ '{index_name}'ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
            print(f"   ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ìœ ì§€í•˜ê³  ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")
            print(f"   ì¸ë±ìŠ¤ë¥¼ ì¬ìƒì„±í•˜ë ¤ë©´ --force-recreate í”Œë˜ê·¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
            return
    
    print(f"âœ¨ ì¸ë±ìŠ¤ '{index_name}' ìƒì„± ì¤‘...")
    
    # ì¸ë±ìŠ¤ ë§¤í•‘ ì •ì˜ (ë™ì  ë§¤í•‘ í—ˆìš©)
    mappings = {
        "properties": {
            # ê¸°ë³¸ í•„ë“œ
            "user_id": {
                "type": "keyword"
            },
            "timestamp": {
                "type": "date"
            },
            
            # ë©”íƒ€ë°ì´í„° í•„ë“œ
            "meta_gender": {
                "type": "keyword"
            },
            "meta_age": {
                "type": "integer"
            },
            "meta_age_group": {
                "type": "keyword"
            },
            "meta_region": {
                "type": "keyword"
            },
            "meta_sub_region": {
                "type": "keyword"
            },
            "meta_panel": {
                "type": "keyword"
            },
            "meta_birth_year": {
                "type": "keyword"
            },
            "meta_survey_datetime": {
                "type": "date"
            }
        },
        # ë™ì  ë§¤í•‘: q_* í•„ë“œë“¤ì€ ìë™ìœ¼ë¡œ ë§¤í•‘ë¨
        "dynamic": True,
        "dynamic_templates": [
            {
                # ë¬¸ìì—´ í•„ë“œ (ë‹¨ì¼ ë‹µë³€)
                "question_string_fields": {
                    "match": "q_*",
                    "match_mapping_type": "string",
                    "mapping": {
                        "type": "text",
                        "fields": {
                            "keyword": {
                                "type": "keyword"
                            }
                        }
                    }
                }
            },
            {
                # ë°°ì—´ í•„ë“œ (ë‹¤ì¤‘ ì„ íƒ ë‹µë³€)
                "question_array_fields": {
                    "match": "q_*",
                    "match_mapping_type": "object",
                    "mapping": {
                        "type": "keyword"
                    }
                }
            }
        ]
    }
    
    # ì¸ë±ìŠ¤ ì„¤ì •
    settings = {
        "number_of_shards": 3,
        "number_of_replicas": 1,
        "refresh_interval": "30s",
        "analysis": {
            "analyzer": {
                "standard": {
                    "type": "standard"
                }
            }
        }
    }
    
    body = {
        "settings": settings,
        "mappings": mappings
    }
    
    try:
        client.indices.create(index=index_name, body=body)
        print(f"âœ… ì¸ë±ìŠ¤ '{index_name}' ìƒì„± ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        raise


def scroll_all_documents(client: OpenSearch, index_name: str, batch_size: int = 1000):
    """ìŠ¤í¬ë¡¤ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ë¬¸ì„œ ì½ê¸°"""
    print(f"ğŸ“– ì¸ë±ìŠ¤ '{index_name}'ì—ì„œ ë°ì´í„° ì½ê¸° ì‹œì‘...")
    
    scroll_time = "5m"
    total_docs = 0
    
    try:
        # ì´ˆê¸° ê²€ìƒ‰ (match_all)
        response = client.search(
            index=index_name,
            body={"query": {"match_all": {}}},
            scroll=scroll_time,
            size=batch_size
        )
        
        scroll_id = response.get('_scroll_id')
        hits = response['hits']['hits']
        total_hits = response['hits']['total']['value']
        
        print(f"ğŸ“Š ì´ ë¬¸ì„œ ìˆ˜: {total_hits:,}ê±´")
        
        while hits:
            for hit in hits:
                yield hit['_source']
                total_docs += 1
            
            if total_docs % 10000 == 0:
                print(f"  ì§„í–‰: {total_docs:,} / {total_hits:,} ({total_docs*100//total_hits}%)")
            
            # ë‹¤ìŒ ë°°ì¹˜
            response = client.scroll(
                scroll_id=scroll_id,
                scroll=scroll_time
            )
            scroll_id = response.get('_scroll_id')
            hits = response['hits']['hits']
        
        # ìŠ¤í¬ë¡¤ ì •ë¦¬
        if scroll_id:
            client.clear_scroll(scroll_id=scroll_id)
        
        print(f"âœ… ë°ì´í„° ì½ê¸° ì™„ë£Œ: {total_docs:,}ê±´")
        
    except Exception as e:
        print(f"âŒ ìŠ¤í¬ë¡¤ ì½ê¸° ì‹¤íŒ¨: {e}")
        raise


def transform_data(documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ì›ë³¸ ë°ì´í„°ë¥¼ ì‹œê°í™”ìš© êµ¬ì¡°ë¡œ ë³€í™˜ (ì‚¬ìš©ìë³„ ë¬¸ì„œ)"""
    print("ğŸ”„ ë°ì´í„° ë³€í™˜ ì¤‘...")
    
    transformed_docs = []
    processed_users = 0
    
    for doc in documents:
        user_id = doc.get("user_id", "")
        timestamp = doc.get("timestamp", "")
        metadata = doc.get("metadata", {})
        qa_pairs = doc.get("qa_pairs", [])
        
        if not user_id:
            continue
        
        # ìƒˆ ë¬¸ì„œ ìƒì„±
        new_doc = {
            "user_id": user_id,
            "timestamp": timestamp,
        }
        
        # [1] ë©”íƒ€ë°ì´í„°ë¥¼ flatí•˜ê²Œ ë³€í™˜
        if metadata:
            new_doc["meta_gender"] = metadata.get("gender", "")
            new_doc["meta_age"] = metadata.get("age")
            new_doc["meta_age_group"] = metadata.get("age_group", "")
            new_doc["meta_region"] = metadata.get("region", "")
            new_doc["meta_sub_region"] = metadata.get("sub_region", "")
            new_doc["meta_panel"] = metadata.get("panel", "")
            new_doc["meta_birth_year"] = metadata.get("birth_year", "")
            if metadata.get("survey_datetime"):
                new_doc["meta_survey_datetime"] = metadata.get("survey_datetime")
        
        # [2] ì§ˆë¬¸-ë‹µë³€ ìŒì„ ì˜ë¬¸ í‚¤ë¡œ í•„ë“œë¡œ ìŠ¹ê²©
        for qa in qa_pairs:
            q_text = qa.get("q_text", "").strip()
            if not q_text:
                continue
            
            answer = qa.get("answer", "")
            answer_normalized = normalize_answer(answer)
            
            # ë‹µë³€ì´ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
            if answer_normalized is None:
                continue
            
            # ì§ˆë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¬¸ í•„ë“œëª…ìœ¼ë¡œ ë³€í™˜
            field_name = text_to_field_name(q_text)
            field_key = f"q_{field_name}"
            
            # ë‹¤ì¤‘ ì„ íƒì¸ ê²½ìš° ë°°ì—´ë¡œ, ë‹¨ì¼ ì„ íƒì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ì €ì¥
            if isinstance(answer_normalized, list):
                new_doc[field_key] = answer_normalized
            else:
                new_doc[field_key] = answer_normalized
        
        transformed_docs.append(new_doc)
        processed_users += 1
        
        if processed_users % 10000 == 0:
            print(f"  ë³€í™˜ ì§„í–‰: {processed_users:,}ëª…")
    
    print(f"âœ… ë³€í™˜ ì™„ë£Œ: {processed_users:,}ê°œ ë¬¸ì„œ ìƒì„±")
    return transformed_docs


def bulk_index_documents(client: OpenSearch, index_name: str, documents: List[Dict[str, Any]], batch_size: int = 1000):
    """Bulk APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ì¸ë±ì‹±"""
    print(f"ğŸ“ ì¸ë±ìŠ¤ '{index_name}'ì— ë¬¸ì„œ ì €ì¥ ì¤‘...")
    
    actions = []
    total_indexed = 0
    
    for i, doc in enumerate(documents):
        # ë¬¸ì„œ ID ìƒì„± (user_id ì‚¬ìš©)
        doc_id = doc.get('user_id', f"doc_{i}")
        
        action = {
            "_index": index_name,
            "_id": doc_id,
            "_source": doc
        }
        actions.append(action)
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¸ë±ì‹±
        if len(actions) >= batch_size:
            try:
                success, failed = bulk(client, actions, raise_on_error=False)
                total_indexed += success
                if failed:
                    print(f"  âš ï¸ {len(failed)}ê°œ ë¬¸ì„œ ì¸ë±ì‹± ì‹¤íŒ¨")
                actions = []
                
                if total_indexed % 10000 == 0:
                    print(f"  ì§„í–‰: {total_indexed:,} / {len(documents):,} ({total_indexed*100//len(documents)}%)")
            except Exception as e:
                print(f"  âŒ ë°°ì¹˜ ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
                actions = []
    
    # ë‚¨ì€ ë¬¸ì„œ ì¸ë±ì‹±
    if actions:
        try:
            success, failed = bulk(client, actions, raise_on_error=False)
            total_indexed += success
            if failed:
                print(f"  âš ï¸ {len(failed)}ê°œ ë¬¸ì„œ ì¸ë±ì‹± ì‹¤íŒ¨")
        except Exception as e:
            print(f"  âŒ ë§ˆì§€ë§‰ ë°°ì¹˜ ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
    
    print(f"âœ… ì¸ë±ì‹± ì™„ë£Œ: {total_indexed:,} / {len(documents):,}ê±´")
    
    # ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨
    client.indices.refresh(index=index_name)
    print("âœ… ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨ ì™„ë£Œ")


def main() -> int:
    parser = argparse.ArgumentParser(
        description="OpenSearch ìŠ¤í¬ë¡¤ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°í™”ìš© ì¸ë±ìŠ¤ ìƒì„±"
    )
    parser.add_argument("--host", default=DEFAULT_HOST, help="OpenSearch host")
    parser.add_argument("--port", type=int, default=DEFAULT_PORT, help="OpenSearch port")
    parser.add_argument("--user", default=DEFAULT_USER, help="OpenSearch username")
    parser.add_argument("--password", default=DEFAULT_PASSWORD, help="OpenSearch password")
    parser.add_argument(
        "--source-index",
        default=SOURCE_INDEX,
        help=f"ì›ë³¸ ì¸ë±ìŠ¤ ì´ë¦„ (default: {SOURCE_INDEX})"
    )
    parser.add_argument(
        "--target-index",
        default=TARGET_INDEX,
        help=f"ëŒ€ìƒ ì¸ë±ìŠ¤ ì´ë¦„ (default: {TARGET_INDEX})"
    )
    parser.add_argument(
        "--force-recreate",
        action="store_true",
        help="âš ï¸ ëŒ€ìƒ ì¸ë±ìŠ¤(target_index)ê°€ ìˆìœ¼ë©´ ì‚­ì œí•˜ê³  ì¬ìƒì„± (ì›ë³¸ ì¸ë±ìŠ¤ëŠ” ì ˆëŒ€ ê±´ë“œë¦¬ì§€ ì•ŠìŒ)"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1000,
        help="ìŠ¤í¬ë¡¤ ë°°ì¹˜ í¬ê¸° (default: 1000)"
    )
    
    args = parser.parse_args()
    
    try:
        # í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        print("ğŸ”Œ OpenSearch ì—°ê²° ì¤‘...")
        client = build_client(args.host, args.port, args.user, args.password)
        
        # ì—°ê²° í™•ì¸
        info = client.info()
        print(f"âœ… OpenSearch ì—°ê²° ì„±ê³µ: v{info['version']['number']}")
        
        # ì›ë³¸ ì¸ë±ìŠ¤ í™•ì¸ (ì ˆëŒ€ ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
        if not client.indices.exists(index=args.source_index):
            print(f"âŒ ì›ë³¸ ì¸ë±ìŠ¤ '{args.source_index}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return 1
        
        print(f"âœ… ì›ë³¸ ì¸ë±ìŠ¤ '{args.source_index}' í™•ì¸ ì™„ë£Œ (ì½ê¸° ì „ìš©)")
        print(f"ğŸ“ ëŒ€ìƒ ì¸ë±ìŠ¤: '{args.target_index}'")
        
        # ì›ë³¸ê³¼ ëŒ€ìƒì´ ê°™ì€ì§€ í™•ì¸ (ì‹¤ìˆ˜ ë°©ì§€)
        if args.source_index == args.target_index:
            print(f"\nâŒ ì˜¤ë¥˜: ì›ë³¸ ì¸ë±ìŠ¤ì™€ ëŒ€ìƒ ì¸ë±ìŠ¤ê°€ ê°™ìŠµë‹ˆë‹¤!")
            print(f"   ì›ë³¸ ì¸ë±ìŠ¤ '{args.source_index}'ëŠ” ì ˆëŒ€ ìˆ˜ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"   ë‹¤ë¥¸ ëŒ€ìƒ ì¸ë±ìŠ¤ ì´ë¦„ì„ ì§€ì •í•˜ì„¸ìš”.")
            return 1
        
        # ìƒˆ ì¸ë±ìŠ¤ ìƒì„± (ëŒ€ìƒ ì¸ë±ìŠ¤ë§Œ)
        create_analysis_index(client, args.target_index, args.force_recreate)
        
        # ìŠ¤í¬ë¡¤ë¡œ ëª¨ë“  ë¬¸ì„œ ì½ê¸°
        documents = list(scroll_all_documents(client, args.source_index, args.batch_size))
        
        if not documents:
            print("âš ï¸ ì½ì„ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        # ë°ì´í„° ë³€í™˜
        transformed_docs = transform_data(documents)
        
        if not transformed_docs:
            print("âš ï¸ ë³€í™˜ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        # ìƒˆ ì¸ë±ìŠ¤ì— ì €ì¥
        bulk_index_documents(client, args.target_index, transformed_docs, args.batch_size)
        
        # ìµœì¢… í†µê³„
        stats = client.indices.stats(index=args.target_index)
        doc_count = stats['indices'][args.target_index]['total']['docs']['count']
        print(f"\nğŸ“Š ìµœì¢… í†µê³„:")
        print(f"  - ì¸ë±ìŠ¤: {args.target_index}")
        print(f"  - ë¬¸ì„œ ìˆ˜: {doc_count:,}ê±´")
        print(f"  - ì›ë³¸ ì‚¬ìš©ì ìˆ˜: {len(documents):,}ëª…")
        print(f"  - ë³€í™˜ëœ ë¬¸ì„œ ìˆ˜: {len(transformed_docs):,}ê±´")
        
        print("\nâœ… ì‘ì—… ì™„ë£Œ!")
        return 0
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return 1
    except Exception as exc:
        print(f"âŒ ì‘ì—… ì‹¤íŒ¨: {exc}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    raise SystemExit(main())

