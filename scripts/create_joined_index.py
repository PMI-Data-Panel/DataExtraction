"""
welcome_1stì™€ welcome_2ndë¥¼ ì¡°ì¸í•˜ì—¬ í†µí•© ì¸ë±ìŠ¤ ìƒì„±

ì‚¬ì „ ì¡°ì¸ì˜ ì¥ì :
1. RRF ê²°í•© ì •í™•ë„ ìƒìŠ¹ - ê°™ì€ user_id ê¸°ë°˜ìœ¼ë¡œ ì‰½ê²Œ ë³‘í•©
2. í•„í„°ë§ íš¨ìœ¨ í–¥ìƒ - must í•„í„°ë¥¼ í•œ ë²ˆì— ì ìš© ê°€ëŠ¥
3. ê²€ìƒ‰ ì†ë„ ê°œì„  - ì¡°ì¸ ë¹„ìš©ì´ ì‚¬ë¼ì§
4. ê²°ê³¼ ì¼ê´€ì„± í™•ë³´ - í•„í„° ëˆ„ë½/ë¶ˆì¼ì¹˜ ë¬¸ì œ í•´ê²°
"""

import logging
from opensearchpy import OpenSearch
from typing import Dict, Any, Optional
import sys
import os
from opensearchpy.helpers import bulk

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, PROJECT_ROOT)

from rag_query_analyzer.config import get_config

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_joined_index_mapping() -> Dict[str, Any]:
    """ì¡°ì¸ëœ ì¸ë±ìŠ¤ì˜ ë§¤í•‘ ì •ì˜"""
    return {
        "mappings": {
            "properties": {
                "user_id": {
                    "type": "keyword"
                },
                "metadata": {
                    "type": "object",
                    "properties": {
                        # welcome_1stì—ì„œ ê°€ì ¸ì˜¨ í•„ë“œ
                        "age_group": {
                            "type": "keyword"
                        },
                        "gender": {
                            "type": "keyword"
                        },
                        "birth_year": {
                            "type": "keyword"
                        },
                        "region": {
                            "type": "keyword"
                        },
                        # welcome_2ndì—ì„œ ê°€ì ¸ì˜¨ í•„ë“œ
                        "occupation": {
                            "type": "keyword"
                        },
                        "job_category": {
                            "type": "keyword"
                        }
                    }
                },
                "qa_pairs": {
                    "type": "nested",
                    "properties": {
                        "q_text": {
                            "type": "text",
                            "analyzer": "nori"
                        },
                        "q_code": {
                            "type": "keyword"
                        },
                        "answer": {
                            "type": "text",
                            "analyzer": "nori"
                        },
                        "answer_text": {
                            "type": "text",
                            "analyzer": "nori"
                        },
                        "answer_vector": {
                            "type": "dense_vector",
                            "dims": 1024,
                            "index": True,
                            "similarity": "cosine"
                        }
                    }
                },
                "timestamp": {
                    "type": "date"
                },
                # ì›ë³¸ ì¸ë±ìŠ¤ ì •ë³´ (ë””ë²„ê¹…ìš©)
                "source_indices": {
                    "type": "keyword"
                }
            }
        },
        "settings": {
            "number_of_shards": 3,
            "number_of_replicas": 1,
            "refresh_interval": "30s",
            "analysis": {
                "analyzer": {
                    "nori": {
                        "type": "nori",
                        "decompound_mode": "mixed"
                    }
                }
            }
        }
    }


def join_documents(
    os_client: OpenSearch,
    source_index_1st: str = "s_welcome_1st",
    source_index_2nd: str = "s_welcome_2nd",
    target_index: str = "s_users_joined",
    batch_size: int = 1000
) -> Dict[str, Any]:
    """
    welcome_1stì™€ welcome_2ndë¥¼ ì¡°ì¸í•˜ì—¬ í†µí•© ì¸ë±ìŠ¤ ìƒì„±
    
    Args:
        os_client: OpenSearch í´ë¼ì´ì–¸íŠ¸
        source_index_1st: welcome_1st ì¸ë±ìŠ¤ ì´ë¦„
        source_index_2nd: welcome_2nd ì¸ë±ìŠ¤ ì´ë¦„
        target_index: ìƒì„±í•  ì¡°ì¸ëœ ì¸ë±ìŠ¤ ì´ë¦„
        batch_size: ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°
    
    Returns:
        ì¡°ì¸ í†µê³„ ì •ë³´
    """
    stats = {
        "total_processed": 0,
        "successful_joins": 0,
        "missing_1st": 0,
        "missing_2nd": 0,
        "errors": 0
    }
    
    # 1. ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸
    if not os_client.indices.exists(index=source_index_1st):
        raise ValueError(f"ì†ŒìŠ¤ ì¸ë±ìŠ¤ '{source_index_1st}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    if not os_client.indices.exists(index=source_index_2nd):
        raise ValueError(f"ì†ŒìŠ¤ ì¸ë±ìŠ¤ '{source_index_2nd}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    # 2. íƒ€ê²Ÿ ì¸ë±ìŠ¤ ìƒì„± (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì‚­ì œ í›„ ì¬ìƒì„±)
    if os_client.indices.exists(index=target_index):
        logger.warning(f"âš ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ '{target_index}' ì‚­ì œ ì¤‘...")
        os_client.indices.delete(index=target_index)
    
    logger.info(f"âœ¨ ì¡°ì¸ëœ ì¸ë±ìŠ¤ '{target_index}' ìƒì„± ì¤‘...")
    mapping = create_joined_index_mapping()
    os_client.indices.create(index=target_index, body=mapping)
    logger.info(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    # 3. welcome_1stì˜ ëª¨ë“  user_id ìˆ˜ì§‘
    logger.info(f"ğŸ“Š '{source_index_1st}'ì—ì„œ user_id ìˆ˜ì§‘ ì¤‘...")
    user_ids_1st = set()
    
    query = {
        "size": 0,
        "aggs": {
            "user_ids": {
                "terms": {
                    "field": "user_id",
                    "size": 10000  # ìµœëŒ€ 10000ê°œ (í•„ìš”ì‹œ ì¦ê°€)
                }
            }
        }
    }
    
    response = os_client.search(index=source_index_1st, body=query)
    for bucket in response['aggregations']['user_ids']['buckets']:
        user_ids_1st.add(bucket['key'])
    
    logger.info(f"âœ… '{source_index_1st}'ì—ì„œ {len(user_ids_1st)}ê°œì˜ user_id ë°œê²¬")
    
    # 4. welcome_2ndì˜ ëª¨ë“  user_id ìˆ˜ì§‘
    logger.info(f"ğŸ“Š '{source_index_2nd}'ì—ì„œ user_id ìˆ˜ì§‘ ì¤‘...")
    user_ids_2nd = set()
    
    response = os_client.search(index=source_index_2nd, body=query)
    for bucket in response['aggregations']['user_ids']['buckets']:
        user_ids_2nd.add(bucket['key'])
    
    logger.info(f"âœ… '{source_index_2nd}'ì—ì„œ {len(user_ids_2nd)}ê°œì˜ user_id ë°œê²¬")
    
    # 5. ëª¨ë“  user_idì˜ í•©ì§‘í•© (ì–‘ìª½ ì¸ë±ìŠ¤ì— ìˆëŠ” ëª¨ë“  ì‚¬ìš©ì)
    all_user_ids = user_ids_1st.union(user_ids_2nd)
    logger.info(f"ğŸ“Š ì´ {len(all_user_ids)}ê°œì˜ ê³ ìœ  user_id ë°œê²¬")
    
    # 6. ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¡°ì¸ ë° ì¸ë±ì‹±
    logger.info(f"ğŸ”„ ì¡°ì¸ ë° ì¸ë±ì‹± ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {batch_size})...")
    
    user_id_list = list(all_user_ids)
    for i in range(0, len(user_id_list), batch_size):
        batch = user_id_list[i:i + batch_size]
        batch_docs = []
        
        for user_id in batch:
            try:
                # welcome_1stì—ì„œ ë¬¸ì„œ ì¡°íšŒ
                doc_1st = None
                try:
                    response = os_client.get(index=source_index_1st, id=user_id, ignore=[404])
                    if response.get('found'):
                        doc_1st = response['_source']
                except Exception as e:
                    logger.debug(f"âš ï¸ '{source_index_1st}'ì—ì„œ {user_id} ì¡°íšŒ ì‹¤íŒ¨: {e}")
                
                # welcome_2ndì—ì„œ ë¬¸ì„œ ì¡°íšŒ
                doc_2nd = None
                try:
                    response = os_client.get(index=source_index_2nd, id=user_id, ignore=[404])
                    if response.get('found'):
                        doc_2nd = response['_source']
                except Exception as e:
                    logger.debug(f"âš ï¸ '{source_index_2nd}'ì—ì„œ {user_id} ì¡°íšŒ ì‹¤íŒ¨: {e}")
                
                # ì¡°ì¸ëœ ë¬¸ì„œ ìƒì„±
                joined_doc = {
                    "user_id": user_id,
                    "metadata": {},
                    "qa_pairs": [],
                    "source_indices": []
                }
                
                # welcome_1st ë°ì´í„° ë³‘í•©
                if doc_1st:
                    metadata_1st = doc_1st.get('metadata', {})
                    if metadata_1st:
                        joined_doc["metadata"].update({
                            "age_group": metadata_1st.get("age_group"),
                            "gender": metadata_1st.get("gender"),
                            "birth_year": metadata_1st.get("birth_year"),
                            "region": metadata_1st.get("region")
                        })
                    
                    # qa_pairs ë³‘í•©
                    qa_pairs_1st = doc_1st.get('qa_pairs', [])
                    if qa_pairs_1st:
                        joined_doc["qa_pairs"].extend(qa_pairs_1st)
                    
                    joined_doc["source_indices"].append(source_index_1st)
                    
                    # timestampëŠ” welcome_1st ê²ƒì„ ì‚¬ìš© (ì—†ìœ¼ë©´ welcome_2nd)
                    if 'timestamp' in doc_1st:
                        joined_doc["timestamp"] = doc_1st.get("timestamp")
                else:
                    stats["missing_1st"] += 1
                
                # welcome_2nd ë°ì´í„° ë³‘í•©
                if doc_2nd:
                    metadata_2nd = doc_2nd.get('metadata', {})
                    if metadata_2nd:
                        joined_doc["metadata"].update({
                            "occupation": metadata_2nd.get("occupation"),
                            "job_category": metadata_2nd.get("job_category")
                        })
                    
                    # qa_pairs ë³‘í•©
                    qa_pairs_2nd = doc_2nd.get('qa_pairs', [])
                    if qa_pairs_2nd:
                        joined_doc["qa_pairs"].extend(qa_pairs_2nd)
                    
                    joined_doc["source_indices"].append(source_index_2nd)
                    
                    # timestampê°€ ì—†ìœ¼ë©´ welcome_2nd ê²ƒì„ ì‚¬ìš©
                    if 'timestamp' not in joined_doc and 'timestamp' in doc_2nd:
                        joined_doc["timestamp"] = doc_2nd.get("timestamp")
                else:
                    stats["missing_2nd"] += 1
                
                # ìµœì†Œí•œ í•˜ë‚˜ì˜ ì¸ë±ìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™”ìœ¼ë©´ ì¸ë±ì‹±
                if doc_1st or doc_2nd:
                    batch_docs.append({
                        "_index": target_index,
                        "_id": user_id,
                        "_source": joined_doc
                    })
                    stats["successful_joins"] += 1
                else:
                    stats["errors"] += 1
                
                stats["total_processed"] += 1
                
            except Exception as e:
                logger.error(f"âŒ user_id={user_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                stats["errors"] += 1
                stats["total_processed"] += 1
        
        # ë°°ì¹˜ ì¸ë±ì‹±
        if batch_docs:
            success, failed = bulk(os_client, batch_docs, raise_on_error=False)
            logger.info(f"âœ… ë°°ì¹˜ {i//batch_size + 1}: {success}ê±´ ì„±ê³µ, {len(failed)}ê±´ ì‹¤íŒ¨")
            
            if failed:
                for item in failed:
                    logger.error(f"âŒ ì¸ë±ì‹± ì‹¤íŒ¨: {item}")
    
    # 7. ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨
    logger.info("ğŸ”„ ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨ ì¤‘...")
    os_client.indices.refresh(index=target_index)
    
    # 8. í†µê³„ ì¶œë ¥
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š ì¡°ì¸ í†µê³„")
    logger.info("="*60)
    logger.info(f"ì´ ì²˜ë¦¬: {stats['total_processed']}ê±´")
    logger.info(f"ì„±ê³µì  ì¡°ì¸: {stats['successful_joins']}ê±´")
    logger.info(f"welcome_1st ëˆ„ë½: {stats['missing_1st']}ê±´")
    logger.info(f"welcome_2nd ëˆ„ë½: {stats['missing_2nd']}ê±´")
    logger.info(f"ì˜¤ë¥˜: {stats['errors']}ê±´")
    logger.info("="*60)
    
    return stats


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="welcome_1stì™€ welcome_2ndë¥¼ ì¡°ì¸í•˜ì—¬ í†µí•© ì¸ë±ìŠ¤ ìƒì„±")
    parser.add_argument(
        "--source-1st",
        default="s_welcome_1st",
        help="welcome_1st ì¸ë±ìŠ¤ ì´ë¦„ (ê¸°ë³¸ê°’: s_welcome_1st)"
    )
    parser.add_argument(
        "--source-2nd",
        default="s_welcome_2nd",
        help="welcome_2nd ì¸ë±ìŠ¤ ì´ë¦„ (ê¸°ë³¸ê°’: s_welcome_2nd)"
    )
    parser.add_argument(
        "--target",
        default="s_users_joined",
        help="ìƒì„±í•  ì¡°ì¸ëœ ì¸ë±ìŠ¤ ì´ë¦„ (ê¸°ë³¸ê°’: s_users_joined)"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1000,
        help="ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (ê¸°ë³¸ê°’: 1000)"
    )
    
    args = parser.parse_args()
    
    # ì„¤ì • ë¡œë“œ ë° OpenSearch í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    config = get_config()
    os_client = OpenSearch(
        hosts=[{'host': config.OPENSEARCH_HOST, 'port': config.OPENSEARCH_PORT}],
        http_auth=(config.OPENSEARCH_USER, config.OPENSEARCH_PASSWORD),
        use_ssl=config.OPENSEARCH_USE_SSL,
        verify_certs=False,
        ssl_show_warn=False,
        request_timeout=30
    )
    
    if not os_client or not os_client.ping():
        logger.error("âŒ OpenSearch ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    logger.info("="*60)
    logger.info("ğŸš€ ì¡°ì¸ëœ ì¸ë±ìŠ¤ ìƒì„± ì‹œì‘")
    logger.info("="*60)
    logger.info(f"ì†ŒìŠ¤ ì¸ë±ìŠ¤ 1: {args.source_1st}")
    logger.info(f"ì†ŒìŠ¤ ì¸ë±ìŠ¤ 2: {args.source_2nd}")
    logger.info(f"íƒ€ê²Ÿ ì¸ë±ìŠ¤: {args.target}")
    logger.info(f"ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    logger.info("="*60)
    
    try:
        stats = join_documents(
            os_client=os_client,
            source_index_1st=args.source_1st,
            source_index_2nd=args.source_2nd,
            target_index=args.target,
            batch_size=args.batch_size
        )
        
        logger.info("\nâœ… ì¡°ì¸ëœ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")
        logger.info(f"ğŸ“ ì¸ë±ìŠ¤ ì´ë¦„: {args.target}")
        logger.info(f"ğŸ“Š ì´ ë¬¸ì„œ ìˆ˜: {stats['successful_joins']}ê±´")
        
    except Exception as e:
        logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    main()

