"""ë©”ì¸ FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜"""
import os
import logging
import torch
from typing import Optional
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from opensearchpy import OpenSearch, AsyncOpenSearch
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
import redis
import anthropic
# ì„¤ì •
from rag_query_analyzer.config import get_config, Config

# ë¼ìš°í„°
from indexer.router import router as indexer_router
from .search_api import router as search_router
from .visualization_api import router as visualization_router
from .visualization_qa_api import router as visualization_qa_router
from .search.refine_api import router as refine_router

# --- ë¡œê¹… ì„¤ì • ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- ì „ì—­ ë³€ìˆ˜ ---
config: Config = None
os_client: OpenSearch = None
async_os_client: Optional[AsyncOpenSearch] = None
qdrant_client: QdrantClient = None
embedding_model: SentenceTransformer = None


def create_app() -> FastAPI:
    """
    FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„± ë° ì´ˆê¸°í™”

    Returns:
        ì´ˆê¸°í™”ëœ FastAPI ì•± ì¸ìŠ¤í„´ìŠ¤
    """
    global config, os_client, async_os_client, qdrant_client, embedding_model

    try:
        # ì„¤ì • ë¡œë“œ
        config = get_config()

        # FastAPI ì•± ìƒì„±
        app = FastAPI(
            title="RAG Query Analyzer API",
            description="OpenSearch ê¸°ë°˜ ì„¤ë¬¸ì¡°ì‚¬ ë°ì´í„° ìƒ‰ì¸ ë° ê²€ìƒ‰ API",
            version="4.0.0 (Refactored)"
        )

        # CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
        # ê°œë°œ í™˜ê²½ì—ì„œëŠ” ëª¨ë“  ì˜¤ë¦¬ì§„ í—ˆìš©, í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ ì§€ì •
        is_production = os.getenv("APP_ENV", "development").lower() == "production"
        
        if is_production:
            # í”„ë¡œë•ì…˜: ëª…ì‹œì ìœ¼ë¡œ í—ˆìš©í•  ì˜¤ë¦¬ì§„ ì§€ì •
            allowed_origins = [
                "http://localhost:5173",
                "http://localhost:5174",
                "http://localhost:3000",
                "http://localhost:8080",
                "http://127.0.0.1:5173",
                "http://127.0.0.1:5174",
                "http://127.0.0.1:3000",
                "http://127.0.0.1:8080",

                # í”„ë¡ íŠ¸ì—”ë“œ í”„ë¡œë•ì…˜ ë„ë©”ì¸
                "https://data-panel-fe-six.vercel.app",
            ]
            # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì¶”ê°€ ì˜¤ë¦¬ì§„ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ
            extra_origins = os.getenv("CORS_ORIGINS", "").split(",")
            allowed_origins.extend([origin.strip() for origin in extra_origins if origin.strip()])
            allow_creds = True
        else:
            # ê°œë°œ í™˜ê²½: ëª¨ë“  ì˜¤ë¦¬ì§„ í—ˆìš© (ì™€ì¼ë“œì¹´ë“œ ì‚¬ìš© ì‹œ credentialsëŠ” False)
            allowed_origins = ["*"]
            allow_creds = False
        
        app.add_middleware(
            CORSMiddleware,
            allow_origins=allowed_origins,
            allow_credentials=True,
            allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "PATCH"],
            allow_headers=["*"],  # ëª¨ë“  í—¤ë” í—ˆìš©
            expose_headers=["*"],  # ì‘ë‹µ í—¤ë” ë…¸ì¶œ
        )

        # Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì„¤ì •
        try:
            from prometheus_fastapi_instrumentator import Instrumentator
            instrumentator = Instrumentator()
            instrumentator.instrument(app).expose(app, endpoint="/metrics")
            logger.info("[OK] Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í™œì„±í™”: /metrics")
        except ImportError:
            logger.warning("âš ï¸ prometheus-fastapi-instrumentatorê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë‹ˆí„°ë§ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

        # OpenSearch í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        logger.info("OpenSearch í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
        common_os_kwargs = dict(
            hosts=[{'host': config.OPENSEARCH_HOST, 'port': config.OPENSEARCH_PORT}],
            http_auth=(config.OPENSEARCH_USER, config.OPENSEARCH_PASSWORD),
            use_ssl=config.OPENSEARCH_USE_SSL,
            verify_certs=config.OPENSEARCH_VERIFY_CERTS,
            ssl_assert_hostname=config.OPENSEARCH_SSL_ASSERT_HOSTNAME,
            ssl_show_warn=False,
            request_timeout=180  # â­ íƒ€ì„ì•„ì›ƒ ì¦ê°€: ëŒ€ëŸ‰ ë°ì´í„° ì¡°íšŒ ëŒ€ì‘ (60ì´ˆ â†’ 180ì´ˆ, ì „ì²´ ë°ì´í„° ì•½ 35000ê°œ)
        )
        os_client = OpenSearch(**common_os_kwargs)
        logger.info("OpenSearch client initialized with settings: %s", common_os_kwargs)
        async_os_client = AsyncOpenSearch(**common_os_kwargs)
        logger.info("AsyncOpenSearch client initialized with settings: %s", common_os_kwargs)
        logger.info("[OK] OpenSearch í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (sync/async)")
        
        # â­ ì¸ë±ìŠ¤ max_result_window ì„¤ì • í™•ì¸ ë° ì—…ë°ì´íŠ¸ (ì „ì²´ ë°ì´í„° ì•½ 35000ê°œ ëŒ€ì‘)
        try:
            from rag_query_analyzer.utils.opensearch_utils import ensure_max_result_window
            default_index = config.OPENSEARCH_INDEX if hasattr(config, 'OPENSEARCH_INDEX') else "survey_responses_merged"
            if os_client.indices.exists(index=default_index):
                ensure_max_result_window(os_client, default_index, max_result_window=50000)
            else:
                logger.warning(f"âš ï¸ ì¸ë±ìŠ¤ {default_index}ê°€ ì¡´ì¬í•˜ì§€ ì•Šì•„ max_result_window ì„¤ì •ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        except Exception as e:
            logger.warning(f"âš ï¸ max_result_window ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ê³„ì† ì§„í–‰): {e}")

        # Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        logger.info("Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
        qdrant_host = os.getenv("QDRANT_HOST", "104.248.144.17")
        qdrant_port = int(os.getenv("QDRANT_PORT", "6333"))
        qdrant_client = QdrantClient(host=qdrant_host, port=qdrant_port, timeout=30)
        logger.info(f"[OK] Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {qdrant_host}:{qdrant_port}")

        # Redis í´ë¼ì´ì–¸íŠ¸ (ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ ë° ë¡œê·¸)
        redis_client = None
        if config.REDIS_URL:
            try:
                redis_client = redis.Redis.from_url(config.REDIS_URL, decode_responses=True)
                redis_client.ping()
                logger.info(f"[OK] Redis ì—°ê²° ì„±ê³µ: {config.REDIS_URL}")
            except Exception as e:
                redis_client = None
                logger.warning(f"âš ï¸ Redis ì—°ê²° ì‹¤íŒ¨ ({config.REDIS_URL}): {e}")

        # Claude/Anthropic í´ë¼ì´ì–¸íŠ¸ (ì¬ì‚¬ìš©)
        anthropic_client = None
        if config.CLAUDE_API_KEY:
            try:
                anthropic_client = anthropic.Anthropic(api_key=config.CLAUDE_API_KEY)
                logger.info("[OK] Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                anthropic_client = None
                logger.warning(f"âš ï¸ Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

        # ì„ë² ë”© ëª¨ë¸ ë¡œë”© (KURE-v1)
        logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {config.EMBEDDING_MODEL}")
        embedding_model = SentenceTransformer(
            config.EMBEDDING_MODEL,
            device='cuda' if torch.cuda.is_available() else 'cpu'
        )
        embedding_model.max_seq_length = 512
        logger.info(f"[OK] KURE-v1 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì°¨ì›: {config.EMBEDDING_DIM}, ì¥ì¹˜: {embedding_model.device})")

        # ë¼ìš°í„°ì— ì˜ì¡´ì„± ì£¼ì…
        indexer_router.os_client = os_client
        indexer_router.embedding_model = embedding_model
        search_router.os_client = os_client
        search_router.async_os_client = async_os_client
        search_router.qdrant_client = qdrant_client
        search_router.embedding_model = embedding_model
        search_router.embedding_model_factory = lambda: embedding_model
        search_router.config = config
        search_router.redis_client = redis_client
        search_router.cache_ttl_seconds = config.SEARCH_CACHE_TTL_SECONDS
        search_router.cache_max_results = config.SEARCH_CACHE_MAX_RESULTS
        search_router.cache_prefix = "search:results"
        search_router.conversation_history_prefix = config.CONVERSATION_HISTORY_PREFIX
        search_router.conversation_history_ttl_seconds = config.CONVERSATION_HISTORY_TTL_SECONDS
        search_router.conversation_history_max_messages = config.CONVERSATION_HISTORY_MAX_MESSAGES
        search_router.search_history_prefix = config.SEARCH_HISTORY_PREFIX
        search_router.search_history_ttl_seconds = config.SEARCH_HISTORY_TTL_SECONDS
        search_router.search_history_max_entries = config.SEARCH_HISTORY_MAX_ENTRIES
        search_router.enable_search_summary = config.ENABLE_SEARCH_SUMMARY
        search_router.search_summary_max_results = config.SEARCH_SUMMARY_MAX_RESULTS
        search_router.search_summary_max_chars = config.SEARCH_SUMMARY_MAX_CHARS
        search_router.search_summary_model = (
            config.SEARCH_SUMMARY_MODEL or config.CLAUDE_MODEL
        )
        search_router.anthropic_client = anthropic_client

        # Visualization ë¼ìš°í„°ì— ì˜ì¡´ì„± ì£¼ì…
        visualization_router.os_client = os_client
        visualization_qa_router.os_client = os_client


        # Refine ë¼ìš°í„°ì— ì˜ì¡´ì„± ì£¼ì… (search_routerì™€ ë™ì¼í•œ ì˜ì¡´ì„± ê³µìœ )
        refine_router.os_client = os_client
        refine_router.anthropic_client = anthropic_client
        refine_router.config = config
        refine_router.redis_client = redis_client
        refine_router.conversation_history_prefix = config.CONVERSATION_HISTORY_PREFIX

        # ì‹œì‘ ì´ë²¤íŠ¸ ë“±ë¡
        @app.on_event("startup")
        async def startup_event():
            """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ì—°ê²° ìƒíƒœ í™•ì¸"""
            import asyncio
            try:
                logger.info("=" * 60)
                logger.info("RAG Query Analyzer API ì‹œì‘")
                logger.info("=" * 60)
                
                # ë™ì˜ì–´ í™•ì¥ê¸° ì´ˆê¸°í™” (ì •ì  ì‚¬ì „ + Qdrant ë™ì  í™•ì¥)
                try:
                    from rag_query_analyzer.utils.synonym_expander import get_synonym_expander
                    expander = get_synonym_expander(
                        qdrant_client=qdrant_client,
                        embedding_model=embedding_model
                    )
                    stats = expander.get_stats()
                    logger.info("ğŸ“š ë™ì˜ì–´ í™•ì¥ê¸° ì •ë³´:")
                    logger.info(f"   - Terms: {stats['total_terms']}ê°œ")
                    logger.info(f"   - ë™ì˜ì–´: {stats['total_synonyms']}ê°œ")
                    logger.info(f"   - í‰ê· : {stats['avg_synonyms']:.1f}ê°œ/term")
                    logger.info(f"   - íŒŒì¼: {stats['loaded_from']}")
                    logger.info(f"   - Qdrant ë™ì  í™•ì¥: {'í™œì„±í™”' if stats['dynamic_enabled'] else 'ë¹„í™œì„±í™”'}")
                    if stats['dynamic_enabled']:
                        logger.info(f"   - ë™ì  ìºì‹œ í¬ê¸°: {stats['dynamic_cache_size']}/{stats['cache_size_limit']}")
                except Exception as e:
                    logger.warning(f"âš ï¸  ë™ì˜ì–´ í™•ì¥ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    logger.info(f"   ìƒì„± ë°©ë²•: python scripts/generate_synonyms.py")
                logger.info("=" * 60)

                # OpenSearch ì—°ê²° í™•ì¸
                try:
                    if os_client.ping():
                        logger.info("[OK] OpenSearch ì—°ê²° ì„±ê³µ")
                        info = os_client.info()
                        logger.info(f"   - ë²„ì „: {info['version']['number']}")
                        logger.info(f"   - í´ëŸ¬ìŠ¤í„°: {info['cluster_name']}")
                    else:
                        logger.warning("[WARNING] OpenSearch ì—°ê²° ì‹¤íŒ¨")
                except Exception as e:
                    logger.warning(f"[WARNING] OpenSearch ì—°ê²° ì‹¤íŒ¨: {e}")

                # Async OpenSearch ì—°ê²° í™•ì¸
                try:
                    if async_os_client and await async_os_client.ping():
                        logger.info("[OK] Async OpenSearch ì—°ê²° ì„±ê³µ")
                except Exception as e:
                    logger.warning(f"[WARNING] Async OpenSearch ì—°ê²° ì‹¤íŒ¨: {e}")

                # â­ Panel ë°ì´í„° ë©”ëª¨ë¦¬ í”„ë¦¬ë¡œë“œ (ì´ˆê³ ì† ê²€ìƒ‰ì„ ìœ„í•œ ìµœì í™”)
                try:
                    logger.info("=" * 60)
                    logger.info("âš¡ Panel ë°ì´í„° ë©”ëª¨ë¦¬ í”„ë¦¬ë¡œë“œ ì‹œì‘...")
                    logger.info("=" * 60)

                    from connectors.data_fetcher import DataFetcher
                    from .search_api import panel_cache

                    data_fetcher = DataFetcher(
                        opensearch_client=os_client,
                        qdrant_client=qdrant_client,
                        async_opensearch_client=async_os_client
                    )

                    await panel_cache.initialize(data_fetcher, index_name="survey_responses_merged")

                    logger.info("=" * 60)
                    logger.info("âœ… Panel ë°ì´í„° í”„ë¦¬ë¡œë“œ ì™„ë£Œ!")
                    logger.info(f"   - ì „ì²´ íŒ¨ë„: {panel_cache.total_count:,}ëª…")
                    logger.info(f"   - ë¡œë”© ì‹œê°„: {panel_cache.load_time:.2f}ì´ˆ")
                    logger.info(f"   - ì´í›„ ê²€ìƒ‰ì€ 0.05-0.2ì´ˆ ì´ë‚´ ì‘ë‹µ ì˜ˆìƒ")
                    logger.info("=" * 60)
                except Exception as e:
                    logger.error(f"âŒ Panel ë°ì´í„° í”„ë¦¬ë¡œë“œ ì‹¤íŒ¨: {e}")
                    logger.warning("   â†’ ê¸°ì¡´ Scroll API ë°©ì‹ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤ (ëŠë¦¼)")

                # â­ RAG Query Analyzer ì‚¬ì „ ë¡œë”© (ì²« ê²€ìƒ‰ ì‘ë‹µ ì†ë„ ê°œì„ )
                try:
                    logger.info("=" * 60)
                    logger.info("ğŸ§  RAG Query Analyzer ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì‹œì‘...")
                    logger.info("=" * 60)

                    from rag_query_analyzer.config import get_config
                    from rag_query_analyzer.analyzers.main_analyzer import AdvancedRAGQueryAnalyzer
                    from .search_api import router as search_router

                    # Config ì´ˆê¸°í™”
                    config = get_config()
                    search_router.config = config

                    # Analyzer ì´ˆê¸°í™” (ëª¨ë“  ëª¨ë¸ ë¡œë”©)
                    analyzer = AdvancedRAGQueryAnalyzer(config)
                    search_router.analyzer = analyzer

                    logger.info("=" * 60)
                    logger.info("âœ… RAG Query Analyzer ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì™„ë£Œ!")
                    logger.info("   - SemanticModel, QueryRewriter, Reranker ë“± ëª¨ë‘ ë¡œë“œë¨")
                    logger.info("   - ì²« ê²€ìƒ‰ ìš”ì²­ë¶€í„° ë¹ ë¥¸ ì‘ë‹µ ê°€ëŠ¥")
                    logger.info("=" * 60)
                except Exception as e:
                    logger.error(f"âŒ RAG Query Analyzer ì‚¬ì „ ë¡œë”© ì‹¤íŒ¨: {e}")
                    logger.warning("   â†’ ì²« ê²€ìƒ‰ ìš”ì²­ ì‹œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤ (ì•½ê°„ ëŠë¦¼)")

                logger.info("\nì‚¬ìš© ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸:")
                logger.info("   - GET  /                          : API í™˜ì˜ ë©”ì‹œì§€")
                logger.info("   - GET  /health                    : í—¬ìŠ¤ ì²´í¬")
                logger.info("   - GET  /system-status             : ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸")
                logger.info("   - POST /indexer/index-survey-data : ì„¤ë¬¸ ë°ì´í„° ìƒ‰ì¸")
                logger.info("   - DELETE /indexer/index/{name}    : ì¸ë±ìŠ¤ ì‚­ì œ")
                logger.info("   - POST /search/nl                : ìì—°ì–´ ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰")
                logger.info("   - GET  /docs                      : API ë¬¸ì„œ (Swagger UI)")
                logger.info("   - GET  /redoc                     : API ë¬¸ì„œ (ReDoc)")
                logger.info("=" * 60 + "\n")
            except asyncio.CancelledError:
                # ì •ìƒì ì¸ ì¢…ë£Œ ê³¼ì •ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì·¨ì†Œ ì—ëŸ¬ëŠ” ë¬´ì‹œ
                pass
            except Exception as e:
                logger.error(f"âš ï¸ Startup ì´ë²¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

        @app.on_event("shutdown")
        async def shutdown_event():
            """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
            import asyncio
            logger.info("ğŸ›‘ ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ: ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
            try:
                if async_os_client:
                    try:
                        await asyncio.wait_for(async_os_client.close(), timeout=2.0)
                        logger.info("[OK] Async OpenSearch í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ")
                    except asyncio.CancelledError:
                        logger.info("[INFO] Async OpenSearch í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ ì·¨ì†Œë¨")
                    except asyncio.TimeoutError:
                        logger.warning("âš ï¸ Async OpenSearch í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ íƒ€ì„ì•„ì›ƒ")
            except asyncio.CancelledError:
                # ì •ìƒì ì¸ ì¢…ë£Œ ê³¼ì •ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì·¨ì†Œ ì—ëŸ¬ëŠ” ë¬´ì‹œ
                pass
            except Exception as e:
                logger.warning(f"âš ï¸ Async OpenSearch ì¢…ë£Œ ì‹¤íŒ¨: {e}")

        # ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸
        @app.get("/", summary="API í™˜ì˜ ë©”ì‹œì§€")
        def read_root():
            """API ê¸°ë³¸ ì •ë³´"""
            return {
                
                "message": "RAG Query Analyzer APIì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!",
                "version": "4.0.0",
                "description": "ì„¤ë¬¸ì¡°ì‚¬ ë°ì´í„°ë¥¼ OpenSearchì— ìƒ‰ì¸í•˜ê³  ê²€ìƒ‰í•©ë‹ˆë‹¤.",
                "endpoints": {
                    "docs": "/docs",
                    "redoc": "/redoc",
                    "health": "/health",
                    "system_status": "/system-status",
                    "indexer": "/indexer",
                    "search": "/search",
                    "visualization": "/visualization"
                }
            }

        
        # ë¼ìš°í„° ë“±ë¡
        app.include_router(indexer_router)
        app.include_router(search_router)
        app.include_router(visualization_router)
        app.include_router(visualization_qa_router)
        app.include_router(refine_router)

        return app

    except Exception as e:
        logger.critical(f"[ERROR] ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        raise


# ì•± ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
app = create_app()


if __name__ == "__main__":
    import uvicorn
    logger.info("FastAPI ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    uvicorn.run(app, host="0.0.0.0", port=8000)
