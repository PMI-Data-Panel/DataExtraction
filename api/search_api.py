"""ê²€ìƒ‰ API ë¼ìš°í„°"""
import asyncio
import json
import logging
import hashlib
from collections import defaultdict, OrderedDict
from time import perf_counter
from typing import List, Dict, Any, Optional, Set, Tuple
from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel, Field
from opensearchpy import OpenSearch

# ë¶„ì„ê¸° ë° ì¿¼ë¦¬ ë¹Œë”
from rag_query_analyzer.analyzers.main_analyzer import AdvancedRAGQueryAnalyzer
from rag_query_analyzer.analyzers.demographic_extractor import DemographicExtractor
from rag_query_analyzer.models.entities import DemographicType, DemographicEntity
from connectors.hybrid_searcher import OpenSearchHybridQueryBuilder, calculate_rrf_score
from connectors.data_fetcher import DataFetcher
from connectors.qdrant_helper import search_qdrant_async, search_qdrant_collections_async

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

router = APIRouter(
    prefix="/search",
    tags=["Search"]
)

# âš ï¸ ì„ì‹œ í™•ì¥ íƒ€ì„ì•„ì›ƒ (ì¤‘ì²© í•„í„° ì œê±° ì „ê¹Œì§€ 8~10ì´ˆ ìœ ì§€)
DEFAULT_OS_TIMEOUT = 10

# ëŸ°íƒ€ì„ ê³µìœ  ê°ì²´ (í•œ ë²ˆë§Œ ì´ˆê¸°í™” í›„ ì¬ì‚¬ìš©)
router.analyzer = None  # type: ignore[attr-defined]
router.embedding_model = None  # type: ignore[attr-defined]
router.config = None  # type: ignore[attr-defined]

# ê°„ë‹¨í•œ ì¸ë©”ëª¨ë¦¬ LRU ìºì‹œ (welcome ì¸ë±ìŠ¤ ì „ìš©)
_WELCOME_CACHE_MAX = 5000
_welcome_cache: Dict[str, OrderedDict] = {
    "s_welcome_1st": OrderedDict(),
    "s_welcome_2nd": OrderedDict(),
}


def _cache_get_welcome_doc(index_name: str, user_id: str) -> Optional[Dict[str, Any]]:
    bucket = _welcome_cache.get(index_name)
    if bucket is None:
        return None
    doc = bucket.get(user_id)
    if doc is not None:
        # ìµœê·¼ ì‚¬ìš© ì—…ë°ì´íŠ¸
        bucket.move_to_end(user_id)
    return doc


def _cache_put_welcome_doc(index_name: str, user_id: str, doc: Dict[str, Any]) -> None:
    bucket = _welcome_cache.get(index_name)
    if bucket is None:
        return
    bucket[user_id] = doc
    bucket.move_to_end(user_id)
    if len(bucket) > _WELCOME_CACHE_MAX:
        bucket.popitem(last=False)


def calculate_rrf_score_adaptive(
    keyword_results: List[Dict[str, Any]],
    vector_results: List[Dict[str, Any]],
    query_intent: Optional[str],
    has_filters: bool,
    use_vector_search: bool,
) -> Tuple[List[Dict[str, Any]], int, str]:
    """ì¿¼ë¦¬ íŠ¹ì„±ì— ë”°ë¼ RRF k ê°’ì„ ì¡°ì •"""
    k = 60
    reason = "ê· í˜• ìœ ì§€ (k=60)"

    if has_filters:
        k = 40
        reason = "í•„í„° ì ìš© â†’ ì •í™•ë„ ì¤‘ì‹œ (k=40)"
    elif use_vector_search and query_intent and query_intent.lower() in {"semantic", "semantic_search"}:
        k = 80
        reason = f"ì˜ë„={query_intent} â†’ ë²¡í„° ê°€ì¤‘ (k=80)"

    combined = calculate_rrf_score(
        keyword_results=keyword_results,
        vector_results=vector_results,
        k=k,
    )
    return combined, k, reason


def _sort_dict_recursive(obj: Any) -> Any:
    """ë”•ì…”ë„ˆë¦¬/ë¦¬ìŠ¤íŠ¸ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì •ë ¬"""
    if isinstance(obj, dict):
        return {key: _sort_dict_recursive(obj[key]) for key in sorted(obj)}
    if isinstance(obj, list):
        normalized_items = [_sort_dict_recursive(item) for item in obj]
        try:
            return sorted(
                normalized_items,
                key=lambda item: json.dumps(item, ensure_ascii=False, sort_keys=True),
            )
        except TypeError:
            return normalized_items
    return obj


def _normalize_filters_for_cache(filters: List[Dict[str, Any]]) -> str:
    """í•„í„° ëª©ë¡ì„ ì•ˆì •ì ì¸ ë¬¸ìì—´ë¡œ ë³€í™˜"""
    if not filters:
        return ""

    normalized_strings = []
    for filter_item in filters:
        normalized = _sort_dict_recursive(filter_item)
        normalized_strings.append(json.dumps(normalized, ensure_ascii=False, sort_keys=True))

    normalized_strings.sort()
    return "|".join(normalized_strings)


def _make_cache_key(
    *,
    prefix: str,
    query: str,
    index_name: str,
    page_size: int,
    use_vector: bool,
    must_terms: List[str],
    should_terms: List[str],
    must_not_terms: List[str],
    filters_signature: Optional[str] = None,
) -> str:
    """ìƒì„±ëœ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©í•˜ê¸° ìœ„í•œ ìºì‹œ í‚¤ ìƒì„± (ì•ˆì •í™”)"""
    stable_must = sorted(must_terms) if must_terms else []
    stable_should = sorted(should_terms) if should_terms else []
    stable_must_not = sorted(must_not_terms) if must_not_terms else []

    payload = {
        "query": query.strip().lower(),
        "index": index_name,
        "page_size": page_size,
        "use_vector": use_vector,
        "must_terms": stable_must,
        "should_terms": stable_should,
        "must_not_terms": stable_must_not,
        "filters_signature": filters_signature or "",
    }
    raw = json.dumps(payload, ensure_ascii=False, sort_keys=True)
    digest = hashlib.sha256(raw.encode("utf-8")).hexdigest()
    key = f"{prefix}:{digest}"

    logger.debug(f"ğŸ”‘ Cache key generated: {key}")
    logger.debug(f"   - must_terms: {stable_must}")
    logger.debug(f"   - should_terms: {stable_should}")
    logger.debug(f"   - filters_signature: {(filters_signature or '')[:100]}...")

    return key


def _serialize_result(result: "SearchResult") -> Dict[str, Any]:
    """SearchResultë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ dictë¡œ ë³€í™˜"""
    return result.model_dump()


def _deserialize_result(payload: Dict[str, Any]) -> "SearchResult":
    """dictë¥¼ SearchResult ê°ì²´ë¡œ ì—­ì§ë ¬í™”"""
    return SearchResult(**payload)


def _slice_results(
    serialized_items: List[Dict[str, Any]],
    page: int,
    page_size: int,
) -> Tuple[List["SearchResult"], bool]:
    """í˜ì´ì§€ ì •ë³´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²°ê³¼ë¥¼ ìŠ¬ë¼ì´ì‹±"""
    if page <= 0:
        page = 1
    start = (page - 1) * page_size
    end = start + page_size

    if start >= len(serialized_items):
        return [], False

    page_items = serialized_items[start:end]
    results = [_deserialize_result(item) for item in page_items]
    has_more = end < len(serialized_items)
    return results, has_more


def _build_cached_response(
    *,
    payload: Dict[str, Any],
    request: "SearchRequest",
    analysis,
    filters_for_response: List[Dict[str, Any]],
    overall_start: float,
    extracted_entities_dict: Optional[Dict[str, Any]] = None,
) -> "SearchResponse":
    """Redis ìºì‹œì—ì„œ ë¶ˆëŸ¬ì˜¨ ê²°ê³¼ë¡œ SearchResponse êµ¬ì„±"""
    total_hits = payload.get("total_hits", 0)
    max_score = payload.get("max_score", 0.0)
    serialized_items = payload.get("items", [])

    page_results, has_more_local = _slice_results(serialized_items, request.page, request.size)
    has_more = has_more_local and ((request.page * request.size) < total_hits)
    total_duration_ms = (perf_counter() - overall_start) * 1000

    timings = {
        "cache_hit": 1.0,
        "total_ms": total_duration_ms,
    }

    query_analysis = {
        "intent": analysis.intent,
        "must_terms": analysis.must_terms,
        "should_terms": analysis.should_terms,
        "alpha": analysis.alpha,
        "confidence": analysis.confidence,
        "filters": filters_for_response,
        "size": request.size,
        "timings_ms": timings,
    }
    if extracted_entities_dict is not None:
        query_analysis["extracted_entities"] = extracted_entities_dict

    return SearchResponse(
        query=request.query,
        total_hits=total_hits,
        max_score=max_score,
        results=page_results,
        query_analysis=query_analysis,
        took_ms=int(total_duration_ms),
        page=request.page,
        page_size=request.size,
        has_more=has_more,
    )


def build_occupation_dsl_filter(occupation_entities: List["DemographicEntity"]) -> Dict[str, Any]:
    """ì§ì—… DemographicEntity ë¦¬ìŠ¤íŠ¸ë¥¼ OpenSearch DSL í•„í„°ë¡œ ë³€í™˜"""
    if not occupation_entities:
        return {"match_all": {}}

    occupation_values: Set[str] = set()
    for demo in occupation_entities:
        for candidate in (
            getattr(demo, "raw_value", None),
            getattr(demo, "value", None),
        ):
            if candidate:
                occupation_values.add(str(candidate))
        synonyms = getattr(demo, "synonyms", None)
        if synonyms:
            for syn in synonyms:
                if syn:
                    occupation_values.add(str(syn))

    occupation_values = {value.strip() for value in occupation_values if value and value.strip()}
    if not occupation_values:
        return {"match_all": {}}

    values_list = sorted(occupation_values)

    question_should = [
        {"match_phrase": {"qa_pairs.q_text": "ì§ì—…"}},
        {"match_phrase": {"qa_pairs.q_text": "ì§ë¬´"}},
        {"match_phrase": {"qa_pairs.q_text": "occupation"}},
    ]

    answer_should = [
        {"terms": {"qa_pairs.answer.keyword": values_list}},
        {"terms": {"qa_pairs.answer_text.keyword": values_list}},
    ]
    for value in values_list:
        answer_should.append({"match": {"qa_pairs.answer": value}})
        answer_should.append({"match": {"qa_pairs.answer_text": value}})

    nested_filter = {
        "nested": {
            "path": "qa_pairs",
            "query": {
                "bool": {
                    "must": [
                        {
                            "bool": {
                                "should": question_should,
                                "minimum_should_match": 1,
                            }
                        },
                        {
                            "bool": {
                                "should": answer_should,
                                "minimum_should_match": 1,
                            }
                        },
                    ]
                }
            }
        }
    }

    metadata_terms = {
        "terms": {
            "metadata.occupation.keyword": values_list,
        }
    }
    metadata_job_terms = {
        "terms": {
            "metadata.job.keyword": values_list,
        }
    }

    return {
        "bool": {
            "should": [
                metadata_terms,
                metadata_job_terms,
                nested_filter,
            ],
            "minimum_should_match": 1,
        }
    }


def get_adaptive_score_threshold(
    query: str,
    has_filters: bool,
    must_terms_count: int,
) -> Tuple[float, str]:
    """ì¿¼ë¦¬ íŠ¹ì„±ì— ë”°ë¼ Qdrant score_threshold ì¡°ì •"""
    threshold = 0.30
    reason = "ê¸°ë³¸ê°’ 0.30"

    if has_filters:
        threshold = 0.25
        reason = "í•„í„° ì ìš© â†’ í›„ë³´ í™•ë³´ (threshold=0.25)"
    elif must_terms_count >= 3:
        threshold = 0.35
        reason = "í‚¤ì›Œë“œ ë‹¤ìˆ˜ â†’ ì •í™•ë„ ì¤‘ì‹œ (threshold=0.35)"
    elif len(query.split()) <= 3:
        threshold = 0.35
        reason = "ì§§ì€ ì¿¼ë¦¬ â†’ ì •ë°€ë„ ì¤‘ì‹œ (threshold=0.35)"

    return threshold, reason


def _collect_text_from_doc(doc: Dict[str, Any]) -> str:
    text_fragments: List[str] = []

    source = doc.get("_source") or doc.get("source") or {}
    if not source and "doc" in doc:
        source = doc.get("doc", {}).get("_source", {})
    if not source and "payload" in doc:
        payload = doc["payload"]
        if isinstance(payload, dict):
            source = {
                "payload_text": payload.get("text"),
                "payload": payload,
            }

    if isinstance(source, dict):
        for key in ("qa_pairs", "qaPairs"):
            qa_pairs = source.get(key, [])
            if isinstance(qa_pairs, list):
                for qa in qa_pairs:
                    if not isinstance(qa, dict):
                        continue
                    q_text = qa.get("q_text") or qa.get("question")
                    if q_text:
                        text_fragments.append(str(q_text).lower())
                    answer = qa.get("answer") or qa.get("answer_text") or qa.get("value")
                    if answer:
                        if isinstance(answer, list):
                            text_fragments.extend(str(item).lower() for item in answer)
                        else:
                            text_fragments.append(str(answer).lower())

        for key in ("metadata", "demographic_info", "payload"):
            meta = source.get(key)
            if isinstance(meta, dict):
                for value in meta.values():
                    if value:
                        text_fragments.append(str(value).lower())

        for key in ("title", "text", "content", "payload_text"):
            value = source.get(key)
            if value:
                text_fragments.append(str(value).lower())

    if "payload" in doc and isinstance(doc["payload"], dict):
        payload = doc["payload"]
        for key in ("text", "keywords"):
            value = payload.get(key)
            if isinstance(value, list):
                text_fragments.extend(str(item).lower() for item in value)
            elif value:
                text_fragments.append(str(value).lower())

    return " ".join(text_fragments)


def contains_must_terms(doc: Dict[str, Any], must_terms: List[str]) -> bool:
    if not must_terms:
        return True

    combined_text = _collect_text_from_doc(doc)
    if not combined_text:
        return False

    for term in must_terms:
        normalized = term.lower().strip()
        if normalized and normalized not in combined_text:
            return False
    return True


def _qa_contains_terms(qa: Dict[str, Any], terms_lower: List[str]) -> bool:
    if not isinstance(qa, dict):
        return False

    text_candidates: List[str] = []
    q_text = qa.get("q_text") or qa.get("question")
    if q_text:
        text_candidates.append(str(q_text).lower())

    answer = qa.get("answer") or qa.get("answer_text") or qa.get("value")
    if answer:
        if isinstance(answer, list):
            text_candidates.extend(str(item).lower() for item in answer)
        else:
            text_candidates.append(str(answer).lower())

    if not text_candidates:
        return False

    combined = " ".join(text_candidates)
    return all(term in combined for term in terms_lower if term)


def extract_matched_qa_pairs(source: Dict[str, Any], must_terms: List[str], limit: int = 5) -> List[Dict[str, Any]]:
    if not must_terms:
        return []
    qa_pairs = source.get("qa_pairs")
    if not isinstance(qa_pairs, list):
        return []

    terms_lower = [term.lower().strip() for term in must_terms if term]
    matched: List[Dict[str, Any]] = []
    for qa in qa_pairs:
        if _qa_contains_terms(qa, terms_lower):
            matched.append(qa)
            if len(matched) >= limit:
                break
    return matched


def get_display_qa_pairs(source: Dict[str, Any], must_terms: List[str], limit: int = 5) -> List[Dict[str, Any]]:
    qa_pairs = source.get("qa_pairs")
    if not isinstance(qa_pairs, list):
        return []

    if not must_terms:
        return qa_pairs[:limit]

    terms_lower = [term.lower().strip() for term in must_terms if term]
    matched = []
    others = []
    for qa in qa_pairs:
        if _qa_contains_terms(qa, terms_lower):
            matched.append(qa)
        else:
            others.append(qa)

    ordered = matched + others
    return ordered[:limit]


def extract_inner_hit_matches(hit: Dict[str, Any]) -> List[Dict[str, Any]]:
    inner_hits = hit.get("inner_hits")
    if not isinstance(inner_hits, dict):
        return []

    collected: List[Dict[str, Any]] = []
    for inner_name, inner_data in inner_hits.items():
        hits_obj = inner_data.get("hits", {}) if isinstance(inner_data, dict) else {}
        for inner_hit in hits_obj.get("hits", []):
            inner_source = inner_hit.get("_source", {}) or {}
            if "qa_pairs" in inner_source and isinstance(inner_source["qa_pairs"], dict):
                qa_entry = inner_source["qa_pairs"].copy()
            else:
                qa_entry = inner_source.copy()

            if not isinstance(qa_entry, dict):
                continue

            if "_score" in inner_hit and "match_score" not in qa_entry:
                qa_entry["match_score"] = inner_hit["_score"]
            if "highlight" in inner_hit and "highlights" not in qa_entry:
                qa_entry["highlights"] = inner_hit["highlight"]

            qa_entry.setdefault("inner_hit_name", inner_name)
            collected.append(qa_entry)

    return collected


def reorder_with_matches(full_list: List[Dict[str, Any]], matched: List[Dict[str, Any]], limit: int) -> List[Dict[str, Any]]:
    if not isinstance(full_list, list):
        return []

    if not matched:
        return full_list[:limit]

    def _key(qa: Dict[str, Any]) -> tuple:
        return (
            qa.get("q_text") or qa.get("question") or "",
            str(qa.get("answer") or qa.get("answer_text") or qa.get("value") or "")
        )

    seen = set()
    ordered: List[Dict[str, Any]] = []

    for qa in matched:
        key = _key(qa)
        if key not in seen:
            ordered.append(qa)
            seen.add(key)

    for qa in full_list:
        key = _key(qa)
        if key not in seen:
            ordered.append(qa)
            seen.add(key)

    return ordered[:limit]


class SearchRequest(BaseModel):
    """ê²€ìƒ‰ ìš”ì²­"""
    query: str = Field(..., description="ê²€ìƒ‰ ì¿¼ë¦¬")
    index_name: str = Field(
        default="welcome_all",
        description="ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ì´ë¦„ (ê¸°ë³¸ê°’: alias 'welcome_all'; ì™€ì¼ë“œì¹´ë“œ ì‚¬ìš© ê°€ëŠ¥)"
    )
    size: int = Field(default=10, ge=1, le=100, description="ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜")
    use_vector_search: bool = Field(default=True, description="ë²¡í„° ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€")
    page: int = Field(default=1, ge=1, description="ìš”ì²­í•  í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)")


class SearchResult(BaseModel):
    """ê²€ìƒ‰ ê²°ê³¼ í•­ëª©"""
    user_id: str
    score: float
    timestamp: Optional[str] = None
    demographic_info: Optional[Dict[str, Any]] = Field(default=None, description="ì¸êµ¬í†µê³„ ì •ë³´ (welcome_1st, welcome_2ndì—ì„œ ì¡°íšŒ)")
    qa_pairs: Optional[List[Dict[str, Any]]] = None
    matched_qa_pairs: Optional[List[Dict[str, Any]]] = None
    highlights: Optional[Dict[str, Any]] = None


class SearchResponse(BaseModel):
    """ê²€ìƒ‰ ì‘ë‹µ"""
    query: str
    total_hits: int
    max_score: Optional[float]
    results: List[SearchResult]
    query_analysis: Optional[Dict[str, Any]] = None
    took_ms: int
    page: int = Field(default=1, description="í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸")
    page_size: int = Field(default=10, description="í˜ì´ì§€ ë‹¹ ê²°ê³¼ ìˆ˜")
    has_more: bool = Field(default=False, description="ì¶”ê°€ í˜ì´ì§€ ì¡´ì¬ ì—¬ë¶€")



@router.get("/", summary="Search API ìƒíƒœ")
def search_root():
    """Search API ê¸°ë³¸ ì •ë³´"""
    return {
        "message": "Search API ì‹¤í–‰ ì¤‘",
        "version": "1.0",
        "endpoints": [
            "/search/query",
            "/search/similar"
        ]
    }



@router.post("/query", response_model=SearchResponse, summary="ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰")
async def search_query(
    request: SearchRequest,
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """
    ìì—°ì–´ ì¿¼ë¦¬ë¡œ ì„¤ë¬¸ ë°ì´í„° ê²€ìƒ‰

    - ì¿¼ë¦¬ ë¶„ì„ (ì˜ë„ íŒŒì•…, í‚¤ì›Œë“œ ì¶”ì¶œ)
    - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í‚¤ì›Œë“œ + ë²¡í„°)
    - ì¸êµ¬í†µê³„ í•„í„°ë§
    - ê²°ê³¼ ë­í‚¹ ë° í¬ë§¤íŒ…
    """
    try:
        # OpenSearch ì—°ê²° í™•ì¸
        if not os_client or not os_client.ping():
            raise HTTPException(
                status_code=503,
                detail="OpenSearch ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )

        # ì„ë² ë”© ëª¨ë¸ ë° ì„¤ì • í™•ì¸
        embedding_model = getattr(router, 'embedding_model', None)
        config = getattr(router, 'config', None)
        if config is None:
            from rag_query_analyzer.config import get_config
            config = get_config()
            router.config = config
        if embedding_model is None and hasattr(router, 'embedding_model_factory'):
            embedding_model = router.embedding_model_factory()
            router.embedding_model = embedding_model

        logger.info(f"\n{'='*60}")
        logger.info(f"[SEARCH] ê²€ìƒ‰ ì¿¼ë¦¬: '{request.query}'")
        logger.info(f"{'='*60}")

        # 1ë‹¨ê³„: ì¿¼ë¦¬ ë¶„ì„
        logger.info("\n[1/3] ì¿¼ë¦¬ ë¶„ì„ ì¤‘...")
        analyzer = getattr(router, 'analyzer', None)
        if analyzer is None:
            analyzer = AdvancedRAGQueryAnalyzer(config)
            router.analyzer = analyzer
        query_analysis = analyzer.analyze_query(request.query)
        analysis = query_analysis

        logger.info(f"   - ì˜ë„: {query_analysis.intent}")
        logger.info(f"   - must_terms: {query_analysis.must_terms}")
        logger.info(f"   - should_terms: {query_analysis.should_terms}")
        logger.info(f"   - alpha: {query_analysis.alpha}")

        timings: Dict[str, float] = {}
        overall_start = perf_counter()

        # í˜ì´ì§€ ë° ìºì‹œ ì„¤ì •
        page_size = max(1, request.size)
        page = max(1, request.page)
        requested_window = page_size * page
        cache_client = getattr(router, "redis_client", None)
        cache_ttl = getattr(router, "cache_ttl_seconds", 0)
        cache_limit = getattr(router, "cache_max_results", requested_window)
        cache_prefix = getattr(router, "cache_prefix", "search:results")
        cache_enabled = bool(cache_client) and cache_ttl > 0
        window_size = max(page_size, requested_window)
        if cache_limit > 0:
            window_size = min(window_size, cache_limit)
        filters_for_response: List[Dict[str, Any]] = []
        filters_signature = _normalize_filters_for_cache(filters_for_response)
        cache_key = None
        cache_hit = False

        if cache_enabled:
            try:
                cache_key = _make_cache_key(
                    prefix=cache_prefix,
                    query=request.query,
                    index_name=request.index_name,
                    page_size=page_size,
                    use_vector=request.use_vector_search,
                    must_terms=analysis.must_terms or [],
                    should_terms=analysis.should_terms or [],
                    must_not_terms=getattr(analysis, "must_not_terms", []) or [],
                    filters_signature=filters_signature,
                )
                cached_raw = cache_client.get(cache_key)
                if cached_raw:
                    cache_payload = json.loads(cached_raw)
                    cache_hit = True
                    logger.info(f"ğŸ” Redis ê²€ìƒ‰ ìºì‹œ íˆíŠ¸: key={cache_key}")
                    return _build_cached_response(
                        payload=cache_payload,
                        request=request,
                        analysis=analysis,
                        filters_for_response=filters_for_response,
                        overall_start=overall_start,
                    )
            except Exception as cache_exc:
                logger.warning(f"âš ï¸ Redis ê²€ìƒ‰ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {cache_exc}")
                cache_key = None
                cache_enabled = False

        # 2ë‹¨ê³„: ì¿¼ë¦¬ ë¹Œë“œ
        logger.info("\n[2/3] ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ì¤‘...")
        query_builder = OpenSearchHybridQueryBuilder(config)

        # ì„ë² ë”© ë²¡í„° ìƒì„±
        query_vector = None
        if request.use_vector_search and embedding_model:
            query_vector = embedding_model.encode(request.query).tolist()
            logger.info(f"   - ì¿¼ë¦¬ ë²¡í„° ìƒì„± ì™„ë£Œ (dim: {len(query_vector)})")

        # OpenSearch ì¿¼ë¦¬ ìƒì„±
        os_query = query_builder.build_query(
            analysis=query_analysis,
            query_vector=query_vector,
            size=window_size
        )

        # 3ë‹¨ê³„: ê²€ìƒ‰ ì‹¤í–‰
        logger.info("\n[3/3] ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")

        # OpenSearchëŠ” bodyì— ë°˜ë“œì‹œ ê°ì²´ í˜•íƒœì˜ queryê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        # í•˜ì´ë¸Œë¦¬ë“œ ë¹Œë”ê°€ í‚¤ì›Œë“œê°€ ì—†ì„ ë•Œ {'query': None}ì„ ëŒë ¤ì£¼ëŠ” ê²½ìš°ê°€ ìˆì–´,
        # ê·¸ëŒ€ë¡œ ì „ë‹¬í•˜ë©´ parsing_exceptionì´ ë°œìƒí•˜ë¯€ë¡œ match_allë¡œ ì¹˜í™˜í•©ë‹ˆë‹¤.
        if os_query.get("query") in (None, {}):
            logger.warning("âš ï¸ ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ë¹„ì–´ ìˆì–´ match_all ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤")
            os_query["query"] = {"match_all": {}}

        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (OpenSearch + Qdrant + RRF)
        if request.use_vector_search and query_vector and hasattr(router, 'qdrant_client'):
            logger.info("   - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ëª¨ë“œ (OpenSearch + Qdrant + RRF)")

            # OpenSearch í‚¤ì›Œë“œ ê²€ìƒ‰
            logger.info("   - [1/3] OpenSearch í‚¤ì›Œë“œ ê²€ìƒ‰...")
            
            # OpenSearch _source filtering: í•„ìš”í•œ í•„ë“œë§Œ ì¡°íšŒ
            source_filter = {
                "includes": ["user_id", "metadata", "timestamp"],
                "excludes": []
            }

            # ------------------------------------------------------------
            # OpenSearch ê²€ìƒ‰ (í•„ìš”ì‹œ ë³‘ë ¬ ì‹¤í–‰)
            # ------------------------------------------------------------
           

            data_fetcher = DataFetcher(
                opensearch_client=os_client,
                qdrant_client=getattr(router, 'qdrant_client', None),
                async_opensearch_client=getattr(router, 'async_os_client', None)
            )
            # â­ í•„í„°ê°€ ìˆëŠ” ê²½ìš°, êµì§‘í•©ì„ ìœ„í•´ ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì•¼ í•¨
            has_filters = bool(os_query.get('query', {}).get('bool', {}).get('must'))
            
            # Qdrant top-N ì œí•œ: í•„í„° ìœ ë¬´ì— ë”°ë¼ ë¶„ê¸°
            if has_filters:
                # í•„í„° ìˆìŒ: í›„ë³´ ìˆ˜ë¥¼ ì¤„ì—¬ í›„ì²˜ë¦¬ ë¶€ë‹´ ì™„í™”
                qdrant_limit = min(300, max(150, window_size * 5 // max(page, 1)))
                search_size = max(500, min(window_size * 15, 3000))
                logger.info(f"ğŸ” í•„í„° ì ìš©: OpenSearch size={search_size}, Qdrant limit={qdrant_limit} (size*5 ì „ëµ)")
            else:
                # í•„í„° ì—†ìŒ: ì†ŒëŸ‰ë§Œ ì½ê¸°
                qdrant_limit = min(150, max(60, window_size * 2 // max(page, 1)))
                search_size = max(window_size * 2, 200)
                logger.info(f"ğŸ” í•„í„° ì—†ìŒ: OpenSearch size={search_size}, Qdrant limit={qdrant_limit}")
            
            # OpenSearch _source filtering: í•„ìš”í•œ í•„ë“œë§Œ ì¡°íšŒ
            source_filter = {
                "includes": ["user_id", "metadata", "timestamp"],
                "excludes": []  # í•„ìš”ì‹œ ì œì™¸í•  í•„ë“œ ì¶”ê°€
            }
            
            os_response = data_fetcher.search_opensearch(
                index_name=request.index_name,
                query=os_query,
                size=search_size,
                source_filter=source_filter,
                request_timeout=DEFAULT_OS_TIMEOUT,
            )
            logger.info(f"      â†’ OpenSearch: {len(os_response['hits']['hits'])}ê±´")

            inner_hits_map: Dict[str, List[Dict[str, Any]]] = {}
            for hit in os_response['hits']['hits']:
                user_id = hit.get('_source', {}).get('user_id') or hit.get('_id')
                if not user_id:
                    continue
                matches = extract_inner_hit_matches(hit)
                if matches:
                    inner_hits_map[user_id] = matches
                    logger.debug(
                        "[inner_hits_map] user_id=%s matches=%d", user_id, len(matches)
                    )

            # Qdrant ë²¡í„° ê²€ìƒ‰ (ëª¨ë“  ì»¬ë ‰ì…˜)
            logger.info("   - [2/3] Qdrant ë²¡í„° ê²€ìƒ‰ (ëª¨ë“  ì»¬ë ‰ì…˜)...")
            qdrant_client = router.qdrant_client

            collection_names: List[str] = []
            try:
                collections = qdrant_client.get_collections()
                collection_names = [col.name for col in collections.collections]
                logger.info(f"      â†’ ê²€ìƒ‰í•  ì»¬ë ‰ì…˜: {collection_names}")
            except Exception as e:
                logger.warning(f"      â†’ Qdrant ì»¬ë ‰ì…˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")

            qdrant_results_raw = []
            if collection_names:
                try:
                    qdrant_start = perf_counter()
                    adaptive_threshold, threshold_reason = get_adaptive_score_threshold(
                        query=request.query,
                        has_filters=has_demographic_filters,
                        must_terms_count=len(getattr(analysis, "must_terms", []) or []),
                    )
                    results_map = await search_qdrant_collections_async(
                        qdrant_client=qdrant_client,
                        collection_names=collection_names,
                        query_vector=query_vector,
                        limit=qdrant_limit,
                        score_threshold=adaptive_threshold,
                    )
                    duration_ms = (perf_counter() - qdrant_start) * 1000
                    for name, items in results_map.items():
                        logger.info(f"      â†’ {name}: {len(items)}ê±´ (limit={qdrant_limit})")
                        qdrant_results_raw.extend(items)
                    logger.info(
                        f"      â†’ ë³‘ë ¬ Qdrant ê²€ìƒ‰ ì™„ë£Œ: {len(qdrant_results_raw)}ê±´ "
                        f"(ì´ ì»¬ë ‰ì…˜ {len(collection_names)}ê°œ, {duration_ms:.1f}ms)"
                    )
                except Exception as e:
                    logger.warning(f"      â†’ Qdrant ë³‘ë ¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            qdrant_results_raw.sort(key=lambda x: x.get('_score', 0.0), reverse=True)
            qdrant_results_raw = qdrant_results_raw[:qdrant_limit]

            # RRFë¡œ ê²°í•©
            logger.info("   - [3/3] RRF ê²°í•© ì¤‘...")
            keyword_results = os_response['hits']['hits']
            vector_results = [
                {
                    '_id': item.get('_id'),
                    '_score': item.get('_score'),
                    '_source': item.get('_source', {})
                }
                for item in qdrant_results_raw
            ]

            combined_results, rrf_k_used, rrf_reason = calculate_rrf_score_adaptive(
                keyword_results=keyword_results,
                vector_results=vector_results,
                query_intent=getattr(analysis, "intent", None),
                has_filters=has_demographic_filters,
                use_vector_search=request.use_vector_search,
            )

            must_terms: List[str] = []
            if getattr(analysis, "must_terms", None):
                must_terms = [term for term in analysis.must_terms if term]
                if must_terms:
                    logger.info(f"   - Must-term ê²€ì¦ ì‹œì‘: {must_terms}")
                    before_count = len(combined_results)
                    combined_results = [
                        doc for doc in combined_results if contains_must_terms(doc, must_terms)
                    ]
                    removed = before_count - len(combined_results)
                    logger.info(
                        f"   - Must-term ê²€ì¦ ì™„ë£Œ: {len(combined_results)}/{before_count}ê±´ ìœ ì§€"
                    )
                    if removed > 0:
                        logger.warning(
                            f"     âš ï¸ Must-term ë¯¸ì¼ì¹˜ ë¬¸ì„œ {removed}ê±´ ì œê±° (Qdrant ë­í¬ ì œì™¸)"
                        )

            # ìƒìœ„ Nê°œë§Œ ì„ íƒ
            final_hits = combined_results[:window_size]
            logger.info(f"      â†’ RRF ê²°í•© ì™„ë£Œ: {len(final_hits)}ê±´")

            # ìµœì¢… ìƒì„¸ ì •ë³´ (_mget) ì¡°íšŒ
            user_docs_map: Dict[str, Dict[str, Any]] = {}
            user_ids_for_mget: List[str] = []
            for doc in final_hits:
                source_candidate = doc.get('_source') or {}
                if not source_candidate and 'doc' in doc:
                    source_candidate = doc.get('doc', {}).get('_source', {})
                user_id_candidate = (
                    source_candidate.get('user_id')
                    or doc.get('_id')
                    or doc.get('id')
                )
                if not user_id_candidate and 'payload' in doc:
                    payload = doc['payload']
                    if isinstance(payload, dict):
                        user_id_candidate = payload.get('user_id')
                if user_id_candidate and user_id_candidate not in user_docs_map:
                    user_docs_map[user_id_candidate] = source_candidate if isinstance(source_candidate, dict) else {}
                    user_ids_for_mget.append(user_id_candidate)

            if user_ids_for_mget:
                try:
                    final_docs_raw = await data_fetcher.multi_get_documents_async(
                        index_name=request.index_name,
                        doc_ids=user_ids_for_mget,
                        source_fields=["user_id", "metadata", "demographic_info", "qa_pairs", "timestamp"],
                    )
                    for doc_item in final_docs_raw:
                        if not isinstance(doc_item, dict):
                            continue
                        if not doc_item.get('found'):
                            continue
                        doc_id = doc_item.get('_id')
                        src = doc_item.get('_source', {})
                        if doc_id and isinstance(src, dict):
                            user_docs_map[doc_id] = src
                except Exception as e:
                    logger.warning(f"     âš ï¸ ìµœì¢… ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨: {e}")

            # ê²°ê³¼ í¬ë§¤íŒ… (RRF ìˆœì„œ ìœ ì§€)
            results = []
            for doc in final_hits:
                source = doc.get('_source') or {}
                if not source and 'doc' in doc:
                    source = doc.get('doc', {}).get('_source', {})
                if not source and 'payload' in doc:
                    source = doc['payload']

                if isinstance(source, dict) and 'payload' in source and 'user_id' not in source:
                    payload = source['payload']
                    user_id = payload.get('user_id', '') if isinstance(payload, dict) else ''
                else:
                    user_id = source.get('user_id') or doc.get('_id', '')

                if user_id and user_id in user_docs_map:
                    merged_source = {}
                    if isinstance(source, dict):
                        merged_source.update(source)
                    merged_source.update(user_docs_map[user_id])
                    source = merged_source
                    logger.debug(
                        "[mget_merge] user_id=%s qa_pairs=%d", 
                        user_id,
                        len(source.get('qa_pairs', [])) if isinstance(source, dict) else -1,
                    )
                elif isinstance(source, dict) and user_id:
                    user_docs_map.setdefault(user_id, source)

                qa_pairs_display = get_display_qa_pairs(source, must_terms, limit=10)
                matched_qa_pairs = inner_hits_map.get(user_id, [])
                if not matched_qa_pairs:
                    matched_qa_pairs = extract_matched_qa_pairs(source, must_terms)

                qa_pairs_display = reorder_with_matches(
                    source.get('qa_pairs', []),
                    matched_qa_pairs,
                    limit=10
                ) if isinstance(source, dict) else qa_pairs_display

                demographic_info = None
                if isinstance(source, dict):
                    demographic_info = source.get('demographic_info') or source.get('metadata')

                result = SearchResult(
                    user_id=user_id,
                    score=doc.get('_score', 0.0),
                    timestamp=source.get('timestamp') if isinstance(source, dict) else None,
                    demographic_info=demographic_info,
                    qa_pairs=qa_pairs_display[:5],
                    matched_qa_pairs=matched_qa_pairs,
                    highlights=None
                )
                results.append(result)
                logger.debug(
                    "[match_check] user_id=%s inner_hits=%d matched=%d", 
                    user_id,
                    len(inner_hits_map.get(user_id, [])),
                    len(matched_qa_pairs),
                )

            total_hits = max(os_response['hits']['total']['value'], len(qdrant_results_raw))
            max_score = final_hits[0].get('_score', 0.0) if final_hits else 0.0
            took_ms = os_response['took']

        else:
            # ê¸°ì¡´ OpenSearch ë‹¨ë… ê²€ìƒ‰
            logger.info("   - OpenSearch í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ ì‚¬ìš©")
            data_fetcher = DataFetcher(
                opensearch_client=os_client,
                qdrant_client=getattr(router, 'qdrant_client', None),
                async_opensearch_client=getattr(router, 'async_os_client', None)
            )
            search_response = data_fetcher.search_opensearch(
                index_name=request.index_name,
                query=os_query,
                size=window_size
            )

            # ê²°ê³¼ í¬ë§¤íŒ…
            results = []
            for hit in search_response['hits']['hits']:
                # inner_hitsì—ì„œ ë§¤ì¹­ëœ qa_pairs ì¶”ì¶œ
                matched_qa = []
                if 'inner_hits' in hit and 'qa_pairs' in hit['inner_hits']:
                    for inner_hit in hit['inner_hits']['qa_pairs']['hits']['hits']:
                        qa_data = inner_hit['_source'].copy()
                        qa_data['match_score'] = inner_hit['_score']
                        if 'highlight' in inner_hit:
                            qa_data['highlights'] = inner_hit['highlight']
                        matched_qa.append(qa_data)

                if not matched_qa and must_terms:
                    matched_qa = extract_matched_qa_pairs(hit['_source'], must_terms)

                qa_pairs_display = reorder_with_matches(
                    hit['_source'].get('qa_pairs', []) if isinstance(hit['_source'], dict) else [],
                    matched_qa,
                    limit=10
                )

                demographic_info = None
                if isinstance(hit['_source'], dict):
                    demographic_info = hit['_source'].get('demographic_info') or hit['_source'].get('metadata')

                result = SearchResult(
                    user_id=hit['_source'].get('user_id', ''),
                    score=hit['_score'],
                    timestamp=hit['_source'].get('timestamp'),
                    demographic_info=demographic_info,
                    qa_pairs=qa_pairs_display[:5],
                    matched_qa_pairs=matched_qa,
                    highlights=hit.get('highlight')
                )
                results.append(result)

            total_hits = search_response['hits']['total']['value']
            max_score = search_response['hits']['max_score']
            took_ms = search_response['took']

        total_duration_ms = (perf_counter() - overall_start) * 1000
        timings = {
            "opensearch_ms": took_ms,
            "total_ms": total_duration_ms,
            "cache_hit": 1.0 if cache_hit else 0.0,
        }

        serialized_results = [_serialize_result(res) for res in results]
        stored_items = serialized_results
        if cache_enabled and cache_limit > 0:
            stored_items = serialized_results[:cache_limit]
        page_results, has_more_local = _slice_results(stored_items, page, page_size)
        has_more = has_more_local and ((page * page_size) < total_hits)

        if cache_enabled and cache_key and stored_items:
            cache_payload = {
                "total_hits": total_hits,
                "max_score": max_score,
                "items": stored_items,
                "filters": filters_for_response,
                "extracted_entities": extracted_entities.to_dict(),
            }
            try:
                cache_client.setex(
                    cache_key,
                    cache_ttl,
                    json.dumps(cache_payload, ensure_ascii=False),
                )
                logger.info(f"ğŸ’¾ Redis ê²€ìƒ‰ ìºì‹œ ì €ì¥: key={cache_key}, ttl={cache_ttl}s")
            except Exception as cache_exc:
                logger.warning(f"âš ï¸ Redis ê²€ìƒ‰ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {cache_exc}")

        logger.info(f"\n[OK] ê²€ìƒ‰ ì™„ë£Œ: {len(page_results)}ê±´ ë°˜í™˜ (page={page}, size={page_size})")
        logger.info(f"{'='*60}\n")

        return SearchResponse(
            query=request.query,
            total_hits=total_hits,
            max_score=max_score,
            results=page_results,
            query_analysis={
                "intent": query_analysis.intent,
                "must_terms": query_analysis.must_terms,
                "should_terms": query_analysis.should_terms,
                "alpha": query_analysis.alpha,
                "confidence": query_analysis.confidence,
                "filters": filters_for_response,
                "size": page_size,
                "timings_ms": timings,
            },
            took_ms=int(total_duration_ms),
            page=page,
            page_size=page_size,
            has_more=has_more,
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[ERROR] ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )


class NLSearchRequest(BaseModel):
    """ìì—°ì–´ ê¸°ë°˜ ê²€ìƒ‰ ìš”ì²­ (í•„í„°/size ìë™ ì¶”ì¶œ)"""
    query: str = Field(..., description="ìì—°ì–´ ì¿¼ë¦¬ (ì˜ˆ: '30ëŒ€ ì‚¬ë¬´ì§ 300ëª… ë°ì´í„° ë³´ì—¬ì¤˜')")
    index_name: str = Field(
        default="welcome_all",
        description="ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ì´ë¦„ (ê¸°ë³¸ê°’: alias 'welcome_all'; ì™€ì¼ë“œì¹´ë“œ ì‚¬ìš© ê°€ëŠ¥)"
    )
    use_vector_search: bool = Field(default=True, description="ë²¡í„° ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€")
    page: int = Field(default=1, ge=1, description="ìš”ì²­í•  í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)")


@router.post("/nl", response_model=SearchResponse, summary="ìì—°ì–´ ì¿¼ë¦¬: ìë™ ì¶”ì¶œ+ê²€ìƒ‰")
async def search_natural_language(
    request: NLSearchRequest,
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """
    ìì—°ì–´ ì…ë ¥ì—ì„œ ì¸êµ¬í†µê³„(ì—°ë ¹/ì„±ë³„/ì§ì—…)ì™€ ìš”ì²­ ìˆ˜ëŸ‰ì„ ì¶”ì¶œí•˜ì—¬
    ê²€ìƒ‰ ì¿¼ë¦¬ì™€ sizeì— ë°˜ì˜í•œ ë’¤ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        logger.info("ğŸŸ¢ /search/nl ìš”ì²­ ì‹œì‘")

        if not os_client or not os_client.ping():
            raise HTTPException(status_code=503, detail="OpenSearch ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        config = getattr(router, 'config', None)
        if config is None:
            from rag_query_analyzer.config import get_config
            config = get_config()
            router.config = config

        analyzer = getattr(router, 'analyzer', None)
        if analyzer is None:
            analyzer = AdvancedRAGQueryAnalyzer(config)
            router.analyzer = analyzer
        analysis = analyzer.analyze_query(request.query)
        if analysis is None:
            raise RuntimeError("Query analysis returned None")
        query_analysis = analysis

        embedding_model = getattr(router, 'embedding_model', None)
        if embedding_model is None and hasattr(router, 'embedding_model_factory'):
            embedding_model = router.embedding_model_factory()
            router.embedding_model = embedding_model
        data_fetcher = DataFetcher(
            opensearch_client=os_client,
            qdrant_client=getattr(router, 'qdrant_client', None),
            async_opensearch_client=getattr(router, 'async_os_client', None)
        )

        timings: Dict[str, float] = {}
        overall_start = perf_counter()

        # 1) ì¶”ì¶œ: filters + size
        extractor = DemographicExtractor()
        extracted_entities, requested_size = extractor.extract_with_size(request.query)
        filters: List[Dict[str, Any]] = []
        for demo in extracted_entities.demographics:
            metadata_only = demo.demographic_type in {DemographicType.AGE, DemographicType.GENDER}
            include_nested_fallback = demo.demographic_type not in {DemographicType.OCCUPATION}
            filter_clause = demo.to_opensearch_filter(
                metadata_only=metadata_only,
                include_qa_fallback=include_nested_fallback,
            )
            if filter_clause and filter_clause != {"match_all": {}}:
                filters.append(filter_clause)
        filters_for_response = list(filters)
        filters_signature = _normalize_filters_for_cache(filters_for_response)

        page_size = max(1, min(requested_size, 100))
        page = max(1, request.page)
        requested_window = page_size * page
        cache_client = getattr(router, "redis_client", None)
        cache_ttl = getattr(router, "cache_ttl_seconds", 0)
        cache_limit = getattr(router, "cache_max_results", requested_window)
        cache_prefix = getattr(router, "cache_prefix", "search:results")
        cache_enabled = bool(cache_client) and cache_ttl > 0
        window_size = max(page_size, requested_window)
        if cache_limit > 0:
            window_size = min(window_size, cache_limit)
        size = window_size
        cache_key = None
        cache_hit = False

        if cache_enabled:
            try:
                cache_key = _make_cache_key(
                    prefix=cache_prefix,
                    query=request.query,
                    index_name=request.index_name,
                    page_size=page_size,
                    use_vector=request.use_vector_search,
                    must_terms=analysis.must_terms or [],
                    should_terms=analysis.should_terms or [],
                    must_not_terms=getattr(analysis, "must_not_terms", []) or [],
                    filters_signature=filters_signature,
                )
                cached_raw = cache_client.get(cache_key)
                if cached_raw:
                    cache_payload = json.loads(cached_raw)
                    cache_hit = True
                    logger.info(f"ğŸ” Redis ê²€ìƒ‰ ìºì‹œ íˆíŠ¸: key={cache_key}")
                    extracted_entities_dict = cache_payload.get("extracted_entities")
                    if extracted_entities_dict is None:
                        extracted_entities_dict = extracted_entities.to_dict()
                    return _build_cached_response(
                        payload=cache_payload,
                        request=request,
                        analysis=analysis,
                        filters_for_response=filters_for_response,
                        overall_start=overall_start,
                        extracted_entities_dict=extracted_entities_dict,
                    )
            except Exception as cache_exc:
                logger.warning(f"âš ï¸ Redis ê²€ìƒ‰ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {cache_exc}")
                cache_key = None
                cache_enabled = False
        
        age_gender_filters = [f for f in filters if is_age_or_gender_filter(f)]
        occupation_filters = [f for f in filters if is_occupation_filter(f)]
        other_filters = [f for f in filters if f not in age_gender_filters and f not in occupation_filters]

        filters_os = age_gender_filters + other_filters
        filters = filters_os  # ìœ ì§€ë³´ìˆ˜: ê¸°ì¡´ ë¡œì§ê³¼ í˜¸í™˜ì„±ì„ ìœ„í•´
        has_demographic_filters = bool(filters_for_response)
        occupation_filter_handled = False

        logger.info("ğŸ” í•„í„° ìƒíƒœ ì²´í¬:")
        logger.info(f"  - age_gender_filters: {len(age_gender_filters)}ê°œ")
        logger.info(f"  - occupation_filters: {len(occupation_filters)}ê°œ")
        logger.info(f"  - other_filters: {len(other_filters)}ê°œ")

        two_phase_applicable = bool(age_gender_filters and occupation_filters)
        two_phase_response: Optional[SearchResponse] = None
        if two_phase_applicable:
            logger.info("âœ… 2ë‹¨ê³„ ê²€ìƒ‰ ì¡°ê±´ ì¶©ì¡± â€“ ë‘ ë‹¨ê³„ ê²€ìƒ‰ ì‹œë„")

            try:
                response = await run_two_phase_demographic_search(
                    request=request,
                    analysis=analysis,
                    extracted_entities=extracted_entities,
                    filters=filters_for_response,
                    size=size,
                    age_gender_filters=age_gender_filters,
                    occupation_filters=occupation_filters,
                    data_fetcher=data_fetcher,
                    timings=timings,
                    overall_start=overall_start,
                )

                if response is not None:
                    two_phase_response = response
                    logger.info("âœ… 2ë‹¨ê³„ ê²€ìƒ‰ ì„±ê³µ! ê²°ê³¼ ë°˜í™˜")
                    logger.info(f"ğŸ”µ /search/nl ìš”ì²­ ì™„ë£Œ: ê²°ê³¼ {len(response.results)}ê±´, took_ms={response.took_ms}")
            except Exception as e:
                logger.warning(f"âš ï¸ 2ë‹¨ê³„ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}, ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì§„í–‰")

        if two_phase_response is not None:
            return two_phase_response

        # 2) ë¶„ì„ + ì¿¼ë¦¬ ë¹Œë“œ
        # â­ ìµœì¢… í‚¤ì›Œë“œ ì •ì œ: ë©”íƒ€ í‚¤ì›Œë“œ, ìˆ˜ëŸ‰ íŒ¨í„´, Demographics ì œê±°
        import re

        def strip_korean_particles(term: str) -> str:
            if not term:
                return term
            particles = [
                'ì—ëŠ”', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë„', 'ì€', 'ëŠ”', 'ì´', 'ê°€',
                'ì„', 'ë¥¼', 'ì™€', 'ê³¼', 'ì¸'
            ]
            normalized = term
            for _ in range(10):
                changed = False
                for particle in particles:
                    if normalized.endswith(particle) and len(normalized) > len(particle):
                        normalized = normalized[:-len(particle)]
                        changed = True
                        break
                if not changed or len(normalized) <= 1:
                    break
            return normalized

        meta_keywords = {
            'ì„¤ë¬¸ì¡°ì‚¬', 'ì„¤ë¬¸', 'ë°ì´í„°', 'ìë£Œ', 'ì •ë³´',
            'ë³´ì—¬ì¤˜', 'ë³´ì—¬ì£¼ì„¸ìš”', 'ì•Œë ¤ì¤˜', 'ì•Œë ¤ì£¼ì„¸ìš”',
            'ê²€ìƒ‰', 'ì°¾ì•„ì¤˜', 'ì°¾ì•„ì£¼ì„¸ìš”', 'ì¡°íšŒ',
            'ì„', 'ë¥¼', 'ì´', 'ê°€', 'ì˜', 'ì—', 'ì—ì„œ',
            'ì™€', 'ê³¼', 'ì—ê²Œ', 'í•œí…Œ', 'ëª…', 'ê°œ', 'ê±´',
            'ì‚¬ëŒ', 'ì¸', 'ë¶„', 'ì¤‘', 'ì¤‘ì—', 'ì¤‘ì—ì„œ'
        }

        quantity_pattern = re.compile(r'\d+\s*(ëª…|ê±´)')

        extracted_keywords = set()
        for demo in extracted_entities.demographics:
            extracted_keywords.add(demo.raw_value)
            extracted_keywords.update(demo.synonyms)

        extracted_keywords_stripped = set(strip_korean_particles(k) for k in extracted_keywords)

        if analysis is None:
            raise RuntimeError("Query analysis not initialized")

        original_must = analysis.must_terms.copy()
        original_should = analysis.should_terms.copy()

        def is_demographic_term(term: str) -> bool:
            if term in extracted_keywords:
                return True
            stripped = strip_korean_particles(term)
            return stripped in extracted_keywords or stripped in extracted_keywords_stripped

        analysis.must_terms = [
            t for t in analysis.must_terms
            if (
                t not in meta_keywords and
                not quantity_pattern.search(t) and
                not is_demographic_term(t)
            )
        ]

        analysis.should_terms = [
            t for t in analysis.should_terms
            if (
                t not in meta_keywords and
                not quantity_pattern.search(t) and
                not is_demographic_term(t)
            )
        ]

        removed_meta = [t for t in (original_must + original_should) if t in meta_keywords]
        removed_demo = [t for t in (original_must + original_should) if is_demographic_term(t)]
        removed_quantity = [t for t in (original_must + original_should) if quantity_pattern.search(t)]

        logger.info(f"ğŸ” ìµœì¢… í‚¤ì›Œë“œ ì •ì œ:")
        logger.info(f"  - Must terms: {analysis.must_terms} (ì›ë³¸: {original_must})")
        logger.info(f"  - Should terms: {analysis.should_terms} (ì›ë³¸: {original_should})")
        if removed_meta:
            logger.info(f"  - âŒ ì œê±°ëœ ë©”íƒ€ í‚¤ì›Œë“œ: {removed_meta}")
        if removed_demo:
            logger.info(f"  - âŒ ì œê±°ëœ Demographics: {removed_demo} (í•„í„°ë¡œë§Œ ì²˜ë¦¬)")
        if removed_quantity:
            logger.info(f"  - âŒ ì œê±°ëœ ìˆ˜ëŸ‰ íŒ¨í„´: {removed_quantity}")
        logger.info(f"  - âœ… Demographics í•„í„°: {[d.raw_value for d in extracted_entities.demographics]}")

        query_builder = OpenSearchHybridQueryBuilder(config)
        query_vector = None
        if embedding_model:
            # ì™„ì „ ë™ì  ì„ë² ë”© ê¸°ë°˜ ë™ì˜ì–´ í™•ì¥ (ë„ë©”ì¸ ë¬´ê´€, ë²”ìš©)
            def _enrich_query_vector() -> Optional[list]:
                """ì„ì‹œ: ë™ì˜ì–´ í™•ì¥ ë¹„í™œì„±í™” (ì„±ëŠ¥ ìµœì í™”)"""
                try:
                    vec = embedding_model.encode(request.query).tolist()
                    logger.info("  âš ï¸ ë™ì˜ì–´ í™•ì¥ ë¹„í™œì„±í™” (ì„±ëŠ¥ ìµœì í™”)")
                    return vec
                except Exception:
                    return None

            query_vector = _enrich_query_vector()

        base_query = query_builder.build_query(
            analysis=analysis,
            query_vector=query_vector,
            size=size,
        )

        # 3) í•„í„° ì ìš© ì „ëµ: í•„í„°ëŠ” mustë¡œ, í‚¤ì›Œë“œëŠ” shouldë¡œ ì™„í™”
        # - í•„í„°(30ëŒ€, ì‚¬ë¬´ì§)ëŠ” ë°˜ë“œì‹œ ë§¤ì¹­ë˜ì–´ì•¼ í•¨
        # - í‚¤ì›Œë“œ ê²€ìƒ‰ì€ shouldë¡œ ì™„í™” (í•˜ë‚˜ë§Œ ë§¤ì¹­ë˜ì–´ë„ OK)
        final_query = base_query
        
        # â­ match_all/match_none/None ì œê±°: base_queryì—ì„œ match_all, match_none, Noneì´ ìˆìœ¼ë©´ ì œê±°
        existing_query = final_query.get('query', {"match_all": {}})
        if existing_query is None or existing_query == {"match_all": {}} or existing_query == {"match_none": {}}:
            # match_all/match_none/None ì œê±°
            removed_type = "None" if existing_query is None else ("match_all" if existing_query == {"match_all": {}} else "match_none")
            
            # â­ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í‚¤ì›Œë“œ ì¿¼ë¦¬ ìƒì„± (í•„í„°ë§Œ ìˆëŠ” ê²½ìš°ë¥¼ ìœ„í•´)
            if analysis.must_terms or analysis.should_terms:
                # í‚¤ì›Œë“œ ì¿¼ë¦¬ ì¬ìƒì„±
                keyword_queries = []
                if analysis.must_terms:
                    for term in analysis.must_terms:
                        keyword_queries.append({
                            "nested": {
                                "path": "qa_pairs",
                                "query": {"match": {"qa_pairs.answer_text": term}},
                                "score_mode": "max"
                            }
                        })
                
                if analysis.should_terms:
                    should_keywords = [{
                        "nested": {
                            "path": "qa_pairs",
                            "query": {"match": {"qa_pairs.answer_text": term}},
                            "score_mode": "max"
                        }
                    } for term in analysis.should_terms]
                    
                    if keyword_queries:
                        # mustì™€ should ëª¨ë‘ ìˆëŠ” ê²½ìš°
                        existing_query = {
                            "bool": {
                                "must": keyword_queries,
                                "should": should_keywords,
                                "minimum_should_match": 1
                            }
                        }
                    else:
                        # shouldë§Œ ìˆëŠ” ê²½ìš°
                        existing_query = {
                            "bool": {
                                "should": should_keywords,
                                "minimum_should_match": 1
                            }
                        }
                else:
                    # mustë§Œ ìˆëŠ” ê²½ìš°
                    if len(keyword_queries) == 1:
                        existing_query = keyword_queries[0]
                    else:
                        existing_query = {
                            "bool": {
                                "must": keyword_queries
                            }
                        }
                
                logger.info(f"âš ï¸ {removed_type} ì œê±°, í‚¤ì›Œë“œ ì¿¼ë¦¬ ì¬ìƒì„±: must={len(analysis.must_terms)}, should={len(analysis.should_terms)}")
            else:
                existing_query = None
                logger.info(f"âš ï¸ {removed_type} ì œê±°: í•„í„°ë§Œ ì‚¬ìš© (í‚¤ì›Œë“œ ì—†ìŒ)")
        
        # â­ inner_hits ì œê±° í•¨ìˆ˜ (ì¤‘ë³µ ë°©ì§€)
        def remove_inner_hits(query_dict):
            """ì¬ê·€ì ìœ¼ë¡œ inner_hits ì œê±° (í•„í„°ì—ì„œëŠ” ë§¤ì¹­ë§Œ í™•ì¸í•˜ë©´ ë˜ë¯€ë¡œ)"""
            import copy
            cleaned = copy.deepcopy(query_dict)
            
            if isinstance(cleaned, dict):
                # nested ì¿¼ë¦¬ì—ì„œ inner_hits ì œê±°
                if 'nested' in cleaned:
                    if 'inner_hits' in cleaned['nested']:
                        del cleaned['nested']['inner_hits']
                    # ì¬ê·€ì ìœ¼ë¡œ query ë‚´ë¶€ë„ ì •ì œ
                    if 'query' in cleaned['nested']:
                        cleaned['nested']['query'] = remove_inner_hits(cleaned['nested']['query'])
                
                # bool ì¿¼ë¦¬ ë‚´ë¶€ë„ ì¬ê·€ì ìœ¼ë¡œ ì •ì œ
                if 'bool' in cleaned:
                    for key in ['must', 'should', 'must_not', 'filter']:
                        if key in cleaned['bool']:
                            if isinstance(cleaned['bool'][key], list):
                                cleaned['bool'][key] = [remove_inner_hits(item) for item in cleaned['bool'][key]]
                            else:
                                cleaned['bool'][key] = remove_inner_hits(cleaned['bool'][key])
            
            return cleaned
        
        if filters_os:
            # â­ inner_hits ì œê±° (ì¤‘ë³µ ë°©ì§€)
            cleaned_filters = [remove_inner_hits(f) for f in filters_os]
            
            filter_by_type = {}
            for f in cleaned_filters:
                # í•„í„° íƒ€ì… ì¶”ì¶œ (ìƒˆë¡œìš´ bool ì¿¼ë¦¬ í˜•íƒœ ì§€ì›)
                filter_type = None
                
                # 1. bool ì¿¼ë¦¬ í˜•íƒœ (metadata OR qa_pairs)
                if 'bool' in f and 'should' in f['bool']:
                    should_clauses = f['bool']['should']
                    for clause in should_clauses:
                        # term í•„í„°ì—ì„œ íƒ€ì… ì¶”ì¶œ
                        if 'term' in clause:
                            term_key = list(clause['term'].keys())[0]
                            if 'age_group' in term_key:
                                filter_type = 'age'
                                break
                            elif 'gender' in term_key:
                                filter_type = 'gender'
                                break
                            elif 'occupation' in term_key:
                                filter_type = 'occupation'
                                break
                        # nested í•„í„°ì—ì„œ íƒ€ì… ì¶”ì¶œ
                        elif 'nested' in clause:
                            nested_q = clause['nested'].get('query', {}).get('bool', {}).get('must', [])
                            for nq in nested_q:
                                if isinstance(nq, dict) and 'bool' in nq and 'should' in nq['bool']:
                                    # q_text ë§¤ì¹­ í™•ì¸
                                    for sq in nq['bool']['should']:
                                        if 'match' in sq:
                                            match_key = list(sq['match'].keys())[0]
                                            if 'q_text' in match_key:
                                                q_text_val = sq['match'][match_key]
                                                if 'ì—°ë ¹' in str(q_text_val) or 'ë‚˜ì´' in str(q_text_val):
                                                    filter_type = 'age'
                                                    break
                                                elif 'ì„±ë³„' in str(q_text_val):
                                                    filter_type = 'gender'
                                                    break
                                                elif 'ì§ì—…' in str(q_text_val):
                                                    filter_type = 'occupation'
                                                    break
                                elif 'match' in nq:
                                    match_key = list(nq['match'].keys())[0]
                                    if 'q_text' in match_key:
                                        q_text_val = nq['match'][match_key]
                                        if 'ì—°ë ¹' in str(q_text_val) or 'ë‚˜ì´' in str(q_text_val):
                                            filter_type = 'age'
                                            break
                                        elif 'ì„±ë³„' in str(q_text_val):
                                            filter_type = 'gender'
                                            break
                                        elif 'ì§ì—…' in str(q_text_val):
                                            filter_type = 'occupation'
                                            break
                        if filter_type:
                            break
                
                # 2. ê¸°ì¡´ í˜•íƒœ (í•˜ìœ„ í˜¸í™˜ì„±)
                elif 'term' in f:
                    term_key = list(f['term'].keys())[0]
                    if 'age_group' in term_key:
                        filter_type = 'age'
                    elif 'gender' in term_key:
                        filter_type = 'gender'
                    elif 'occupation' in term_key:
                        filter_type = 'occupation'
                elif 'nested' in f:
                    nested_q = f['nested'].get('query', {}).get('bool', {}).get('must', [])
                    for nq in nested_q:
                        if 'match' in nq:
                            match_key = list(nq['match'].keys())[0]
                            if 'q_text' in match_key:
                                q_text_val = nq['match'][match_key]
                                if 'ì—°ë ¹' in str(q_text_val) or 'ë‚˜ì´' in str(q_text_val):
                                    filter_type = 'age'
                                elif 'ì„±ë³„' in str(q_text_val):
                                    filter_type = 'gender'
                                elif 'ì§ì—…' in str(q_text_val):
                                    filter_type = 'occupation'
                
                if filter_type:
                    if filter_type not in filter_by_type:
                        filter_by_type[filter_type] = []
                    filter_by_type[filter_type].append(f)
                else:
                    # íƒ€ì…ì„ ì•Œ ìˆ˜ ì—†ëŠ” í•„í„°ëŠ” ê·¸ëŒ€ë¡œ ì¶”ê°€
                    if 'unknown' not in filter_by_type:
                        filter_by_type['unknown'] = []
                    filter_by_type['unknown'].append(f)
            
            # â­ í•„í„°ë¥¼ should ì¡°ê±´ìœ¼ë¡œ ì „í™˜ (ì ìˆ˜ ë¶€ìŠ¤íŒ… í¬í•¨)
            # ê° íƒ€ì…ë³„ë¡œ OR, íƒ€ì… ê°„ì€ AND (shouldë¡œ ì™„í™”)
            should_filters = []
            for filter_type, type_filters in filter_by_type.items():
                if len(type_filters) == 1:
                    # ë‹¨ì¼ í•„í„°: í•„í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì´ë¯¸ bool ì¿¼ë¦¬ í˜•íƒœ)
                    filter_item = type_filters[0]
                    should_filters.append(filter_item)
                else:
                    # ê°™ì€ íƒ€ì… í•„í„°ëŠ” OR
                    should_filters.append({
                        'bool': {
                            'should': type_filters,
                            "minimum_should_match": 1
                        }
                    })
            
            # â­ ê¸°ì¡´ ì¿¼ë¦¬ì™€ í•„í„° ê²°í•© (mustë¡œ ê²°í•©: ëª¨ë“  í•„í„°ë¥¼ ë§Œì¡±í•´ì•¼ í•¨)
            # welcome_1st: ì—°ë ¹/ì„±ë³„, welcome_2nd: ì§ì—… ì •ë³´
            # ê° ì¸ë±ìŠ¤ì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì™€ì•¼ í•˜ë¯€ë¡œ mustë¡œ ê²°í•©
            if existing_query is None or existing_query == {"match_all": {}} or existing_query == {"match_none": {}}:
                # í‚¤ì›Œë“œ ì¿¼ë¦¬ê°€ ì—†ê±°ë‚˜ match_all/match_noneì¸ ê²½ìš°: í•„í„°ë¥¼ mustë¡œ ì‚¬ìš©
                final_query['query'] = {
                    'bool': {
                        'must': should_filters  # ëª¨ë“  í•„í„°ë¥¼ ë§Œì¡±í•´ì•¼ í•¨
                    }
                }
                logger.info(f"âœ… í•„í„°ë¥¼ mustë¡œ ì ìš© (ëª¨ë“  í•„í„° ë§Œì¡± í•„ìš”): {len(should_filters)}ê°œ í•„í„°")
            elif isinstance(existing_query, dict) and existing_query.get('bool'):
                # ê¸°ì¡´ bool ì¿¼ë¦¬ì— í•„í„°ë¥¼ mustë¡œ ì¶”ê°€
                if 'must' not in existing_query['bool']:
                    existing_query['bool']['must'] = []
                existing_query['bool']['must'].extend(should_filters)
                final_query['query'] = existing_query
                logger.info(f"âœ… í•„í„°ë¥¼ mustë¡œ ì¶”ê°€ (ëª¨ë“  í•„í„° ë§Œì¡± í•„ìš”): {len(should_filters)}ê°œ í•„í„°")
            else:
                # ê¸°ì¡´ ì¿¼ë¦¬ë¥¼ boolë¡œ ê°ì‹¸ê¸° (mustë¡œ ê²°í•©)
                final_query['query'] = {
                    'bool': {
                        'must': [existing_query] + should_filters
                    }
                }
                logger.info(f"âœ… í•„í„°ë¥¼ mustë¡œ ì¶”ê°€ (ëª¨ë“  í•„í„° ë§Œì¡± í•„ìš”): {len(should_filters)}ê°œ í•„í„°")
        
        if 'size' not in final_query:
            final_query['size'] = size

        if filters_os:
            logger.info(f"ğŸ” ì ìš©ëœ í•„í„° ({len(filters_os)}ê°œ):")
            for i, f in enumerate(filters_os, 1):
                logger.info(f"  í•„í„° {i}: {json.dumps(f, ensure_ascii=False, indent=2)}")
            logger.info(f"ğŸ” ìµœì¢… ì¿¼ë¦¬ êµ¬ì¡°:")
            logger.info(f"  {json.dumps(final_query, ensure_ascii=False, indent=2)}")
        else:
            logger.info(f"ğŸ” ìµœì¢… ì¿¼ë¦¬ êµ¬ì¡° (í•„í„° ì—†ìŒ):")
            logger.info(f"  {json.dumps(final_query, ensure_ascii=False, indent=2)}")

        # â­ Qdrant top-N ì œí•œ: í•„í„° ìœ ë¬´ì— ë”°ë¼ ë¶„ê¸°
        has_filters = bool(filters_os or occupation_filters)
        rrf_k_used: Optional[int] = None
        rrf_reason: str = ""
        adaptive_threshold: Optional[float] = None
        threshold_reason: str = ""
        if has_filters:
            qdrant_limit = min(300, max(150, size * 5))
            search_size = max(500, min(size * 15, 3000))
            logger.info(f"ğŸ” í•„í„° ì ìš©: OpenSearch size={search_size}, Qdrant limit={qdrant_limit} (size*5 ì „ëµ)")
        else:
            qdrant_limit = min(150, max(60, size * 2))
            search_size = max(size * 2, 200)
            logger.info(f"ğŸ” í•„í„° ì—†ìŒ: OpenSearch size={search_size}, Qdrant limit={qdrant_limit}")

        # 4) ì‹¤í–‰: í•˜ì´ë¸Œë¦¬ë“œ (OpenSearch + ì„ íƒì  Qdrant) with RRF
        # â­ STEP 1: welcome_1stì™€ welcome_2ndë¥¼ ê°ê° ë³„ë„ë¡œ ê²€ìƒ‰
        
        # OpenSearch _source filtering: í•„ìš”í•œ í•„ë“œë§Œ ì¡°íšŒ
        source_filter = {
            "includes": ["user_id", "metadata", "timestamp"],
            "excludes": []  # í•„ìš”ì‹œ ì œì™¸í•  í•„ë“œ ì¶”ê°€
        }
        
        # welcome_1stì™€ welcome_2ndë¥¼ ë³„ë„ë¡œ ê²€ìƒ‰í• ì§€ ê²°ì •
        # í•„í„°ì— ì—°ë ¹/ì„±ë³„ì´ ìˆìœ¼ë©´ welcome_1st ê²€ìƒ‰, ì§ì—…ì´ ìˆìœ¼ë©´ welcome_2nd ê²€ìƒ‰
        search_welcome_1st = False
        search_welcome_2nd = False
        search_other_indices = True
        
        if filters:
            for demo in extracted_entities.demographics:
                if demo.demographic_type == DemographicType.AGE or demo.demographic_type == DemographicType.GENDER:
                    search_welcome_1st = True
                elif demo.demographic_type == DemographicType.OCCUPATION:
                    search_welcome_2nd = True
        
        # í•„í„°ê°€ ì—†ê±°ë‚˜ ëª¨ë“  ì¸ë±ìŠ¤ë¥¼ ê²€ìƒ‰í•´ì•¼ í•˜ëŠ” ê²½ìš°
        if not filters or request.index_name == '*':
            search_welcome_1st = True
            search_welcome_2nd = True
        
        logger.info(f"ğŸ” ì¸ë±ìŠ¤ë³„ ê²€ìƒ‰ ì „ëµ:")
        logger.info(f"  - welcome_1st ê²€ìƒ‰: {search_welcome_1st}")
        logger.info(f"  - welcome_2nd ê²€ìƒ‰: {search_welcome_2nd}")
        logger.info(f"  - ê¸°íƒ€ ì¸ë±ìŠ¤ ê²€ìƒ‰: {search_other_indices}")
        
        # â­ ì¸ë±ìŠ¤ë³„ í•„í„° ë¶„ë¦¬: welcome_1stëŠ” ì—°ë ¹/ì„±ë³„ë§Œ, welcome_2ndëŠ” ì§ì—…ë§Œ
        logger.info(f"ğŸ” ì¸ë±ìŠ¤ë³„ ê²€ìƒ‰ ì „ëµ:")
        logger.info(f"  - welcome_1st ê²€ìƒ‰: {search_welcome_1st}")
        logger.info(f"  - welcome_2nd ê²€ìƒ‰: {search_welcome_2nd}")
        logger.info(f"  - ê¸°íƒ€ ì¸ë±ìŠ¤ ê²€ìƒ‰: {search_other_indices}")

        def create_safe_query_template(size_value: int) -> Dict[str, Any]:
            """ì•ˆì „í•œ ê¸°ë³¸ ì¿¼ë¦¬ ìƒì„±"""
            return {
                'query': {'match_all': {}},
                'size': size_value,
                '_source': {
                'includes': ['user_id', 'metadata', 'timestamp']
                }
            }

        welcome_1st_query = create_safe_query_template(search_size)
        welcome_2nd_query = create_safe_query_template(search_size)

        if filters:
            logger.info(f"ğŸ” ì¸ë±ìŠ¤ë³„ í•„í„° ë¶„ë¦¬ ì¤‘...")

            age_gender_filters_split = [f for f in filters if is_age_or_gender_filter(f)]
            occupation_filters_split = [f for f in filters if is_occupation_filter(f)]
            occupation_entities = [
                demo for demo in extracted_entities.demographics
                if demo.demographic_type == DemographicType.OCCUPATION
            ]

            logger.info(f"  - ì—°ë ¹/ì„±ë³„ í•„í„°: {len(age_gender_filters_split)}ê°œ")
            logger.info(f"  - ì§ì—… í•„í„°: {len(occupation_filters_split)}ê°œ")

            if age_gender_filters_split:
                welcome_1st_query['query'] = {
                    'bool': {
                        'must': age_gender_filters_split
                    }
                }
                logger.info(f"  âœ… welcome_1st: ì—°ë ¹/ì„±ë³„ í•„í„° {len(age_gender_filters_split)}ê°œ ì ìš©")
            else:
                logger.info(f"  âš ï¸ welcome_1st: í•„í„° ì—†ìŒ, match_all ì‚¬ìš©")

            occupation_dsl_filter = build_occupation_dsl_filter(occupation_entities)
            if occupation_dsl_filter and occupation_dsl_filter != {"match_all": {}}:
                welcome_2nd_query['query'] = {
                    'bool': {
                        'must': [occupation_dsl_filter]
                    }
                }
                occupation_filter_handled = True
                logger.info(f"  âœ… welcome_2nd: ì§ì—… í•„í„° DSL ì ìš© ({len(occupation_entities)}ê°œ)")
            elif occupation_filters_split:
                welcome_2nd_query['query'] = {
                    'bool': {
                        'must': occupation_filters_split
                    }
                }
                logger.info(f"  âš ï¸ welcome_2nd: DSL ìƒì„± ì‹¤íŒ¨ â†’ ê¸°ì¡´ í•„í„° {len(occupation_filters_split)}ê°œ ì ìš©")
            else:
                logger.info(f"  âš ï¸ welcome_2nd: í•„í„° ì—†ìŒ, match_all ì‚¬ìš©")
        else:
            logger.info(f"  âš ï¸ í•„í„° ì—†ìŒ: ëª¨ë“  ì¸ë±ìŠ¤ì—ì„œ match_all ì‚¬ìš©")

        logger.info(f"ğŸ“‹ ìµœì¢… ì¿¼ë¦¬ í™•ì¸:")
        logger.info(f"  welcome_1st: {json.dumps(welcome_1st_query, ensure_ascii=False)[:200]}...")
        logger.info(f"  welcome_2nd: {json.dumps(welcome_2nd_query, ensure_ascii=False)[:200]}...")

        welcome_1st_keyword_results: List[Dict[str, Any]] = []
        welcome_1st_vector_results: List[Dict[str, Any]] = []
        if search_welcome_1st:
            logger.info(f"ğŸ“Š [1/3] welcome_1st ê²€ìƒ‰ ì¤‘...")
            try:
                if 'query' not in welcome_1st_query or not welcome_1st_query['query']:
                    raise ValueError("welcome_1st_queryì— 'query' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤")

                os_response_1st = data_fetcher.search_opensearch(
                    index_name="s_welcome_1st",
                    query=remove_inner_hits(welcome_1st_query),
                    size=search_size,
                    source_filter=source_filter,
                    request_timeout=DEFAULT_OS_TIMEOUT,
                )
                welcome_1st_keyword_results = os_response_1st['hits']['hits']
                logger.info(f"  âœ… OpenSearch: {len(welcome_1st_keyword_results)}ê±´")
                
                # Qdrant ë²¡í„° ê²€ìƒ‰
                if request.use_vector_search and query_vector and hasattr(router, 'qdrant_client'):
                    qdrant_client = router.qdrant_client
                    try:
                        r = qdrant_client.search(
                            collection_name="s_welcome_1st",
                            query_vector=query_vector,
                            limit=qdrant_limit,  # í•„í„° ìœ ë¬´ì— ë”°ë¼ ë¶„ê¸°ëœ limit ì‚¬ìš©
                            score_threshold=0.3,
                        )
                        for item in r:
                            welcome_1st_vector_results.append({
                                '_id': str(item.id),
                                '_score': item.score,
                                '_source': item.payload,
                                '_index': 's_welcome_1st',
                            })
                        logger.info(f"  âœ… Qdrant: {len(welcome_1st_vector_results)}ê±´")
                    except Exception as e:
                        logger.debug(f"  âš ï¸ Qdrant ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            except Exception as e:
                logger.warning(f"  âš ï¸ welcome_1st ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # welcome_2nd ê²€ìƒ‰
        welcome_2nd_keyword_results: List[Dict[str, Any]] = []
        welcome_2nd_vector_results: List[Dict[str, Any]] = []
        if search_welcome_2nd:
            logger.info(f"ğŸ“Š [2/3] welcome_2nd ê²€ìƒ‰ ì¤‘...")
            try:
                os_response_2nd = data_fetcher.search_opensearch(
                    index_name="s_welcome_2nd",
                    query=remove_inner_hits(welcome_2nd_query),
                    size=search_size,
                    source_filter=source_filter,
                    request_timeout=DEFAULT_OS_TIMEOUT,
                )
                welcome_2nd_keyword_results = os_response_2nd['hits']['hits']
                logger.info(f"  âœ… OpenSearch: {len(welcome_2nd_keyword_results)}ê±´")
                
                # Qdrant ë²¡í„° ê²€ìƒ‰
                if request.use_vector_search and query_vector and hasattr(router, 'qdrant_client'):
                    qdrant_client = router.qdrant_client
                    try:
                        r = qdrant_client.search(
                            collection_name="s_welcome_2nd",
                            query_vector=query_vector,
                            limit=qdrant_limit,  # í•„í„° ìœ ë¬´ì— ë”°ë¼ ë¶„ê¸°ëœ limit ì‚¬ìš©
                            score_threshold=0.3,
                        )
                        for item in r:
                            welcome_2nd_vector_results.append({
                                '_id': str(item.id),
                                '_score': item.score,
                                '_source': item.payload,
                                '_index': 's_welcome_2nd',
                            })
                        logger.info(f"  âœ… Qdrant: {len(welcome_2nd_vector_results)}ê±´")
                    except Exception as e:
                        logger.debug(f"  âš ï¸ Qdrant ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            except Exception as e:
                logger.warning(f"  âš ï¸ welcome_2nd ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # ê¸°íƒ€ ì¸ë±ìŠ¤ ê²€ìƒ‰ (survey_* ë“±)
        other_keyword_results: List[Dict[str, Any]] = []
        other_vector_results: List[Dict[str, Any]] = []
        if search_other_indices:
            logger.info(f"ğŸ“Š [3/3] ê¸°íƒ€ ì¸ë±ìŠ¤ ê²€ìƒ‰ ì¤‘...")
            # welcome_1st, welcome_2ndë¥¼ ì œì™¸í•œ ì¸ë±ìŠ¤ ê²€ìƒ‰
            other_index_pattern = request.index_name
            if request.index_name == '*':
                # survey_* íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰ (welcome_1st, welcome_2nd ì œì™¸)
                other_index_pattern = "survey_*"
            elif 's_welcome_1st' in request.index_name or 's_welcome_2nd' in request.index_name:
                # welcome ì¸ë±ìŠ¤ë¥¼ ì œì™¸í•œ íŒ¨í„´ ìƒì„±
                indices = [idx.strip() for idx in request.index_name.split(',')]
                other_indices = [idx for idx in indices if idx not in ['s_welcome_1st', 's_welcome_2nd']]
                if other_indices:
                    other_index_pattern = ','.join(other_indices)
                else:
                    search_other_indices = False
            
            if search_other_indices:
                try:
                    other_query_body = final_query.copy()
                    if not isinstance(other_query_body.get('query'), dict):
                        logger.warning("  âš ï¸ ê¸°íƒ€ ì¸ë±ìŠ¤ ì¿¼ë¦¬ê°€ ë¹„ì–´ ìˆì–´ match_allë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤")
                        other_query_body['query'] = {"match_all": {}}
                    os_response_other = data_fetcher.search_opensearch(
                        index_name=other_index_pattern,
                        query=other_query_body,
                        size=search_size,
                        source_filter=source_filter,
                        request_timeout=DEFAULT_OS_TIMEOUT,
                    )
                    other_keyword_results = os_response_other['hits']['hits']
                    logger.info(f"  âœ… OpenSearch: {len(other_keyword_results)}ê±´")
                    
                    # Qdrant ë²¡í„° ê²€ìƒ‰ (ê¸°íƒ€ ì»¬ë ‰ì…˜)
                    if request.use_vector_search and query_vector and hasattr(router, 'qdrant_client'):
                        qdrant_client = router.qdrant_client
                        try:
                            collections = qdrant_client.get_collections()
                            for col in collections.collections:
                                if col.name not in ['s_welcome_1st', 's_welcome_2nd']:
                                    try:
                                        r = qdrant_client.search(
                                            collection_name=col.name,
                                            query_vector=query_vector,
                                            limit=qdrant_limit,  # í•„í„° ìœ ë¬´ì— ë”°ë¼ ë¶„ê¸°ëœ limit ì‚¬ìš©
                                            score_threshold=0.3,
                                        )
                                        for item in r:
                                            other_vector_results.append({
                                                '_id': str(item.id),
                                                '_score': item.score,
                                                '_source': item.payload,
                                                '_index': col.name,
                                            })
                                    except Exception:
                                        continue
                            logger.info(f"  âœ… Qdrant: {len(other_vector_results)}ê±´")
                        except Exception as e:
                            logger.debug(f"  âš ï¸ Qdrant ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                except Exception as e:
                    logger.warning(f"  âš ï¸ ê¸°íƒ€ ì¸ë±ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # user_id ë° _id -> ì›ë³¸ ë¬¸ì„œ ë§¤í•‘ ìƒì„± (ëª¨ë“  ì¸ë±ìŠ¤ ê²°ê³¼ì—ì„œ)
        user_doc_map = {}
        id_doc_map = {}  # _id ê¸°ë°˜ ë§¤í•‘ë„ ì¶”ê°€
        
        # welcome_1st ë§¤í•‘
        for hit in welcome_1st_keyword_results:
            source = hit.get('_source', {})
            user_id = source.get('user_id')
            doc_id = hit.get('_id')
            
            doc_info = {
                'source': source,
                'inner_hits': hit.get('inner_hits', {}),
                'highlight': hit.get('highlight'),
                'index': 's_welcome_1st'
            }
            
            if user_id:
                user_doc_map[user_id] = doc_info
            if doc_id:
                id_doc_map[doc_id] = doc_info
        
        # welcome_2nd ë§¤í•‘
        for hit in welcome_2nd_keyword_results:
            source = hit.get('_source', {})
            user_id = source.get('user_id')
            doc_id = hit.get('_id')
            
            doc_info = {
                'source': source,
                'inner_hits': hit.get('inner_hits', {}),
                'highlight': hit.get('highlight'),
                'index': 's_welcome_2nd'
            }
            
            if user_id:
                user_doc_map[user_id] = doc_info
            if doc_id:
                id_doc_map[doc_id] = doc_info
        
        # ê¸°íƒ€ ì¸ë±ìŠ¤ ë§¤í•‘
        for hit in other_keyword_results:
            source = hit.get('_source', {})
            user_id = source.get('user_id')
            doc_id = hit.get('_id')
            
            doc_info = {
                'source': source,
                'inner_hits': hit.get('inner_hits', {}),
                'highlight': hit.get('highlight'),
                'index': hit.get('_index', 'unknown')
            }
            
            if user_id:
                user_doc_map[user_id] = doc_info
            if doc_id:
                id_doc_map[doc_id] = doc_info

        # â­ í†µí•© RRF ê²°í•©: í•œ ë²ˆë§Œ ì‹¤í–‰
        logger.info(f"\n{'='*60}")
        logger.info("ğŸ“Š RRF ê²°í•©: ë‹¨ì¼ íŒ¨ìŠ¤ ì‹¤í–‰")
        logger.info(f"{'='*60}")

        # ë²¡í„° ê²°ê³¼ì— ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„° ë³´ê°•
        for doc in welcome_1st_vector_results:
            doc.setdefault('_index', 's_welcome_1st')
        for doc in welcome_2nd_vector_results:
            doc.setdefault('_index', 's_welcome_2nd')
        for doc in other_vector_results:
            if '_index' not in doc:
                doc['_index'] = doc.get('collection', doc.get('_source', {}).get('index', 'unknown'))

        all_keyword_results: List[Dict[str, Any]] = []
        all_vector_results: List[Dict[str, Any]] = []

        if welcome_1st_keyword_results:
            all_keyword_results.extend(welcome_1st_keyword_results)
        if welcome_2nd_keyword_results:
            all_keyword_results.extend(welcome_2nd_keyword_results)
        if other_keyword_results:
            all_keyword_results.extend(other_keyword_results)

        if welcome_1st_vector_results:
            all_vector_results.extend(welcome_1st_vector_results)
        if welcome_2nd_vector_results:
            all_vector_results.extend(welcome_2nd_vector_results)
        if other_vector_results:
            all_vector_results.extend(other_vector_results)

        logger.info(f"  - ì´ í‚¤ì›Œë“œ ê²°ê³¼: {len(all_keyword_results)}ê±´")
        logger.info(f"  - ì´ ë²¡í„°   ê²°ê³¼: {len(all_vector_results)}ê±´")

        rrf_start = perf_counter()

        if request.use_vector_search and all_vector_results:
            combined_rrf, rrf_k_used, rrf_reason = calculate_rrf_score_adaptive(
                keyword_results=all_keyword_results,
                vector_results=all_vector_results,
                query_intent=getattr(analysis, "intent", None),
                has_filters=has_filters,
                use_vector_search=request.use_vector_search,
            )
        else:
            combined_rrf = all_keyword_results
            if request.use_vector_search:
                rrf_reason = "ë²¡í„° ê²°ê³¼ ì—†ìŒ â†’ í‚¤ì›Œë“œ ê²°ê³¼ ì‚¬ìš©"
            else:
                rrf_reason = "ë²¡í„° ê²€ìƒ‰ ë¹„í™œì„±í™” â†’ í‚¤ì›Œë“œ ê²°ê³¼ ì‚¬ìš©"
            rrf_k_used = 0

        user_rrf_variants: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        for doc in combined_rrf:
            user_id = get_user_id_from_doc(doc)
            if not user_id:
                continue
            user_rrf_variants[user_id].append(doc)
        
        user_rrf_map: Dict[str, List[Dict[str, Any]]] = {}
        final_rrf_results: List[Dict[str, Any]] = []
        for user_id, docs in user_rrf_variants.items():
            def _score(doc: Dict[str, Any]) -> float:
                score = doc.get('_score')
                if score is None:
                    score = doc.get('rrf_score', 0.0)
                return float(score or 0.0)

            best_doc_original = max(docs, key=_score)
            total_rrf_score = sum(_score(doc) for doc in docs)
            sources = [doc.get('_index', 'unknown') for doc in docs]

            best_doc = dict(best_doc_original)
            best_doc['_score'] = total_rrf_score
            best_doc['_rrf_details'] = {
                'combined_score': total_rrf_score,
                'source_count': len(docs),
                'sources': sources,
            }

            final_rrf_results.append(best_doc)
            # best_docë¥¼ ì²« ë²ˆì§¸ë¡œ ìœ ì§€í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” ì°¸ê³ ìš©ìœ¼ë¡œ ë³´ê´€
            others = [doc for doc in docs if doc is not best_doc_original]
            user_rrf_map[user_id] = [best_doc] + others
        
        final_rrf_results.sort(
            key=lambda d: d.get('_score', 0.0) or d.get('rrf_score', 0.0),
            reverse=True
        )
        
        rrf_results = final_rrf_results
        took_ms = 0  # ì—¬ëŸ¬ ê²€ìƒ‰ì˜ í•©ì´ë¯€ë¡œ ì •í™•í•œ ì‹œê°„ ì¸¡ì •ì€ ì–´ë ¤ì›€
        
        logger.info(f"  âœ… ë‹¨ì¼ RRF ê²°í•© ì™„ë£Œ: {len(rrf_results)}ê±´ (ê³ ìœ  user_id: {len(user_rrf_map)}ê°œ)")
        timings['rrf_recombination_ms'] = (perf_counter() - rrf_start) * 1000

        # í›„ë³´ ë¬¸ì„œ ìˆ˜ ì œí•œ (í›„ì²˜ë¦¬ ë¶€ë‹´ ì™„í™”)
        fetch_size = window_size
        candidate_cap = max(fetch_size * 5, 300)
        if len(rrf_results) > candidate_cap:
            logger.info(
                f"  - í›„ë³´ ë¬¸ì„œ ì œí•œ ì ìš©: {len(rrf_results)} â†’ {candidate_cap} (size={fetch_size})"
            )
            rrf_results = rrf_results[:candidate_cap]
        if len(rrf_results) < fetch_size:
            backup_cap = max(fetch_size * 6, fetch_size + 50)
            logger.info(
                f"  - í›„ë³´ ìˆ˜ê°€ sizeë³´ë‹¤ ì‘ì•„ ì¦ê°€ ì‹œë„: {len(rrf_results)} â†’ {min(len(final_rrf_results), backup_cap)}"
            )
            rrf_results = final_rrf_results[:backup_cap]
        
        # RRF ì ìˆ˜ ë””ë²„ê¹…: ìƒìœ„ 10ê°œ ì¶œë ¥
        if rrf_results:
            logger.info(f"  - RRF ì ìˆ˜ ìƒìœ„ 10ê°œ:")
            for i, doc in enumerate(rrf_results[:10], 1):
                rrf_score = doc.get('_score') or doc.get('rrf_score', 0.0)
                rrf_details = doc.get('_rrf_details', {})
                doc_index = doc.get('_index', 'unknown')
                logger.info(f"    {i}. doc_id={doc.get('_id', 'N/A')}, index={doc_index}, RRF={rrf_score:.6f}, "
                          f"keyword_rank={rrf_details.get('keyword_rank')}, vector_rank={rrf_details.get('vector_rank')}")
        
        demographic_filters: Dict[DemographicType, List["DemographicEntity"]] = defaultdict(list)
        for demo in extracted_entities.demographics:
            demographic_filters[demo.demographic_type].append(demo)

        filtered_rrf_results: List[Dict[str, Any]] = rrf_results

        occupation_display_map: Dict[str, str] = {}
        doc_user_map: Dict[int, str] = {}
        welcome_1st_batch: Dict[str, Dict[str, Any]] = {}
        welcome_2nd_batch: Dict[str, Dict[str, Any]] = {}
        synonym_cache: Dict[str, List[str]] = {}

        PLACEHOLDER_TOKENS: Set[str] = {
            "",
            "ë¯¸ì •",
            "ì—†ìŒ",
            "ë¬´ì‘ë‹µ",
            "í•´ë‹¹ì—†ìŒ",
            "n/a",
            "na",
            "null",
            "none",
            "unknown",
            "ë¯¸ì„ íƒ",
            "ë¯¸ê¸°ì¬",
        }
        PLACEHOLDER_TOKENS = {token.strip().lower() for token in PLACEHOLDER_TOKENS}

        def normalize_value(value: Any) -> str:
            if value is None:
                return ""
            if isinstance(value, bool):
                value_str = str(value)
            elif isinstance(value, (int, float)):
                try:
                    if value.is_integer():  # type: ignore[attr-defined]
                        value = int(value)
                except AttributeError:
                    pass
                value_str = str(value)
            else:
                value_str = str(value)

            cleaned = value_str.strip()
            lower = cleaned.lower()
            if lower in PLACEHOLDER_TOKENS:
                return ""
            return lower

        def build_expected_values(demo: "DemographicEntity") -> Set[str]:
            key = f"{demo.demographic_type.value}:{demo.raw_value}"
            expected: Set[str] = set()
            expected.add(demo.raw_value)
            expected.add(demo.value)
            expected.update(demo.synonyms or set())
            expected.update(synonym_cache.get(key, []))
            normalized_expected = {normalize_value(v) for v in expected if v}

            if demo.demographic_type == DemographicType.GENDER:
                male_aliases = {
                    normalize_value(v)
                    for v in {"m", "male", "man", "ë‚¨", "ë‚¨ì„±", "ë‚¨ì", "ë‚¨ì„±í˜•"}
                }
                female_aliases = {
                    normalize_value(v)
                    for v in {"f", "female", "woman", "ì—¬", "ì—¬ì„±", "ì—¬ì", "ì—¬ì„±í˜•"}
                }
                if normalized_expected & male_aliases:
                    normalized_expected.update(male_aliases)
                if normalized_expected & female_aliases:
                    normalized_expected.update(female_aliases)

            return normalized_expected

        def values_match(values: Set[str], expected: Set[str]) -> bool:
            if not values or not expected:
                return False
            for val in values:
                if not val:
                    continue
                for exp in expected:
                    if not exp:
                        continue
                    if val == exp or val in exp or exp in val:
                        return True
            return False

        def expand_gender_aliases(values: Set[str]) -> None:
            male_aliases = {"m", "ë‚¨", "ë‚¨ì„±", "male", "man", "ë‚¨ì"}
            female_aliases = {"f", "ì—¬", "ì—¬ì„±", "female", "woman", "ì—¬ì"}
            if values & male_aliases:
                values.update(male_aliases)
            if values & female_aliases:
                values.update(female_aliases)

        def add_age_decade(values: Set[str], age_value: Any) -> None:
            if age_value in (None, ""):
                return
            try:
                age_int = int(age_value)
                decade = (age_int // 10) * 10
                for candidate in (f"{decade}ëŒ€", f"{decade}s", str(age_int)):
                    normalized_candidate = normalize_value(candidate)
                    if normalized_candidate:
                        values.add(normalized_candidate)
            except (ValueError, TypeError):
                pass

        if has_demographic_filters:
            filter_start = perf_counter()

            gender_dsl_handled = bool(demographic_filters.get(DemographicType.GENDER)) and search_welcome_1st
            age_dsl_handled = bool(demographic_filters.get(DemographicType.AGE)) and search_welcome_1st
            occupation_dsl_handled = bool(demographic_filters.get(DemographicType.OCCUPATION)) and occupation_filter_handled

            filters_to_validate: List[DemographicType] = []
            if demographic_filters.get(DemographicType.GENDER) and not gender_dsl_handled:
                filters_to_validate.append(DemographicType.GENDER)
            if demographic_filters.get(DemographicType.AGE) and not age_dsl_handled:
                filters_to_validate.append(DemographicType.AGE)
            if demographic_filters.get(DemographicType.OCCUPATION) and not occupation_dsl_handled:
                filters_to_validate.append(DemographicType.OCCUPATION)

            if not filters_to_validate and demographic_filters:
                filters_to_validate = list(demographic_filters.keys())
                logger.info("  âš ï¸ DSLì—ì„œ í•„í„°ë¥¼ ì ìš©í–ˆì§€ë§Œ ì•ˆì „ì„± í™•ì¸ì„ ìœ„í•´ í›„ì²˜ë¦¬ ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")

            for demo in extracted_entities.demographics:
                cache_key = f"{demo.demographic_type.value}:{demo.raw_value}"
                if demo.demographic_type in {DemographicType.GENDER, DemographicType.OCCUPATION}:
                    try:
                        from rag_query_analyzer.utils.synonym_expander import get_synonym_expander
                        expander = get_synonym_expander()
                        synonym_cache[cache_key] = expander.expand(demo.raw_value)
                    except Exception:
                        synonyms = [demo.raw_value]
                        synonyms.extend([syn for syn in demo.synonyms if syn])
                        synonym_cache[cache_key] = synonyms
                else:
                    synonym_cache[cache_key] = [demo.raw_value]
            
            user_ids_to_fetch: Set[str] = set()
            doc_user_map.clear()
            welcome_1st_batch.clear()
            welcome_2nd_batch.clear()
            
            logger.info(f"ğŸ” user_id ìˆ˜ì§‘ ì¤‘: RRF ê²°ê³¼ {len(rrf_results)}ê±´...")
            for doc in rrf_results:
                source = doc.get('_source', {})
                if not source and 'doc' in doc:
                    source = doc.get('doc', {}).get('_source', {})
                if not source or not isinstance(source, dict):
                    if 'payload' in doc:
                        payload = doc.get('payload', {})
                        if isinstance(payload, dict) and payload:
                            source = payload
                    elif isinstance(source, dict) and 'payload' in source:
                        payload = source.get('payload', {})
                        if isinstance(payload, dict) and payload:
                            source = payload
                user_id = None
                if isinstance(source, dict):
                    user_id = source.get('user_id')
                if not user_id:
                    user_id = doc.get('_id', '')
                if not user_id and 'payload' in doc:
                    payload = doc.get('payload', {})
                    if isinstance(payload, dict):
                        user_id = payload.get('user_id')
                if user_id:
                    user_ids_to_fetch.add(user_id)
                    doc_user_map[id(doc)] = user_id
            
            logger.info(f"  âœ… ìˆ˜ì§‘ëœ user_id: {len(user_ids_to_fetch)}ê±´")
            
            if user_ids_to_fetch:
                user_ids_list = list(user_ids_to_fetch)
                total_batches = (len(user_ids_list) + 199) // 200
                logger.info(f"ğŸ” ë°°ì¹˜ ì¡°íšŒ: welcome_1st/welcome_2nd {len(user_ids_list)}ê±´ ì¡°íšŒ ì¤‘...")

                for uid in list(user_ids_list):
                    cached_1 = _cache_get_welcome_doc("s_welcome_1st", uid)
                    if cached_1:
                        welcome_1st_batch[uid] = cached_1
                    cached_2 = _cache_get_welcome_doc("s_welcome_2nd", uid)
                    if cached_2:
                        welcome_2nd_batch[uid] = cached_2

                uncached_1st = [uid for uid in user_ids_list if uid not in welcome_1st_batch]
                uncached_2nd = [uid for uid in user_ids_list if uid not in welcome_2nd_batch]

                try:
                    if data_fetcher.os_async_client:
                        if uncached_1st:
                            raw_welcome_1st_docs = await data_fetcher.multi_get_documents_async(
                                index_name="s_welcome_1st",
                                doc_ids=uncached_1st,
                                source_fields=["metadata", "user_id", "qa_pairs"],
                            ) or []
                            fetched_map = data_fetcher.docs_to_user_map(raw_welcome_1st_docs)
                            welcome_1st_batch.update(fetched_map)
                            for uid, doc_item in fetched_map.items():
                                _cache_put_welcome_doc("s_welcome_1st", uid, doc_item)

                        if uncached_2nd:
                            raw_welcome_2nd_docs = await data_fetcher.multi_get_documents_async(
                                index_name="s_welcome_2nd",
                                doc_ids=uncached_2nd,
                                source_fields=["metadata", "user_id", "qa_pairs"],
                            ) or []
                            fetched_map = data_fetcher.docs_to_user_map(raw_welcome_2nd_docs)
                            welcome_2nd_batch.update(fetched_map)
                            for uid, doc_item in fetched_map.items():
                                _cache_put_welcome_doc("s_welcome_2nd", uid, doc_item)
                    else:
                        if uncached_1st:
                            for batch_idx in range(0, len(uncached_1st), 200):
                                batch_ids = uncached_1st[batch_idx:batch_idx + 200]
                                batch_num = (batch_idx // 200) + 1
                                try:
                                    mget_body = [{"_index": "s_welcome_1st", "_id": uid} for uid in batch_ids]
                                    mget_response = os_client.mget(body={"docs": mget_body}, ignore=[404], request_timeout=60)
                                    for item in mget_response.get('docs', []):
                                        if item.get('found'):
                                            welcome_1st_batch[item['_id']] = item['_source']
                                            _cache_put_welcome_doc("s_welcome_1st", item['_id'], item['_source'])
                                    logger.debug(f"  ğŸ“¦ welcome_1st ë°°ì¹˜ {batch_num}/{total_batches}: {len([d for d in mget_response.get('docs', []) if d.get('found')])}/{len(batch_ids)}ê±´")
                                except Exception as e:
                                    logger.warning(f"  âš ï¸ welcome_1st ë°°ì¹˜ {batch_num}/{total_batches} ì‹¤íŒ¨: {e}")
                                    continue

                        if uncached_2nd:
                            for batch_idx in range(0, len(uncached_2nd), 200):
                                batch_ids = uncached_2nd[batch_idx:batch_idx + 200]
                                batch_num = (batch_idx // 200) + 1
                                try:
                                    mget_body = [{"_index": "s_welcome_2nd", "_id": uid} for uid in batch_ids]
                                    mget_response = os_client.mget(body={"docs": mget_body}, ignore=[404], request_timeout=60)
                                    for item in mget_response.get('docs', []):
                                        if item.get('found'):
                                            welcome_2nd_batch[item['_id']] = item['_source']
                                            _cache_put_welcome_doc("s_welcome_2nd", item['_id'], item['_source'])
                                    logger.debug(f"  ğŸ“¦ welcome_2nd ë°°ì¹˜ {batch_num}/{total_batches}: {len([d for d in mget_response.get('docs', []) if d.get('found')])}/{len(batch_ids)}ê±´")
                                except Exception as e:
                                    logger.warning(f"  âš ï¸ welcome_2nd ë°°ì¹˜ {batch_num}/{total_batches} ì‹¤íŒ¨: {e}")
                                    continue

                    missing_1st = user_ids_to_fetch - set(welcome_1st_batch.keys())
                    missing_2nd = user_ids_to_fetch - set(welcome_2nd_batch.keys())

                    if missing_1st and len(missing_1st) <= 1000:
                        logger.info(f"  ğŸ” welcome_1st ì¶”ê°€ ì¡°íšŒ ì‹œë„: {len(missing_1st)}ê±´...")
                        missing_ids = list(missing_1st)
                        if data_fetcher.os_async_client:
                            extra_docs_raw = await data_fetcher.multi_get_documents_async(
                                index_name="s_welcome_1st",
                                doc_ids=missing_ids,
                                source_fields=["metadata", "user_id", "qa_pairs"],
                            )
                            welcome_1st_batch.update(data_fetcher.docs_to_user_map(extra_docs_raw))
                        else:
                            response = os_client.mget(
                                index="s_welcome_1st",
                                body={"ids": missing_ids},
                                _source=["metadata", "user_id", "qa_pairs"],
                                request_timeout=60,
                                ignore=[404]
                            )
                            for doc_item in response.get('docs', []):
                                if doc_item.get('found'):
                                    welcome_1st_batch[doc_item['_id']] = doc_item['_source']
                                    _cache_put_welcome_doc("s_welcome_1st", doc_item['_id'], doc_item['_source'])
                        logger.info(f"  âœ… welcome_1st ì¶”ê°€ ì¡°íšŒ í›„ ì´ {len(welcome_1st_batch)}ê±´")

                    if missing_2nd and len(missing_2nd) <= 1000:
                        logger.info(f"  ğŸ” welcome_2nd ì¶”ê°€ ì¡°íšŒ ì‹œë„: {len(missing_2nd)}ê±´...")
                        missing_ids = list(missing_2nd)
                        if data_fetcher.os_async_client:
                            extra_docs_raw = await data_fetcher.multi_get_documents_async(
                                index_name="s_welcome_2nd",
                                doc_ids=missing_ids,
                                source_fields=["metadata", "user_id", "qa_pairs"],
                            )
                            welcome_2nd_batch.update(data_fetcher.docs_to_user_map(extra_docs_raw))
                        else:
                            response = os_client.mget(
                                index="s_welcome_2nd",
                                body={"ids": missing_ids},
                                _source=["metadata", "user_id", "qa_pairs"],
                                request_timeout=60,
                                ignore=[404]
                            )
                            for doc_item in response.get('docs', []):
                                if doc_item.get('found'):
                                    welcome_2nd_batch[doc_item['_id']] = doc_item['_source']
                                    _cache_put_welcome_doc("s_welcome_2nd", doc_item['_id'], doc_item['_source'])
                        logger.info(f"  âœ… welcome_2nd ì¶”ê°€ ì¡°íšŒ í›„ ì´ {len(welcome_2nd_batch)}ê±´")

                except Exception as e:
                    logger.warning(f"  âš ï¸ ë°°ì¹˜ ì¡°íšŒ ì‹¤íŒ¨: {e}, ê°œë³„ ì¡°íšŒë¡œ fallback")
            
            if not filters_to_validate:
                timings["post_filter_ms"] = (perf_counter() - filter_start) * 1000
                filtered_rrf_results = rrf_results
                logger.info("  âœ… Demographic í•„í„° ì—†ìŒ: Python í›„ì²˜ë¦¬ ìƒëµ")
            else:
                def collect_doc_values(
                    user_id: str,
                    source: Dict[str, Any],
                    metadata_1st: Dict[str, Any],
                    metadata_2nd: Dict[str, Any],
                ) -> Tuple[Dict[DemographicType, Set[str]], Dict[DemographicType, bool]]:
                    doc_values: Dict[DemographicType, Set[str]] = {
                        DemographicType.GENDER: set(),
                        DemographicType.AGE: set(),
                        DemographicType.OCCUPATION: set(),
                    }
                    metadata_presence: Dict[DemographicType, bool] = {
                        DemographicType.GENDER: False,
                        DemographicType.AGE: False,
                        DemographicType.OCCUPATION: False,
                    }

                    metadata_candidates = [
                        metadata_1st,
                        metadata_2nd,
                        source.get("metadata", {}) if isinstance(source, dict) else {},
                    ]

                    payload = {}
                    if isinstance(source, dict):
                        payload_candidate = source.get("payload")
                        if isinstance(payload_candidate, dict):
                            payload = payload_candidate
                    if not payload and isinstance(source, dict) and "doc" in source:
                        doc_payload = source.get("doc", {}).get("payload")
                        if isinstance(doc_payload, dict):
                            payload = doc_payload
                    if not payload and isinstance(source, dict):
                        payload = source

                    if isinstance(payload, dict):
                        metadata_candidates.append(payload.get("metadata", {}))

                    for candidate in metadata_candidates:
                        if not isinstance(candidate, dict):
                            continue

                        candidate_sources: List[Dict[str, Any]] = [candidate]
                        nested_meta = candidate.get("metadata")
                        if isinstance(nested_meta, dict):
                            candidate_sources.append(nested_meta)

                        for meta_source in candidate_sources:
                            gender_val = meta_source.get("gender") or meta_source.get("gender_code")
                            if gender_val:
                                normalized_gender = normalize_value(gender_val)
                                if normalized_gender:
                                    doc_values[DemographicType.GENDER].add(normalized_gender)
                                    metadata_presence[DemographicType.GENDER] = True

                            age_group_val = meta_source.get("age_group")
                            if age_group_val:
                                normalized_age_group = normalize_value(age_group_val)
                                if normalized_age_group:
                                    doc_values[DemographicType.AGE].add(normalized_age_group)
                                    metadata_presence[DemographicType.AGE] = True

                            age_val = meta_source.get("age")
                            if age_val:
                                pre_count = len(doc_values[DemographicType.AGE])
                                add_age_decade(doc_values[DemographicType.AGE], age_val)
                                if len(doc_values[DemographicType.AGE]) > pre_count:
                                    metadata_presence[DemographicType.AGE] = True

                            birth_year_val = meta_source.get("birth_year")
                            if birth_year_val:
                                normalized_birth_year = normalize_value(birth_year_val)
                                if normalized_birth_year:
                                    doc_values[DemographicType.AGE].add(normalized_birth_year)
                                    metadata_presence[DemographicType.AGE] = True

                            occupation_val = meta_source.get("occupation") or meta_source.get("job")
                            if occupation_val:
                                normalized_occupation = normalize_value(occupation_val)
                                if normalized_occupation:
                                    doc_values[DemographicType.OCCUPATION].add(normalized_occupation)
                                    metadata_presence[DemographicType.OCCUPATION] = True

                    if user_id:
                        qa_sources: List[List[Dict[str, Any]]] = []
                        if isinstance(source, dict):
                            qa_sources.append(source.get("qa_pairs", []) or [])
                        if isinstance(welcome_2nd_batch.get(user_id), dict):
                            qa_sources.append(welcome_2nd_batch[user_id].get("qa_pairs", []) or [])
                        if isinstance(welcome_1st_batch.get(user_id), dict):
                            qa_sources.append(welcome_1st_batch[user_id].get("qa_pairs", []) or [])

                        for qa_pairs in qa_sources:
                            for qa in qa_pairs:
                                if not isinstance(qa, dict):
                                    continue
                                q_text = normalize_value(qa.get("q_text"))
                                answer_candidates = [
                                    qa.get("answer"),
                                    qa.get("answer_text"),
                                    qa.get("value"),
                                ]
                                answers: List[str] = []
                                for candidate in answer_candidates:
                                    if candidate is None:
                                        continue
                                    if isinstance(candidate, list):
                                        answers.extend(str(item) for item in candidate if item)
                                    else:
                                        answers.append(str(candidate))
                                normalized_answers = {normalize_value(ans) for ans in answers if ans}

                                if q_text and normalized_answers:
                                    if q_text in {"ì§ì—…", "ì§ë¬´", "occupation"}:
                                        doc_values[DemographicType.OCCUPATION].update(normalized_answers)

                    return doc_values, metadata_presence

                filtered_list = []
                source_not_found_count = 0
                gender_filter_failed = 0
                age_filter_failed = 0
                occupation_filter_failed = 0
                gender_metadata_missing = 0
                age_metadata_missing = 0
                occupation_metadata_missing = 0

                for doc in rrf_results:
                    user_id = doc_user_map.get(id(doc))
                    if not user_id:
                        continue

                    source = user_rrf_map.get(user_id, [{}])[0].get('_source', {})
                    if not isinstance(source, dict):
                        source = {}

                    metadata_1st = welcome_1st_batch.get(user_id, {}) if isinstance(welcome_1st_batch.get(user_id), dict) else {}
                    metadata_2nd = welcome_2nd_batch.get(user_id, {}) if isinstance(welcome_2nd_batch.get(user_id), dict) else {}
                    metadata_2nd_full = welcome_2nd_batch.get(user_id, {})

                    doc_values, metadata_presence = collect_doc_values(user_id, source, metadata_1st, metadata_2nd)

                    gender_pass = True
                    age_pass = True
                    occupation_pass = True

                    if DemographicType.GENDER in filters_to_validate:
                        expected = set()
                        for demo in demographic_filters[DemographicType.GENDER]:
                            expected.update(build_expected_values(demo))
                        expand_gender_aliases(doc_values[DemographicType.GENDER])
                        gender_pass = values_match(doc_values[DemographicType.GENDER], expected)
                        if not gender_pass:
                            gender_filter_failed += 1
                            if logger.isEnabledFor(logging.DEBUG):
                                logger.debug(
                                    "ğŸš« ì„±ë³„ ë¶ˆì¼ì¹˜ | user_id=%s | doc_values=%s | expected=%s | metadata=%s",
                                    user_id,
                                    doc_values[DemographicType.GENDER],
                                    expected,
                                    {
                                        "metadata_1st": metadata_1st,
                                        "metadata_2nd": metadata_2nd,
                                    },
                                )

                    if gender_pass and DemographicType.AGE in filters_to_validate:
                        expected = set()
                        for demo in demographic_filters[DemographicType.AGE]:
                            expected.update(build_expected_values(demo))
                        age_pass = values_match(doc_values[DemographicType.AGE], expected)
                        if not age_pass:
                            age_filter_failed += 1

                    if gender_pass and age_pass and DemographicType.OCCUPATION in filters_to_validate:
                        expected = set()
                        for demo in demographic_filters[DemographicType.OCCUPATION]:
                            expected.update(build_expected_values(demo))
                        if not metadata_presence[DemographicType.OCCUPATION]:
                            occupation_metadata_missing += 1
                        occupation_pass = values_match(doc_values[DemographicType.OCCUPATION], expected)
                        if not occupation_pass:
                            occupation_filter_failed += 1
                        else:
                            display_occupation = None
                            occupation_candidates = [
                                metadata_2nd.get("occupation"),
                                metadata_2nd.get("job"),
                                metadata_2nd.get("occupation_group"),
                            ]
                            for candidate in occupation_candidates:
                                normalized_candidate = normalize_value(candidate)
                                if normalized_candidate and values_match({normalized_candidate}, expected):
                                    display_occupation = str(candidate)
                                    break
                            if not display_occupation:
                                qa_sources: List[List[Dict[str, Any]]] = []
                                if isinstance(source, dict):
                                    qa_sources.append(source.get("qa_pairs", []) or [])
                                if isinstance(metadata_2nd_full, dict):
                                    qa_sources.append(metadata_2nd_full.get("qa_pairs", []) or [])
                                for qa_pairs in qa_sources:
                                    for qa in qa_pairs:
                                        if not isinstance(qa, dict):
                                            continue
                                        q_text = str(qa.get("q_text", "")).lower()
                                        if not any(keyword in q_text for keyword in ("ì§ì—…", "ì§ë¬´", "occupation", "ì§ì¢…")):
                                            continue
                                        answer = qa.get("answer")
                                        if answer is None:
                                            answer = qa.get("answer_text")
                                        if answer is None:
                                            continue
                                        candidate_value = str(answer)
                                        normalized_candidate = normalize_value(candidate_value)
                                        if normalized_candidate and values_match({normalized_candidate}, expected):
                                            display_occupation = candidate_value
                                            break
                                    if display_occupation:
                                        break
                            if display_occupation:
                                occupation_display_map[user_id] = display_occupation

                    if gender_pass and age_pass and occupation_pass:
                        filtered_list.append(doc)

                filter_duration_ms = (perf_counter() - filter_start) * 1000
                timings["post_filter_ms"] = filter_duration_ms
                filtered_rrf_results = filtered_list

                logger.info(f"  - ì†ŒìŠ¤ ëˆ„ë½ ë¬¸ì„œ: {source_not_found_count}ê±´")
                if DemographicType.GENDER in filters_to_validate:
                    logger.info(f"  - ì„±ë³„ metadata ì—†ìŒ: {gender_metadata_missing}ê±´")
                    logger.info(f"  - ì„±ë³„ í•„í„° ë¯¸ì¶©ì¡±: {gender_filter_failed}ê±´")
                if DemographicType.AGE in filters_to_validate:
                    logger.info(f"  - ì—°ë ¹ metadata ì—†ìŒ: {age_metadata_missing}ê±´")
                    logger.info(f"  - ì—°ë ¹ í•„í„° ë¯¸ì¶©ì¡±: {age_filter_failed}ê±´")
                if DemographicType.OCCUPATION in filters_to_validate:
                    logger.info(f"  - ì§ì—… metadata ì—†ìŒ: {occupation_metadata_missing}ê±´")
                    logger.info(f"  - ì§ì—… í•„í„° ë¯¸ì¶©ì¡±: {occupation_filter_failed}ê±´")
                logger.info(f"  - í•„í„° ì¡°ê±´ ì¶©ì¡± ë¬¸ì„œ: {len(filtered_rrf_results)}ê±´")
        else:
            timings.setdefault('post_filter_ms', timings.get('post_filter_ms', 0.0))

            def collect_doc_values(
                user_id: str,
                source: Dict[str, Any],
                metadata_1st: Dict[str, Any],
                metadata_2nd: Dict[str, Any],
            ) -> Tuple[Dict[DemographicType, Set[str]], Dict[DemographicType, bool]]:
                doc_values: Dict[DemographicType, Set[str]] = {
                    DemographicType.GENDER: set(),
                    DemographicType.AGE: set(),
                    DemographicType.OCCUPATION: set(),
                }
                metadata_presence: Dict[DemographicType, bool] = {
                    DemographicType.GENDER: False,
                    DemographicType.AGE: False,
                    DemographicType.OCCUPATION: False,
                }

                # Common metadata sources
                metadata_candidates = [
                    metadata_1st,
                    metadata_2nd,
                    source.get("metadata", {}) if isinstance(source, dict) else {},
                ]

                payload = {}
                if isinstance(source, dict):
                    payload_candidate = source.get("payload")
                    if isinstance(payload_candidate, dict):
                        payload = payload_candidate
                if not payload and isinstance(source, dict) and "doc" in source:
                    doc_payload = source.get("doc", {}).get("payload")
                    if isinstance(doc_payload, dict):
                        payload = doc_payload
                if not payload and isinstance(source, dict):
                    payload = source

                if isinstance(payload, dict):
                    metadata_candidates.append(payload.get("metadata", {}))

                for candidate in metadata_candidates:
                    if not isinstance(candidate, dict):
                        continue

                    gender_val = candidate.get("gender") or candidate.get("gender_code")
                    if gender_val:
                        normalized_gender = normalize_value(gender_val)
                        if normalized_gender:
                            doc_values[DemographicType.GENDER].add(normalized_gender)
                            metadata_presence[DemographicType.GENDER] = True

                    age_group_val = candidate.get("age_group")
                    if age_group_val:
                        normalized_age_group = normalize_value(age_group_val)
                        if normalized_age_group:
                            doc_values[DemographicType.AGE].add(normalized_age_group)
                            metadata_presence[DemographicType.AGE] = True

                    age_val = candidate.get("age")
                    if age_val:
                        pre_count = len(doc_values[DemographicType.AGE])
                        add_age_decade(doc_values[DemographicType.AGE], age_val)
                        if len(doc_values[DemographicType.AGE]) > pre_count:
                            metadata_presence[DemographicType.AGE] = True

                    birth_year_val = candidate.get("birth_year")
                    if birth_year_val:
                        normalized_birth_year = normalize_value(birth_year_val)
                        if normalized_birth_year:
                            doc_values[DemographicType.AGE].add(normalized_birth_year)
                            metadata_presence[DemographicType.AGE] = True

                    occupation_val = candidate.get("occupation") or candidate.get("job")
                    if occupation_val:
                        normalized_occupation = normalize_value(occupation_val)
                        if normalized_occupation:
                            doc_values[DemographicType.OCCUPATION].add(normalized_occupation)
                            metadata_presence[DemographicType.OCCUPATION] = True

                    job_group_val = candidate.get("job_group") or candidate.get("occupation_group")
                    if job_group_val:
                        normalized_job_group = normalize_value(job_group_val)
                        if normalized_job_group:
                            doc_values[DemographicType.OCCUPATION].add(normalized_job_group)
                            metadata_presence[DemographicType.OCCUPATION] = True

                # QA ê¸°ë°˜ ë³´ì™„ (ì§ì—…) - ë©”íƒ€ë°ì´í„°ê°€ ë¹„ì—ˆì„ ë•Œë§Œ ì‚¬ìš©
                if not metadata_presence[DemographicType.OCCUPATION]:
                    qa_sources: List[List[Dict[str, Any]]] = []
                    if isinstance(source, dict):
                        qa_sources.append(source.get("qa_pairs", []) or [])
                    welcome_2nd_doc = welcome_2nd_batch.get(user_id, {})
                    if isinstance(welcome_2nd_doc, dict):
                        qa_sources.append(welcome_2nd_doc.get("qa_pairs", []) or [])

                    for qa_pairs in qa_sources:
                        for qa in qa_pairs:
                            if not isinstance(qa, dict):
                                continue
                            q_text = str(qa.get("q_text", "")).lower()
                            answer_text = qa.get("answer") or qa.get("answer_text")
                            if not answer_text:
                                continue
                            if any(keyword in q_text for keyword in ("ì§ì—…", "ì§ë¬´", "occupation", "ì§ì¢…")):
                                normalized_answer = normalize_value(answer_text)
                                if normalized_answer:
                                    doc_values[DemographicType.OCCUPATION].add(normalized_answer)

                # Normalize
                for demo_type, values in doc_values.items():
                    normalized = {normalize_value(v) for v in values if v}
                    if demo_type == DemographicType.GENDER:
                        expand_gender_aliases(normalized)
                    doc_values[demo_type] = normalized

                return doc_values, metadata_presence

            filtered_list: List[Dict[str, Any]] = []
            source_not_found_count = 0
            gender_filter_failed = 0
            age_filter_failed = 0
            occupation_filter_failed = 0
            gender_metadata_missing = 0
            age_metadata_missing = 0
            occupation_metadata_missing = 0
            for doc in rrf_results:
                source = doc.get("_source")
                if not source and "doc" in doc:
                    source = doc.get("doc", {}).get("_source")
                if not source and "payload" in doc:
                    source = doc.get("payload")

                if not isinstance(source, dict):
                    source_not_found_count += 1
                    continue

                user_id = source.get("user_id") or doc.get("_id") or doc.get("id")
                if not user_id and "payload" in doc and isinstance(doc["payload"], dict):
                    user_id = doc["payload"].get("user_id")

                if not user_id:
                    source_not_found_count += 1
                    continue

                welcome_1st_doc_full = welcome_1st_batch.get(user_id, {})
                metadata_1st = welcome_1st_doc_full.get("metadata", {}) if isinstance(welcome_1st_doc_full, dict) else {}
                welcome_2nd_doc_full = welcome_2nd_batch.get(user_id, {})
                metadata_2nd = welcome_2nd_doc_full.get("metadata", {}) if isinstance(welcome_2nd_doc_full, dict) else {}

                doc_values, metadata_presence = collect_doc_values(user_id, source, metadata_1st, metadata_2nd)

                gender_pass = True
                age_pass = True
                occupation_pass = True

                if demographic_filters.get(DemographicType.GENDER):
                    expected = set()
                    for demo in demographic_filters[DemographicType.GENDER]:
                        expected.update(build_expected_values(demo))
                    if not metadata_presence[DemographicType.GENDER]:
                        gender_metadata_missing += 1
                    gender_pass = values_match(doc_values[DemographicType.GENDER], expected)
                    if not gender_pass:
                        gender_filter_failed += 1

                if gender_pass and demographic_filters.get(DemographicType.AGE):
                    expected = set()
                    for demo in demographic_filters[DemographicType.AGE]:
                        expected.update(build_expected_values(demo))
                    if not metadata_presence[DemographicType.AGE]:
                        age_metadata_missing += 1
                    age_pass = values_match(doc_values[DemographicType.AGE], expected)
                    if not age_pass:
                        age_filter_failed += 1

                if gender_pass and age_pass and demographic_filters.get(DemographicType.OCCUPATION):
                    expected = set()
                    for demo in demographic_filters[DemographicType.OCCUPATION]:
                        expected.update(build_expected_values(demo))
                    if not metadata_presence[DemographicType.OCCUPATION]:
                        occupation_metadata_missing += 1
                    occupation_pass = values_match(doc_values[DemographicType.OCCUPATION], expected)
                    if not occupation_pass:
                        occupation_filter_failed += 1
                    else:
                        display_occupation = None
                        occupation_candidates = [
                            metadata_2nd.get("occupation"),
                            metadata_2nd.get("job"),
                            metadata_2nd.get("occupation_group"),
                        ]
                        for candidate in occupation_candidates:
                            normalized_candidate = normalize_value(candidate)
                            if normalized_candidate and values_match({normalized_candidate}, expected):
                                display_occupation = str(candidate)
                                break
                        if not display_occupation:
                            qa_sources: List[List[Dict[str, Any]]] = []
                            if isinstance(source, dict):
                                qa_sources.append(source.get("qa_pairs", []) or [])
                            if isinstance(welcome_2nd_doc_full, dict):
                                qa_sources.append(welcome_2nd_doc_full.get("qa_pairs", []) or [])
                            for qa_pairs in qa_sources:
                                for qa in qa_pairs:
                                    if not isinstance(qa, dict):
                                        continue
                                    q_text = str(qa.get("q_text", "")).lower()
                                    if not any(keyword in q_text for keyword in ("ì§ì—…", "ì§ë¬´", "occupation", "ì§ì¢…")):
                                        continue
                                    answer = qa.get("answer")
                                    if answer is None:
                                        answer = qa.get("answer_text")
                                    if answer is None:
                                        continue
                                    candidate = str(answer)
                                    normalized_candidate = normalize_value(candidate)
                                    if normalized_candidate and values_match({normalized_candidate}, expected):
                                        display_occupation = candidate
                                        break
                                if display_occupation:
                                    break
                        if display_occupation:
                            occupation_display_map[user_id] = display_occupation

                if gender_pass and age_pass and occupation_pass:
                    filtered_list.append(doc)

            filter_duration_ms = (perf_counter() - filter_start) * 1000
            timings["post_filter_ms"] = filter_duration_ms
            filtered_rrf_results = filtered_list

            logger.info(f"  - ì†ŒìŠ¤ ëˆ„ë½ ë¬¸ì„œ: {source_not_found_count}ê±´")
            if demographic_filters.get(DemographicType.GENDER):
                logger.info(f"  - ì„±ë³„ metadata ì—†ìŒ: {gender_metadata_missing}ê±´")
            logger.info(f"  - ì„±ë³„ í•„í„° ë¯¸ì¶©ì¡±: {gender_filter_failed}ê±´")
            if demographic_filters.get(DemographicType.AGE):
                logger.info(f"  - ì—°ë ¹ metadata ì—†ìŒ: {age_metadata_missing}ê±´")
            logger.info(f"  - ì—°ë ¹ í•„í„° ë¯¸ì¶©ì¡±: {age_filter_failed}ê±´")
            if demographic_filters.get(DemographicType.OCCUPATION):
                logger.info(f"  - ì§ì—… metadata ì—†ìŒ: {occupation_metadata_missing}ê±´")
            logger.info(f"  - ì§ì—… í•„í„° ë¯¸ì¶©ì¡±: {occupation_filter_failed}ê±´")
            logger.info(f"  - í•„í„° ì¡°ê±´ ì¶©ì¡± ë¬¸ì„œ: {len(filtered_rrf_results)}ê±´")

        lazy_join_start = perf_counter()
        final_hits = filtered_rrf_results[:window_size]
        results: List[SearchResult] = []
        inner_hits_map: Dict[str, List[Dict[str, Any]]] = {}

        for doc in final_hits:
            source = doc.get("_source")
            if not source and "doc" in doc:
                source = doc.get("doc", {}).get("_source")
            if not source and "payload" in doc:
                source = doc.get("payload")
            if not isinstance(source, dict):
                source = {}

            payload = {}
            payload_candidate = source.get("payload")
            if isinstance(payload_candidate, dict):
                payload = payload_candidate
            elif isinstance(doc.get("payload"), dict):
                payload = doc["payload"]

            user_id = (
                source.get("user_id")
                or payload.get("user_id")
                or doc.get("_id")
                or doc.get("id")
            )

            doc_info = None
            if user_id and user_id in user_doc_map:
                doc_info = user_doc_map[user_id]
            elif doc.get("_id") and doc.get("_id") in id_doc_map:
                doc_info = id_doc_map[doc.get("_id")]

            if doc_info:
                src_info = doc_info.get("source")
                if isinstance(src_info, dict):
                    merged_source = {}
                    merged_source.update(src_info)
                    merged_source.update(source)
                    source = merged_source
                    logger.debug(
                        "[mget_merge] user_id=%s qa_pairs=%d", 
                        user_id,
                        len(source.get('qa_pairs', [])) if isinstance(source, dict) else -1,
                    )
                inner_hit_wrapper = {"inner_hits": doc_info.get("inner_hits", {})}
            else:
                inner_hit_wrapper = doc

            metadata_2nd = source.get("metadata", {}) if isinstance(source, dict) else {}
            if not metadata_2nd and isinstance(payload, dict):
                metadata_2nd = payload.get("metadata", {}) or {}

            welcome_1st_doc = welcome_1st_batch.get(user_id, {}) if user_id else {}
            metadata_1st = (
                welcome_1st_doc.get("metadata", {}) if isinstance(welcome_1st_doc, dict) else {}
            )

            welcome_2nd_doc = welcome_2nd_batch.get(user_id, {}) if user_id else {}
            metadata_2nd_cached = (
                welcome_2nd_doc.get("metadata", {}) if isinstance(welcome_2nd_doc, dict) else {}
            )

            demographic_info: Dict[str, Any] = {}
            if metadata_1st:
                demographic_info["age_group"] = metadata_1st.get("age_group")
                demographic_info["gender"] = metadata_1st.get("gender")
                demographic_info["birth_year"] = metadata_1st.get("birth_year")

            occupation_candidate = metadata_2nd.get("occupation") if isinstance(metadata_2nd, dict) else None
            if not occupation_candidate and isinstance(metadata_2nd_cached, dict):
                occupation_candidate = metadata_2nd_cached.get("occupation")
            if not occupation_candidate and isinstance(payload, dict):
                occupation_candidate = payload.get("occupation")
            if occupation_candidate:
                demographic_info["occupation"] = occupation_candidate

            occupation_expected = set()
            for demo in demographic_filters.get(DemographicType.OCCUPATION, []):
                occupation_expected.update(build_expected_values(demo))

            if ("occupation" not in demographic_info or not demographic_info["occupation"]) and user_id:
                mapped_occupation = occupation_display_map.get(user_id) if has_demographic_filters else None
                if mapped_occupation:
                    demographic_info["occupation"] = mapped_occupation

            def occupation_matches(candidate: str) -> bool:
                normalized_candidate = normalize_value(candidate)
                if not normalized_candidate:
                    return False
                for expected in occupation_expected:
                    if not expected:
                        continue
                    if normalized_candidate == expected or normalized_candidate in expected or expected in normalized_candidate:
                        return True
                return False

            if ("occupation" not in demographic_info or not demographic_info["occupation"]) and isinstance(source, dict):
                qa_pairs_for_occ = source.get("qa_pairs", [])
                for qa in qa_pairs_for_occ:
                    if not isinstance(qa, dict):
                        continue
                    q_text = str(qa.get("q_text", "")).lower()
                    answer = qa.get("answer")
                    if answer is None:
                        answer = qa.get("answer_text")
                    if answer is None:
                        continue
                    answer_str = str(answer)
                    if any(keyword in q_text for keyword in ("ì§ì—…", "ì§ë¬´", "occupation", "ì§ì¢…")) and occupation_matches(answer_str):
                        demographic_info["occupation"] = answer_str
                        break

            matched_qa_pairs: List[Dict[str, Any]] = extract_inner_hit_matches(inner_hit_wrapper)
            if not matched_qa_pairs and analysis.must_terms:
                matched_qa_pairs = extract_matched_qa_pairs(source, analysis.must_terms)

            qa_pairs_display = reorder_with_matches(
                source.get("qa_pairs", []) if isinstance(source, dict) else [],
                matched_qa_pairs,
                limit=10
            )

            results.append(
                SearchResult(
                    user_id=user_id,
                    score=doc.get("_score", 0.0),
                    timestamp=source.get("timestamp") if isinstance(source, dict) else None,
                    demographic_info=demographic_info if demographic_info else None,
                    qa_pairs=qa_pairs_display[:5],
                    matched_qa_pairs=matched_qa_pairs,
                    highlights=doc.get("highlight"),
                )
            )
            logger.debug(
                "[match_check] user_id=%s inner_hits=%d matched=%d", 
                user_id,
                len(inner_hits_map.get(user_id, [])),
                len(matched_qa_pairs),
            )

        timings["lazy_join_ms"] = (perf_counter() - lazy_join_start) * 1000
        timings.setdefault('post_filter_ms', timings.get('post_filter_ms', 0.0))
        timings.setdefault('rrf_recombination_ms', 0.0)
        timings.setdefault('qdrant_parallel_ms', 0.0)
        timings.setdefault(
            'opensearch_parallel_ms',
            timings.get('two_phase_stage1_ms', 0.0) + timings.get('two_phase_stage2_ms', 0.0)
        )

        total_duration_ms = (perf_counter() - overall_start) * 1000
        timings['total_ms'] = total_duration_ms
        timings['cache_hit'] = 1.0 if cache_hit else 0.0

        serialized_results = [_serialize_result(res) for res in results]
        stored_items = serialized_results
        if cache_enabled and cache_limit > 0:
            stored_items = serialized_results[:cache_limit]
        page_results, has_more_local = _slice_results(stored_items, page, page_size)
        has_more = has_more_local and ((page * page_size) < total_hits)
        total_hits = len(filtered_rrf_results)
        max_score = results[0].score if results else 0.0
        response_took_ms = int(total_duration_ms)

        logger.info("ğŸ“ˆ ì„±ëŠ¥ ì¸¡ì • ìš”ì•½ (ms):")
        for key in sorted(timings.keys()):
            logger.info(f"  - {key}: {timings[key]:.2f}")

        summary_parts = [
            f"returned={len(page_results)}/{total_hits}",
            f"total_ms={response_took_ms}",
        ]
        if rrf_k_used is not None:
            summary_parts.append(f"rrf_k={rrf_k_used}")
        if adaptive_threshold is not None:
            summary_parts.append(f"qdrant_threshold={adaptive_threshold:.2f}")
        logger.info("âœ… ìµœì¢… ìš”ì•½: " + ", ".join(summary_parts))
        if rrf_reason:
            logger.info(f"   â€¢ RRF: {rrf_reason}")
        if threshold_reason:
            logger.info(f"   â€¢ Qdrant: {threshold_reason}")

        if cache_enabled and cache_key and stored_items:
            cache_payload = {
                "total_hits": total_hits,
                "max_score": max_score,
                "items": stored_items,
                "filters": filters_for_response,
                "extracted_entities": extracted_entities.to_dict(),
            }
            try:
                cache_client.setex(
                    cache_key,
                    cache_ttl,
                    json.dumps(cache_payload, ensure_ascii=False),
                )
                logger.info(f"ğŸ’¾ Redis ê²€ìƒ‰ ìºì‹œ ì €ì¥: key={cache_key}, ttl={cache_ttl}s")
            except Exception as cache_exc:
                logger.warning(f"âš ï¸ Redis ê²€ìƒ‰ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {cache_exc}")

        return SearchResponse(
            query=request.query,
            total_hits=total_hits,
            max_score=max_score,
            results=page_results,
            query_analysis={
                "intent": analysis.intent,
                "must_terms": analysis.must_terms,
                "should_terms": analysis.should_terms,
                "alpha": analysis.alpha,
                "confidence": analysis.confidence,
                "filters": filters_for_response,
                "size": page_size,
                "timings_ms": timings,
                "extracted_entities": extracted_entities.to_dict(),
            },
            took_ms=response_took_ms,
            page=page,
            page_size=page_size,
            has_more=has_more,
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[ERROR] ìì—°ì–´ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# -----------------------------
# Qdrant ì§„ë‹¨/í—¬ìŠ¤ ì—”ë“œí¬ì¸íŠ¸ (ì½ê¸° ì „ìš©)
# -----------------------------

@router.get("/debug/welcome-1st", summary="welcome_1st ì¸ë±ìŠ¤ ìƒ˜í”Œ ë°ì´í„° í™•ì¸ (ë””ë²„ê¹…ìš©)")
async def get_welcome_1st_samples(
    user_id: str = None,
    age_group: str = None,
    size: int = 5,
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """
    welcome_1st ì¸ë±ìŠ¤ì˜ ìƒ˜í”Œ ë°ì´í„° í™•ì¸ (ë””ë²„ê¹…ìš©)
    
    - user_idë¡œ íŠ¹ì • ì‚¬ìš©ì ì¡°íšŒ
    - age_groupìœ¼ë¡œ í•„í„°ë§
    - metadata êµ¬ì¡° í™•ì¸
    """
    try:
        if not os_client or not os_client.ping():
            raise HTTPException(status_code=503, detail="OpenSearch ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        query = {"match_all": {}}
        
        if user_id:
            # íŠ¹ì • user_id ì¡°íšŒ
            query = {"term": {"_id": user_id}}
        elif age_group:
            # age_groupìœ¼ë¡œ í•„í„°ë§
            query = {
                "term": {
                    "metadata.age_group.keyword": age_group
                }
            }
        
        response = os_client.search(
            index="s_welcome_1st",
            body={
                "query": query,
                "size": size,
                "_source": {
                    "includes": ["user_id", "metadata", "qa_pairs"]
                }
            }
        )
        
        results = []
        for hit in response['hits']['hits']:
            source = hit.get('_source', {})
            results.append({
                "_id": hit.get('_id'),
                "user_id": source.get('user_id'),
                "metadata": source.get('metadata', {}),
                "qa_pairs_sample": source.get('qa_pairs', [])[:10] if source.get('qa_pairs') else []
            })
        
        return {
            "index_name": "s_welcome_1st",
            "query": {
                "user_id": user_id,
                "age_group": age_group
            },
            "total_hits": response['hits']['total']['value'],
            "samples": results
        }
    
    except Exception as e:
        logger.error(f"[ERROR] welcome_1st ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/debug/welcome-2nd", summary="welcome_2nd ì¸ë±ìŠ¤ ìƒ˜í”Œ ë°ì´í„° í™•ì¸ (ë””ë²„ê¹…ìš©)")
async def get_welcome_2nd_samples(
    user_id: str = None,
    occupation: str = None,
    size: int = 5,
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """
    welcome_2nd ì¸ë±ìŠ¤ì˜ ìƒ˜í”Œ ë°ì´í„° í™•ì¸ (ë””ë²„ê¹…ìš©)
    
    - user_idë¡œ íŠ¹ì • ì‚¬ìš©ì ì¡°íšŒ
    - occupationìœ¼ë¡œ í•„í„°ë§ (qa_pairsì—ì„œ)
    - metadata êµ¬ì¡° í™•ì¸
    """
    try:
        if not os_client or not os_client.ping():
            raise HTTPException(status_code=503, detail="OpenSearch ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        query = {"match_all": {}}
        
        if user_id:
            # íŠ¹ì • user_id ì¡°íšŒ
            query = {"term": {"_id": user_id}}
        elif occupation:
            # qa_pairsì—ì„œ ì§ì—…ìœ¼ë¡œ í•„í„°ë§
            query = {
                "nested": {
                    "path": "qa_pairs",
                    "query": {
                        "bool": {
                            "must": [
                                {"match": {"qa_pairs.q_text": "ì§ì—…"}},
                                {"match": {"qa_pairs.answer_text": occupation}}
                            ]
                        }
                    }
                }
            }
        
        response = os_client.search(
            index="s_welcome_2nd",
            body={
                "query": query,
                "size": size,
                "_source": {
                    "includes": ["user_id", "metadata", "qa_pairs"]
                }
            }
        )
        
        results = []
        for hit in response['hits']['hits']:
            source = hit.get('_source', {})
            results.append({
                "_id": hit.get('_id'),
                "user_id": source.get('user_id'),
                "metadata": source.get('metadata', {}),
                "qa_pairs_sample": source.get('qa_pairs', [])[:10] if source.get('qa_pairs') else []
            })
        
        return {
            "index_name": "s_welcome_2nd",
            "query": {
                "user_id": user_id,
                "occupation": occupation
            },
            "total_hits": response['hits']['total']['value'],
            "samples": results
        }
    
    except Exception as e:
        logger.error(f"[ERROR] welcome_2nd ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/debug/sample-data", summary="ì¸ë±ìŠ¤ë³„ ìƒ˜í”Œ ë°ì´í„° í™•ì¸ (ë””ë²„ê¹…ìš©)")
async def get_sample_data(
    index_name: str = "*",
    question_keyword: str = None,
    answer_keyword: str = None,
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """
    ì¸ë±ìŠ¤ë³„ ìƒ˜í”Œ ë°ì´í„° í™•ì¸ (ë””ë²„ê¹…ìš©)
    
    - íŠ¹ì • ì§ˆë¬¸ í‚¤ì›Œë“œë¡œ ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ
    - íŠ¹ì • ë‹µë³€ í‚¤ì›Œë“œë¡œ ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ
    - ì‹¤ì œ ë‹µë³€ í˜•ì‹ í™•ì¸
    """
    try:
        if not os_client or not os_client.ping():
            raise HTTPException(status_code=503, detail="OpenSearch ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        query = {"match_all": {}}
        if question_keyword or answer_keyword:
            nested_query = {}
            if question_keyword and answer_keyword:
                nested_query = {
                    "bool": {
                        "must": [
                            {"match": {"qa_pairs.q_text": question_keyword}},
                            {
                                "bool": {
                                    "should": [
                                        {"match_phrase": {"qa_pairs.answer_text": answer_keyword}},
                                        {"match_phrase": {"qa_pairs.answer": answer_keyword}},
                                        {"match": {"qa_pairs.answer_text": {"query": answer_keyword, "operator": "or"}}},
                                        {"match": {"qa_pairs.answer": {"query": answer_keyword, "operator": "or"}}}
                                    ],
                                    "minimum_should_match": 1
                                }
                            }
                        ]
                    }
                }
            elif question_keyword:
                nested_query = {"match": {"qa_pairs.q_text": question_keyword}}
            elif answer_keyword:
                nested_query = {
                    "bool": {
                        "should": [
                            {"match_phrase": {"qa_pairs.answer_text": answer_keyword}},
                            {"match_phrase": {"qa_pairs.answer": answer_keyword}},
                            {"match": {"qa_pairs.answer_text": {"query": answer_keyword, "operator": "or"}}},
                            {"match": {"qa_pairs.answer": {"query": answer_keyword, "operator": "or"}}}
                        ],
                        "minimum_should_match": 1
                    }
                }
            
            query = {
                "nested": {
                    "path": "qa_pairs",
                    "query": nested_query,
                    "inner_hits": {
                        "size": 5,
                        "_source": {"includes": ["qa_pairs.q_text", "qa_pairs.answer_text", "qa_pairs.answer"]}
                    }
                }
            }
        
        response = os_client.search(
            index=index_name,
            body={
                "query": query,
                "size": 5,
                "_source": {"includes": ["user_id", "metadata", "qa_pairs"]}
            }
        )
        
        results = []
        for hit in response['hits']['hits']:
            source = hit.get('_source', {})
            result = {
                "index": hit.get('_index'),
                "user_id": source.get('user_id'),
                "metadata": source.get('metadata', {}),
                "qa_pairs_sample": source.get('qa_pairs', [])[:5]
            }
            
            if (question_keyword or answer_keyword) and 'inner_hits' in hit:
                result['matched_qa_pairs'] = []
                for inner_hit in hit['inner_hits']['qa_pairs']['hits']['hits']:
                    result['matched_qa_pairs'].append(inner_hit.get('_source', {}))
            
            results.append(result)
        
        return {
            "index_name": index_name,
            "question_keyword": question_keyword,
            "answer_keyword": answer_keyword,
            "total_hits": response['hits']['total']['value'],
            "samples": results
        }
    
    except Exception as e:
        logger.error(f"[ERROR] ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


class TestFiltersRequest(BaseModel):
    """í•„í„° í…ŒìŠ¤íŠ¸ ìš”ì²­"""
    filters: List[Dict[str, Any]] = Field(..., description="í…ŒìŠ¤íŠ¸í•  í•„í„° ë¦¬ìŠ¤íŠ¸")
    index_name: str = Field(default="*", description="ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ì´ë¦„")


@router.post("/debug/test-filters", summary="í•„í„° ê°œë³„ í…ŒìŠ¤íŠ¸ (ë””ë²„ê¹…ìš©)")
async def test_filters(
    request: TestFiltersRequest,
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """
    í•„í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ì—¬ ì–´ë–¤ ì¸ë±ìŠ¤ì—ì„œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
    
    - ê° í•„í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì‹¤í–‰
    - ì¸ë±ìŠ¤ë³„ ê²°ê³¼ ê°œìˆ˜ í™•ì¸
    
    ì‚¬ìš© ì˜ˆì‹œ:
    ```json
    {
      "filters": [
        {
          "bool": {
            "should": [
              {"term": {"metadata.age_group.keyword": "30ëŒ€"}}
            ],
            "minimum_should_match": 1
          }
        }
      ],
      "index_name": "*"
    }
    ```
    """
    try:
        if not os_client or not os_client.ping():
            raise HTTPException(status_code=503, detail="OpenSearch ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        filters = request.filters
        index_name = request.index_name
        
        results = []
        for i, filter_dict in enumerate(filters):
            # ê° í•„í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            query = {
                "query": {
                    "bool": {
                        "must": [filter_dict]
                    }
                },
                "size": 0,  # ê°œìˆ˜ë§Œ í™•ì¸
                "aggs": {
                    "by_index": {
                        "terms": {
                            "field": "_index",
                            "size": 20
                        }
                    }
                }
            }
            
            response = os_client.search(
                index=request.index_name,
                body=query
            )
            
            # ì¸ë±ìŠ¤ë³„ ê²°ê³¼ ê°œìˆ˜
            index_counts = {}
            if 'aggregations' in response and 'by_index' in response['aggregations']:
                for bucket in response['aggregations']['by_index']['buckets']:
                    index_counts[bucket['key']] = bucket['doc_count']
            
            results.append({
                "filter_index": i,
                "filter": filter_dict,
                "total_hits": response['hits']['total']['value'],
                "index_counts": index_counts
            })
        
        # ëª¨ë“  í•„í„°ë¥¼ ANDë¡œ ê²°í•©í•œ ê²°ê³¼ë„ í…ŒìŠ¤íŠ¸
        if len(filters) > 1:
            combined_query = {
                "query": {
                    "bool": {
                        "must": filters
                    }
                },
                "size": 0,
                "aggs": {
                    "by_index": {
                        "terms": {
                            "field": "_index",
                            "size": 20
                        }
                    }
                }
            }
            
            combined_response = os_client.search(
                index=request.index_name,
                body=combined_query
            )
            
            combined_index_counts = {}
            if 'aggregations' in combined_response and 'by_index' in combined_response['aggregations']:
                for bucket in combined_response['aggregations']['by_index']['buckets']:
                    combined_index_counts[bucket['key']] = bucket['doc_count']
            
            results.append({
                "filter_index": "combined",
                "filter": "ALL FILTERS (AND)",
                "total_hits": combined_response['hits']['total']['value'],
                "index_counts": combined_index_counts
            })
        
        return {
            "index_name": request.index_name,
            "results": results
        }
    
    except Exception as e:
        logger.error(f"[ERROR] í•„í„° í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/qdrant/collections", summary="Qdrant ì»¬ë ‰ì…˜ ëª©ë¡ ë° í†µê³„")
async def list_qdrant_collections():
    qdrant_client = getattr(router, 'qdrant_client', None)
    if not qdrant_client:
        raise HTTPException(status_code=503, detail="Qdrant í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    try:
        cols = qdrant_client.get_collections()
        items = []
        for c in cols.collections:
            try:
                info = qdrant_client.get_collection(c.name)
                items.append({
                    "name": c.name,
                    "vectors_count": info.vectors_count if hasattr(info, 'vectors_count') else None,
                    "points_count": getattr(info, 'points_count', None),
                    "config": getattr(info, 'config', None).__dict__ if hasattr(info, 'config') else None,
                })
            except Exception:
                items.append({"name": c.name})
        return {"collections": items}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


class QdrantTestSearchRequest(BaseModel):
    query: str = Field(..., description="ì„ë² ë”©ìœ¼ë¡œ ê²€ìƒ‰í•  í…ìŠ¤íŠ¸")
    limit: int = Field(5, ge=1, le=100)


@router.post("/qdrant/test-search", summary="Qdrant ì „ ì»¬ë ‰ì…˜ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ (ì½ê¸° ì „ìš©)")
async def qdrant_test_search(req: QdrantTestSearchRequest):
    qdrant_client = getattr(router, 'qdrant_client', None)
    embedding_model = getattr(router, 'embedding_model', None)
    if not qdrant_client:
        raise HTTPException(status_code=503, detail="Qdrant í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if not embedding_model:
        raise HTTPException(status_code=503, detail="ì„ë² ë”© ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    try:
        qvec = embedding_model.encode(req.query).tolist()
        cols = qdrant_client.get_collections()
        results = []
        for c in cols.collections:
            try:
                r = qdrant_client.search(
                    collection_name=c.name,
                    query_vector=qvec,
                    limit=req.limit,
                )
                results.append({
                    "collection": c.name,
                    "hits": [
                        {
                            "id": str(h.id),
                            "score": h.score,
                            "payload": getattr(h, 'payload', None)
                        } for h in r
                    ]
                })
            except Exception as e:
                results.append({"collection": c.name, "error": str(e)})
        return {"query": req.query, "results": results}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/similar", summary="ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (í”Œë ˆì´ìŠ¤í™€ë”)")
async def search_similar(
    user_id: str,
    index_name: str = "s_welcome_2nd",
    size: int = 10
):
    """
    íŠ¹ì • ì‚¬ìš©ìì™€ ìœ ì‚¬í•œ ì‘ë‹µì„ ê°€ì§„ ì‚¬ìš©ì ê²€ìƒ‰ (í–¥í›„ êµ¬í˜„)
    """
    raise HTTPException(
        status_code=501,
        detail="ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ê¸°ëŠ¥ì€ í–¥í›„ êµ¬í˜„ ì˜ˆì •ì…ë‹ˆë‹¤."
    )


@router.get("/stats/{index_name}", summary="ê²€ìƒ‰ í†µê³„")
async def get_search_stats(
    index_name: str,
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """ì¸ë±ìŠ¤ ê²€ìƒ‰ í†µê³„ ì¡°íšŒ"""
    try:
        if not os_client.indices.exists(index=index_name):
            raise HTTPException(
                status_code=404,
                detail=f"ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {index_name}"
            )

        stats = os_client.indices.stats(index=index_name)
        count = os_client.count(index=index_name)

        return {
            "index_name": index_name,
            "doc_count": count['count'],
            "size_mb": round(stats['_all']['total']['store']['size_in_bytes'] / 1024 / 1024, 2),
            "search_total": stats['_all']['total']['search']['query_total'],
            "search_time_ms": stats['_all']['total']['search']['query_time_in_millis']
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[ERROR] í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


def _filter_to_string(filter_dict: Dict[str, Any]) -> str:
    try:
        return json.dumps(filter_dict, ensure_ascii=False)
    except Exception:
        return str(filter_dict)


AGE_GENDER_KEYWORDS = [
    "metadata.age_group", "metadata.gender", "birth_year", "ì—°ë ¹", "ë‚˜ì´", "ì„±ë³„"
]
OCCUPATION_KEYWORDS = [
    "metadata.occupation", "occupation", "ì§ì—…", "ì§ë¬´"
]


def is_age_or_gender_filter(filter_dict: Dict[str, Any]) -> bool:
    filter_str = _filter_to_string(filter_dict)
    return any(keyword in filter_str for keyword in AGE_GENDER_KEYWORDS)


def is_occupation_filter(filter_dict: Dict[str, Any]) -> bool:
    filter_str = _filter_to_string(filter_dict)
    return any(keyword in filter_str for keyword in OCCUPATION_KEYWORDS)


async def run_two_phase_demographic_search(
    request,
    analysis,
    extracted_entities,
    filters: List[Dict[str, Any]],
    size: int,
    age_gender_filters: List[Dict[str, Any]],
    occupation_filters: List[Dict[str, Any]],
    data_fetcher: "DataFetcher",
    timings: Dict[str, float],
    overall_start: float,
) -> Optional[SearchResponse]:
    """ë‘ ë‹¨ê³„ ê²€ìƒ‰ìœ¼ë¡œ user_idë¥¼ ë¨¼ì € ì¢íˆê³  ì •ë°€ ì¡°íšŒ"""
    logger.info("ğŸš€ ë‘ ë‹¨ê³„ ì¸êµ¬í†µê³„ ìµœì í™” ì‹¤í–‰")

    async_client = data_fetcher.os_async_client
    sync_client = data_fetcher.os_client
    if not (async_client or sync_client):
        logger.warning("âš ï¸ OpenSearch í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ 2ë‹¨ê³„ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤")
        return None

    stage1_start = perf_counter()
    stage1_query_size = min(max(size * 50, 2000), 10000)
    stage1_query = {
        "query": {
            "bool": {
                "must": age_gender_filters
            }
        },
        "size": stage1_query_size,
        "_source": ["user_id"],
        "track_total_hits": True
    }

    try:
        if async_client:
            response_1st = await data_fetcher.search_opensearch_async(
                index_name="s_welcome_1st",
                query=stage1_query,
                size=stage1_query_size,
                source_filter=None,
                request_timeout=DEFAULT_OS_TIMEOUT,
            )
        else:
            response_1st = data_fetcher.search_opensearch(
                index_name="s_welcome_1st",
                query=stage1_query,
                size=stage1_query_size,
                source_filter=None,
                request_timeout=DEFAULT_OS_TIMEOUT,
            )
    except Exception as e:
        logger.warning(f"âš ï¸ 2ë‹¨ê³„ ê²€ìƒ‰ Stage1 ì‹¤íŒ¨: {e}")
        return None

    timings['two_phase_stage1_ms'] = (perf_counter() - stage1_start) * 1000
    hits_1st = response_1st.get('hits', {}).get('hits', [])
    total_stage1 = response_1st.get('hits', {}).get('total', {}).get('value', len(hits_1st))

    if not hits_1st:
        logger.info("   âš ï¸ Stage1ì—ì„œ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” user_idê°€ ì—†ìŠµë‹ˆë‹¤")
        total_time = (perf_counter() - overall_start) * 1000
        timings['total_ms'] = total_time
        timings.setdefault('two_phase_stage2_ms', 0.0)
        timings.setdefault('two_phase_fetch_demographics_ms', 0.0)
        timings.setdefault('lazy_join_ms', 0.0)
        timings.setdefault('post_filter_ms', 0.0)
        timings.setdefault('rrf_recombination_ms', 0.0)
        timings.setdefault('qdrant_parallel_ms', 0.0)
        timings.setdefault('opensearch_parallel_ms', timings['two_phase_stage1_ms'])
        logger.info("ğŸ“ˆ ì„±ëŠ¥ ì¸¡ì • ìš”ì•½ (ms):")
        for key in sorted(timings.keys()):
            logger.info(f"  - {key}: {timings[key]:.2f}")
        return SearchResponse(
            query=request.query,
            total_hits=0,
            max_score=0.0,
            results=[],
            query_analysis={
                "intent": analysis.intent,
                "must_terms": analysis.must_terms,
                "should_terms": analysis.should_terms,
                "alpha": analysis.alpha,
                "confidence": analysis.confidence,
                "extracted_entities": extracted_entities.to_dict(),
                "filters": filters,
                "size": size,
                "timings_ms": timings,
            },
            took_ms=int(total_time)
        )

    user_ids_filtered = []
    for hit in hits_1st:
        src = hit.get('_source', {})
        uid = src.get('user_id') or hit.get('_id')
        if uid:
            user_ids_filtered.append(uid)
    user_ids_filtered = list(dict.fromkeys(user_ids_filtered))

    logger.info(f"   âœ… Stage1 user_id ì¶”ì¶œ: {len(user_ids_filtered)}/{total_stage1}ê±´")
    if total_stage1 > len(user_ids_filtered):
        logger.warning("   âš ï¸ Stage1 size ì œí•œìœ¼ë¡œ ì¼ë¶€ user_idê°€ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤")

    if not user_ids_filtered:
        total_time = (perf_counter() - overall_start) * 1000
        timings['two_phase_stage2_ms'] = 0.0
        timings['two_phase_fetch_demographics_ms'] = 0.0
        timings['lazy_join_ms'] = 0.0
        timings['post_filter_ms'] = 0.0
        timings['rrf_recombination_ms'] = 0.0
        timings.setdefault('opensearch_parallel_ms', timings['two_phase_stage1_ms'])
        timings['total_ms'] = total_time
        logger.info("ğŸ“ˆ ì„±ëŠ¥ ì¸¡ì • ìš”ì•½ (ms):")
        for key in sorted(timings.keys()):
            logger.info(f"  - {key}: {timings[key]:.2f}")
        return SearchResponse(
            query=request.query,
            total_hits=0,
            max_score=0.0,
            results=[],
            query_analysis={
                "intent": analysis.intent,
                "must_terms": analysis.must_terms,
                "should_terms": analysis.should_terms,
                "alpha": analysis.alpha,
                "confidence": analysis.confidence,
                "extracted_entities": extracted_entities.to_dict(),
                "filters": filters,
                "size": size,
                "timings_ms": timings,
            },
            took_ms=int(total_time)
        )

    max_terms = 10000
    if len(user_ids_filtered) > max_terms:
        logger.warning(f"   âš ï¸ user_idê°€ {len(user_ids_filtered)}ê±´ì…ë‹ˆë‹¤. ìƒìœ„ {max_terms}ê±´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤")
        user_ids_filtered = user_ids_filtered[:max_terms]

    detail_size = max(size * 2, min(len(user_ids_filtered), 500))
    stage2_query = {
        "query": {
            "bool": {
                "must": [
                    {"terms": {"_id": user_ids_filtered}},
                ]
            }
        },
        "size": detail_size,
        "_source": {
            "includes": ["user_id", "metadata", "qa_pairs", "timestamp"]
        },
        "track_total_hits": True
    }

    stage2_start = perf_counter()
    try:
        if async_client:
            response_2nd = await data_fetcher.search_opensearch_async(
                index_name="s_welcome_2nd",
                query=stage2_query,
                size=detail_size,
                source_filter=None,
                request_timeout=DEFAULT_OS_TIMEOUT,
            )
        else:
            response_2nd = data_fetcher.search_opensearch(
                index_name="s_welcome_2nd",
                query=stage2_query,
                size=detail_size,
                source_filter=None,
                request_timeout=DEFAULT_OS_TIMEOUT,
            )
    except Exception as e:
        logger.warning(f"âš ï¸ 2ë‹¨ê³„ ê²€ìƒ‰ Stage2 ì‹¤íŒ¨: {e}")
        return None

    timings['two_phase_stage2_ms'] = (perf_counter() - stage2_start) * 1000
    hits_2nd = response_2nd.get('hits', {}).get('hits', [])
    total_stage2 = response_2nd.get('hits', {}).get('total', {}).get('value', len(hits_2nd))
    logger.info(f"   âœ… Stage2 ê²°ê³¼: {len(hits_2nd)}ê±´ (ì´ {total_stage2}ê±´)")

    if not hits_2nd:
        total_time = (perf_counter() - overall_start) * 1000
        timings.setdefault('two_phase_fetch_demographics_ms', 0.0)
        timings['lazy_join_ms'] = 0.0
        timings['post_filter_ms'] = 0.0
        timings['rrf_recombination_ms'] = 0.0
        timings.setdefault('opensearch_parallel_ms', timings.get('two_phase_stage1_ms', 0.0))
        timings['total_ms'] = total_time
        logger.info("ğŸ“ˆ ì„±ëŠ¥ ì¸¡ì • ìš”ì•½ (ms):")
        for key in sorted(timings.keys()):
            logger.info(f"  - {key}: {timings[key]:.2f}")
        return SearchResponse(
            query=request.query,
            total_hits=0,
            max_score=0.0,
            results=[],
            query_analysis={
                "intent": analysis.intent,
                "must_terms": analysis.must_terms,
                "should_terms": analysis.should_terms,
                "alpha": analysis.alpha,
                "confidence": analysis.confidence,
                "extracted_entities": extracted_entities.to_dict(),
                "filters": filters,
                "size": size,
                "timings_ms": timings,
            },
            took_ms=int(total_time)
        )

    final_hits = hits_2nd[:size]
    final_user_ids = [hit.get('_id') or hit.get('_source', {}).get('user_id') for hit in final_hits]

    fetch_start = perf_counter()
    welcome_1st_docs: Dict[str, Dict[str, Any]] = {}
    welcome_2nd_docs: Dict[str, Dict[str, Any]] = {}

    if final_user_ids:
        if async_client:
            welcome_1st_docs = await data_fetcher.multi_get_documents_async(
                index_name="s_welcome_1st",
                doc_ids=final_user_ids,
                batch_size=200,
                source_fields=["metadata", "user_id", "qa_pairs"],
            )
            welcome_2nd_docs = await data_fetcher.multi_get_documents_async(
                index_name="s_welcome_2nd",
                doc_ids=final_user_ids,
                batch_size=200,
                source_fields=["metadata", "user_id", "qa_pairs"],
            )
        else:
            response = sync_client.mget(index="s_welcome_1st", body={"ids": final_user_ids}, _source=["metadata", "user_id", "qa_pairs"])
            for doc in response.get('docs', []):
                if doc.get('found'):
                    welcome_1st_docs[doc['_id']] = doc['_source']
            response = sync_client.mget(index="s_welcome_2nd", body={"ids": final_user_ids}, _source=["metadata", "user_id", "qa_pairs"])
            for doc in response.get('docs', []):
                if doc.get('found'):
                    welcome_2nd_docs[doc['_id']] = doc['_source']
    timings['two_phase_fetch_demographics_ms'] = (perf_counter() - fetch_start) * 1000

    results: List[SearchResult] = []
    lazy_join_start = perf_counter()
    final_hits = final_hits if 'final_hits' in locals() else []
    for doc in final_hits:
        source = doc.get('_source', {}) or {}
        user_id = source.get('user_id') or hit.get('_id', '')
        metadata_2nd = source.get('metadata', {}) if isinstance(source, dict) else {}
        welcome_1st_doc = welcome_1st_docs.get(user_id, {})
        metadata_1st = welcome_1st_doc.get('metadata', {}) if isinstance(welcome_1st_doc, dict) else {}

        demographic_info: Dict[str, Any] = {}
        if metadata_1st:
            demographic_info['age_group'] = metadata_1st.get('age_group')
            demographic_info['gender'] = metadata_1st.get('gender')
            demographic_info['birth_year'] = metadata_1st.get('birth_year')
        if metadata_2nd:
            demographic_info['occupation'] = metadata_2nd.get('occupation')

        if 'occupation' not in demographic_info or not demographic_info['occupation']:
            qa_pairs_for_occ = source.get('qa_pairs', []) if isinstance(source, dict) else []
            for qa in qa_pairs_for_occ:
                if isinstance(qa, dict):
                    q_text = qa.get('q_text', '')
                    answer = str(qa.get('answer', qa.get('answer_text', '')))
                    if 'ì§ì—…' in q_text or 'occupation' in q_text.lower() or 'ì§ë¬´' in q_text:
                        if answer:
                            demographic_info['occupation'] = answer
                        break

        matched_qa = []
        inner_hits = hit.get('inner_hits', {}).get('qa_pairs', {}).get('hits', {}).get('hits', [])
        for inner_hit in inner_hits:
            qa_data = inner_hit.get('_source', {}).copy()
            qa_data['match_score'] = inner_hit.get('_score')
            if 'highlight' in inner_hit:
                qa_data['highlights'] = inner_hit['highlight']
            matched_qa.append(qa_data)

        results.append(
            SearchResult(
                user_id=user_id,
                score=hit.get('_score', 0.0),
                timestamp=source.get('timestamp') if isinstance(source, dict) else None,
                demographic_info=demographic_info if demographic_info else None,
                qa_pairs=source.get('qa_pairs', [])[:5] if isinstance(source, dict) else [],
                matched_qa_pairs=matched_qa,
                highlights=hit.get('highlight'),
            )
        )
    timings['lazy_join_ms'] = (perf_counter() - lazy_join_start) * 1000

    timings.setdefault('post_filter_ms', 0.0)
    timings.setdefault('rrf_recombination_ms', 0.0)
    timings.setdefault('qdrant_parallel_ms', 0.0)
    timings.setdefault('opensearch_parallel_ms', timings.get('two_phase_stage1_ms', 0.0) + timings.get('two_phase_stage2_ms', 0.0))

    total_duration_ms = (perf_counter() - overall_start) * 1000
    timings['total_ms'] = total_duration_ms

    logger.info("ğŸ“ˆ ì„±ëŠ¥ ì¸¡ì • ìš”ì•½ (ms):")
    for key in sorted(timings.keys()):
        logger.info(f"  - {key}: {timings[key]:.2f}")

    response_took_ms = int(total_duration_ms)
    total_hits = len(final_hits)
    max_score = final_hits[0].get('_score', 0.0) if final_hits else 0.0

    response_payload = SearchResponse(
        query=request.query,
        total_hits=total_hits,
        max_score=max_score,
        results=results,
        query_analysis={
            "intent": analysis.intent,
            "must_terms": analysis.must_terms,
            "should_terms": analysis.should_terms,
            "alpha": analysis.alpha,
            "confidence": analysis.confidence,
            "extracted_entities": extracted_entities.to_dict(),
            "filters": filters,
            "size": size,
            "timings_ms": timings,
        },
        took_ms=response_took_ms,
    )
    return response_payload

def get_user_id_from_doc(doc: Dict[str, Any]) -> Optional[str]:
    if not isinstance(doc, dict):
        return None
    source = doc.get('_source')
    if isinstance(source, dict):
        uid = source.get('user_id')
        if uid:
            return uid
        payload = source.get('payload')
        if isinstance(payload, dict):
            uid = payload.get('user_id')
            if uid:
                return uid
    uid = doc.get('_id')
    if uid:
        return uid
    payload = doc.get('payload')
    if isinstance(payload, dict):
        return payload.get('user_id')
    return None
