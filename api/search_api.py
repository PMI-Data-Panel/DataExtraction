"""ê²€ìƒ‰ API ë¼ìš°í„°"""
import asyncio
import json
import logging
from collections import defaultdict
from time import perf_counter
from typing import List, Dict, Any, Optional, Set, Tuple
from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel, Field
from opensearchpy import OpenSearch
import os
# ë¶„ì„ê¸° ë° ì¿¼ë¦¬ ë¹Œë”
from rag_query_analyzer.analyzers.main_analyzer import AdvancedRAGQueryAnalyzer
from rag_query_analyzer.analyzers.demographic_extractor import DemographicExtractor
from rag_query_analyzer.models.entities import DemographicType, DemographicEntity
from connectors.hybrid_searcher import OpenSearchHybridQueryBuilder, calculate_rrf_score
from connectors.data_fetcher import DataFetcher
from connectors.qdrant_helper import search_qdrant_async, search_qdrant_collections_async

from redis_celery.celery_app import celery_app
from celery.result import AsyncResult
from redis_celery.tasks.search_tasks import search_with_rrf_task
import redis


logger = logging.getLogger(__name__)

router = APIRouter(
    prefix="/search",
    tags=["Search"]
)

# âš ï¸ ì„ì‹œ í™•ì¥ íƒ€ì„ì•„ì›ƒ (ì¤‘ì²© í•„í„° ì œê±° ì „ê¹Œì§€ 8~10ì´ˆ ìœ ì§€)
DEFAULT_OS_TIMEOUT = 10
class NLSearchRequest(BaseModel):
    """ìì—°ì–´ ê¸°ë°˜ ê²€ìƒ‰ ìš”ì²­ (í•„í„°/size ìë™ ì¶”ì¶œ)"""
    query: str = Field(..., description="ìì—°ì–´ ì¿¼ë¦¬ (ì˜ˆ: '30ëŒ€ ì‚¬ë¬´ì§ 300ëª… ë°ì´í„° ë³´ì—¬ì¤˜')")
    index_name: str = Field(default="*", description="ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ì´ë¦„ (ê¸°ë³¸ê°’: ì „ì²´ ì¸ë±ìŠ¤ '*')")
    use_vector_search: bool = Field(default=True, description="ë²¡í„° ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€")


class SearchResult(BaseModel):
    """ê²€ìƒ‰ ê²°ê³¼ í•­ëª©"""
    user_id: str
    score: float
    timestamp: Optional[str] = None
    demographic_info: Optional[Dict[str, Any]] = Field(default=None, description="ì¸êµ¬í†µê³„ ì •ë³´ (welcome_1st, welcome_2ndì—ì„œ ì¡°íšŒ)")
    qa_pairs: Optional[List[Dict[str, Any]]] = None
    matched_qa_pairs: Optional[List[Dict[str, Any]]] = None
    highlights: Optional[Dict[str, Any]] = None


class SearchResponse(BaseModel):
    """ê²€ìƒ‰ ì‘ë‹µ"""
    query: str
    total_hits: int
    max_score: Optional[float]
    results: List[SearchResult]
    query_analysis: Optional[Dict[str, Any]] = None
    took_ms: int


@router.get("/", summary="Search API ìƒíƒœ")
def search_root():
    """Search API ê¸°ë³¸ ì •ë³´"""
    return {
        "message": "Search API ì‹¤í–‰ ì¤‘",
        "version": "1.0",
        "endpoints": [
            "/search/query",
            "/search/similar"
        ]
    }
    

def get_redis_client() -> redis.StrictRedis:
    """Redis ì—°ê²° ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    REDIS_HOST = os.getenv('REDIS_HOST', 'redis') # Docker Compose ì„œë¹„ìŠ¤ ì´ë¦„ ì‚¬ìš©
    REDIS_PORT = int(os.getenv('REDIS_PORT', '6379'))
    # ìºì‹œ DB 2ë²ˆ ì‚¬ìš© (docker-compose.yml ì°¸ê³ )
    CACHE_DB = int(os.getenv('CACHE_DB', '2')) 
    try:
        r = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, db=CACHE_DB, decode_responses=True)
        r.ping()
        return r
    except Exception as e:
        logger.error(f"Redis ì—°ê²° ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=503, detail="Redis ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
@router.post("/nl-async", response_model=Dict[str, Any], summary="ìì—°ì–´ ê²€ìƒ‰ (ë™ê¸°)")
async def search_natural_language_async(request: NLSearchRequest):
    """
    ë³µì¡í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‘ì—…ì„ Celery Workerì— ìœ„ì„í•˜ê³  Task IDë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # 1. Celery íƒœìŠ¤í¬ í˜¸ì¶œ
    task = search_with_rrf_task.delay(
        query=request.query,
        index_name=request.index_name or "*",
        size=100, # ì´ˆê¸° ìš”ì²­ ì‚¬ì´ì¦ˆë¥¼ ë„‰ë„‰í•˜ê²Œ ì„¤ì •
        use_vector_search=request.use_vector_search,
    )

    return {
        "message": "ë¹„ë™ê¸° ê²€ìƒ‰ ì‹œì‘",
        "task_id": task.id,
        "status_url": f"/search/status/{task.id}"
    }
    

@router.get("/status/{task_id}", response_model=Dict[str, Any], summary="Celery ì‘ì—… ìƒíƒœ ë° ê²°ê³¼ ì¡°íšŒ")
async def get_task_status(task_id: str):
    """í´ë¼ì´ì–¸íŠ¸ê°€ ì´ ì—”ë“œí¬ì¸íŠ¸ë¥¼ í´ë§í•˜ì—¬ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    task = AsyncResult(task_id, app=celery_app) # celery_appì„ ì¸ìˆ˜ë¡œ ì „ë‹¬
    
    response = {
        'state': task.state,
        'message': task.status,
        'ready': task.ready()
    }
    
    if task.ready():
        if task.state == 'SUCCESS':
            # SUCCESS ì‘ë‹µì€ SearchResponseì™€ ìœ ì‚¬í•œ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
            # resultëŠ” Celery Taskì—ì„œ ë°˜í™˜ëœ ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤.
            response['result'] = task.result 
        elif task.state == 'FAILURE':
            response['result'] = {
                'error': str(task.result), 
                'error_type': task.result.get('error_type', 'UnknownError') if isinstance(task.result, dict) else 'UnknownError'
            }
        
    return response


from fastapi import Query
@router.get("/scroll/{task_id}", response_model=SearchResponse, summary="ë¬´í•œ ìŠ¤í¬ë¡¤: RRF ê²°ê³¼ í˜ì´ì§• ì¡°íšŒ")
async def get_scrolled_results(
    task_id: str,
    offset: int = Query(0, ge=0),
    limit: int = Query(20, ge=1, le=100),
    r: redis.StrictRedis = Depends(get_redis_client) # Redis í´ë¼ì´ì–¸íŠ¸ DI
):
    """
    Redisì— ì €ì¥ëœ RRF ìˆœì„œ ê¸°ë°˜ì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ offset/limit ë°©ì‹ìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    id_list_key = f"task:{task_id}:ids"
    
    # 1. Redisì—ì„œ ì „ì²´ ID ëª©ë¡ ê¸¸ì´ í™•ì¸
    total_hits = r.llen(id_list_key)
        
    if total_hits == 0:
        raise HTTPException(
            status_code=404, 
            detail="ê²€ìƒ‰ ê²°ê³¼ê°€ ë§Œë£Œë˜ì—ˆê±°ë‚˜ Task IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        )

    # 2. Redis Listì—ì„œ ID ëª©ë¡ ì¶”ì¶œ
    end_index = offset + limit - 1
    user_ids = r.lrange(id_list_key, offset, end_index)
    
    if not user_ids:
        # ëê¹Œì§€ ìŠ¤í¬ë¡¤í•œ ê²½ìš°
        return SearchResponse(
            query=f"Task {task_id} Scrolled",
            total_hits=total_hits,
            max_score=0.0,
            results=[],
            took_ms=0
        )

    # 3. MGETìœ¼ë¡œ ìƒì„¸ JSON ë°ì´í„° ì¼ê´„ ì¡°íšŒ
    data_keys = [f"task:{task_id}:data:{uid}" for uid in user_ids]
    detailed_results_json = r.mget(data_keys)
    
    results: List[SearchResult] = []
    max_score = 0.0
    
    for item_json in detailed_results_json:
        if item_json:
            try:
                data = json.loads(item_json)
                result = SearchResult(**data)
                results.append(result)
                max_score = max(max_score, result.score)
            except json.JSONDecodeError as e:
                logger.error(f"Redis ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {e}")
        
    # 4. ì‘ë‹µ í¬ë§· êµ¬ì„±
    return SearchResponse(
        query=f"Task {task_id} Scrolled (Offset: {offset}, Limit: {limit})",
        total_hits=total_hits,
        max_score=max_score,
        results=results,
        query_analysis={"note": "í˜ì´ì§• ë°ì´í„°ëŠ” ì „ì²´ ë¶„ì„ ê²°ê³¼ë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."},
        took_ms=0
    )

class TestFiltersRequest(BaseModel):
    """í•„í„° í…ŒìŠ¤íŠ¸ ìš”ì²­"""
    filters: List[Dict[str, Any]] = Field(..., description="í…ŒìŠ¤íŠ¸í•  í•„í„° ë¦¬ìŠ¤íŠ¸")
    index_name: str = Field(default="*", description="ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ì´ë¦„")


@router.post("/debug/test-filters", summary="í•„í„° ê°œë³„ í…ŒìŠ¤íŠ¸ (ë””ë²„ê¹…ìš©)")
async def test_filters(
    request: TestFiltersRequest,
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """
    í•„í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ì—¬ ì–´ë–¤ ì¸ë±ìŠ¤ì—ì„œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
    
    - ê° í•„í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì‹¤í–‰
    - ì¸ë±ìŠ¤ë³„ ê²°ê³¼ ê°œìˆ˜ í™•ì¸
    
    ì‚¬ìš© ì˜ˆì‹œ:
    ```json
    {
      "filters": [
        {
          "bool": {
            "should": [
              {"term": {"metadata.age_group.keyword": "30ëŒ€"}}
            ],
            "minimum_should_match": 1
          }
        }
      ],
      "index_name": "*"
    }
    ```
    """
    try:
        if not os_client or not os_client.ping():
            raise HTTPException(status_code=503, detail="OpenSearch ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        filters = request.filters
        index_name = request.index_name
        
        results = []
        for i, filter_dict in enumerate(filters):
            # ê° í•„í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            query = {
                "query": {
                    "bool": {
                        "must": [filter_dict]
                    }
                },
                "size": 0,  # ê°œìˆ˜ë§Œ í™•ì¸
                "aggs": {
                    "by_index": {
                        "terms": {
                            "field": "_index",
                            "size": 20
                        }
                    }
                }
            }
            
            response = os_client.search(
                index=request.index_name,
                body=query
            )
            
            # ì¸ë±ìŠ¤ë³„ ê²°ê³¼ ê°œìˆ˜
            index_counts = {}
            if 'aggregations' in response and 'by_index' in response['aggregations']:
                for bucket in response['aggregations']['by_index']['buckets']:
                    index_counts[bucket['key']] = bucket['doc_count']
            
            results.append({
                "filter_index": i,
                "filter": filter_dict,
                "total_hits": response['hits']['total']['value'],
                "index_counts": index_counts
            })
        
        # ëª¨ë“  í•„í„°ë¥¼ ANDë¡œ ê²°í•©í•œ ê²°ê³¼ë„ í…ŒìŠ¤íŠ¸
        if len(filters) > 1:
            combined_query = {
                "query": {
                    "bool": {
                        "must": filters
                    }
                },
                "size": 0,
                "aggs": {
                    "by_index": {
                        "terms": {
                            "field": "_index",
                            "size": 20
                        }
                    }
                }
            }
            
            combined_response = os_client.search(
                index=request.index_name,
                body=combined_query
            )
            
            combined_index_counts = {}
            if 'aggregations' in combined_response and 'by_index' in combined_response['aggregations']:
                for bucket in combined_response['aggregations']['by_index']['buckets']:
                    combined_index_counts[bucket['key']] = bucket['doc_count']
            
            results.append({
                "filter_index": "combined",
                "filter": "ALL FILTERS (AND)",
                "total_hits": combined_response['hits']['total']['value'],
                "index_counts": combined_index_counts
            })
        
        return {
            "index_name": request.index_name,
            "results": results
        }
    
    except Exception as e:
        logger.error(f"[ERROR] í•„í„° í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/qdrant/collections", summary="Qdrant ì»¬ë ‰ì…˜ ëª©ë¡ ë° í†µê³„")
async def list_qdrant_collections():
    qdrant_client = getattr(router, 'qdrant_client', None)
    if not qdrant_client:
        raise HTTPException(status_code=503, detail="Qdrant í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    try:
        cols = qdrant_client.get_collections()
        items = []
        for c in cols.collections:
            try:
                info = qdrant_client.get_collection(c.name)
                items.append({
                    "name": c.name,
                    "vectors_count": info.vectors_count if hasattr(info, 'vectors_count') else None,
                    "points_count": getattr(info, 'points_count', None),
                    "config": getattr(info, 'config', None).__dict__ if hasattr(info, 'config') else None,
                })
            except Exception:
                items.append({"name": c.name})
        return {"collections": items}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


class QdrantTestSearchRequest(BaseModel):
    query: str = Field(..., description="ì„ë² ë”©ìœ¼ë¡œ ê²€ìƒ‰í•  í…ìŠ¤íŠ¸")
    limit: int = Field(5, ge=1, le=100)


@router.post("/qdrant/test-search", summary="Qdrant ì „ ì»¬ë ‰ì…˜ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ (ì½ê¸° ì „ìš©)")
async def qdrant_test_search(req: QdrantTestSearchRequest):
    qdrant_client = getattr(router, 'qdrant_client', None)
    embedding_model = getattr(router, 'embedding_model', None)
    if not qdrant_client:
        raise HTTPException(status_code=503, detail="Qdrant í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if not embedding_model:
        raise HTTPException(status_code=503, detail="ì„ë² ë”© ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    try:
        qvec = embedding_model.encode(req.query).tolist()
        cols = qdrant_client.get_collections()
        results = []
        for c in cols.collections:
            try:
                r = qdrant_client.search(
                    collection_name=c.name,
                    query_vector=qvec,
                    limit=req.limit,
                )
                results.append({
                    "collection": c.name,
                    "hits": [
                        {
                            "id": str(h.id),
                            "score": h.score,
                            "payload": getattr(h, 'payload', None)
                        } for h in r
                    ]
                })
            except Exception as e:
                results.append({"collection": c.name, "error": str(e)})
        return {"query": req.query, "results": results}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/similar", summary="ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (í”Œë ˆì´ìŠ¤í™€ë”)")
async def search_similar(
    user_id: str,
    index_name: str = "s_welcome_2nd",
    size: int = 10
):
    """
    íŠ¹ì • ì‚¬ìš©ìì™€ ìœ ì‚¬í•œ ì‘ë‹µì„ ê°€ì§„ ì‚¬ìš©ì ê²€ìƒ‰ (í–¥í›„ êµ¬í˜„)
    """
    raise HTTPException(
        status_code=501,
        detail="ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ê¸°ëŠ¥ì€ í–¥í›„ êµ¬í˜„ ì˜ˆì •ì…ë‹ˆë‹¤."
    )


@router.get("/stats/{index_name}", summary="ê²€ìƒ‰ í†µê³„")
async def get_search_stats(
    index_name: str,
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """ì¸ë±ìŠ¤ ê²€ìƒ‰ í†µê³„ ì¡°íšŒ"""
    try:
        if not os_client.indices.exists(index=index_name):
            raise HTTPException(
                status_code=404,
                detail=f"ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {index_name}"
            )

        stats = os_client.indices.stats(index=index_name)
        count = os_client.count(index=index_name)

        return {
            "index_name": index_name,
            "doc_count": count['count'],
            "size_mb": round(stats['_all']['total']['store']['size_in_bytes'] / 1024 / 1024, 2),
            "search_total": stats['_all']['total']['search']['query_total'],
            "search_time_ms": stats['_all']['total']['search']['query_time_in_millis']
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[ERROR] í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


def _filter_to_string(filter_dict: Dict[str, Any]) -> str:
    try:
        return json.dumps(filter_dict, ensure_ascii=False)
    except Exception:
        return str(filter_dict)


AGE_GENDER_KEYWORDS = [
    "metadata.age_group", "metadata.gender", "birth_year", "ì—°ë ¹", "ë‚˜ì´", "ì„±ë³„"
]
OCCUPATION_KEYWORDS = [
    "metadata.occupation", "occupation", "ì§ì—…", "ì§ë¬´"
]


def is_age_or_gender_filter(filter_dict: Dict[str, Any]) -> bool:
    filter_str = _filter_to_string(filter_dict)
    return any(keyword in filter_str for keyword in AGE_GENDER_KEYWORDS)


def is_occupation_filter(filter_dict: Dict[str, Any]) -> bool:
    filter_str = _filter_to_string(filter_dict)
    return any(keyword in filter_str for keyword in OCCUPATION_KEYWORDS)


async def run_two_phase_demographic_search(
    request,
    analysis,
    extracted_entities,
    filters: List[Dict[str, Any]],
    size: int,
    age_gender_filters: List[Dict[str, Any]],
    occupation_filters: List[Dict[str, Any]],
    data_fetcher: "DataFetcher",
    timings: Dict[str, float],
    overall_start: float,
) -> Optional[SearchResponse]:
    """ë‘ ë‹¨ê³„ ê²€ìƒ‰ìœ¼ë¡œ user_idë¥¼ ë¨¼ì € ì¢íˆê³  ì •ë°€ ì¡°íšŒ"""
    logger.info("ğŸš€ ë‘ ë‹¨ê³„ ì¸êµ¬í†µê³„ ìµœì í™” ì‹¤í–‰")

    async_client = data_fetcher.os_async_client
    sync_client = data_fetcher.os_client
    if not (async_client or sync_client):
        logger.warning("âš ï¸ OpenSearch í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ 2ë‹¨ê³„ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤")
        return None

    stage1_start = perf_counter()
    stage1_query_size = min(max(size * 50, 2000), 10000)
    stage1_query = {
        "query": {
            "bool": {
                "must": age_gender_filters
            }
        },
        "size": stage1_query_size,
        "_source": ["user_id"],
        "track_total_hits": True
    }

    try:
        if async_client:
            response_1st = await data_fetcher.search_opensearch_async(
                index_name="s_welcome_1st",
                query=stage1_query,
                size=stage1_query_size,
                source_filter=None,
                request_timeout=DEFAULT_OS_TIMEOUT,
            )
        else:
            response_1st = data_fetcher.search_opensearch(
                index_name="s_welcome_1st",
                query=stage1_query,
                size=stage1_query_size,
                source_filter=None,
                request_timeout=DEFAULT_OS_TIMEOUT,
            )
    except Exception as e:
        logger.warning(f"âš ï¸ 2ë‹¨ê³„ ê²€ìƒ‰ Stage1 ì‹¤íŒ¨: {e}")
        return None

    timings['two_phase_stage1_ms'] = (perf_counter() - stage1_start) * 1000
    hits_1st = response_1st.get('hits', {}).get('hits', [])
    total_stage1 = response_1st.get('hits', {}).get('total', {}).get('value', len(hits_1st))

    if not hits_1st:
        logger.info("   âš ï¸ Stage1ì—ì„œ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” user_idê°€ ì—†ìŠµë‹ˆë‹¤")
        total_time = (perf_counter() - overall_start) * 1000
        timings['total_ms'] = total_time
        timings.setdefault('two_phase_stage2_ms', 0.0)
        timings.setdefault('two_phase_fetch_demographics_ms', 0.0)
        timings.setdefault('lazy_join_ms', 0.0)
        timings.setdefault('post_filter_ms', 0.0)
        timings.setdefault('rrf_recombination_ms', 0.0)
        timings.setdefault('qdrant_parallel_ms', 0.0)
        timings.setdefault('opensearch_parallel_ms', timings['two_phase_stage1_ms'])
        logger.info("ğŸ“ˆ ì„±ëŠ¥ ì¸¡ì • ìš”ì•½ (ms):")
        for key in sorted(timings.keys()):
            logger.info(f"  - {key}: {timings[key]:.2f}")
        return SearchResponse(
            query=request.query,
            total_hits=0,
            max_score=0.0,
            results=[],
            query_analysis={
                "intent": analysis.intent,
                "must_terms": analysis.must_terms,
                "should_terms": analysis.should_terms,
                "alpha": analysis.alpha,
                "confidence": analysis.confidence,
                "extracted_entities": extracted_entities.to_dict(),
                "filters": filters,
                "size": size,
                "timings_ms": timings,
            },
            took_ms=int(total_time)
        )

    user_ids_filtered = []
    for hit in hits_1st:
        src = hit.get('_source', {})
        uid = src.get('user_id') or hit.get('_id')
        if uid:
            user_ids_filtered.append(uid)
    user_ids_filtered = list(dict.fromkeys(user_ids_filtered))

    logger.info(f"   âœ… Stage1 user_id ì¶”ì¶œ: {len(user_ids_filtered)}/{total_stage1}ê±´")
    if total_stage1 > len(user_ids_filtered):
        logger.warning("   âš ï¸ Stage1 size ì œí•œìœ¼ë¡œ ì¼ë¶€ user_idê°€ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤")

    if not user_ids_filtered:
        total_time = (perf_counter() - overall_start) * 1000
        timings['two_phase_stage2_ms'] = 0.0
        timings['two_phase_fetch_demographics_ms'] = 0.0
        timings['lazy_join_ms'] = 0.0
        timings['post_filter_ms'] = 0.0
        timings['rrf_recombination_ms'] = 0.0
        timings.setdefault('opensearch_parallel_ms', timings['two_phase_stage1_ms'])
        timings['total_ms'] = total_time
        logger.info("ğŸ“ˆ ì„±ëŠ¥ ì¸¡ì • ìš”ì•½ (ms):")
        for key in sorted(timings.keys()):
            logger.info(f"  - {key}: {timings[key]:.2f}")
        return SearchResponse(
            query=request.query,
            total_hits=0,
            max_score=0.0,
            results=[],
            query_analysis={
                "intent": analysis.intent,
                "must_terms": analysis.must_terms,
                "should_terms": analysis.should_terms,
                "alpha": analysis.alpha,
                "confidence": analysis.confidence,
                "extracted_entities": extracted_entities.to_dict(),
                "filters": filters,
                "size": size,
                "timings_ms": timings,
            },
            took_ms=int(total_time)
        )

    max_terms = 10000
    if len(user_ids_filtered) > max_terms:
        logger.warning(f"   âš ï¸ user_idê°€ {len(user_ids_filtered)}ê±´ì…ë‹ˆë‹¤. ìƒìœ„ {max_terms}ê±´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤")
        user_ids_filtered = user_ids_filtered[:max_terms]

    detail_size = max(size * 2, min(len(user_ids_filtered), 500))
    stage2_query = {
        "query": {
            "bool": {
                "must": [
                    {"terms": {"_id": user_ids_filtered}},
                ]
            }
        },
        "size": detail_size,
        "_source": {
            "includes": ["user_id", "metadata", "qa_pairs", "timestamp"]
        },
        "track_total_hits": True
    }

    stage2_start = perf_counter()
    try:
        if async_client:
            response_2nd = await data_fetcher.search_opensearch_async(
                index_name="s_welcome_2nd",
                query=stage2_query,
                size=detail_size,
                source_filter=None,
                request_timeout=DEFAULT_OS_TIMEOUT,
            )
        else:
            response_2nd = data_fetcher.search_opensearch(
                index_name="s_welcome_2nd",
                query=stage2_query,
                size=detail_size,
                source_filter=None,
                request_timeout=DEFAULT_OS_TIMEOUT,
            )
    except Exception as e:
        logger.warning(f"âš ï¸ 2ë‹¨ê³„ ê²€ìƒ‰ Stage2 ì‹¤íŒ¨: {e}")
        return None

    timings['two_phase_stage2_ms'] = (perf_counter() - stage2_start) * 1000
    hits_2nd = response_2nd.get('hits', {}).get('hits', [])
    total_stage2 = response_2nd.get('hits', {}).get('total', {}).get('value', len(hits_2nd))
    logger.info(f"   âœ… Stage2 ê²°ê³¼: {len(hits_2nd)}ê±´ (ì´ {total_stage2}ê±´)")

    if not hits_2nd:
        total_time = (perf_counter() - overall_start) * 1000
        timings.setdefault('two_phase_fetch_demographics_ms', 0.0)
        timings['lazy_join_ms'] = 0.0
        timings['post_filter_ms'] = 0.0
        timings['rrf_recombination_ms'] = 0.0
        timings.setdefault('opensearch_parallel_ms', timings.get('two_phase_stage1_ms', 0.0))
        timings['total_ms'] = total_time
        logger.info("ğŸ“ˆ ì„±ëŠ¥ ì¸¡ì • ìš”ì•½ (ms):")
        for key in sorted(timings.keys()):
            logger.info(f"  - {key}: {timings[key]:.2f}")
        return SearchResponse(
            query=request.query,
            total_hits=0,
            max_score=0.0,
            results=[],
            query_analysis={
                "intent": analysis.intent,
                "must_terms": analysis.must_terms,
                "should_terms": analysis.should_terms,
                "alpha": analysis.alpha,
                "confidence": analysis.confidence,
                "extracted_entities": extracted_entities.to_dict(),
                "filters": filters,
                "size": size,
                "timings_ms": timings,
            },
            took_ms=int(total_time)
        )

    final_hits = hits_2nd[:size]
    final_user_ids = [hit.get('_id') or hit.get('_source', {}).get('user_id') for hit in final_hits]

    fetch_start = perf_counter()
    welcome_1st_docs: Dict[str, Dict[str, Any]] = {}
    welcome_2nd_docs: Dict[str, Dict[str, Any]] = {}

    if final_user_ids:
        if async_client:
            welcome_1st_docs = await data_fetcher.multi_get_documents_async(
                index_name="s_welcome_1st",
                doc_ids=final_user_ids,
                batch_size=200
            )
            welcome_2nd_docs = await data_fetcher.multi_get_documents_async(
                index_name="s_welcome_2nd",
                doc_ids=final_user_ids,
                batch_size=200
            )
        else:
            response = sync_client.mget(index="s_welcome_1st", body={"ids": final_user_ids}, _source=["metadata", "user_id", "qa_pairs"])
            for doc in response.get('docs', []):
                if doc.get('found'):
                    welcome_1st_docs[doc['_id']] = doc['_source']
            response = sync_client.mget(index="s_welcome_2nd", body={"ids": final_user_ids}, _source=["metadata", "user_id", "qa_pairs"])
            for doc in response.get('docs', []):
                if doc.get('found'):
                    welcome_2nd_docs[doc['_id']] = doc['_source']
    timings['two_phase_fetch_demographics_ms'] = (perf_counter() - fetch_start) * 1000

    results: List[SearchResult] = []
    lazy_join_start = perf_counter()
    final_hits = final_hits if 'final_hits' in locals() else []
    for doc in final_hits:
        source = doc.get('_source', {}) or {}
        user_id = source.get('user_id') or hit.get('_id', '')
        metadata_2nd = source.get('metadata', {}) if isinstance(source, dict) else {}
        welcome_1st_doc = welcome_1st_docs.get(user_id, {})
        metadata_1st = welcome_1st_doc.get('metadata', {}) if isinstance(welcome_1st_doc, dict) else {}

        demographic_info = {}
        if metadata_1st:
            demographic_info['age_group'] = metadata_1st.get('age_group')
            demographic_info['gender'] = metadata_1st.get('gender')
            demographic_info['birth_year'] = metadata_1st.get('birth_year')
        if metadata_2nd:
            demographic_info['occupation'] = metadata_2nd.get('occupation')

        if 'occupation' not in demographic_info or not demographic_info['occupation']:
            qa_pairs_for_occ = source.get('qa_pairs', []) if isinstance(source, dict) else []
            for qa in qa_pairs_for_occ:
                if isinstance(qa, dict):
                    q_text = qa.get('q_text', '')
                    answer = str(qa.get('answer', qa.get('answer_text', '')))
                    if 'ì§ì—…' in q_text or 'occupation' in q_text.lower() or 'ì§ë¬´' in q_text:
                        if answer:
                            demographic_info['occupation'] = answer
                        break

        matched_qa = []
        inner_hits = hit.get('inner_hits', {}).get('qa_pairs', {}).get('hits', {}).get('hits', [])
        for inner_hit in inner_hits:
            qa_data = inner_hit.get('_source', {}).copy()
            qa_data['match_score'] = inner_hit.get('_score')
            if 'highlight' in inner_hit:
                qa_data['highlights'] = inner_hit['highlight']
            matched_qa.append(qa_data)

        results.append(
            SearchResult(
                user_id=user_id,
                score=hit.get('_score', 0.0),
                timestamp=source.get('timestamp') if isinstance(source, dict) else None,
                demographic_info=demographic_info if demographic_info else None,
                qa_pairs=source.get('qa_pairs', [])[:5] if isinstance(source, dict) else [],
                matched_qa_pairs=matched_qa,
                highlights=hit.get('highlight'),
            )
        )
    timings['lazy_join_ms'] = (perf_counter() - lazy_join_start) * 1000

    timings.setdefault('post_filter_ms', 0.0)
    timings.setdefault('rrf_recombination_ms', 0.0)
    timings.setdefault('qdrant_parallel_ms', 0.0)
    timings.setdefault('opensearch_parallel_ms', timings.get('two_phase_stage1_ms', 0.0) + timings.get('two_phase_stage2_ms', 0.0))

    total_duration_ms = (perf_counter() - overall_start) * 1000
    timings['total_ms'] = total_duration_ms

    logger.info("ğŸ“ˆ ì„±ëŠ¥ ì¸¡ì • ìš”ì•½ (ms):")
    for key in sorted(timings.keys()):
        logger.info(f"  - {key}: {timings[key]:.2f}")

    response_took_ms = int(total_duration_ms)
    total_hits = len(final_hits)
    max_score = final_hits[0].get('_score', 0.0) if final_hits else 0.0

    response_payload = SearchResponse(
        query=request.query,
        total_hits=total_hits,
        max_score=max_score,
        results=results,
        query_analysis={
            "intent": analysis.intent,
            "must_terms": analysis.must_terms,
            "should_terms": analysis.should_terms,
            "alpha": analysis.alpha,
            "confidence": analysis.confidence,
            "extracted_entities": extracted_entities.to_dict(),
            "filters": filters,
            "size": size,
            "timings_ms": timings,
        },
        took_ms=response_took_ms,
    )
    return response_payload

def get_user_id_from_doc(doc: Dict[str, Any]) -> Optional[str]:
    if not isinstance(doc, dict):
        return None
    source = doc.get('_source')
    if isinstance(source, dict):
        uid = source.get('user_id')
        if uid:
            return uid
        payload = source.get('payload')
        if isinstance(payload, dict):
            uid = payload.get('user_id')
            if uid:
                return uid
    uid = doc.get('_id')
    if uid:
        return uid
    payload = doc.get('payload')
    if isinstance(payload, dict):
        return payload.get('user_id')
    return None