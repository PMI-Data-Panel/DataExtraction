"""ê²€ìƒ‰ API ë¼ìš°í„°"""
import asyncio
import json
import logging
import hashlib
import re
import gzip
import pickle
from collections import defaultdict, OrderedDict
from time import perf_counter
from datetime import datetime, timezone
from uuid import uuid4
from typing import List, Dict, Any, Optional, Set, Tuple, Literal, Union
from fastapi import APIRouter, HTTPException, Depends, Query, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from opensearchpy import OpenSearch
import pandas as pd
import numpy as np
from cachetools import TTLCache

# ë¶„ì„ê¸° ë° ì¿¼ë¦¬ ë¹Œë”
from rag_query_analyzer.analyzers.main_analyzer import AdvancedRAGQueryAnalyzer
from rag_query_analyzer.analyzers.demographic_extractor import DemographicExtractor
from rag_query_analyzer.models.entities import DemographicType, DemographicEntity
from connectors.hybrid_searcher import OpenSearchHybridQueryBuilder, calculate_rrf_score
from connectors.data_fetcher import DataFetcher
from connectors.qdrant_helper import search_qdrant_async, search_qdrant_collections_async

logger = logging.getLogger(__name__)
# main_api.pyì—ì„œ ì´ë¯¸ basicConfigë¡œ ë£¨íŠ¸ ë¡œê±°ê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë¯€ë¡œ
# propagateë§Œ Trueë¡œ ì„¤ì •í•˜ì—¬ ë£¨íŠ¸ ë¡œê±°ë¡œ ì „íŒŒë˜ë„ë¡ í•¨
logger.propagate = True


def _mask_user_ids_in_query(query: Dict[str, Any]) -> Dict[str, Any]:
    """ì¿¼ë¦¬ì—ì„œ user_id ë¦¬ìŠ¤íŠ¸ë¥¼ ë§ˆìŠ¤í‚¹í•˜ì—¬ ë¡œê¹…ìš© ë³µì‚¬ë³¸ ìƒì„±"""
    import copy
    masked = copy.deepcopy(query)
    
    def mask_recursive(obj):
        if isinstance(obj, dict):
            for key, value in obj.items():
                if key in ("_id", "user_id") and isinstance(value, list):
                    # user_id ë¦¬ìŠ¤íŠ¸ë¥¼ ê°œìˆ˜ë§Œ í‘œì‹œí•˜ë„ë¡ ë§ˆìŠ¤í‚¹
                    obj[key] = f"[{len(value)} user_ids masked]"
                elif isinstance(value, (dict, list)):
                    mask_recursive(value)
        elif isinstance(obj, list):
            for item in obj:
                if isinstance(item, (dict, list)):
                    mask_recursive(item)
    
    mask_recursive(masked)
    return masked


# ============================================================================
# â­ ì „ì—­ ë©”ëª¨ë¦¬ ìºì‹œ: ì„œë²„ ì‹œì‘ ì‹œ 1ë²ˆë§Œ ë¡œë“œ (ë©”ëª¨ë¦¬ í”„ë¦¬ë¡œë“œ ìµœì í™”)
# ============================================================================

class PanelDataCache:
    """Survey panel ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ìºì‹± - ì´ˆê³ ì† ê²€ìƒ‰ì„ ìœ„í•œ í”„ë¦¬ë¡œë“œ"""

    def __init__(self):
        self.df: pd.DataFrame = None  # Pandas DataFrame (ë²¡í„°í™” í•„í„°ë§ìš©)
        self.user_map: Dict[str, Dict] = {}  # user_id â†’ full document ë§¤í•‘
        self.loaded = False
        self.load_time = None
        self.total_count = 0

    async def initialize(self, data_fetcher, index_name="survey_responses_merged"):
        """ì„œë²„ ì‹œì‘ ì‹œ ì „ì²´ ë°ì´í„° ë¡œë“œ (1ë²ˆë§Œ ì‹¤í–‰)"""
        if self.loaded:
            logger.info("âœ… Panel data already loaded")
            return

        start = perf_counter()
        logger.info("ğŸ”„ Loading all panel data into memory... (ë©”ëª¨ë¦¬ í”„ë¦¬ë¡œë“œ)")

        # â­ Scroll APIë¡œ ì „ì²´ ë°ì´í„° ì¡°íšŒ (í•„ìš”í•œ í•„ë“œë§Œ)
        query = {
            "query": {"match_all": {}},
            "_source": {
                "includes": [
                    "user_id",
                    "metadata",   # demographics
                    "qa_pairs",   # occupation, marital, behavioral
                    "timestamp",
                    "text"
                ],
                "excludes": []
            }
        }

        try:
            all_docs = await data_fetcher.scroll_search_async(
                index_name=index_name,
                query=query,
                batch_size=2000,
                scroll_time="5m",
                num_slices=8,  # ë³‘ë ¬ 8ê°œ (ë¹ ë¥¸ ë¡œë”©)
                request_timeout=300,
            )

            # â­â­â­ Pandas DataFrameìœ¼ë¡œ ë³€í™˜ + ëª¨ë“  ì¡°ê±´ ì‚¬ì „ ì¶”ì¶œ (ì´ˆê³ ì†!)
            extract_start = perf_counter()

            # â­ ëª¨ë“  behavioral í‚¤ (BEHAVIORAL_KEYWORD_MAPì—ì„œ ìë™ ìƒì„±!)
            all_behavioral_keys = list(BEHAVIORAL_KEYWORD_MAP.keys())

            records = []
            for idx, doc in enumerate(all_docs):
                if idx % 5000 == 0 and idx > 0:
                    logger.info(f"    ì§„í–‰: {idx}/{len(all_docs)}...")

                source = doc.get('_source', {})
                metadata = source.get('metadata', {}) if isinstance(source.get('metadata'), dict) else {}

                # â­ qa_pairs ì²˜ë¦¬: dict â†’ list ë³€í™˜ (OpenSearch êµ¬ì¡° ëŒ€ì‘!)
                qa_pairs_raw = source.get('qa_pairs', {})
                if isinstance(qa_pairs_raw, dict):
                    # dict â†’ list of values
                    qa_pairs = list(qa_pairs_raw.values())
                elif isinstance(qa_pairs_raw, list):
                    # ì´ë¯¸ listë©´ ê·¸ëŒ€ë¡œ
                    qa_pairs = qa_pairs_raw
                else:
                    # ê¸°íƒ€ íƒ€ì…ì€ ë¹ˆ ë¦¬ìŠ¤íŠ¸
                    qa_pairs = []

                # â­ Occupation ì‚¬ì „ ì¶”ì¶œ
                occupation_value = None

                # ğŸ” ë””ë²„ê·¸: ì²« 3ê°œ ë¬¸ì„œì˜ qa_pairs êµ¬ì¡° í™•ì¸
                if idx < 3:
                    logger.info(f"\nğŸ” [DEBUG] Document #{idx}: user_id={source.get('user_id')}")
                    logger.info(f"   - qa_pairs type: {type(qa_pairs)}")
                    logger.info(f"   - qa_pairs count: {len(qa_pairs)}")
                    if qa_pairs and len(qa_pairs) > 0:
                        logger.info(f"   - First QA pair structure: {qa_pairs[0]}")
                        logger.info(f"   - QA pair keys: {qa_pairs[0].keys() if isinstance(qa_pairs[0], dict) else 'NOT A DICT'}")
                        # ì²˜ìŒ 5ê°œ ì§ˆë¬¸ ì¶œë ¥
                        for i, qa in enumerate(qa_pairs[:5]):
                            if isinstance(qa, dict):
                                # â­ ìˆ˜ì •: ì‹¤ì œ í‚¤ëŠ” 'q_text'
                                q_text_raw = qa.get("q_text", "") or qa.get("question", "") or qa.get("question_text", "") or ""
                                logger.info(f"   - Q{i+1}: '{q_text_raw[:100]}'")  # ì²˜ìŒ 100ìë§Œ

                if qa_pairs:
                    for qa in qa_pairs:
                        if not isinstance(qa, dict):
                            continue
                        # â­ ìˆ˜ì •: ì‹¤ì œ í‚¤ëŠ” 'q_text'
                        q_text = str(qa.get("q_text", "") or qa.get("question", "") or qa.get("question_text", "")).lower()
                        if any(keyword in q_text for keyword in ("ì§ì—…", "ì§ë¬´", "occupation", "ì§ì¢…")):
                            answer = qa.get("answer") or qa.get("answer_text")
                            if answer:
                                occupation_value = str(answer).strip()
                                if idx < 3:
                                    logger.info(f"   âœ… Found occupation: '{occupation_value}' from q_text: '{q_text[:50]}'")
                                break

                    # ë””ë²„ê·¸: ì²« 3ê°œ ë¬¸ì„œì—ì„œ occupation ì°¾ì§€ ëª»í•œ ê²½ìš°
                    if idx < 3 and occupation_value is None:
                        logger.info(f"   âŒ No occupation found in {len(qa_pairs)} QA pairs")

                # â­ Marital status ì‚¬ì „ ì¶”ì¶œ
                marital_value = metadata.get("marital_status")
                if not marital_value and qa_pairs:
                    for qa in qa_pairs:
                        if not isinstance(qa, dict):
                            continue
                        # â­ ìˆ˜ì •: ì‹¤ì œ í‚¤ëŠ” 'q_text'
                        q_text = str(qa.get("q_text", "") or qa.get("question", "") or qa.get("question_text", "")).lower()
                        if any(keyword in q_text for keyword in ("ê²°í˜¼", "í˜¼ì¸", "marital")):
                            answer = qa.get("answer") or qa.get("answer_text")
                            if answer:
                                marital_value = str(answer).strip()
                                break

                # â­â­â­ ëª¨ë“  Behavioral ì¡°ê±´ ì‚¬ì „ ì¶”ì¶œ (77ê°œ) - ë°°ì¹˜ ìµœì í™”!
                # qa_pairsë¥¼ í•œ ë²ˆë§Œ ìˆœíšŒí•˜ì—¬ ëª¨ë“  íŒ¨í„´ì„ ë™ì‹œì— ì¶”ì¶œ (77ë°° ì†ë„ í–¥ìƒ!)
                behavioral_values = extract_all_behaviors_batch(qa_pairs)

                # DataFrame ë ˆì½”ë“œ ìƒì„±
                record = {
                    'user_id': source.get('user_id'),
                    'gender': metadata.get('gender'),
                    'age_group': metadata.get('age_group'),
                    'birth_year': metadata.get('birth_year'),
                    'region': metadata.get('region'),
                    'sub_region': metadata.get('sub_region'),
                    'occupation': occupation_value,  # â­ ì‚¬ì „ ì¶”ì¶œ!
                    'marital_status': marital_value,  # â­ ì‚¬ì „ ì¶”ì¶œ!
                    'timestamp': source.get('timestamp'),
                    # â­ ì „ì²´ ë°ì´í„° ë³´ê´€ (í•„ìš” ì‹œ ì ‘ê·¼)
                    '_full_source': source,
                    '_doc': doc,
                }

                # â­â­â­ Behavioral ì»¬ëŸ¼ ì¶”ê°€ (33ê°œ)
                record.update(behavioral_values)

                records.append(record)

                # user_id â†’ full document ë§¤í•‘ (ë¹ ë¥¸ ì¡°íšŒìš©)
                if source.get('user_id'):
                    self.user_map[source['user_id']] = doc

            extract_duration = perf_counter() - extract_start

            self.df = pd.DataFrame(records)
            self.total_count = len(self.df)
            self.loaded = True
            self.load_time = perf_counter() - start

            memory_mb = self.df.memory_usage(deep=True).sum() / 1024**2
            logger.info(f"âœ… Panel data loaded: {self.total_count}ê±´, {self.load_time:.2f}ì´ˆ")
            logger.info(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©: {memory_mb:.2f} MB")
            logger.info(f"   ì»¬ëŸ¼: {list(self.df.columns)}")

            # â­ Occupation í†µê³„ (ë””ë²„ê¹…ìš©)
            occupation_stats = self.df['occupation'].value_counts()
            logger.info(f"\nğŸ“Š Occupation í†µê³„:")
            logger.info(f"   - Total: {self.total_count}ê±´")
            logger.info(f"   - None: {self.df['occupation'].isna().sum()}ê±´")
            logger.info(f"   - ê³ ìœ ê°’: {occupation_stats.nunique()}ê°œ")
            logger.info(f"   - ìƒìœ„ 20ê°œ:")
            for occ, count in occupation_stats.head(20).items():
                if occ:
                    logger.info(f"      * {occ}: {count}ê±´")

            # â­ "í•™ìƒ" ê´€ë ¨ occupation
            student_mask = self.df['occupation'].str.contains('í•™ìƒ', na=False, case=False)
            student_count = student_mask.sum()
            logger.info(f"\n   - 'í•™ìƒ' í¬í•¨: {student_count}ê±´")
            if student_count > 0:
                student_occupations = self.df[student_mask]['occupation'].value_counts()
                for occ, count in student_occupations.items():
                    logger.info(f"      * {occ}: {count}ê±´")

            # â­â­â­ Behavioral íŒ¨í„´ í†µê³„ (ë””ë²„ê¹…ìš©)
            logger.info(f"\nğŸ“Š Behavioral íŒ¨í„´ í†µê³„:")

            # late_night_snack_method í†µê³„
            if 'late_night_snack_method' in self.df.columns:
                lns_stats = self.df['late_night_snack_method'].value_counts()
                lns_count = self.df['late_night_snack_method'].notna().sum()
                logger.info(f"\n   [late_night_snack_method]")
                logger.info(f"   - Total: {self.total_count}ê±´")
                logger.info(f"   - None: {self.df['late_night_snack_method'].isna().sum()}ê±´")
                logger.info(f"   - ê°’ ìˆìŒ: {lns_count}ê±´")
                logger.info(f"   - ê³ ìœ ê°’: {lns_stats.nunique()}ê°œ")
                for value, count in lns_stats.items():
                    logger.info(f"      * '{value}': {count}ê±´")
            else:
                logger.warning(f"   âš ï¸ 'late_night_snack_method' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")

            # uses_food_delivery í†µê³„
            if 'uses_food_delivery' in self.df.columns:
                fd_stats = self.df['uses_food_delivery'].value_counts()
                logger.info(f"\n   [uses_food_delivery]")
                logger.info(f"   - True: {(self.df['uses_food_delivery'] == True).sum()}ê±´")
                logger.info(f"   - False: {(self.df['uses_food_delivery'] == False).sum()}ê±´")
                logger.info(f"   - None: {self.df['uses_food_delivery'].isna().sum()}ê±´")

        except Exception as e:
            logger.error(f"âŒ Panel data ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def filter_all(
        self,
        gender: Optional[str] = None,
        age_group: Optional[str] = None,
        region: Optional[str] = None,
        sub_region: Optional[str] = None,
        occupation: Optional[str] = None,
        marital_status: Optional[str] = None,
        behavioral_conditions: Optional[Dict[str, Union[bool, str]]] = None,
    ) -> pd.DataFrame:
        """âš¡ Pandas ë²¡í„°í™” í•„í„°ë§ (ì´ˆê³ ì† - metadata + occupation + marital + behavioral ì „ë¶€!)

        Args:
            gender, age_group, region, sub_region: Demographics í•„í„°
            occupation: ì§ì—… í•„í„° (ë¶€ë¶„ ë§¤ì¹­)
            marital_status: ê²°í˜¼ ì—¬ë¶€ í•„í„°
            behavioral_conditions: Behavioral í•„í„°
                - bool: {'smoker': True, 'has_vehicle': False}
                - str: {'winter_vacation_memory': 'ì¹œêµ¬ë“¤ê³¼ ë³´ë‚¸ ì¦ê±°ìš´ ì‹œê°„'}

        Returns:
            í•„í„°ë§ëœ DataFrame
        """
        if not self.loaded:
            raise RuntimeError("Panel data not loaded")

        mask = pd.Series([True] * len(self.df))

        # â­ Demographics í•„í„° (metadata)
        if gender:
            mask &= (self.df['gender'] == gender)

        if age_group:
            mask &= (self.df['age_group'] == age_group)

        if region:
            mask &= (self.df['region'] == region)

        if sub_region:
            mask &= (self.df['sub_region'] == sub_region)

        # â­ Occupation í•„í„° (ë¶€ë¶„ ë§¤ì¹­)
        if occupation:
            # "ì˜ì—…ì§" â†’ "ì˜ì—…" í¬í•¨ í™•ì¸
            keyword = occupation.replace('ì§', '')
            if keyword:
                mask &= self.df['occupation'].notna() & self.df['occupation'].str.contains(
                    keyword, case=False, na=False, regex=False
                )

        # â­ Marital status í•„í„°
        if marital_status:
            mask &= (self.df['marital_status'] == marital_status)

        # â­â­â­ Behavioral í•„í„° (39ê°œ ì¡°ê±´)
        if behavioral_conditions:
            for behavior_key, expected_value in behavioral_conditions.items():
                if expected_value is None:
                    continue  # Noneì€ ì²´í¬ ì•ˆí•¨

                if behavior_key in self.df.columns:
                    if isinstance(expected_value, bool):
                        # â­ Boolean ì²´í¬ (ë²¡í„°í™”!)
                        mask &= (self.df[behavior_key] == expected_value)
                    elif isinstance(expected_value, str):
                        # â­ ë¬¸ìì—´ ë§¤ì¹­ (ë¶€ë¶„ ë§¤ì¹­, ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
                        mask &= self.df[behavior_key].notna() & self.df[behavior_key].str.contains(
                            expected_value, case=False, na=False, regex=False
                        )

        return self.df[mask]

    def get_user_docs(self, user_ids: List[str]) -> List[Dict]:
        """user_id ë¦¬ìŠ¤íŠ¸ë¡œ ì „ì²´ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°"""
        return [self.user_map[uid] for uid in user_ids if uid in self.user_map]

    def get_all_user_ids(self) -> List[str]:
        """ëª¨ë“  user_id ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        return self.df['user_id'].tolist() if self.loaded else []


# â­ ì „ì—­ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤
panel_cache = PanelDataCache()


router = APIRouter(
    prefix="/search",
    tags=["Search"]
)

# â­ 1ì°¨ ë©”ëª¨ë¦¬ ìºì‹œ (ì´ˆê³ ì†!)
# - maxsize: ìµœëŒ€ 100ê°œ ê²€ìƒ‰ ê²°ê³¼ ìºì‹±
# - ttl: 300ì´ˆ (5ë¶„) í›„ ìë™ ë§Œë£Œ
memory_cache = TTLCache(maxsize=100, ttl=300)

# â­ LLM ì¿¼ë¦¬ ê²°ê³¼ ìºì‹œ (í–‰ë™ íŒ¨í„´ ì¶”ì¶œ)
llm_query_cache = TTLCache(maxsize=1000, ttl=300)

# â­ íŒ¨í„´ ìš°ì„ ìˆœìœ„ ì •ì˜ (êµ¬ì²´ì  > ì¼ë°˜ì )
PATTERN_HIERARCHY = {
    "overseas_travel_preference": ["travels"],
    "travel_style": ["travels"],
    "happy_consumption": [],
    "winter_vacation_memory": ["travels"],
    "skin_satisfaction": ["uses_beauty_products"],
    "skincare_spending": ["uses_beauty_products"],
    "skincare_priority": ["uses_beauty_products"],
    "plastic_bag_reduction": ["cares_about_environment"],
    "rewards_attention": ["cares_about_rewards"],
    "privacy_protection_habit": ["privacy_conscious"],
    "summer_fashion_essential": ["shops_fashion"],
    "pet_experience": ["has_pet"],
    "traditional_market_frequency": ["visits_traditional_market"],
    "stress_source": ["has_stress"],
    "stress_relief_method": ["has_stress"],
    "exercise_type": ["exercises"],
    "fast_delivery_product": ["uses_fast_delivery"],
    "late_night_snack_method": ["uses_food_delivery"],  # â­ ì•¼ì‹ ë°©ë²• > ë°°ë‹¬ ì—¬ë¶€
    "ott_count": ["ott_user"],  # â­ OTT ê°œìˆ˜ > OTT ì‚¬ìš© ì—¬ë¶€
    "solo_dining_frequency": ["dines_out"],  # â­ í˜¼ë°¥ ë¹ˆë„ > ì™¸ì‹ ì—¬ë¶€
}

# OpenSearch ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ë³µì¡í•œ ì¿¼ë¦¬ë‚˜ ëŒ€ìš©ëŸ‰ ê²€ìƒ‰ì„ ìœ„í•´ 30ì´ˆë¡œ ì„¤ì •)
DEFAULT_OS_TIMEOUT = 180  # ëŒ€ëŸ‰ ë°ì´í„° ì¡°íšŒ ëŒ€ì‘ (ì „ì²´ ë°ì´í„° ì•½ 35000ê°œ)

# ëŸ°íƒ€ì„ ê³µìœ  ê°ì²´ (í•œ ë²ˆë§Œ ì´ˆê¸°í™” í›„ ì¬ì‚¬ìš©)
router.analyzer = None  # type: ignore[attr-defined]
router.embedding_model = None  # type: ignore[attr-defined]
router.config = None  # type: ignore[attr-defined]

# â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ welcome ì¸ë±ìŠ¤ ìºì‹œ ì œê±°

_SUMMARY_RESPONSE_TEMPLATE = (
    "{\n"
    '  "highlights": ["ìš”ì•½1", "ìš”ì•½2", "ìš”ì•½3"],\n'
    '  "demographic_summary": "ì£¼ìš” ì¸êµ¬í†µê³„ ì¸ì‚¬ì´íŠ¸",\n'
    '  "behavioral_summary": "ì£¼ìš” í–‰ë™/ìŠµê´€ ì¸ì‚¬ì´íŠ¸",\n'
    '  "data_signals": ["ì£¼ìš” ìˆ˜ì¹˜ë‚˜ íŒ¨í„´"],\n'
    '  "follow_up_questions": ["í›„ì†ìœ¼ë¡œ íƒìƒ‰í•˜ë©´ ì¢‹ì„ ì§ˆë¬¸"]\n'
    "}"
)

_DEFAULT_SUMMARY_INSTRUCTIONS = (
    "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ì œê³µí•˜ì„¸ìš”. "
    "ì •ëŸ‰ì  ì§€í‘œ(ì‘ë‹µì ìˆ˜, ë¹„ìœ¨ ë“±)ê°€ ìˆì„ ê²½ìš° ëª…ì‹œí•˜ê³ , ë°ì´í„°ì˜ í¸í–¥ì´ë‚˜ í•œê³„ë„ ì–¸ê¸‰í•˜ì„¸ìš”. "
    "âš ï¸ ì¤‘ìš”: ëª¨ë“  ìš”ì•½ í•„ë“œ(highlights, demographic_summary, behavioral_summary ë“±)ëŠ” ê°ê° ìµœëŒ€ 2ì¤„ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”."
)


def _utc_now_iso() -> str:
    return datetime.now(timezone.utc).replace(tzinfo=timezone.utc).isoformat()


def _truncate_text(value: Any, max_length: int = 4000) -> str:
    text = value if isinstance(value, str) else json.dumps(value, ensure_ascii=False)
    if len(text) <= max_length:
        return text
    return text[: max_length - 3] + "..."


def _redis_list_append(
    client,
    key: str,
    payload: Dict[str, Any],
    max_length: Optional[int],
    ttl_seconds: Optional[int],
) -> None:
    if not client or not key:
        return
    try:
        serialized = json.dumps(payload, ensure_ascii=False, default=str)
        pipeline = client.pipeline()
        pipeline.rpush(key, serialized)
        if max_length and max_length > 0:
            pipeline.ltrim(key, -max_length, -1)
        if ttl_seconds and ttl_seconds > 0:
            pipeline.expire(key, ttl_seconds)
        pipeline.execute()
    except Exception as exc:
        logger.warning(f"âš ï¸ Redis ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: key={key}, error={exc}")


def _make_conversation_key(prefix: Optional[str], session_id: Optional[str]) -> Optional[str]:
    if not prefix or not session_id:
        return None
    return f"{prefix}:{session_id}"


def _make_history_key(prefix: Optional[str], owner_id: Optional[str]) -> Optional[str]:
    if not prefix or not owner_id:
        return None
    return f"{prefix}:{owner_id}"


def _extract_json_from_text(text: str) -> Optional[Dict[str, Any]]:
    if not text:
        return None
    try:
        fenced = re.search(r"```json\s*(.*?)\s*```", text, re.DOTALL)
        if fenced:
            return json.loads(fenced.group(1))
        fallback = re.search(r"\{.*\}", text, re.DOTALL)
        if fallback:
            return json.loads(fallback.group(0))
        return json.loads(text)
    except Exception:
        return None


def _ensure_request_defaults(request: Any) -> None:
    """ìš”ì²­ì— í•„ìˆ˜ ê¸°ë³¸ê°’ì„ ì±„ì›Œ ì‚¬ìš©ìê°€ queryë§Œ ë³´ë‚´ë„ ë™ì‘í•˜ë„ë¡ ë³´ì •."""
    session_id = getattr(request, "session_id", None)
    if not session_id:
        session_id = str(uuid4())
        request.session_id = session_id

    user_id = getattr(request, "user_id", None)
    if not user_id:
        request.user_id = session_id

    # ëª¨ë“  ë³´ì¡° ê¸°ëŠ¥ ê¸°ë³¸ í™œì„±í™”
    request.log_conversation = True
    request.log_search_history = True
    request.request_llm_summary = True

def _prepare_summary_results(
    results: List["SearchResult"],
    max_results: int,
    max_chars: int,
) -> List[Dict[str, Any]]:
    trimmed: List[Dict[str, Any]] = []
    total_chars = 0
    for idx, result in enumerate(results[:max_results], start=1):
        if hasattr(result, "model_dump"):
            item = result.model_dump()
        elif isinstance(result, dict):
            item = result
        else:
            continue

        item_copy = {
            "rank": idx,
            "user_id": item.get("user_id"),
            "score": item.get("score"),
            "timestamp": item.get("timestamp"),
            "demographic_info": item.get("demographic_info"),
            "behavioral_info": item.get("behavioral_info"),
            "qa_pairs": (item.get("qa_pairs") or [])[:3],
            "matched_qa_pairs": (item.get("matched_qa_pairs") or [])[:3],
            "highlights": item.get("highlights"),
        }

        serialized = json.dumps(item_copy, ensure_ascii=False)
        prospective_total = total_chars + len(serialized)
        if prospective_total > max_chars and trimmed:
            break
        trimmed.append(item_copy)
        total_chars = prospective_total
    return trimmed


def _maybe_generate_llm_summary(
    *,
    request,
    response: "SearchResponse",
    analysis,
) -> Optional[Dict[str, Any]]:
    if not getattr(request, "request_llm_summary", False):
        return None

    if not getattr(router, "enable_search_summary", False):
        logger.info("LLM ìš”ì•½ ë¹„í™œì„±í™” ì„¤ì •ìœ¼ë¡œ ì¸í•´ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    client = getattr(router, "anthropic_client", None)
    if client is None:
        logger.warning("Anthropic í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ LLM ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    config = getattr(router, "config", None)
    if config is None:
        logger.warning("Config ì„¤ì •ì´ ì—†ì–´ LLM ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    model_name = getattr(router, "search_summary_model", None) or config.CLAUDE_MODEL
    if not model_name:
        logger.warning("ìš”ì•½ì— ì‚¬ìš©í•  ëª¨ë¸ëª…ì´ ì„¤ì •ë˜ì§€ ì•Šì•„ LLM ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    max_results = getattr(router, "search_summary_max_results", 10)
    max_chars = getattr(router, "search_summary_max_chars", 16000)
    prepared_results = _prepare_summary_results(response.results, max_results, max_chars)
    if not prepared_results:
        logger.info("LLM ìš”ì•½ì„ ìœ„í•œ ê²°ê³¼ê°€ ì—†ì–´ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    instructions = getattr(request, "llm_summary_instructions", None) or _DEFAULT_SUMMARY_INSTRUCTIONS

    # â­â­â­ í™œì„±í™”ëœ behavioral í•„í„° ì¶”ì¶œ
    active_behavioral = {
        k: v for k, v in getattr(analysis, 'behavioral_conditions', {}).items()
        if v is not None and v is not False  # Noneê³¼ False ì œì™¸
    }

    # í•œê¸€ ì„¤ëª…ìœ¼ë¡œ ë³€í™˜
    behavioral_text = ""
    if active_behavioral:
        items = []
        for k, v in active_behavioral.items():
            config = BEHAVIORAL_KEYWORD_MAP.get(k, {})
            label = config.get('question_text', k)
            # ê°„ê²°í•˜ê²Œ í‘œì‹œ
            if isinstance(v, bool):
                items.append(f"- {label}: {'ì˜ˆ' if v else 'ì•„ë‹ˆì˜¤'}")
            else:
                items.append(f"- {label}: {v}")
        behavioral_text = "\n".join(items)

    prompt = (
        "ë‹¹ì‹ ì€ ì„¤ë¬¸ì¡°ì‚¬ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
        "ì£¼ì–´ì§„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”.\n\n"
        f"ì‚¬ìš©ì ì§ˆì˜: {request.query}\n"
        f"ì˜ˆìƒ ê²€ìƒ‰ ì˜ë„: {getattr(analysis, 'intent', 'N/A')}\n"
        f"ì¶”ì¶œëœ must_terms: {getattr(analysis, 'must_terms', [])}\n"
        f"ì¶”ì¶œëœ should_terms: {getattr(analysis, 'should_terms', [])}\n\n"
        # â­â­â­ ì ìš©ëœ í–‰ë™ í•„í„° ì •ë³´ ì¶”ê°€!
        f"ğŸ“‹ ì ìš©ëœ í–‰ë™ í•„í„°:\n{behavioral_text or 'ì—†ìŒ'}\n\n"
        f"âš ï¸ ë§¤ìš° ì¤‘ìš”: ìœ„ í–‰ë™ í•„í„°ê°€ ì ìš©ë˜ì–´ ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ëŠ” ì´ë¯¸ í•„í„°ë§ëœ ìƒíƒœì…ë‹ˆë‹¤.\n"
        f"ê²€ìƒ‰ëœ ëª¨ë“  ì‘ë‹µìëŠ” ìœ„ ì¡°ê±´ì„ ë§Œì¡±í•©ë‹ˆë‹¤. behavioral_summary ì‘ì„± ì‹œ ë°˜ë“œì‹œ ì´ë¥¼ ë°˜ì˜í•˜ì„¸ìš”.\n"
        f"ì˜ˆ: 'í¡ì—° ì—¬ë¶€: ì˜ˆ' í•„í„° ì ìš© ì‹œ â†’ 'ëª¨ë“  ì‘ë‹µìëŠ” í¡ì—°ìì…ë‹ˆë‹¤'\n"
        f"ì˜ˆ: 'OTT ì„œë¹„ìŠ¤ ê°œìˆ˜: 2ê°œ' í•„í„° ì ìš© ì‹œ â†’ 'ëª¨ë“  ì‘ë‹µìëŠ” OTT ì„œë¹„ìŠ¤ 2ê°œë¥¼ ì´ìš©í•©ë‹ˆë‹¤'\n\n"
        f"ì´ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {response.total_hits}\n"
        f"í˜„ì¬ ë°˜í™˜ëœ ê²°ê³¼ ìˆ˜: {len(response.results)}\n\n"
        f"ìš”ì•½ ì§€ì¹¨: {instructions}\n\n"
        "âš ï¸ ì¤‘ìš”: ëª¨ë“  ìš”ì•½ í•„ë“œëŠ” ê°ê° ìµœëŒ€ 2ì¤„ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n"
        "- highlights: ê° í•­ëª©ì€ 1ì¤„ë¡œ, ìµœëŒ€ 2ê°œ í•­ëª©\n"
        "- demographic_summary: ìµœëŒ€ 2ì¤„\n"
        "- behavioral_summary: ìµœëŒ€ 2ì¤„ (â­ ì ìš©ëœ í–‰ë™ í•„í„°ë¥¼ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì„¸ìš”!)\n"
        "- data_signals: ê° í•­ëª©ì€ 1ì¤„ë¡œ, ìµœëŒ€ 2ê°œ í•­ëª©\n"
        "- follow_up_questions: ê° í•­ëª©ì€ 1ì¤„ë¡œ, ìµœëŒ€ 2ê°œ í•­ëª©\n\n"
        "ê²€ìƒ‰ ê²°ê³¼(ìµœëŒ€ ì¼ë¶€) JSON:\n"
        f"{json.dumps(prepared_results, ensure_ascii=False, indent=2)}\n\n"
        "ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. í˜•ì‹ ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n"
        f"{_SUMMARY_RESPONSE_TEMPLATE}\n"
    )

    max_tokens = min(1200, getattr(config, "CLAUDE_MAX_TOKENS", 1500))
    temperature = getattr(config, "CLAUDE_TEMPERATURE", 0.1)

    try:
        message = client.messages.create(
            model=model_name,
            max_tokens=max_tokens,
            temperature=temperature,
            messages=[{"role": "user", "content": prompt}],
        )
        content = ""
        if message and getattr(message, "content", None):
            parts = getattr(message, "content", [])
            if parts:
                # Anthropics SDK returns list of blocks with .text
                first = parts[0]
                content = getattr(first, "text", "") or ""
        summary_json = _extract_json_from_text(content)
        if summary_json is None:
            logger.warning("LLM ìš”ì•½ ì‘ë‹µì—ì„œ JSONì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return {
                "model": model_name,
                "generated_at": _utc_now_iso(),
                "raw_text": content,
            }
        return {
            "model": model_name,
            "generated_at": _utc_now_iso(),
            "summary": summary_json,
        }
    except Exception as exc:
        logger.warning(f"âš ï¸ LLM ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {exc}")
        return None


def _extract_response_timings(response: "SearchResponse", fallback: Optional[Dict[str, Any]]) -> Dict[str, Any]:
    if response.query_analysis and isinstance(response.query_analysis, dict):
        timings = response.query_analysis.get("timings_ms")
        if isinstance(timings, dict):
            return timings
    return fallback or {}


def _persist_search_logs(
    *,
    request,
    response: "SearchResponse",
    analysis,
    cache_hit: bool,
    timings: Dict[str, Any],
) -> None:
    client = getattr(router, "redis_client", None)
    if client is None:
        return

    timestamp = _utc_now_iso()
    session_id = getattr(request, "session_id", None)
    user_id = getattr(request, "user_id", None)
    request_id = getattr(request, "request_id", None)
    request_metadata = getattr(request, "metadata", None)
    conversation_prefix = getattr(router, "conversation_history_prefix", None)
    conversation_ttl = getattr(router, "conversation_history_ttl_seconds", None)
    conversation_max = getattr(router, "conversation_history_max_messages", None)
    search_history_prefix = getattr(router, "search_history_prefix", None)
    search_history_ttl = getattr(router, "search_history_ttl_seconds", None)
    search_history_max = getattr(router, "search_history_max_entries", None)

    top_user_ids = [
        getattr(result, "user_id", None) for result in (response.results or [])[:5]
        if getattr(result, "user_id", None)
    ]

    if getattr(request, "log_conversation", True):
        conversation_key = _make_conversation_key(conversation_prefix, session_id)
        if conversation_key:
            user_entry = {
                "role": "user",
                "timestamp": timestamp,
                "content": _truncate_text(request.query, 4000),
                "session_id": session_id,
                "user_id": user_id,
                "request_id": request_id,
                "metadata": request_metadata,
            }
            _redis_list_append(client, conversation_key, user_entry, conversation_max, conversation_ttl)

            # ì „ì²´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì§ë ¬í™”í•˜ì—¬ í¬í•¨
            serialized_results = None
            if response.results:
                try:
                    serialized_results = [_serialize_result(result) for result in response.results]
                except Exception as exc:
                    logger.warning(f"âš ï¸ ëŒ€í™” ë¡œê·¸ìš© ê²€ìƒ‰ ê²°ê³¼ ì§ë ¬í™” ì‹¤íŒ¨: {exc}")
                    serialized_results = None
            
            assistant_payload: Dict[str, Any] = {
                "requested_count": getattr(response, "requested_count", None),
                "query": request.query,
                "total_hits": response.total_hits,
                "max_score": getattr(response, "max_score", None),
                "results": serialized_results,  # ì „ì²´ ê²€ìƒ‰ ê²°ê³¼ í¬í•¨
                "returned_count": len(response.results or []),
                "cache_hit": cache_hit,
                "top_user_ids": top_user_ids,
                "took_ms": getattr(response, "took_ms", None),
                "page": response.page,
                "page_size": response.page_size,
                "has_more": getattr(response, "has_more", False),
            }
            if response.llm_summary:
                assistant_payload["llm_summary"] = response.llm_summary

            assistant_entry = {
                "role": "assistant",
                "timestamp": timestamp,
                "content": assistant_payload,  # ì „ì²´ ë”•ì…”ë„ˆë¦¬ ì €ì¥ (truncate ì œê±°)
                "session_id": session_id,
                "user_id": user_id,
                "request_id": request_id,
            }
            _redis_list_append(client, conversation_key, assistant_entry, conversation_max, conversation_ttl)

    if getattr(request, "log_search_history", True):
        owner_id = user_id or session_id or "default"
        history_key = _make_history_key(search_history_prefix, owner_id)
        if history_key:
            # ì „ì²´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì§ë ¬í™”í•˜ì—¬ í¬í•¨
            serialized_results = None
            if response.results:
                try:
                    serialized_results = [_serialize_result(result) for result in response.results]
                except Exception as exc:
                    logger.warning(f"âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì§ë ¬í™” ì‹¤íŒ¨: {exc}")
                    serialized_results = None
            
            history_entry = {
                "timestamp": timestamp,
                "user_id": user_id,
                "session_id": session_id,
                "request_id": request_id,
                "query": request.query,
                "intent": getattr(analysis, "intent", None),
                "must_terms": getattr(analysis, "must_terms", []),
                "should_terms": getattr(analysis, "should_terms", []),
                "page": response.page,
                "page_size": response.page_size,
                "total_hits": response.total_hits,
                "returned_count": len(response.results or []),
                "cache_hit": cache_hit,
                "timings": timings,
                "top_user_ids": top_user_ids,
                "llm_summary": response.llm_summary,
                "metadata": request_metadata,
                "results": serialized_results,  # ì „ì²´ ê²€ìƒ‰ ê²°ê³¼ í¬í•¨
            }
            _redis_list_append(client, history_key, history_entry, search_history_max, search_history_ttl)


class ConversationMessage(BaseModel):
    role: str
    timestamp: str
    session_id: Optional[str] = None
    user_id: Optional[str] = None
    request_id: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None
    content: Any


class SearchHistoryEntry(BaseModel):
    timestamp: str
    user_id: Optional[str] = None
    session_id: Optional[str] = None
    request_id: Optional[str] = None
    query: str
    intent: Optional[str] = None
    must_terms: List[str] = Field(default_factory=list)
    should_terms: List[str] = Field(default_factory=list)
    page: int
    page_size: int
    total_hits: int
    returned_count: int
    cache_hit: bool
    timings: Dict[str, Any] = Field(default_factory=dict)
    top_user_ids: List[str] = Field(default_factory=list)
    llm_summary: Optional[Dict[str, Any]] = None
    metadata: Optional[Dict[str, Any]] = None
    results: Optional[List[Dict[str, Any]]] = Field(default=None, description="ì „ì²´ ê²€ìƒ‰ ê²°ê³¼ (ëª¨ë“  ì‚¬ìš©ì ë°ì´í„° í¬í•¨)")


def _parse_conversation_record(item: str) -> Optional[ConversationMessage]:
    if not item:
        return None
    try:
        payload = json.loads(item)
    except Exception as exc:
        logger.warning(f"âš ï¸ ëŒ€í™” ë¡œê·¸ JSON íŒŒì‹± ì‹¤íŒ¨: {exc}")
        return None

    content = payload.get("content")
    # assistant ë©”ì‹œì§€ì˜ contentê°€ ë¬¸ìì—´ì´ë©´ JSONìœ¼ë¡œ íŒŒì‹± ì‹œë„
    # (ì´ì „ ë²„ì „ í˜¸í™˜ì„±: _truncate_textë¡œ ì €ì¥ëœ ê²½ìš°)
    if payload.get("role") == "assistant" and isinstance(content, str):
        try:
            content = json.loads(content)
        except Exception:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ìœ ì§€
            pass
    # contentê°€ ì´ë¯¸ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš© (ìƒˆ ë²„ì „)
    
    return ConversationMessage(
        role=payload.get("role"),
        timestamp=payload.get("timestamp"),
        session_id=payload.get("session_id"),
        user_id=payload.get("user_id"),
        request_id=payload.get("request_id"),
        metadata=payload.get("metadata"),
        content=content,
    )


def _parse_search_history_record(item: str) -> Optional[SearchHistoryEntry]:
    if not item:
        return None
    try:
        payload = json.loads(item)
    except Exception as exc:
        logger.warning(f"âš ï¸ ê²€ìƒ‰ ì´ë ¥ JSON íŒŒì‹± ì‹¤íŒ¨: {exc}")
        return None

    llm_summary = payload.get("llm_summary")
    if isinstance(llm_summary, str):
        try:
            llm_summary = json.loads(llm_summary)
        except Exception:
            pass

    return SearchHistoryEntry(
        timestamp=payload.get("timestamp"),
        user_id=payload.get("user_id"),
        session_id=payload.get("session_id"),
        request_id=payload.get("request_id"),
        query=payload.get("query", ""),
        intent=payload.get("intent"),
        must_terms=payload.get("must_terms") or [],
        should_terms=payload.get("should_terms") or [],
        page=payload.get("page", 1),
        page_size=payload.get("page_size", 10),
        total_hits=payload.get("total_hits", 0),
        returned_count=payload.get("returned_count", 0),
        cache_hit=bool(payload.get("cache_hit")),
        timings=payload.get("timings") or {},
        top_user_ids=payload.get("top_user_ids") or [],
        llm_summary=llm_summary,
        metadata=payload.get("metadata"),
        results=payload.get("results"),  # ì „ì²´ ê²€ìƒ‰ ê²°ê³¼ í¬í•¨
    )


def _finalize_search_response(
    *,
    request,
    response: "SearchResponse",
    analysis,
    cache_hit: bool,
    timings: Optional[Dict[str, Any]] = None,
) -> "SearchResponse":
    summary_payload = _maybe_generate_llm_summary(
        request=request,
        response=response,
        analysis=analysis,
    )
    if summary_payload:
        response = response.model_copy(update={"llm_summary": summary_payload})

    effective_timings = _extract_response_timings(response, timings)
    _persist_search_logs(
        request=request,
        response=response,
        analysis=analysis,
        cache_hit=cache_hit,
        timings=effective_timings,
    )
    return response


# â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ welcome ì¸ë±ìŠ¤ ìºì‹œ í•¨ìˆ˜ ì œê±°


def calculate_rrf_score_adaptive(
    keyword_results: List[Dict[str, Any]],
    vector_results: List[Dict[str, Any]],
    query_intent: Optional[str],
    has_filters: bool,
    use_vector_search: bool,
) -> Tuple[List[Dict[str, Any]], int, str]:
    """ì¿¼ë¦¬ íŠ¹ì„±ì— ë”°ë¼ RRF k ê°’ê³¼ alpha ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •"""
    k = 60
    alpha = 0.6  # â­â­â­ ë²¡í„° ê²€ìƒ‰ ê°€ì¤‘ì¹˜: 60% (keywordëŠ” 40%)
    reason = "ê· í˜• ìœ ì§€ (k=60, alpha=0.6 â†’ vector 60%)"

    if has_filters:
        k = 40
        alpha = 0.6  # í•„í„°ê°€ ìˆì–´ë„ ë²¡í„° ê²€ìƒ‰ ì¤‘ì‹œ
        reason = "í•„í„° ì ìš© â†’ ë²¡í„° ì¤‘ì‹¬ (k=40, alpha=0.6 â†’ vector 60%)"
    elif use_vector_search and query_intent and query_intent.lower() in {"semantic", "semantic_search"}:
        k = 80
        alpha = 0.7  # ì‹œë§¨í‹± ê²€ìƒ‰ì´ë©´ ë²¡í„° ê°€ì¤‘ì¹˜ ë” ë†’ì„
        reason = f"ì˜ë„={query_intent} â†’ ë²¡í„° ê°•í™” (k=80, alpha=0.7 â†’ vector 70%)"

    combined = calculate_rrf_score(
        keyword_results=keyword_results,
        vector_results=vector_results,
        k=k,
        alpha=alpha,
    )
    return combined, k, reason


def _sort_dict_recursive(obj: Any) -> Any:
    """ë”•ì…”ë„ˆë¦¬/ë¦¬ìŠ¤íŠ¸ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì •ë ¬"""
    if isinstance(obj, dict):
        return {key: _sort_dict_recursive(obj[key]) for key in sorted(obj)}
    if isinstance(obj, list):
        normalized_items = [_sort_dict_recursive(item) for item in obj]
        try:
            return sorted(
                normalized_items,
                key=lambda item: json.dumps(item, ensure_ascii=False, sort_keys=True),
            )
        except TypeError:
            return normalized_items
    return obj


def _normalize_filters_for_cache(filters: List[Dict[str, Any]]) -> str:
    """í•„í„° ëª©ë¡ì„ ì•ˆì •ì ì¸ ë¬¸ìì—´ë¡œ ë³€í™˜"""
    if not filters:
        return ""

    normalized_strings = []
    for filter_item in filters:
        normalized = _sort_dict_recursive(filter_item)
        normalized_strings.append(json.dumps(normalized, ensure_ascii=False, sort_keys=True))

    normalized_strings.sort()
    return "|".join(normalized_strings)


def _make_cache_key(
    *,
    prefix: str,
    query: str,
    index_name: str,
    page_size: int,
    use_vector: bool,
    use_claude: bool,
    must_terms: List[str],
    should_terms: List[str],
    must_not_terms: List[str],
    filters_signature: Optional[str] = None,
    behavior_signature: Optional[str] = None,
) -> str:
    """ìƒì„±ëœ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©í•˜ê¸° ìœ„í•œ ìºì‹œ í‚¤ ìƒì„± (ì•ˆì •í™”)"""
    stable_must = sorted(must_terms) if must_terms else []
    stable_should = sorted(should_terms) if should_terms else []
    stable_must_not = sorted(must_not_terms) if must_not_terms else []

    payload = {
        "query": query.strip().lower(),
        "index": index_name,
        "page_size": page_size,
        "use_vector": use_vector,
        "use_claude": bool(use_claude),
        "must_terms": stable_must,
        "should_terms": stable_should,
        "must_not_terms": stable_must_not,
        "filters_signature": filters_signature or "",
        "behavior_signature": behavior_signature or "",
    }
    raw = json.dumps(payload, ensure_ascii=False, sort_keys=True)
    digest = hashlib.sha256(raw.encode("utf-8")).hexdigest()
    key = f"{prefix}:{digest}"

    logger.debug(f"ğŸ”‘ Cache key generated: {key}")
    logger.debug(f"   - must_terms: {stable_must}")
    logger.debug(f"   - should_terms: {stable_should}")
    logger.debug(f"   - filters_signature: {(filters_signature or '')[:100]}...")
    logger.debug(f"   - behavior_signature: {(behavior_signature or '')[:100]}...")
    logger.debug(f"   - use_claude: {bool(use_claude)}")

    return key


def _serialize_result(result: "SearchResult") -> Dict[str, Any]:
    """SearchResultë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ dictë¡œ ë³€í™˜"""
    return result.model_dump()


def _deserialize_result(payload: Dict[str, Any]) -> "SearchResult":
    """dictë¥¼ SearchResult ê°ì²´ë¡œ ì—­ì§ë ¬í™”"""
    return SearchResult(**payload)


def _slice_results(
    serialized_items: List[Dict[str, Any]],
    page: int,
    page_size: int,
) -> Tuple[List["SearchResult"], bool]:
    """í˜ì´ì§€ ì •ë³´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²°ê³¼ë¥¼ ìŠ¬ë¼ì´ì‹±"""
    if page <= 0:
        page = 1
    start = (page - 1) * page_size
    end = start + page_size

    if start >= len(serialized_items):
        return [], False

    page_items = serialized_items[start:end]
    results = [_deserialize_result(item) for item in page_items]
    has_more = end < len(serialized_items)
    return results, has_more


def _build_cached_response(
    *,
    payload: Dict[str, Any],
    request: "SearchRequest",
    analysis,
    filters_for_response: List[Dict[str, Any]],
    overall_start: float,
    extracted_entities_dict: Optional[Dict[str, Any]] = None,
) -> "SearchResponse":
    """Redis ìºì‹œì—ì„œ ë¶ˆëŸ¬ì˜¨ ê²°ê³¼ë¡œ SearchResponse êµ¬ì„±"""
    total_hits = payload.get("total_hits", 0)
    max_score = payload.get("max_score", 0.0)
    serialized_items = payload.get("items", [])

    cached_page_size = payload.get("page_size")
    request_page_size = getattr(request, "size", None)
    if request_page_size is None:
        request_page_size = getattr(request, "page_size", None)
    page_size = cached_page_size or request_page_size or max(len(serialized_items), 1)

    page_results, has_more_local = _slice_results(serialized_items, request.page, page_size)
    has_more = has_more_local and ((request.page * page_size) < total_hits)
    total_duration_ms = (perf_counter() - overall_start) * 1000

    timings = {
        "cache_hit": 1.0,
        "total_ms": total_duration_ms,
    }

    query_analysis = {
        "intent": analysis.intent,
        "must_terms": analysis.must_terms,
        "should_terms": analysis.should_terms,
        "alpha": analysis.alpha,
        "confidence": analysis.confidence,
        "size": page_size,
        "timings_ms": timings,
        "behavioral_conditions": payload.get("behavioral_conditions", {}),
        "use_claude_analyzer": bool(payload.get("use_claude", False)),
    }
    if extracted_entities_dict is not None:
        query_analysis["extracted_entities"] = extracted_entities_dict

    # requested_count ì¶”ì¶œ (payloadì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ None)
    requested_count = payload.get("requested_count")
    
    return SearchResponse(
        requested_count=requested_count,
        query=request.query,
        session_id=getattr(request, "session_id", None),
        total_hits=total_hits,
        max_score=max_score,
        results=page_results,
        query_analysis=query_analysis,
        took_ms=int(total_duration_ms),
        page=request.page,
        page_size=page_size,
        has_more=has_more,
    )


def _log_final_summary(
    *,
    stage: str,
    query: str,
    analysis,
    total_hits: int,
    returned_count: int,
    page: int,
    page_size: int,
    cache_hit: bool,
    timings: Dict[str, Any],
    took_ms: Optional[float],
    filters: Optional[List[Dict[str, Any]]],
    behavioral_conditions: Optional[Dict[str, Any]],
    use_claude: Optional[bool] = None,
) -> None:
    """ê²€ìƒ‰ ì¢…ë£Œ ì‹œ í•µì‹¬ ì •ë³´ë¥¼ í•œ ë²ˆ ë” ìš”ì•½ ì¶œë ¥."""
    intent = getattr(analysis, "intent", None)
    must_terms = getattr(analysis, "must_terms", [])
    should_terms = getattr(analysis, "should_terms", [])
    filter_count = len(filters or [])
    behavior_info = behavioral_conditions or {}
    important_timings = {k: round(v, 2) if isinstance(v, (int, float)) else v for k, v in (timings or {}).items()}

    lines = [
        "",
        "ğŸ”š ìµœì¢… ìš”ì•½ (í•µì‹¬)",
        f" â€¢ stage: {stage}",
        f" â€¢ query: {query}",
        f" â€¢ intent: {intent}",
        f" â€¢ must_terms: {must_terms}",
        f" â€¢ should_terms: {should_terms}",
        f" â€¢ behavioral_conditions: {behavior_info}",
        f" â€¢ filters: {filter_count}ê°œ",
        f" â€¢ returned/total: {returned_count}/{total_hits}",
        f" â€¢ page: {page} / page_size: {page_size}",
        f" â€¢ cache_hit: {cache_hit}",
        f" â€¢ timings: {important_timings}",
        f" â€¢ total_ms: {round(took_ms, 2) if took_ms is not None else 'N/A'}",
    ]
    if use_claude is not None:
        lines.append(f" â€¢ use_claude_analyzer: {use_claude}")

    logger.info("\n".join(lines))


def build_occupation_dsl_filter(occupation_entities: List["DemographicEntity"]) -> Dict[str, Any]:
    """ì§ì—… DemographicEntity ë¦¬ìŠ¤íŠ¸ë¥¼ OpenSearch DSL í•„í„°ë¡œ ë³€í™˜"""
    if not occupation_entities:
        return {"match_all": {}}

    occupation_values: Set[str] = set()
    for demo in occupation_entities:
        for candidate in (
            getattr(demo, "raw_value", None),
            getattr(demo, "value", None),
        ):
            if candidate:
                occupation_values.add(str(candidate))
        synonyms = getattr(demo, "synonyms", None)
        if synonyms:
            for syn in synonyms:
                if syn:
                    occupation_values.add(str(syn))

    occupation_values = {value.strip() for value in occupation_values if value and value.strip()}
    if not occupation_values:
        return {"match_all": {}}

    values_list = sorted(occupation_values)

    question_should = [
        {"match_phrase": {"qa_pairs.q_text": "ì§ì—…"}},
        {"match_phrase": {"qa_pairs.q_text": "ì§ë¬´"}},
        {"match_phrase": {"qa_pairs.q_text": "occupation"}},
    ]

    answer_should = [
        {"terms": {"qa_pairs.answer.keyword": values_list}},
        {"terms": {"qa_pairs.answer_text.keyword": values_list}},
    ]
    # â­ match ëŒ€ì‹  match_phrase ì‚¬ìš©: "ì „ë¬¸ì§"ê³¼ "ì‚¬ë¬´ì§"ì´ "ì§" í† í°ìœ¼ë¡œ ì˜ëª» ë§¤ì¹­ë˜ëŠ” ê²ƒ ë°©ì§€
    for value in values_list:
        answer_should.append({"match_phrase": {"qa_pairs.answer": value}})
        answer_should.append({"match_phrase": {"qa_pairs.answer_text": value}})

    nested_filter = {
        "nested": {
            "path": "qa_pairs",
            "query": {
                "bool": {
                    "must": [
                        {
                            "bool": {
                                "should": question_should,
                                "minimum_should_match": 1,
                            }
                        },
                        {
                            "bool": {
                                "should": answer_should,
                                "minimum_should_match": 1,
                            }
                        },
                    ]
                }
            }
        }
    }

    metadata_terms = {
        "terms": {
            "metadata.occupation.keyword": values_list,
        }
    }
    metadata_job_terms = {
        "terms": {
            "metadata.job.keyword": values_list,
        }
    }

    return {
        "bool": {
            "should": [
                metadata_terms,
                metadata_job_terms,
                nested_filter,
            ],
            "minimum_should_match": 1,
        }
    }


def get_adaptive_score_threshold(
    query: str,
    has_filters: bool,
    must_terms_count: int,
) -> Tuple[float, str]:
    """ì¿¼ë¦¬ íŠ¹ì„±ì— ë”°ë¼ Qdrant score_threshold ì¡°ì •"""
    threshold = 0.30
    reason = "ê¸°ë³¸ê°’ 0.30"

    if has_filters:
        threshold = 0.25
        reason = "í•„í„° ì ìš© â†’ í›„ë³´ í™•ë³´ (threshold=0.25)"
    elif must_terms_count >= 3:
        threshold = 0.35
        reason = "í‚¤ì›Œë“œ ë‹¤ìˆ˜ â†’ ì •í™•ë„ ì¤‘ì‹œ (threshold=0.35)"
    elif len(query.split()) <= 3:
        threshold = 0.35
        reason = "ì§§ì€ ì¿¼ë¦¬ â†’ ì •ë°€ë„ ì¤‘ì‹œ (threshold=0.35)"

    return threshold, reason


def _collect_text_from_doc(doc: Dict[str, Any]) -> str:
    text_fragments: List[str] = []

    source = doc.get("_source") or doc.get("source") or {}
    if not source and "doc" in doc:
        source = doc.get("doc", {}).get("_source", {})
    if not source and "payload" in doc:
        payload = doc["payload"]
        if isinstance(payload, dict):
            source = {
                "payload_text": payload.get("text"),
                "payload": payload,
            }

    if isinstance(source, dict):
        for key in ("qa_pairs", "qaPairs"):
            qa_pairs = source.get(key, [])
            if isinstance(qa_pairs, list):
                for qa in qa_pairs:
                    if not isinstance(qa, dict):
                        continue
                    q_text = qa.get("q_text") or qa.get("question")
                    if q_text:
                        text_fragments.append(str(q_text).lower())
                    answer = qa.get("answer") or qa.get("answer_text") or qa.get("value")
                    if answer:
                        if isinstance(answer, list):
                            text_fragments.extend(str(item).lower() for item in answer)
                        else:
                            text_fragments.append(str(answer).lower())

        for key in ("metadata", "demographic_info", "payload"):
            meta = source.get(key)
            if isinstance(meta, dict):
                for value in meta.values():
                    if value:
                        text_fragments.append(str(value).lower())

        for key in ("title", "text", "content", "payload_text"):
            value = source.get(key)
            if value:
                text_fragments.append(str(value).lower())

    if "payload" in doc and isinstance(doc["payload"], dict):
        payload = doc["payload"]
        for key in ("text", "keywords"):
            value = payload.get(key)
            if isinstance(value, list):
                text_fragments.extend(str(item).lower() for item in value)
            elif value:
                text_fragments.append(str(value).lower())

    return " ".join(text_fragments)


def contains_must_terms(doc: Dict[str, Any], must_terms: List[str]) -> bool:
    """
    âš ï¸ Deprecated: OpenSearch ì¿¼ë¦¬ì—ì„œ must ì¡°ê±´ ì²˜ë¦¬ë¡œ ëŒ€ì²´ë¨
    ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ Python ë ˆë²¨ ê²€ì¦ì€ ì œê±°ë¨ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìµœì í™”)

    ë ˆê±°ì‹œ ì½”ë“œ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€, ì‚¬ìš© ê¶Œì¥í•˜ì§€ ì•ŠìŒ
    """
    if not must_terms:
        return True

    combined_text = _collect_text_from_doc(doc)
    if not combined_text:
        return False

    for term in must_terms:
        normalized = term.lower().strip()
        if normalized and normalized not in combined_text:
            return False
    return True


def _qa_contains_terms(qa: Dict[str, Any], terms_lower: List[str]) -> bool:
    if not isinstance(qa, dict):
        return False

    text_candidates: List[str] = []
    q_text = qa.get("q_text") or qa.get("question")
    if q_text:
        text_candidates.append(str(q_text).lower())

    answer = qa.get("answer") or qa.get("answer_text") or qa.get("value")
    if answer:
        if isinstance(answer, list):
            text_candidates.extend(str(item).lower() for item in answer)
        else:
            text_candidates.append(str(answer).lower())

    if not text_candidates:
        return False

    combined = " ".join(text_candidates)
    return all(term in combined for term in terms_lower if term)


def extract_matched_qa_pairs(source: Dict[str, Any], must_terms: List[str], limit: int = 5) -> List[Dict[str, Any]]:
    if not must_terms:
        return []
    qa_pairs = source.get("qa_pairs")
    if not isinstance(qa_pairs, list):
        return []

    terms_lower = [term.lower().strip() for term in must_terms if term]
    matched: List[Dict[str, Any]] = []
    for qa in qa_pairs:
        if _qa_contains_terms(qa, terms_lower):
            matched.append(qa)
            if len(matched) >= limit:
                break
    return matched


def get_display_qa_pairs(source: Dict[str, Any], must_terms: List[str], limit: int = 5) -> List[Dict[str, Any]]:
    qa_pairs = source.get("qa_pairs")
    if not isinstance(qa_pairs, list):
        return []

    if not must_terms:
        return qa_pairs[:limit]

    terms_lower = [term.lower().strip() for term in must_terms if term]
    matched = []
    others = []
    for qa in qa_pairs:
        if _qa_contains_terms(qa, terms_lower):
            matched.append(qa)
        else:
            others.append(qa)

    ordered = matched + others
    return ordered[:limit]


def extract_inner_hit_matches(hit: Dict[str, Any]) -> List[Dict[str, Any]]:
    inner_hits = hit.get("inner_hits")
    if not isinstance(inner_hits, dict):
        return []

    collected: List[Dict[str, Any]] = []
    for inner_name, inner_data in inner_hits.items():
        hits_obj = inner_data.get("hits", {}) if isinstance(inner_data, dict) else {}
        for inner_hit in hits_obj.get("hits", []):
            inner_source = inner_hit.get("_source", {}) or {}
            if "qa_pairs" in inner_source and isinstance(inner_source["qa_pairs"], dict):
                qa_entry = inner_source["qa_pairs"].copy()
            else:
                qa_entry = inner_source.copy()

            if not isinstance(qa_entry, dict):
                continue

            if "_score" in inner_hit and "match_score" not in qa_entry:
                qa_entry["match_score"] = inner_hit["_score"]
            if "highlight" in inner_hit and "highlights" not in qa_entry:
                qa_entry["highlights"] = inner_hit["highlight"]

            qa_entry.setdefault("inner_hit_name", inner_name)
            collected.append(qa_entry)

    return collected


def extract_behavioral_qa_pairs(
    source: Dict[str, Any],
    behavioral_conditions: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """
    Behavioral ì¡°ê±´ì— ë§¤ì¹­ëœ qa_pairs ì¶”ì¶œ

    Args:
        source: OpenSearch ë¬¸ì„œ _source (ë˜ëŠ” qa_pairsë§Œ í¬í•¨ëœ dict)
        behavioral_conditions: {"smoker": True, "has_vehicle": True, ...}

    Returns:
        ë§¤ì¹­ëœ qa_pairs ë¦¬ìŠ¤íŠ¸
        [
            {
                "condition_type": "smoker",
                "condition_value": True,
                "q_text": "ê·€í•˜ëŠ” í¡ì—°ì„ í•˜ì‹­ë‹ˆê¹Œ?",
                "answer": "í¡ì—°í•¨",
                "confidence": 1.0
            },
            ...
        ]
    """
    qa_pairs = source.get('qa_pairs', [])
    if not qa_pairs or not behavioral_conditions:
        return []

    matched = []

    # ì¡°ê±´ë³„ í‚¤ì›Œë“œ ë§¤í•‘
    CONDITION_KEYWORDS = {
        'smoker': ['í¡ì—°', 'ë‹´ë°°', 'í”¼ìš°', 'í”¼ì›€'],
        'has_vehicle': ['ì°¨ëŸ‰', 'ì°¨', 'ìë™ì°¨', 'ë³´ìœ ì°¨ëŸ‰'],
        'alcohol_preference': ['ì£¼ë¥˜', 'ìŒì£¼', 'ìˆ ', 'ë§¥ì£¼', 'ì†Œì£¼', 'ì™€ì¸', 'ë§‰ê±¸ë¦¬'],
        'exercise_frequency': ['ìš´ë™', 'í—¬ìŠ¤', 'ì²´ìœ¡'],
        'pet_ownership': ['ë°˜ë ¤ë™ë¬¼', 'í«', 'ì• ì™„ë™ë¬¼', 'ê°•ì•„ì§€', 'ê³ ì–‘ì´'],
    }

    for condition_type, condition_value in behavioral_conditions.items():
        # ì´ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” í‚¤ì›Œë“œë“¤
        keywords = CONDITION_KEYWORDS.get(condition_type, [])
        if not keywords:
            continue

        # qa_pairsì—ì„œ ì´ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ì§ˆë¬¸ ì°¾ê¸°
        for qa in qa_pairs:
            q_text = qa.get('q_text', '').lower()
            answer = qa.get('answer', '')

            # í‚¤ì›Œë“œ ë§¤ì¹­
            if any(kw in q_text for kw in keywords):
                # Boolean ì¡°ê±´ (smoker, has_vehicle)
                if isinstance(condition_value, bool):
                    answer_lower = answer.lower()
                    is_positive = any(pos in answer_lower for pos in BEHAVIOR_YES_TOKENS)
                    is_negative = any(neg in answer_lower for neg in BEHAVIOR_NO_TOKENS)

                    # ì¡°ê±´ê°’ê³¼ ë‹µë³€ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                    if condition_value and is_positive:
                        matched.append({
                            'condition_type': condition_type,
                            'condition_value': condition_value,
                            'q_text': qa.get('q_text', ''),
                            'answer': answer,
                            'confidence': 1.0
                        })
                        break  # ì´ ì¡°ê±´ì— ëŒ€í•´ í•˜ë‚˜ë§Œ
                    elif not condition_value and is_negative:
                        matched.append({
                            'condition_type': condition_type,
                            'condition_value': condition_value,
                            'q_text': qa.get('q_text', ''),
                            'answer': answer,
                            'confidence': 1.0
                        })
                        break

                # String ì¡°ê±´ (alcohol_preference)
                else:
                    # ë‹µë³€ì— ì¡°ê±´ê°’ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ë§¤ì¹­
                    if str(condition_value).lower() in answer.lower():
                        matched.append({
                            'condition_type': condition_type,
                            'condition_value': condition_value,
                            'q_text': qa.get('q_text', ''),
                            'answer': answer,
                            'confidence': 0.8
                        })
                        break

    return matched


def reorder_with_matches(full_list: List[Dict[str, Any]], matched: List[Dict[str, Any]], limit: int) -> List[Dict[str, Any]]:
    if not isinstance(full_list, list):
        return []

    if not matched:
        return full_list[:limit]

    def _key(qa: Dict[str, Any]) -> tuple:
        return (
            qa.get("q_text") or qa.get("question") or "",
            str(qa.get("answer") or qa.get("answer_text") or qa.get("value") or "")
        )

    seen = set()
    ordered: List[Dict[str, Any]] = []

    for qa in matched:
        key = _key(qa)
        if key not in seen:
            ordered.append(qa)
            seen.add(key)

    for qa in full_list:
        key = _key(qa)
        if key not in seen:
            ordered.append(qa)
            seen.add(key)

    return ordered[:limit]


class SearchRequest(BaseModel):
    """ê²€ìƒ‰ ìš”ì²­"""
    query: str = Field(..., description="ê²€ìƒ‰ ì¿¼ë¦¬")
    index_name: str = Field(
        default="survey_responses_merged",
        description="ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ì´ë¦„ (ê¸°ë³¸ê°’: survey_responses_merged; ì™€ì¼ë“œì¹´ë“œ ì‚¬ìš© ê°€ëŠ¥)"
    )
    size: int = Field(default=10, ge=1, le=100, description="ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜")
    use_vector_search: bool = Field(default=True, description="ë²¡í„° ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€")
    page: int = Field(default=1, ge=1, description="ìš”ì²­í•  í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)")
    use_claude_analyzer: Optional[bool] = Field(
        default=None,
        description="Claude ë¶„ì„ê¸° ì‚¬ìš© ì—¬ë¶€ (Noneì´ë©´ ì„œë²„ ì„¤ì •ê°’ì„ ë”°ë¦„)"
    )
    session_id: Optional[str] = Field(
        default=None,
        description="ëŒ€í™”/ì„¸ì…˜ ì‹ë³„ì (Redis ëŒ€í™” ë¡œê·¸ í‚¤)"
    )
    user_id: Optional[str] = Field(
        default=None,
        description="ìš”ì²­ ì‚¬ìš©ì ì‹ë³„ì (ê²€ìƒ‰ ì´ë ¥ í‚¤)"
    )
    request_id: Optional[str] = Field(
        default=None,
        description="ìš”ì²­ ì¶”ì ì„ ìœ„í•œ ID"
    )
    metadata: Optional[Dict[str, Any]] = Field(
        default=None,
        description="ì¶”ê°€ ìš”ì²­ ë©”íƒ€ë°ì´í„°"
    )
    log_conversation: bool = Field(
        default=True,
        description="Redis ëŒ€í™” ë¡œê·¸ ì €ì¥ ì—¬ë¶€"
    )
    log_search_history: bool = Field(
        default=True,
        description="Redis ê²€ìƒ‰ ì´ë ¥ ì €ì¥ ì—¬ë¶€"
    )
    request_llm_summary: bool = Field(
        default=False,
        description="LLM ìš”ì•½/ë¶„ì„ ìƒì„± ìš”ì²­ ì—¬ë¶€"
    )
    llm_summary_instructions: Optional[str] = Field(
        default=None,
        description="LLM ìš”ì•½ ì‹œ ì‚¬ìš©í•  ì¶”ê°€ ì§€ì¹¨"
    )


class SearchResult(BaseModel):
    """ê²€ìƒ‰ ê²°ê³¼ í•­ëª©"""
    user_id: str
    score: float
    timestamp: Optional[str] = Field(default=None, description="ì¸ë±ì‹± ì‹œê°„")
    survey_datetime: Optional[str] = Field(default=None, description="ì„¤ë¬¸ì¡°ì‚¬ ì¼ì‹œ (metadata.survey_datetime)")
    demographic_info: Optional[Dict[str, Any]] = Field(default=None, description="ì¸êµ¬í†µê³„ ì •ë³´ (survey_responses_mergedì—ì„œ ì¡°íšŒ)")
    behavioral_info: Optional[Dict[str, Any]] = Field(default=None, description="í–‰ë™/ìŠµê´€ ì •ë³´ (ì˜ˆ: í¡ì—° ì—¬ë¶€, ì°¨ëŸ‰ ë³´ìœ  ì—¬ë¶€)")
    qa_pairs: Optional[List[Dict[str, Any]]] = None
    matched_qa_pairs: Optional[List[Dict[str, Any]]] = None
    highlights: Optional[Dict[str, Any]] = None


class SearchResponse(BaseModel):
    """ê²€ìƒ‰ ì‘ë‹µ"""
    requested_count: Optional[int] = Field(
        default=None,
        description="ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œëœ ìš”ì²­ ì¸ì› ìˆ˜ (ì˜ˆ: 'ì§ì¥ì¸ 5ëª…' â†’ 5, ì¸ì› ì œí•œ ì—†ìœ¼ë©´ None)"
    )
    query: str
    total_hits: int
    max_score: Optional[float]
    results: List[SearchResult]
    session_id: Optional[str] = Field(
        default=None,
        description="ìš”ì²­ì— ì‚¬ìš©ëœ ì„¸ì…˜ ID (ìë™ ìƒì„±/ì „ë‹¬ëœ ê°’)",
    )
    query_analysis: Optional[Dict[str, Any]] = None
    took_ms: int
    page: int = Field(default=1, description="í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸")
    page_size: int = Field(default=10, description="í˜ì´ì§€ ë‹¹ ê²°ê³¼ ìˆ˜")
    has_more: bool = Field(default=False, description="ì¶”ê°€ í˜ì´ì§€ ì¡´ì¬ ì—¬ë¶€")
    llm_summary: Optional[Dict[str, Any]] = Field(
        default=None,
        description="LLM ê¸°ë°˜ ë°ì´í„° ìš”ì•½/ë¶„ì„ ê²°ê³¼"
    )


# ===== ê°„ì†Œí™”ëœ ì‘ë‹µ ëª¨ë¸ (í”„ë¡ íŠ¸ì—”ë“œ ì¹œí™”ì ) =====

class MatchedCondition(BaseModel):
    """Behavioral ì¡°ê±´ ë§¤ì¹­ ì •ë³´"""
    condition_type: str = Field(..., description="ì¡°ê±´ íƒ€ì… (smoker, has_vehicle, alcohol_preference ë“±)")
    condition_value: Any = Field(..., description="ì¡°ê±´ ê°’ (True, False, 'ë§¥ì£¼' ë“±)")
    question: str = Field(..., description="ì‹¤ì œ ì§ˆë¬¸ í…ìŠ¤íŠ¸")
    answer: str = Field(..., description="ì‹¤ì œ ë‹µë³€ í…ìŠ¤íŠ¸")
    confidence: float = Field(default=1.0, description="ë§¤ì¹­ ì‹ ë¢°ë„ (0.0~1.0)")


class SimpleResult(BaseModel):
    """ê°„ì†Œí™”ëœ ê²€ìƒ‰ ê²°ê³¼ (í”„ë¡ íŠ¸ì—”ë“œìš©)"""
    user_id: str = Field(..., description="ì‚¬ìš©ì ID")
    score: float = Field(..., description="ê²€ìƒ‰ ì ìˆ˜")
    demographics: Dict[str, str] = Field(..., description="ì¸êµ¬í†µê³„ ì •ë³´ (gender, age_group, birth_year)")
    matched_conditions: List[MatchedCondition] = Field(
        default_factory=list,
        description="ë§¤ì¹­ëœ behavioral ì¡°ê±´ë“¤"
    )


class SimpleResponse(BaseModel):
    """í”„ë¡ íŠ¸ì—”ë“œ ì¹œí™”ì  ê°„ì†Œí™” ì‘ë‹µ"""
    state: Literal["SUCCESS", "ERROR"] = Field(..., description="ì‘ë‹µ ìƒíƒœ")
    message: str = Field(..., description="ì‘ë‹µ ë©”ì‹œì§€")
    query: str = Field(..., description="ê²€ìƒ‰ ì¿¼ë¦¬")
    total_hits: int = Field(..., description="ì´ ê²°ê³¼ ìˆ˜")
    results: List[SimpleResult] = Field(..., description="ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡")
    query_info: Optional[Dict[str, Any]] = Field(
        default=None,
        description="ì¿¼ë¦¬ ë¶„ì„ ì •ë³´ (keywords, filters_applied, behavioral_conditions)"
    )
    took_ms: int = Field(..., description="ê²€ìƒ‰ ì†Œìš” ì‹œê°„ (ë°€ë¦¬ì´ˆ)")


# ===== ì´ˆê²½ëŸ‰í™” ì‘ë‹µ ëª¨ë¸ (ë¬´í•œ ìŠ¤í¬ë¡¤ìš©) =====

class LightResult(BaseModel):
    """ì´ˆê²½ëŸ‰ ê²€ìƒ‰ ê²°ê³¼ (demographicsë§Œ, qa_pairs ì œì™¸)"""
    user_id: str = Field(..., description="ì‚¬ìš©ì ID")
    score: float = Field(..., description="ê²€ìƒ‰ ì ìˆ˜")
    timestamp: str = Field(..., description="ì‘ë‹µ íƒ€ì„ìŠ¤íƒ¬í”„")
    survey_datetime: Optional[str] = Field(None, description="ì„¤ë¬¸ ì‘ë‹µ ì‹œê°„")
    demographic_info: Dict[str, Optional[str]] = Field(
        ...,
        description="ì¸êµ¬í†µê³„ ì •ë³´ (age_group, gender, birth_year, region, sub_region, occupation, marital_status, panel)"
    )


class SearchResponseLight(BaseModel):
    """ì´ˆê²½ëŸ‰ ê²€ìƒ‰ ì‘ë‹µ (ë¬´í•œ ìŠ¤í¬ë¡¤ í˜ì´ì§€ë„¤ì´ì…˜)"""
    query: str = Field(..., description="ê²€ìƒ‰ ì¿¼ë¦¬")
    total_hits: int = Field(..., description="ì „ì²´ ê²°ê³¼ ìˆ˜ (í•„í„°ë§ í›„)")
    results: List[LightResult] = Field(..., description="í˜„ì¬ í˜ì´ì§€ ê²°ê³¼")
    page: int = Field(..., description="í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)")
    page_size: int = Field(..., description="í˜ì´ì§€ ë‹¹ ê²°ê³¼ ìˆ˜")
    has_more: bool = Field(..., description="ë‹¤ìŒ í˜ì´ì§€ ì¡´ì¬ ì—¬ë¶€")
    took_ms: int = Field(..., description="ê²€ìƒ‰ ì†Œìš” ì‹œê°„ (ë°€ë¦¬ì´ˆ)")
    cache_hit: bool = Field(default=False, description="ìºì‹œ íˆíŠ¸ ì—¬ë¶€")
    cache_type: Optional[str] = Field(None, description="ìºì‹œ íƒ€ì… (memory, redis, none)")


BEHAVIOR_YES_TOKENS = {
    "ìˆë‹¤", "ìˆìŒ", "ìˆì–´ìš”", "yes", "y", "ë³´ìœ ", "ë³´ìœ í•¨", "ë³´ìœ ì¤‘", "í•œë‹¤", "í•©ë‹ˆë‹¤", "í•´ìš”"
}
BEHAVIOR_NO_TOKENS = {
    "ì—†ë‹¤", "ì—†ìŒ", "ì—†ì–´ìš”", "no", "n", "ë¯¸ë³´ìœ ", "ì•ˆí•¨", "ì•ˆí•´ìš”", "í•˜ì§€ì•ŠëŠ”ë‹¤", "í•˜ì§€ ì•ŠëŠ”ë‹¤", "ì•ŠìŒ", "ì•ˆí•©ë‹ˆë‹¤"
}
SMOKER_NEGATIVE_KEYWORDS = {
    "í”¼ì›Œë³¸ ì ì´ ì—†ë‹¤", "í”¼ì›Œë³¸ì ì´ ì—†ë‹¤", "í”¼ì›Œë³¸ì  ì—†ë‹¤", "í”¼ìš°ì§€ ì•ŠëŠ”ë‹¤",
    "í¡ì—°í•˜ì§€ ì•ŠëŠ”ë‹¤", "ë¹„í¡ì—°", "ê¸ˆì—°", "ë‹´ë°°ë¥¼ í”¼ìš°ì§€ ì•ŠëŠ”ë‹¤", "ë‹´ë°°ë¥¼ í”¼ì›Œë³¸ì ì´ ì—†ë‹¤",
    "ë‹´ë°° ì•ˆ í”¼", "ë‹´ë°°ì•ˆí”¼", "í¡ì—° ì•ˆ í•¨", "í¡ì—° ì•ˆí•¨", "ë‹´ë°°ë¥¼ í”¼ìš°ì§€ ì•ŠìŒ", "í”¼ìš°ì§€ ì•ŠìŒ"
}
SMOKER_POSITIVE_KEYWORDS = {
    "í¡ì—°", "ë‹´ë°° í”¼", "ë‹´ë°°í”¼", "ë‹´ë°°ë¥¼ í”¼", "í¡ì—°ì¤‘", "í¡ì—°í•¨", "smoker",
    "í”¼ìš´ë‹¤", "í”¼ì›ë‹ˆë‹¤", "í”¼ì›€", "ì¼ë°˜ ë‹´ë°°", "ì¼ë°˜ë‹´ë°°", "ì „ì ë‹´ë°°",
    "ì „ìë‹´ë°°", "ê¶ë ¨í˜• ì „ìë‹´ë°°", "ê¶ë ¨í˜•ì „ìë‹´ë°°", "ê¶Œë ¨í˜• ì „ìë‹´ë°°",
    "ê¶Œë ¨í˜•ì „ìë‹´ë°°", "ì—°ì´ˆ", "ì‹œê°€í˜• ì „ìë‹´ë°°", "ë‹´ë°°", "ë‹´ë°°ë¥¼ í”¼ì›€",
    "í¡ì—° ê²½í—˜ ìˆìŒ", "í¡ì—°ê²½í—˜ ìˆìŒ"
}
SMOKER_QUESTION_KEYWORDS = {
    "í¡ì—°", "ë‹´ë°°", "í¡ì—°ê²½í—˜", "í¡ì—° ê²½í—˜", "í¡ì—°ê²½í—˜ ë‹´ë°°ë¸Œëœë“œ",
    "ê¶ë ¨í˜• ì „ìë‹´ë°°", "ê¶ë ¨í˜• ì „ìë‹´ë°°/ê°€ì—´ì‹ ì „ìë‹´ë°° ì´ìš©ê²½í—˜",
    "ê°€ì—´ì‹ ì „ìë‹´ë°°", "ì „ìë‹´ë°°"
}
VEHICLE_QUESTION_KEYWORDS = {
    "ë³´ìœ ì°¨ëŸ‰ì—¬ë¶€", "ë³´ìœ ì°¨ëŸ‰", "ì°¨ëŸ‰ì—¬ë¶€", "ì°¨ëŸ‰ ì—¬ë¶€", "ìë™ì°¨", "ì°¨ëŸ‰", "ì°¨ ë³´ìœ ", "ì°¨ëŸ‰ë³´ìœ "  # âœ… "ë³´ìœ ì°¨ëŸ‰ì—¬ë¶€" ì¶”ê°€
}

# ìŒì£¼ ê´€ë ¨ í‚¤ì›Œë“œ
ALCOHOL_QUESTION_KEYWORDS = {
    "ìŒìš©ê²½í—˜ ìˆ ", "ìŒìš©ê²½í—˜", "ìˆ ", "ìŒì£¼", "ìŒì£¼ê²½í—˜", "ì•Œì½œ", "ì•Œì½”ì˜¬"
}
BEER_KEYWORDS = {
    "ë§¥ì£¼", "beer"
}
WINE_KEYWORDS = {
    "ì™€ì¸", "wine"
}
SOJU_KEYWORDS = {
    "ì†Œì£¼", "soju"
}
NON_DRINKER_KEYWORDS = {
    "ìˆ ì„ ë§ˆì‹œì§€ ì•ŠìŒ", "ìˆ  ë§ˆì‹œì§€ ì•ŠìŒ", "ìˆ  ì•ˆ ë§ˆì‹¬", "ìˆ  ì•ˆë§ˆì‹¬", "ìˆ  ëª»ë§ˆì‹¬", "ìˆ  ëª» ë§ˆì‹¬",
    "ë¹„ìŒì£¼", "ê¸ˆì£¼", "ìµœê·¼ 1ë…„ ì´ë‚´ ìˆ ì„ ë§ˆì‹œì§€ ì•ŠìŒ", "ìŒì£¼ ê²½í—˜ ì—†ìŒ", "ìŒì£¼ê²½í—˜ ì—†ìŒ"
}
DRINKER_POSITIVE_KEYWORDS = {
    # ìˆ  ì¢…ë¥˜
    "ë§¥ì£¼", "beer", "ì†Œì£¼", "soju", "ë§‰ê±¸ë¦¬", "íƒì£¼", "ì™€ì¸", "wine",
    "ì–‘ì£¼", "ìœ„ìŠ¤í‚¤", "whiskey", "ë³´ë“œì¹´", "vodka", "ë°í‚¬ë¼", "tequila", "ì§„", "gin",
    "ì €ë„ì£¼", "ì²­ì£¼", "ë§¤ì‹¤ì£¼", "ë³µë¶„ìì£¼", "ê³¼ì¼ì¹µí…Œì¼ì£¼", "KGB", "í›„ì¹˜", "í¬ë£¨ì €",
    "ì¼ë³¸ì²­ì£¼", "ì‚¬ì¼€", "sake", "ì¹µí…Œì¼", "cocktail",
    # ìŒì£¼ ê¸ì • í‘œí˜„
    "ìˆ  ë§ˆì‹¬", "ìˆ  ë§ˆì…”", "ìˆ ë§ˆì‹¬", "ìˆ ë§ˆì…”", "ìŒì£¼í•¨", "ìŒì£¼ ê²½í—˜ ìˆìŒ", "ìŒì£¼ê²½í—˜ ìˆìŒ",
    "ê°€ë” ë§ˆì‹¬", "ìì£¼ ë§ˆì‹¬", "ì£¼ë§ì— ë§ˆì‹¬"
}

# ============================================================================
# â­ ì„¤ë¬¸ ì§ˆë¬¸ ê¸°ë°˜ Behavioral í‚¤ì›Œë“œ ì •ì˜ (ì‹¤ì œ ì„¤ë¬¸ ë°ì´í„° ê¸°ë°˜)
# ============================================================================

# 1. OTT ì„œë¹„ìŠ¤ ì´ìš©
OTT_QUESTION_KEYWORDS = {
    "OTT", "ott", "OTT ì„œë¹„ìŠ¤", "ì´ìš© ì¤‘ì¸ OTT", "í˜„ì¬ ì´ìš© ì¤‘ì¸ OTT",
    "ë™ì˜ìƒ ìŠ¤íŠ¸ë¦¬ë° ì•±", "ë™ì˜ìƒ ìŠ¤íŠ¸ë¦¬ë°", "ì˜ìƒ ìŠ¤íŠ¸ë¦¬ë°", "ìŠ¤íŠ¸ë¦¬ë° ì•±",
    "ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì•±"
}
OTT_POSITIVE_KEYWORDS = {
    "1ê°œ", "2ê°œ", "3ê°œ", "4ê°œ", "4ê°œ ì´ìƒ",
    "ë™ì˜ìƒ ìŠ¤íŠ¸ë¦¬ë° ì•±", "ë™ì˜ìƒ ìŠ¤íŠ¸ë¦¬ë°",
    "ë„·í”Œë¦­ìŠ¤", "ë””ì¦ˆë‹ˆ", "ì¿ íŒ¡í”Œë ˆì´", "ì›¨ì´ë¸Œ", "í‹°ë¹™", "ì™“ì± ", "ìœ íŠœë¸Œ"
}
OTT_NEGATIVE_KEYWORDS = {
    "ì´ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤", "ì´ìš©í•˜ì§€ì•ŠëŠ”ë‹¤", "ì´ìš© ì•ˆí•¨", "ì´ìš©ì•ˆí•¨"
}

# 2. ë°˜ë ¤ë™ë¬¼ ë³´ìœ 
PET_QUESTION_KEYWORDS = {
    "ë°˜ë ¤ë™ë¬¼", "ë°˜ë ¤ê²¬", "ë°˜ë ¤ë¬˜", "ì• ì™„ë™ë¬¼", "í«", "pet"
}
PET_POSITIVE_KEYWORDS = {
    "ë°˜ë ¤ë™ë¬¼ì„ í‚¤ìš°ëŠ” ì¤‘ì´ë‹¤", "ë°˜ë ¤ë™ë¬¼ì„ í‚¤ì›Œë³¸ ì ì´ ìˆë‹¤",
    "í‚¤ìš°ëŠ” ì¤‘", "í‚¤ì›Œë³¸ ì ", "í‚¤ìš°ê³  ìˆ", "í‚¤ì› "
}
PET_NEGATIVE_KEYWORDS = {
    "ë°˜ë ¤ë™ë¬¼ì„ í‚¤ì›Œë³¸ ì ì´ ì—†ë‹¤", "í‚¤ì›Œë³¸ ì ì´ ì—†", "í‚¤ìš´ ì  ì—†"
}

# 3. AI ì„œë¹„ìŠ¤ ì´ìš©
AI_QUESTION_KEYWORDS = {
    "AI", "ai", "ì¸ê³µì§€ëŠ¥", "AI ì„œë¹„ìŠ¤", "AI ì±—ë´‡", "ì±—ë´‡"
}
AI_POSITIVE_KEYWORDS = {
    "ê²€ìƒ‰", "ì •ë³´ íƒìƒ‰", "ë²ˆì—­", "ì™¸êµ­ì–´ í•™ìŠµ", "ì—…ë¬´ ë³´ì¡°", "ë¬¸ì„œ ì‘ì„±",
    "ì´ë¯¸ì§€ ìƒì„±", "ë””ìì¸", "í•™ìŠµ", "ê³µë¶€", "ì½˜í…ì¸  ì œì‘",
    "ChatGPT", "Gemini", "Copilot", "HyperCLOVER", "Claude", "ë”¥ì‹œí¬"
}
AI_NEGATIVE_KEYWORDS = {
    "AI ì„œë² ìŠ¤ë¥¼ ì‚¬ìš©í•´ë³¸ ì  ì—†ë‹¤", "ì‚¬ìš©í•´ ë³¸ ì  ì—†ìŒ",
    "ì‚¬ìš©í•´ë³¸ ì  ì—†", "ì‚¬ìš© ì•ˆí•´", "ì‚¬ìš©í•˜ì§€ ì•Š"
}

# 4. ìš´ë™/ì²´ë ¥ê´€ë¦¬
EXERCISE_QUESTION_KEYWORDS = {
    "ì²´ë ¥ ê´€ë¦¬", "ì²´ë ¥ê´€ë¦¬", "ìš´ë™", "í™œë™", "í”¼íŠ¸ë‹ˆìŠ¤", "í—¬ìŠ¤"
}
EXERCISE_POSITIVE_KEYWORDS = {
    "ë‹¬ë¦¬ê¸°", "ê±·ê¸°", "í™ˆíŠ¸ë ˆì´ë‹", "ë“±ì‚°", "í—¬ìŠ¤", "ìì „ê±°",
    "ìš”ê°€", "í•„ë¼í…ŒìŠ¤", "ìŠ¤í¬ì¸ ", "ì¶•êµ¬", "ë°°ë“œë¯¼í„´", "ìˆ˜ì˜"
}
EXERCISE_NEGATIVE_KEYWORDS = {
    "ì²´ë ¥ê´€ë¦¬ë¥¼ ìœ„í•´ í•˜ê³  ìˆëŠ” í™œë™ì´ ì—†ë‹¤", "í™œë™ì´ ì—†", "í•˜ì§€ ì•Š"
}

# 5. ë¹ ë¥¸ ë°°ì†¡ ì´ìš©
FAST_DELIVERY_QUESTION_KEYWORDS = {
    "ë¹ ë¥¸ ë°°ì†¡", "ë‹¹ì¼ ë°°ì†¡", "ìƒˆë²½ ë°°ì†¡", "ì§ì§„ ë°°ì†¡", "ë¡œì¼“ë°°ì†¡"
}
FAST_DELIVERY_POSITIVE_KEYWORDS = {
    "ì‹ ì„ ì‹í’ˆ", "ê³¼ì¼", "ì±„ì†Œ", "ìœ¡ë¥˜", "ìƒí™œìš©í’ˆ", "ìƒí•„í’ˆ",
    "ìœ„ìƒìš©í’ˆ", "íŒ¨ì…˜", "ë·°í‹°", "ì „ìê¸°ê¸°", "ê°€ì „ì œí’ˆ"
}
FAST_DELIVERY_NEGATIVE_KEYWORDS = {
    "ë¹ ë¥¸ ë°°ì†¡ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•´ ë³¸ ì  ì—†ë‹¤", "ì´ìš©í•´ ë³¸ ì  ì—†", "ì´ìš© ì•ˆí•´"
}

# 6. ì „í†µì‹œì¥ ë°©ë¬¸
TRADITIONAL_MARKET_QUESTION_KEYWORDS = {
    "ì „í†µì‹œì¥", "ì¬ë˜ì‹œì¥", "ì‹œì¥ ë°©ë¬¸"
}
TRADITIONAL_MARKET_POSITIVE_KEYWORDS = {
    "ì¼ì£¼ì¼ì—", "í•œë‹¬ì—", "2ì£¼ì—", "3ê°œì›”ì—", "6ê°œì›”ì—", "1ë…„ì—", "íšŒ ì´ìƒ"
}
TRADITIONAL_MARKET_NEGATIVE_KEYWORDS = {
    "ì „í˜€ ë°©ë¬¸í•˜ì§€ ì•ŠìŒ", "ë°©ë¬¸í•˜ì§€ ì•Š", "ì•ˆ ê°€"
}

# 7. ìŠ¤íŠ¸ë ˆìŠ¤ ìš”ì¸
STRESS_QUESTION_KEYWORDS = {
    "ìŠ¤íŠ¸ë ˆìŠ¤", "ìŠ¤íŠ¸ë ˆìŠ¤ ìš”ì¸", "ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ëŠ”", "ê³ ë¯¼", "ê±±ì •"
}
STRESS_POSITIVE_KEYWORDS = {
    "ì§ì¥", "ì—…ë¬´", "í•™ì—…", "ì„±ì ", "ì·¨ì—…", "ì§„ë¡œ", "ê²½ì œì ", "ì¬ì •ì ", "ê¸ˆì „ì ",
    "ì™¸ëª¨", "ê±´ê°•", "ì§ˆë³‘", "ì¸ê°„ê´€ê³„", "ê°€ì¡±", "ë¶€ëª¨", "ìë…€", "ì—°ì• ", "ê²°í˜¼"
}
STRESS_NEGATIVE_KEYWORDS = {
    "ìŠ¤íŠ¸ë ˆìŠ¤ ì—†ìŒ", "ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ì§€ ì•ŠìŒ", "í•´ë‹¹ ì—†ìŒ"
}

# 8. ì—¬í–‰ ì˜í–¥ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì€ ì˜¬í•´ í•´ì™¸ì—¬í–‰ì„ ê°„ë‹¤ë©´ ì–´ë””ë¡œ ê°€ê³  ì‹¶ë‚˜ìš”?")
TRAVEL_QUESTION_KEYWORDS = {
    "í•´ì™¸ì—¬í–‰", "ì—¬í–‰", "ì–´ë””ë¡œ ê°€ê³  ì‹¶", "ê°€ê³  ì‹¶ë‚˜ìš”"
}
TRAVEL_POSITIVE_KEYWORDS = {
    "ìœ ëŸ½", "ë™ë‚¨ì•„", "ì¼ë³¸", "ì¤‘êµ­", "ë¯¸êµ­", "ìºë‚˜ë‹¤", "ì¼ë³¸/ì¤‘êµ­", "ë¯¸êµ­/ìºë‚˜ë‹¤"
}
TRAVEL_NEGATIVE_KEYWORDS = {
    "í•´ì™¸ì—¬í–‰ì„ ê°€ê³ ì‹¶ì§€ ì•Šë‹¤", "ê°€ê³ ì‹¶ì§€ ì•Šë‹¤", "ê°€ê³  ì‹¶ì§€ ì•Š"
}



# 10. ì»¤í”¼ ì´ìš© (ì‹¤ì œ ì§ˆë¬¸: "ë³´ìœ ê°€ì „ì œí’ˆ")
COFFEE_QUESTION_KEYWORDS = {
    "ë³´ìœ ê°€ì „ì œí’ˆ", "ê°€ì „ì œí’ˆ", "ë³´ìœ ", "ì†Œìœ "
}
COFFEE_POSITIVE_KEYWORDS = {
    "ì»¤í”¼ ë¨¸ì‹ ", "ì»¤í”¼ë¨¸ì‹ ", "ì—ìŠ¤í”„ë ˆì†Œ ë¨¸ì‹ ", "ìº¡ìŠì»¤í”¼ ë¨¸ì‹ ",
    "ìº¡ìŠì»¤í”¼", "ë„¤ìŠ¤í”„ë ˆì†Œ", "ëŒì²´êµ¬ìŠ¤í† "
}
COFFEE_NEGATIVE_KEYWORDS = set()  # ë¹ˆ set: negative í‚¤ì›Œë“œ ì—†ìŒ

# 11. êµ¬ë… ì„œë¹„ìŠ¤ ì´ìš© (ì‹¤ì œ ì§ˆë¬¸: "í• ì¸, ìºì‹œë°±, ë©¤ë²„ì‹­ ë“± í¬ì¸íŠ¸ ì ë¦½ í˜œíƒ")
SUBSCRIPTION_QUESTION_KEYWORDS = {
    "í• ì¸", "ìºì‹œë°±", "ë©¤ë²„ì‹­", "í¬ì¸íŠ¸", "ì ë¦½", "í˜œíƒ", "ì‹ ê²½ ì“°ì‹œë‚˜ìš”"
}
SUBSCRIPTION_POSITIVE_KEYWORDS = {
    "ìì£¼ ì“°ëŠ” ê³³ë§Œ ì±™ê¸´ë‹¤", "ë§¤ìš° ê¼¼ê¼¼í•˜ê²Œ ì±™ê¸´ë‹¤", "ê°€ë” ìƒê°ë‚  ë•Œë§Œ ì±™ê¸´ë‹¤",
    "ì±™ê¸´ë‹¤", "ê¼¼ê¼¼í•˜ê²Œ"
}
SUBSCRIPTION_NEGATIVE_KEYWORDS = {
    "ê±°ì˜ ì‹ ê²½ì“°ì§€ ì•ŠëŠ”ë‹¤", "ì „í˜€ ê´€ì‹¬ ì—†ë‹¤", "ì‹ ê²½ì“°ì§€ ì•ŠëŠ”ë‹¤", "ê´€ì‹¬ ì—†ë‹¤"
}

# 12. ì†Œì…œë¯¸ë””ì–´ ì´ìš© (ì‹¤ì œ ì§ˆë¬¸: "ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì•±ì€ ë¬´ì—‡ì¸ê°€ìš”?")
SOCIAL_MEDIA_QUESTION_KEYWORDS = {
    "ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì•±", "ë§ì´ ì‚¬ìš©í•˜ëŠ” ì•±", "ìš”ì¦˜ ê°€ì¥",
    "ì†Œì…œë¯¸ë””ì–´", "SNS", "ì†Œì…œ ë„¤íŠ¸ì›Œí¬"
}
SOCIAL_MEDIA_POSITIVE_KEYWORDS = {
    "SNS ì•±", "SNS ì•± (ì¸ìŠ¤íƒ€ê·¸ë¨, í˜ì´ìŠ¤ë¶, í‹±í†¡ ë“±)",
    "ì¸ìŠ¤íƒ€ê·¸ë¨", "í˜ì´ìŠ¤ë¶", "íŠ¸ìœ„í„°", "í‹±í†¡",
    "ì¹´ì¹´ì˜¤ìŠ¤í† ë¦¬", "ë„¤ì´ë²„ ë°´ë“œ"
}
SOCIAL_MEDIA_NEGATIVE_KEYWORDS = {
    "SNSë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ", "ì†Œì…œë¯¸ë””ì–´ ì•ˆí•¨", "í•´ë‹¹ ì—†ìŒ"
}

# 13. ê²Œì„ ì´ìš© (ì‹¤ì œ ì§ˆë¬¸: "ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì•±ì€ ë¬´ì—‡ì¸ê°€ìš”?")
GAMING_QUESTION_KEYWORDS = {
    "ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì•±", "ë§ì´ ì‚¬ìš©í•˜ëŠ” ì•±", "ìš”ì¦˜ ê°€ì¥",
    "ê²Œì„", "ê²Œì´ë°", "ëª¨ë°”ì¼ ê²Œì„"
}
GAMING_POSITIVE_KEYWORDS = {
    "ê²Œì„ ì•±", "ê²Œì„ì•±",
    "ë¡¤", "ë¦¬ê·¸ì˜¤ë¸Œë ˆì „ë“œ", "ë°°í‹€ê·¸ë¼ìš´ë“œ", "ë¡œìŠ¤íŠ¸ì•„í¬", "ë©”ì´í”Œ",
    "ëª¨ë°”ì¼ê²Œì„", "PCê²Œì„", "ì½˜ì†”ê²Œì„"
}
GAMING_NEGATIVE_KEYWORDS = {
    "ê²Œì„ì„ í•˜ì§€ ì•ŠìŒ", "ê²Œì„ ì•ˆí•¨", "í•´ë‹¹ ì—†ìŒ"
}

# 14. ë…ì„œ ìŠµê´€
READING_QUESTION_KEYWORDS = {
    "ë…ì„œ", "ì±…", "ë„ì„œ", "ì½ê¸°", "ë…ì„œ ìŠµê´€"
}
READING_POSITIVE_KEYWORDS = {
    "ì†Œì„¤", "ì—ì„¸ì´", "ìê¸°ê³„ë°œ", "ê²½ì œê²½ì˜", "ì¸ë¬¸", "ê³¼í•™",
    "í•œë‹¬ì—", "ì¼ì£¼ì¼ì—", "ê¶Œ", "ìì£¼"
}
READING_NEGATIVE_KEYWORDS = {
    "ì±…ì„ ì½ì§€ ì•ŠìŒ", "ë…ì„œ ì•ˆí•¨", "ê±°ì˜ ì•ˆ ì½ìŒ"
}

# 15. ì˜í™”/ë“œë¼ë§ˆ ì‹œì²­ (ì‹¤ì œ ì§ˆë¬¸: "ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì•±")
MOVIE_DRAMA_QUESTION_KEYWORDS = {
    "ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì•±", "ë§ì´ ì‚¬ìš©í•˜ëŠ” ì•±", "ìš”ì¦˜ ê°€ì¥"
}
MOVIE_DRAMA_POSITIVE_KEYWORDS = {
    "ë™ì˜ìƒ ìŠ¤íŠ¸ë¦¬ë° ì•±", "ë™ì˜ìƒ ìŠ¤íŠ¸ë¦¬ë°",
    "ìœ íŠœë¸Œ", "ë„·í”Œë¦­ìŠ¤", "Youtube", "Netflix"
}
MOVIE_DRAMA_NEGATIVE_KEYWORDS = {
    "ë™ì˜ìƒì„ ë³´ì§€ ì•ŠìŒ", "ìŠ¤íŠ¸ë¦¬ë° ì•ˆí•¨", "ê±°ì˜ ì•ˆ ë´„"
}

# 16. ìŒì•… ìŠ¤íŠ¸ë¦¬ë°
MUSIC_STREAMING_QUESTION_KEYWORDS = {
    "ìŒì•…", "ìŠ¤íŠ¸ë¦¬ë°", "ìŒì›", "ìŒì•… ê°ìƒ"
}
MUSIC_STREAMING_POSITIVE_KEYWORDS = {
    "ë©œë¡ ", "ì§€ë‹ˆ", "ë²…ìŠ¤", "í”Œë¡œ", "ìœ íŠœë¸Œë®¤ì§", "ìŠ¤í¬í‹°íŒŒì´",
    "ë°œë¼ë“œ", "ëŒ„ìŠ¤", "í™í•©", "R&B", "ë¡", "ì¸ë””", "í•˜ë£¨ì—", "ìì£¼"
}
MUSIC_STREAMING_NEGATIVE_KEYWORDS = {
    "ìŒì•…ì„ ë“£ì§€ ì•ŠìŒ", "ìŠ¤íŠ¸ë¦¬ë° ì•ˆí•¨", "í•´ë‹¹ ì—†ìŒ"
}

# 17. ì˜¨ë¼ì¸ êµìœ¡
ONLINE_EDUCATION_QUESTION_KEYWORDS = {
    "ì˜¨ë¼ì¸ êµìœ¡", "ì¸ê°•", "ì˜¨ë¼ì¸ ê°•ì˜", "ì´ëŸ¬ë‹", "ì˜¨ë¼ì¸ í•™ìŠµ"
}
ONLINE_EDUCATION_POSITIVE_KEYWORDS = {
    "ì–´í•™", "ìê²©ì¦", "ì·¨ì—…", "í”„ë¡œê·¸ë˜ë°", "ë””ìì¸", "ë§ˆì¼€íŒ…",
    "ìœ ë°ë¯¸", "í´ë˜ìŠ¤101", "ì¸í”„ëŸ°", "íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤"
}
ONLINE_EDUCATION_NEGATIVE_KEYWORDS = {
    "ì˜¨ë¼ì¸ êµìœ¡ì„ ë°›ì§€ ì•ŠìŒ", "ì¸ê°• ì•ˆ ë“¤ìŒ", "í•´ë‹¹ ì—†ìŒ"
}

# 18. ê¸ˆìœµ ì„œë¹„ìŠ¤ (ì‹¤ì œ ì§ˆë¬¸: "ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì•±")
FINANCIAL_SERVICE_QUESTION_KEYWORDS = {
    "ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì•±", "ë§ì´ ì‚¬ìš©í•˜ëŠ” ì•±", "ìš”ì¦˜ ê°€ì¥"
}
FINANCIAL_SERVICE_POSITIVE_KEYWORDS = {
    "ê¸ˆìœµ ì•±", "ê¸ˆìœµì•±", "ì€í–‰ ì•±", "ì€í–‰ì•±",
    "í† ìŠ¤", "ì¹´ì¹´ì˜¤ë±…í¬", "ì¼€ì´ë±…í¬", "ë±…í‚¹"
}
FINANCIAL_SERVICE_NEGATIVE_KEYWORDS = {
    "ê¸ˆìœµ ì•± ì‚¬ìš©í•˜ì§€ ì•ŠìŒ", "ê¸ˆìœµ ì„œë¹„ìŠ¤ ë¯¸ì‚¬ìš©", "í•´ë‹¹ ì—†ìŒ"
}

# 19. ê±´ê°•ê²€ì§„
HEALTH_CHECKUP_QUESTION_KEYWORDS = {
    "ê±´ê°•ê²€ì§„", "ê²€ì§„", "ê±´ê°•ê²€ì‚¬", "ì •ê¸°ê²€ì§„"
}
HEALTH_CHECKUP_POSITIVE_KEYWORDS = {
    "1ë…„ì—", "2ë…„ì—", "ì •ê¸°ì ", "ë§¤ë…„", "ë°›ìŒ", "ë°›ì€ ì "
}
HEALTH_CHECKUP_NEGATIVE_KEYWORDS = {
    "ê±´ê°•ê²€ì§„ì„ ë°›ì§€ ì•ŠìŒ", "ê²€ì§„ ì•ˆí•¨", "ë°›ì€ ì  ì—†ìŒ"
}

# 20. ë·°í‹°/í™”ì¥í’ˆ (ì‹¤ì œ ì§ˆë¬¸: "í•œ ë‹¬ ê¸°ì¤€ìœ¼ë¡œ ìŠ¤í‚¨ì¼€ì–´ ì œí’ˆì— í‰ê· ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì†Œë¹„í•˜ì‹œë‚˜ìš”?")
BEAUTY_QUESTION_KEYWORDS = {
    "ìŠ¤í‚¨ì¼€ì–´", "ìŠ¤í‚¨ì¼€ì–´ ì œí’ˆ", "í™”ì¥í’ˆ", "ë·°í‹°", "ì†Œë¹„í•˜ì‹œë‚˜ìš”", "ì–¼ë§ˆë‚˜"
}
BEAUTY_POSITIVE_KEYWORDS = {
    "3ë§Œì› ë¯¸ë§Œ", "3ë§Œì› ì´ìƒ", "5ë§Œì› ì´ìƒ", "10ë§Œì› ì´ìƒ", "15ë§Œì› ì´ìƒ",
    "ë§Œì›", "ë¯¸ë§Œ", "ì´ìƒ"
}
BEAUTY_NEGATIVE_KEYWORDS = {
    "0ì›", "ì†Œë¹„í•˜ì§€ ì•Š", "ì‚¬ìš©í•˜ì§€ ì•ŠìŒ", "í™”ì¥í’ˆì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ"
}

# 21. íŒ¨ì…˜ ì‡¼í•‘ (ì‹¤ì œ ì§ˆë¬¸: "ë³¸ì¸ì„ ìœ„í•´ ì†Œë¹„í•˜ëŠ” ê²ƒ ì¤‘ ê°€ì¥ ê¸°ë¶„ ì¢‹ì•„ì§€ëŠ” ì†Œë¹„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?")
FASHION_QUESTION_KEYWORDS = {
    "ë³¸ì¸ì„ ìœ„í•´ ì†Œë¹„", "ê¸°ë¶„ ì¢‹ì•„ì§€ëŠ” ì†Œë¹„", "ì†Œë¹„í•˜ëŠ” ê²ƒ",
    "íŒ¨ì…˜", "ì‡¼í•‘", "ì˜ë¥˜", "ì˜·"
}
FASHION_POSITIVE_KEYWORDS = {
    "ì˜·/íŒ¨ì…˜ê´€ë ¨ ì œí’ˆ êµ¬ë§¤í•˜ê¸°", "ì˜·", "íŒ¨ì…˜", "íŒ¨ì…˜ê´€ë ¨",
    "ìºì£¼ì–¼", "ìŠ¤í¬ì¸ ", "ì •ì¥", "ì•„ì›ƒë„ì–´", "ìŠ¤íŠ¸ë¦¬íŠ¸",
    "ë¬´ì‹ ì‚¬", "ì—ì´ë¸”ë¦¬", "ì§€ê·¸ì¬ê·¸", "ë¸Œëœë””"
}
FASHION_NEGATIVE_KEYWORDS = {
    "ì˜·ì„ ê±°ì˜ ì‚¬ì§€ ì•ŠìŒ", "íŒ¨ì…˜ ì‡¼í•‘ ì•ˆí•¨", "í•´ë‹¹ ì—†ìŒ"
}

# 22. ê°€ì „ì œí’ˆ ê´€ì‹¬
HOME_APPLIANCE_QUESTION_KEYWORDS = {
    "ê°€ì „ì œí’ˆ", "ê°€ì „", "ì „ìì œí’ˆ", "ìŠ¤ë§ˆíŠ¸ ê°€ì „"
}
HOME_APPLIANCE_POSITIVE_KEYWORDS = {
    "TV", "ëƒ‰ì¥ê³ ", "ì„¸íƒê¸°", "ì—ì–´ì»¨", "ì²­ì†Œê¸°", "ê³µê¸°ì²­ì •ê¸°",
    "ë¡œë´‡ì²­ì†Œê¸°", "ì‹ê¸°ì„¸ì²™ê¸°", "ê±´ì¡°ê¸°", "ì¸ë•ì…˜"
}
HOME_APPLIANCE_NEGATIVE_KEYWORDS = {
    "ê°€ì „ì œí’ˆ ê´€ì‹¬ ì—†ìŒ", "êµ¬ë§¤ ê³„íš ì—†ìŒ", "í•´ë‹¹ ì—†ìŒ"
}

# 23. ìŠ¤ë§ˆíŠ¸ ê¸°ê¸° (ì‹¤ì œ ì§ˆë¬¸: "ë³´ìœ ê°€ì „ì œí’ˆ")
SMART_DEVICE_QUESTION_KEYWORDS = {
    "ë³´ìœ ê°€ì „ì œí’ˆ", "ê°€ì „ì œí’ˆ", "ë³´ìœ ", "ì†Œìœ "
}
SMART_DEVICE_POSITIVE_KEYWORDS = {
    "ì¸ê³µì§€ëŠ¥ AI ìŠ¤í”¼ì»¤", "AI ìŠ¤í”¼ì»¤", "AIìŠ¤í”¼ì»¤",
    "ë¡œë´‡ì²­ì†Œê¸°", "ë¡œë´‡ ì²­ì†Œê¸°",
    "ìŠ¤ë§ˆíŠ¸ ì›Œì¹˜", "ìŠ¤ë§ˆíŠ¸ì›Œì¹˜", "ì• í”Œì›Œì¹˜", "ê°¤ëŸ­ì‹œ ì›Œì¹˜",
    "ì‹ê¸°ì„¸ì²™ê¸°", "ì˜ë¥˜ ê´€ë¦¬ê¸°", "ìŠ¤íƒ€ì¼ëŸ¬"
}
SMART_DEVICE_NEGATIVE_KEYWORDS = {
    "ìŠ¤ë§ˆíŠ¸ê¸°ê¸° ê´€ì‹¬ ì—†ìŒ", "ì‚¬ìš© ì•ˆí•¨", "í•´ë‹¹ ì—†ìŒ", "ë³´ìœ í•˜ì§€ ì•ŠìŒ"
}

# 24. í™˜ê²½ ë³´í˜¸ (ì‹¤ì œ ì§ˆë¬¸: "ìŠ¤í‚¨ì¼€ì–´ ì œí’ˆ êµ¬ë§¤ ê³ ë ¤ ìš”ì†Œ", "ë¹„ë‹ë´‰íˆ¬ ì‚¬ìš© ì¤„ì´ê¸°")
ENVIRONMENT_QUESTION_KEYWORDS = {
    "ìŠ¤í‚¨ì¼€ì–´ ì œí’ˆ", "êµ¬ë§¤í•  ë•Œ", "ê³ ë ¤í•˜ëŠ” ìš”ì†Œ",
    "ë¹„ë‹ë´‰íˆ¬", "ì¼íšŒìš©", "ì¤„ì´ê¸°", "ë…¸ë ¥"
}
ENVIRONMENT_POSITIVE_KEYWORDS = {
    "ì¹œí™˜ê²½", "ë¹„ê±´", "ì¹œí™˜ê²½/ë¹„ê±´ ì œí’ˆ ì—¬ë¶€",
    "ì¥ë°”êµ¬ë‹ˆ", "ì—ì½”ë°±", "ì¥ë°”êµ¬ë‹ˆë‚˜ ì—ì½”ë°±ì„ ì±™ê¸´ë‹¤",
    "ì¢…ì´ë´‰íˆ¬", "ë°•ìŠ¤", "ë¹„ë‹ ëŒ€ì‹  ì¢…ì´ë´‰íˆ¬ë‚˜ ë°•ìŠ¤ë¥¼ í™œìš©í•œë‹¤"
}
ENVIRONMENT_NEGATIVE_KEYWORDS = {
    "í™˜ê²½ì— ê´€ì‹¬ ì—†ìŒ", "ì‹¤ì²œ ì•ˆí•¨", "í•´ë‹¹ ì—†ìŒ", "íŠ¹ë³„íˆ ì‹ ê²½ ì“°ì§€ ì•ŠëŠ”ë‹¤"
}

# 25. ê¸°ë¶€/ë´‰ì‚¬ (ì‹¤ì œ ì§ˆë¬¸: "ë²„ë¦¬ê¸° ì•„ê¹Œìš´ ë¬¼ê±´")
CHARITY_QUESTION_KEYWORDS = {
    "ë²„ë¦¬ê¸° ì•„ê¹Œìš´", "ë¬¼ê±´", "ë²„ë¦¬ê¸° ì•„ê¹Œìš´ ë¬¼ê±´", "ì–´ë–»ê²Œ í•˜ì‹œë‚˜ìš”"
}
CHARITY_POSITIVE_KEYWORDS = {
    "ê¸°ë¶€", "ê¸°ë¶€í•œë‹¤", "í•„ìš”í•œ ì‚¬ëŒì—ê²Œ ê¸°ë¶€"
}
CHARITY_NEGATIVE_KEYWORDS = {
    "ë²„ë¦°ë‹¤", "ë°”ë¡œ ë²„ë¦°ë‹¤", "ì¤‘ê³ ë¡œ íŒë§¤", "ì—…ì‚¬ì´í´ë§", "ê¸°ë¶€í•˜ì§€ ì•ŠìŒ"
}

# 26. ìë™ì°¨ ê´€ë ¨ (ì‹¤ì œ ì§ˆë¬¸: "ë³´ìœ ì°¨ëŸ‰ì—¬ë¶€")
CAR_INTEREST_QUESTION_KEYWORDS = {
    "ë³´ìœ ì°¨ëŸ‰ì—¬ë¶€", "ì°¨ëŸ‰", "ë³´ìœ ì°¨ëŸ‰", "ìë™ì°¨", "ì°¨"
}
CAR_INTEREST_POSITIVE_KEYWORDS = {
    "ìˆë‹¤", "ë³´ìœ ", "ì†Œìœ ",
    "í˜„ëŒ€", "ê¸°ì•„", "ì œë„¤ì‹œìŠ¤", "BMW", "ë²¤ì¸ ", "í…ŒìŠ¬ë¼", "ìŒìš©"
}
CAR_INTEREST_NEGATIVE_KEYWORDS = {
    "ì—†ë‹¤", "ë³´ìœ í•˜ì§€ ì•ŠìŒ", "í•´ë‹¹ ì—†ìŒ"
}

# 27. ì£¼ê±° í˜•íƒœ
HOUSING_QUESTION_KEYWORDS = {
    "ì£¼ê±°", "ì£¼íƒ", "ê±°ì£¼", "ì£¼ê±° í˜•íƒœ", "ì§‘"
}
HOUSING_POSITIVE_KEYWORDS = {
    "ì•„íŒŒíŠ¸", "ë¹Œë¼", "ì˜¤í”¼ìŠ¤í…”", "ë‹¨ë…ì£¼íƒ", "ë‹¤ì„¸ëŒ€",
    "ìê°€", "ì „ì„¸", "ì›”ì„¸", "ë³´ì¦ê¸ˆ"
}
HOUSING_NEGATIVE_KEYWORDS = {
    "í•´ë‹¹ ì—†ìŒ"
}

# 28. ë³´í—˜ ê°€ì…
INSURANCE_QUESTION_KEYWORDS = {
    "ë³´í—˜", "ë³´í—˜ ê°€ì…", "ë³´ì¥", "ë³´í—˜ ìƒí’ˆ"
}
INSURANCE_POSITIVE_KEYWORDS = {
    "ìƒëª…ë³´í—˜", "ê±´ê°•ë³´í—˜", "ì‹¤ì†ë³´í—˜", "ì•”ë³´í—˜", "ì—°ê¸ˆë³´í—˜",
    "ìë™ì°¨ë³´í—˜", "ì—¬í–‰ìë³´í—˜", "ê°€ì…í•¨", "ê°€ì… ì¤‘"
}
INSURANCE_NEGATIVE_KEYWORDS = {
    "ë³´í—˜ ê°€ì… ì•ˆí•¨", "ë³´í—˜ ì—†ìŒ", "í•´ë‹¹ ì—†ìŒ"
}

# 29. ì‹ ìš©ì¹´ë“œ ì´ìš©
CREDIT_CARD_QUESTION_KEYWORDS = {
    "ì‹ ìš©ì¹´ë“œ", "ì¹´ë“œ", "ê²°ì œ ìˆ˜ë‹¨", "ì¹´ë“œ ì´ìš©"
}
CREDIT_CARD_POSITIVE_KEYWORDS = {
    "ì‹ ìš©ì¹´ë“œ", "ì²´í¬ì¹´ë“œ", "ì‚¼ì„±ì¹´ë“œ", "í˜„ëŒ€ì¹´ë“œ", "ì‹ í•œì¹´ë“œ",
    "KBì¹´ë“œ", "í•˜ë‚˜ì¹´ë“œ", "ë¡¯ë°ì¹´ë“œ", "ìì£¼ ì‚¬ìš©", "ì£¼ ê²°ì œ"
}
CREDIT_CARD_NEGATIVE_KEYWORDS = {
    "ì¹´ë“œë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ", "í˜„ê¸ˆë§Œ ì‚¬ìš©", "í•´ë‹¹ ì—†ìŒ"
}

# 30. ëŒ€ì¤‘êµí†µ ì´ìš©
PUBLIC_TRANSPORT_QUESTION_KEYWORDS = {
    "ëŒ€ì¤‘êµí†µ", "ì§€í•˜ì² ", "ë²„ìŠ¤", "êµí†µìˆ˜ë‹¨", "í†µê·¼"
}
PUBLIC_TRANSPORT_POSITIVE_KEYWORDS = {
    "ì§€í•˜ì² ", "ë²„ìŠ¤", "ì „ì² ", "ê¸°ì°¨", "íƒì‹œ",
    "í•˜ë£¨ì—", "ë§¤ì¼", "ìì£¼", "ì£¼ë¡œ ì´ìš©"
}
PUBLIC_TRANSPORT_NEGATIVE_KEYWORDS = {
    "ëŒ€ì¤‘êµí†µì„ ì´ìš©í•˜ì§€ ì•ŠìŒ", "ìì°¨ ì´ìš©", "ë„ë³´"
}

# 31. íƒë°°/ë°°ì†¡ ì´ìš© (ì‹¤ì œ ì§ˆë¬¸: "ë¹ ë¥¸ ë°°ì†¡ ì„œë¹„ìŠ¤ë¥¼ ì£¼ë¡œ ì–´ë–¤ ì œí’ˆì„ êµ¬ë§¤í•  ë•Œ ì´ìš©í•˜ì‹œë‚˜ìš”?")
PARCEL_DELIVERY_QUESTION_KEYWORDS = {
    "ë¹ ë¥¸ ë°°ì†¡", "ë‹¹ì¼", "ìƒˆë²½", "ì§ì§„ ë°°ì†¡", "ì–´ë–¤ ì œí’ˆ", "ì´ìš©í•˜ì‹œë‚˜ìš”"
}
PARCEL_DELIVERY_POSITIVE_KEYWORDS = {
    "ì‹ ì„ ì‹í’ˆ", "ê³¼ì¼", "ì±„ì†Œ", "ìœ¡ë¥˜",
    "ìƒí™œìš©í’ˆ", "ìƒí•„í’ˆ", "ìœ„ìƒìš©í’ˆ",
    "íŒ¨ì…˜", "ë·°í‹°", "íŒ¨ì…˜Â·ë·°í‹° ì œí’ˆ",
    "ì „ìê¸°ê¸°", "ê°€ì „ì œí’ˆ", "ì „ìê¸°ê¸° ë° ê°€ì „ì œí’ˆ"
}
PARCEL_DELIVERY_NEGATIVE_KEYWORDS = {
    "ë¹ ë¥¸ ë°°ì†¡ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•´ ë³¸ ì  ì—†ë‹¤", "ì´ìš©í•´ ë³¸ ì  ì—†ë‹¤", "í•´ë‹¹ ì—†ìŒ"
}

# 32. ì™¸ì‹ ë¹ˆë„ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì€ ì™¸ë¶€ ì‹ë‹¹ì—ì„œ í˜¼ì ì‹ì‚¬í•˜ëŠ” ë¹ˆë„ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”?")
DINING_OUT_QUESTION_KEYWORDS = {
    "ì™¸ë¶€ ì‹ë‹¹", "ì™¸ì‹", "ì‹ì‚¬", "í˜¼ì ì‹ì‚¬", "ë¹ˆë„"
}
DINING_OUT_POSITIVE_KEYWORDS = {
    "ì›” 1~2íšŒ ì •ë„", "ì£¼ 1íšŒ ì •ë„", "ì£¼ 2~3íšŒ ì •ë„", "ê±°ì˜ ë§¤ì¼",
    "ì›”", "ì£¼", "íšŒ ì •ë„", "ë§¤ì¼"
}
DINING_OUT_NEGATIVE_KEYWORDS = {
    "ê±°ì˜ í•˜ì§€ ì•Šê±°ë‚˜ í•œ ë²ˆë„ í•´ë³¸ ì  ì—†ë‹¤", "ê±°ì˜ í•˜ì§€ ì•Š", "í•œ ë²ˆë„ í•´ë³¸ ì  ì—†",
    "ì™¸ì‹í•˜ì§€ ì•ŠìŒ", "ê±°ì˜ ì•ˆí•¨"
}

# 33. ìˆ ìë¦¬ ë¹ˆë„
DRINKING_GATHERING_QUESTION_KEYWORDS = {
    "ìˆ ìë¦¬", "ìŒì£¼", "íšŒì‹", "ìˆ ", "ìŒì£¼ ë¹ˆë„"
}
DRINKING_GATHERING_POSITIVE_KEYWORDS = {
    "ì¼ì£¼ì¼ì—", "í•œë‹¬ì—", "ìì£¼", "ê°€ë”", "íšŒ ì´ìƒ"
}
DRINKING_GATHERING_NEGATIVE_KEYWORDS = {
    "ìˆ ìë¦¬ ì—†ìŒ", "ìˆ  ì•ˆ ë§ˆì‹¬", "ì°¸ì„ ì•ˆí•¨"
}

# 34. ì•¼ê·¼ ë¹ˆë„
OVERTIME_QUESTION_KEYWORDS = {
    "ì•¼ê·¼", "ì´ˆê³¼ ê·¼ë¬´", "ì—°ì¥ ê·¼ë¬´", "ì•¼ê·¼ ë¹ˆë„"
}
OVERTIME_POSITIVE_KEYWORDS = {
    "ì¼ì£¼ì¼ì—", "í•œë‹¬ì—", "ìì£¼", "ë§¤ì¼", "ê°€ë”"
}
OVERTIME_NEGATIVE_KEYWORDS = {
    "ì•¼ê·¼ ì—†ìŒ", "ì•¼ê·¼ ì•ˆí•¨", "í•´ë‹¹ ì—†ìŒ"
}

# 35. ì¬íƒê·¼ë¬´
REMOTE_WORK_QUESTION_KEYWORDS = {
    "ì¬íƒê·¼ë¬´", "ì›ê²©ê·¼ë¬´", "ì¬íƒ", "WFH", "í™ˆì˜¤í”¼ìŠ¤"
}
REMOTE_WORK_POSITIVE_KEYWORDS = {
    "ì „ì²´ ì¬íƒ", "ë¶€ë¶„ ì¬íƒ", "í•˜ì´ë¸Œë¦¬ë“œ", "ì£¼ 1íšŒ", "ì£¼ 2íšŒ",
    "ì¼ì£¼ì¼ì—", "ìì£¼", "ê°€ëŠ¥"
}
REMOTE_WORK_NEGATIVE_KEYWORDS = {
    "ì¬íƒê·¼ë¬´ ì—†ìŒ", "ì „ì²´ ì¶œê·¼", "ë¶ˆê°€ëŠ¥"
}

# ============================================================================
# â­ ì‹ ê·œ Behavioral íŒ¨í„´ (ì„¤ë¬¸ ë°ì´í„° ê¸°ë°˜)
# ============================================================================

# 36. í• ì¸/í¬ì¸íŠ¸ ë¯¼ê°ë„ (ì‹¤ì œ ì§ˆë¬¸: "ì†Œë¹„ ì‹œ ê³ ë ¤í•˜ëŠ” ìš”ì¸")
REWARDS_QUESTION_KEYWORDS = {
    "ì†Œë¹„ ì‹œ ê³ ë ¤í•˜ëŠ” ìš”ì¸", "ê³ ë ¤í•˜ëŠ” ìš”ì¸", "ì†Œë¹„", "êµ¬ë§¤", "ì„ íƒ ê¸°ì¤€"
}
REWARDS_POSITIVE_KEYWORDS = {
    "í• ì¸", "ìºì‹œë°±", "ë©¤ë²„ì‹­", "í¬ì¸íŠ¸", "ì ë¦½", "ë¦¬ì›Œë“œ", "í˜œíƒ", "ì¿ í°"
}
REWARDS_NEGATIVE_KEYWORDS = {
    # ë‹¤ë¥¸ ì„ íƒì§€ì—ëŠ” ìˆì§€ë§Œ í• ì¸/í¬ì¸íŠ¸ì™€ ë¬´ê´€í•œ ë‹µë³€
    "ë¸Œëœë“œ", "ë””ìì¸", "í’ˆì§ˆ", "í¸ì˜ì„±", "ì¶”ì²œ"
}

# 37. ì¤‘ê³ ê±°ë˜ ì‚¬ìš© (ì‹¤ì œ ì§ˆë¬¸: "ë²„ë¦¬ê¸° ì•„ê¹Œìš´ ë¬¼ê±´")
SECONDHAND_MARKET_QUESTION_KEYWORDS = {
    "ë²„ë¦¬ê¸° ì•„ê¹Œìš´", "ì•„ê¹Œìš´ ë¬¼ê±´", "ë¬¼ê±´", "ì²˜ë¦¬", "ì¤‘ê³ "
}
SECONDHAND_MARKET_POSITIVE_KEYWORDS = {
    "ì¤‘ê³ ë¡œ íŒë§¤", "ì¤‘ê³  íŒë§¤", "ì¤‘ê³ ê±°ë˜", "ì¤‘ê³ ", "íŒë§¤", "ë‹¹ê·¼ë§ˆì¼“", "ë²ˆê°œì¥í„°"
}
SECONDHAND_MARKET_NEGATIVE_KEYWORDS = {
    "ë²„ë¦°ë‹¤", "íê¸°", "ê¸°ë¶€", "ë³´ê´€", "ì„ ë¬¼"
}

# 38. ë¯¸ë‹ˆë©€ë¦¬ìŠ¤íŠ¸ ì„±í–¥ (ì‹¤ì œ ì§ˆë¬¸: "ë¯¸ë‹ˆë©€ë¦¬ìŠ¤íŠ¸ì™€ ë§¥ì‹œë©€ë¦¬ìŠ¤íŠ¸")
MINIMALIST_QUESTION_KEYWORDS = {
    "ë¯¸ë‹ˆë©€ë¦¬ìŠ¤íŠ¸", "ë§¥ì‹œë©€ë¦¬ìŠ¤íŠ¸", "ë¼ì´í”„ìŠ¤íƒ€ì¼", "ìƒí™œë°©ì‹", "ì„±í–¥"
}
MINIMALIST_POSITIVE_KEYWORDS = {
    "ë¯¸ë‹ˆë©€ë¦¬ìŠ¤íŠ¸", "ë¯¸ë‹ˆë©€", "ì‹¬í”Œ", "ë‹¨ìˆœ", "ìµœì†Œ"
}
MINIMALIST_NEGATIVE_KEYWORDS = {
    "ë§¥ì‹œë©€ë¦¬ìŠ¤íŠ¸", "ë§¥ì‹œë©€", "ë§ì€", "ë‹¤ì–‘"
}

# 39. ê°œì¸ì •ë³´ë³´í˜¸ ì˜ì‹ (ì‹¤ì œ ì§ˆë¬¸: "ê°œì¸ì •ë³´ë³´í˜¸")
PRIVACY_QUESTION_KEYWORDS = {
    "ê°œì¸ì •ë³´", "ê°œì¸ì •ë³´ë³´í˜¸", "í”„ë¼ì´ë²„ì‹œ", "privacy", "ì •ë³´ë³´í˜¸", "ê°œì¸ ì •ë³´"
}
PRIVACY_POSITIVE_KEYWORDS = {
    "ë§¤ìš° ì¤‘ìš”", "ì¤‘ìš”", "ì‹ ê²½", "ë³´í˜¸", "ë¯¼ê°"
}
PRIVACY_NEGATIVE_KEYWORDS = {
    "ì¤‘ìš”í•˜ì§€ ì•Š", "ì‹ ê²½ ì•ˆ", "ë³„ë¡œ", "ë¬´ê´€ì‹¬"
}

# 40. ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œ ë°©ë²• (ì‹¤ì œ ì§ˆë¬¸: "ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ í•´ì†Œí•˜ëŠ” ë°©ë²•")
STRESS_RELIEF_QUESTION_KEYWORDS = {
    "ìŠ¤íŠ¸ë ˆìŠ¤", "ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œ", "í•´ì†Œ", "í•´ì†Œ ë°©ë²•", "ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ í•´ì†Œ"
}
# ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œ ë°©ë²•ì€ ë‹¤ì–‘í•˜ë¯€ë¡œ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜
STRESS_RELIEF_ACTIVE_KEYWORDS = {
    "ìš´ë™", "ì‚°ì±…", "ë“±ì‚°", "ìš”ê°€", "í—¬ìŠ¤", "ëŸ¬ë‹", "ì¡°ê¹…", "ìˆ˜ì˜"
}
STRESS_RELIEF_ENTERTAINMENT_KEYWORDS = {
    "ì˜í™”", "ë“œë¼ë§ˆ", "ê²Œì„", "ìŒì•…", "ë…ì„œ", "ì±…", "ìœ íŠœë¸Œ", "ë„·í”Œë¦­ìŠ¤"
}
STRESS_RELIEF_SOCIAL_KEYWORDS = {
    "ì¹œêµ¬", "ê°€ì¡±", "ëŒ€í™”", "ìˆ˜ë‹¤", "ìˆ ", "ìˆ ìë¦¬", "ëª¨ì„"
}
STRESS_RELIEF_RELAXATION_KEYWORDS = {
    "ìˆ˜ë©´", "ì ", "íœ´ì‹", "ëª…ìƒ", "íë§", "ì—¬í–‰", "ì˜¨ì²œ", "ë§ˆì‚¬ì§€"
}
STRESS_RELIEF_SHOPPING_KEYWORDS = {
    "ì‡¼í•‘", "ì†Œë¹„", "êµ¬ë§¤", "ì¥ë³´ê¸°"
}
STRESS_RELIEF_NEGATIVE_KEYWORDS = {
    "ìŠ¤íŠ¸ë ˆìŠ¤ ì—†ìŒ", "í•´ì†Œ ì•ˆí•¨", "íŠ¹ë³„í•œ ë°©ë²• ì—†ìŒ"
}

# 41. ê²¨ìš¸ë°©í•™ ì¶”ì–µ (ì‹¤ì œ ì§ˆë¬¸: "ì´ˆë“±í•™ìƒ ì‹œì ˆ ê²¨ìš¸ë°©í•™ ë•Œ ê°€ì¥ ê¸°ì–µì— ë‚¨ëŠ” ì¼ì€ ë¬´ì—‡ì¸ê°€ìš”?")
WINTER_VACATION_QUESTION_KEYWORDS = {
    "ì´ˆë“±í•™ìƒ", "ê²¨ìš¸ë°©í•™", "ê¸°ì–µì— ë‚¨ëŠ”", "ì¶”ì–µ"
}
# â­ ë¬¸ìì—´ ê°’ ì €ì¥ (ì¹´í…Œê³ ë¦¬ë³„)
WINTER_VACATION_ANSWER_VALUES = {
    "ì¹œêµ¬ë“¤ê³¼ ë³´ë‚¸ ì¦ê±°ìš´ ì‹œê°„": ["ì¹œêµ¬", "ì¦ê±°ìš´", "ì‹œê°„"],
    "ëˆˆì°ë§¤, ìŠ¤í‚¤ ë“± ê²¨ìš¸ ìŠ¤í¬ì¸ ": ["ëˆˆì°ë§¤", "ìŠ¤í‚¤", "ê²¨ìš¸ ìŠ¤í¬ì¸ ", "ìŠ¤ë…¸ë³´ë“œ"],
    "ëˆˆì‚¬ëŒ ë§Œë“¤ê¸°": ["ëˆˆì‚¬ëŒ", "ëˆˆì‚¬ëŒ ë§Œë“¤ê¸°"],
    "ê°€ì¡±ê³¼ í•¨ê»˜ ë– ë‚œ ì—¬í–‰": ["ê°€ì¡±", "ì—¬í–‰"],
    "ê²¨ìš¸ë°©í•™ ìˆ™ì œë¥¼ ëë‚¸ ìˆœê°„": ["ìˆ™ì œ", "ëë‚¸"],
    "ê¸°íƒ€": ["ê¸°íƒ€"],
    "ë°©í•™ ë™ì•ˆ ë‹¤ë…”ë˜ í•™ì›ì´ë‚˜ íŠ¹ë³„ í™œë™": ["í•™ì›", "íŠ¹ë³„ í™œë™", "ë³´ìŠµí•™ì›"]
}

# 42. í”¼ë¶€ ìƒíƒœ ë§Œì¡±ë„ (ì‹¤ì œ ì§ˆë¬¸: "í˜„ì¬ ë³¸ì¸ì˜ í”¼ë¶€ ìƒíƒœì— ì–¼ë§ˆë‚˜ ë§Œì¡±í•˜ì‹œë‚˜ìš”?")
SKIN_SATISFACTION_QUESTION_KEYWORDS = {
    "í”¼ë¶€", "í”¼ë¶€ ìƒíƒœ", "í”¼ë¶€ìƒíƒœ", "ë§Œì¡±"
}
SKIN_SATISFACTION_ANSWER_VALUES = {
    "ë§¤ìš° ë§Œì¡±í•œë‹¤": ["ë§¤ìš° ë§Œì¡±", "ë§¤ìš°ë§Œì¡±"],
    "ë§Œì¡±í•œë‹¤": ["ë§Œì¡±í•œë‹¤", "ë§Œì¡±"],
    "ë³´í†µì´ë‹¤": ["ë³´í†µ", "ë³´í†µì´ë‹¤"],
    "ë¶ˆë§Œì¡±í•œë‹¤": ["ë¶ˆë§Œì¡±í•œë‹¤", "ë¶ˆë§Œì¡±"],
    "ë§¤ìš° ë¶ˆë§Œì¡±í•œë‹¤": ["ë§¤ìš° ë¶ˆë§Œì¡±", "ë§¤ìš°ë¶ˆë§Œì¡±"]
}

# 43. AI ì„œë¹„ìŠ¤ í™œìš© ë¶„ì•¼ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì€ ìš”ì¦˜ ì–´ë–¤ ë¶„ì•¼ì—ì„œ AI ì„œë¹„ìŠ¤ë¥¼ í™œìš©í•˜ê³  ê³„ì‹ ê°€ìš”?")
AI_SERVICE_FIELD_QUESTION_KEYWORDS = {
    "AI ì„œë¹„ìŠ¤", "AI", "ì¸ê³µì§€ëŠ¥", "í™œìš©", "ì–´ë–¤ ë¶„ì•¼"
}
AI_SERVICE_FIELD_ANSWER_VALUES = {
    "ê²€ìƒ‰/ì •ë³´ íƒìƒ‰": ["ê²€ìƒ‰", "ì •ë³´ íƒìƒ‰", "ì •ë³´íƒìƒ‰"],
    "ë²ˆì—­ì´ë‚˜ ì™¸êµ­ì–´ í•™ìŠµ": ["ë²ˆì—­", "ì™¸êµ­ì–´", "í•™ìŠµ", "ì–¸ì–´"],
    "ì—…ë¬´ ë³´ì¡° (ë¬¸ì„œ ì‘ì„±, ì´ë©”ì¼ ë“±)": ["ì—…ë¬´", "ë¬¸ì„œ", "ì´ë©”ì¼", "ì—…ë¬´ ë³´ì¡°"],
    "ì´ë¯¸ì§€ ìƒì„± ë˜ëŠ” ë””ìì¸ ì°¸ê³ ": ["ì´ë¯¸ì§€", "ë””ìì¸", "ìƒì„±"],
    "í•™ìŠµ/ê³µë¶€ ë³´ì¡°": ["í•™ìŠµ", "ê³µë¶€", "ê³µë¶€ ë³´ì¡°"],
    "ì½˜í…ì¸  ì œì‘ (ë¸”ë¡œê·¸, ì˜ìƒ ê¸°íš ë“±)": ["ì½˜í…ì¸ ", "ë¸”ë¡œê·¸", "ì˜ìƒ"],
    "AI ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•´ë³¸ ì  ì—†ë‹¤": ["ì‚¬ìš©í•´ë³¸ ì  ì—†ë‹¤", "ì—†ë‹¤"]
}

# 44. ê¸°ë¶„ ì¢‹ì€ ì†Œë¹„ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì€ ë³¸ì¸ì„ ìœ„í•´ ì†Œë¹„í•˜ëŠ” ê²ƒ ì¤‘ ê°€ì¥ ê¸°ë¶„ ì¢‹ì•„ì§€ëŠ” ì†Œë¹„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?")
HAPPY_CONSUMPTION_QUESTION_KEYWORDS = {
    "ì†Œë¹„", "ê¸°ë¶„ ì¢‹", "ê¸°ë¶„ì¢‹", "ê°€ì¥ ê¸°ë¶„"
}
HAPPY_CONSUMPTION_ANSWER_VALUES = {
    "ë§›ìˆëŠ” ìŒì‹ ë¨¹ê¸°": ["ìŒì‹", "ë¨¹ê¸°", "ë§›ìˆëŠ”"],
    "ì—¬í–‰ ê°€ê¸°": ["ì—¬í–‰"],
    "ì·¨ë¯¸ê´€ë ¨ ì œí’ˆ êµ¬ë§¤í•˜ê¸°": ["ì·¨ë¯¸", "ì œí’ˆ"],
    "ì˜·/íŒ¨ì…˜ê´€ë ¨ ì œí’ˆ êµ¬ë§¤í•˜ê¸°": ["ì˜·", "íŒ¨ì…˜"]
}

# 45. AI ì±—ë´‡ ì„œë¹„ìŠ¤ ì¢…ë¥˜ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì´ ì‚¬ìš©í•´ ë³¸ AI ì±—ë´‡ ì„œë¹„ìŠ¤ëŠ” ë¬´ì—‡ì¸ê°€ìš”?")
AI_CHATBOT_SERVICE_QUESTION_KEYWORDS = {
    "AI ì±—ë´‡", "ì±—ë´‡", "chatbot", "ì‚¬ìš©í•´ ë³¸"
}
AI_CHATBOT_SERVICE_ANSWER_VALUES = {
    "ChatGPT": ["chatgpt", "ì±—gpt", "gpt"],
    "Gemini (êµ¬ê¸€)": ["gemini", "ì œë¯¸ë‚˜ì´", "êµ¬ê¸€"],
    "Copilot (ë§ˆì´í¬ë¡œì†Œí”„íŠ¸)": ["copilot", "ì½”íŒŒì¼ëŸ¿", "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸"],
    "HyperCLOVER X (ë„¤ì´ë²„)": ["hyperclover", "í•˜ì´í¼í´ë¡œë°”", "ë„¤ì´ë²„"],
    "ë”¥ì‹œí¬": ["ë”¥ì‹œí¬", "deepseek"],
    "Claude (Anthropic)": ["claude", "í´ë¡œë“œ"],
    "ì‚¬ìš©í•´ ë³¸ ì  ì—†ìŒ": ["ì‚¬ìš©í•´ ë³¸ ì  ì—†ìŒ", "ì—†ìŒ"]
}

# 46. í•´ì™¸ì—¬í–‰ ì„ í˜¸ ì§€ì—­ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì€ ì˜¬í•´ í•´ì™¸ì—¬í–‰ì„ ê°„ë‹¤ë©´ ì–´ë””ë¡œ ê°€ê³  ì‹¶ë‚˜ìš”?")
OVERSEAS_TRAVEL_QUESTION_KEYWORDS = {
    "í•´ì™¸ì—¬í–‰", "í•´ì™¸", "ì—¬í–‰", "ê°€ê³  ì‹¶"
}
OVERSEAS_TRAVEL_ANSWER_VALUES = {
    "ìœ ëŸ½": ["ìœ ëŸ½"],
    "ë™ë‚¨ì•„": ["ë™ë‚¨ì•„"],
    "ì¼ë³¸/ì¤‘êµ­": ["ì¼ë³¸", "ì¤‘êµ­"],
    "ë¯¸êµ­/ìºë‚˜ë‹¤": ["ë¯¸êµ­", "ìºë‚˜ë‹¤"],
    "í•´ì™¸ì—¬í–‰ì„ ê°€ê³ ì‹¶ì§€ ì•Šë‹¤": ["ê°€ê³ ì‹¶ì§€ ì•Šë‹¤", "ê°€ê³  ì‹¶ì§€ ì•Šë‹¤"]
}

# 47. OTT ì„œë¹„ìŠ¤ ê°œìˆ˜ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì´ í˜„ì¬ ì´ìš© ì¤‘ì¸ OTT ì„œë¹„ìŠ¤ëŠ” ëª‡ ê°œì¸ê°€ìš”?")
OTT_COUNT_QUESTION_KEYWORDS = {
    "OTT", "OTT ì„œë¹„ìŠ¤", "ëª‡ ê°œ", "ê°œìˆ˜"
}
OTT_COUNT_ANSWER_VALUES = {
    "1ê°œ": ["1ê°œ"],
    "2ê°œ": ["2ê°œ"],
    "3ê°œ": ["3ê°œ"],
    "4ê°œ ì´ìƒ": ["4ê°œ", "4ê°œ ì´ìƒ"]
}

# 48. ë¬¼ê±´ ì²˜ë¦¬ ë°©ë²• (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì€ ë²„ë¦¬ê¸° ì•„ê¹Œìš´ ë¬¼ê±´ì´ ìˆì„ ë•Œ, ì£¼ë¡œ ì–´ë–»ê²Œ í•˜ì‹œë‚˜ìš”?")
DISPOSAL_METHOD_QUESTION_KEYWORDS = {
    "ë²„ë¦¬ê¸° ì•„ê¹Œìš´", "ë¬¼ê±´", "ì²˜ë¦¬"
}
DISPOSAL_METHOD_ANSWER_VALUES = {
    "ê·¸ëƒ¥ ë³´ê´€": ["ë³´ê´€"],
    "ì¤‘ê³ ë¡œ íŒë§¤": ["ì¤‘ê³ ", "íŒë§¤"],
    "ì—…ì‚¬ì´í´ë§(ì¬í™œìš©) ì‹œë„": ["ì—…ì‚¬ì´í´", "ì¬í™œìš©"],
    "ê¸°ë¶€": ["ê¸°ë¶€"],
    "ë°”ë¡œ ë²„ë¦°ë‹¤": ["ë²„ë¦°ë‹¤"]
}

# 49. ì´ì‚¬ ìŠ¤íŠ¸ë ˆìŠ¤ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì€ ì´ì‚¬í•  ë•Œ ê°€ì¥ ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ëŠ” ë¶€ë¶„ì€ ì–´ë–¤ê±¸ê¹Œìš”?")
MOVING_STRESS_QUESTION_KEYWORDS = {
    "ì´ì‚¬", "ìŠ¤íŠ¸ë ˆìŠ¤", "ì´ì‚¬í•  ë•Œ"
}
MOVING_STRESS_ANSWER_VALUES = {
    "ì§ ì‹¸ê³  í’€ê¸°": ["ì§", "ì§ ì‹¸ê³ "],
    "ë¹„ìš© ë¶€ë‹´": ["ë¹„ìš©"],
    "ì´ì‚¬ì—…ì²´ ì„ íƒ": ["ì´ì‚¬ì—…ì²´"],
    "ìƒˆë¡œìš´ í™˜ê²½ ì ì‘": ["í™˜ê²½", "ì ì‘"],
    "ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì§€ ì•ŠëŠ”ë‹¤": ["ë°›ì§€ ì•ŠëŠ”ë‹¤"]
}

# 50. ì„¤ ì„ ë¬¼ ì„ í˜¸ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì´ ê°€ì¥ ì„ í˜¸í•˜ëŠ” ì„¤ ì„ ë¬¼ ìœ í˜•ì€ ë¬´ì—‡ì¸ê°€ìš”?")
LUNAR_GIFT_QUESTION_KEYWORDS = {
    "ì„¤", "ì„ ë¬¼", "ì„¤ ì„ ë¬¼"
}
LUNAR_GIFT_ANSWER_VALUES = {
    "ë°±í™”ì  ìƒí’ˆê¶Œ/í˜„ê¸ˆ": ["ìƒí’ˆê¶Œ", "í˜„ê¸ˆ"],
    "ì „í†µ ì„ ë¬¼ ì„¸íŠ¸(í•œìš°, êµ´ë¹„, ê³¼ì¼ ë“±)": ["ì „í†µ", "í•œìš°", "êµ´ë¹„"],
    "ê±´ê°•ì‹í’ˆ(í™ì‚¼, ë¹„íƒ€ë¯¼ ë“±)": ["ê±´ê°•ì‹í’ˆ", "í™ì‚¼", "ë¹„íƒ€ë¯¼"],
    "ì‹¤ìš©ì ì¸ ìƒí•„í’ˆ(ìƒ´í‘¸, ì„¸ì œ, ì‹ìš©ìœ  ë“±)": ["ìƒí•„í’ˆ", "ìƒ´í‘¸", "ì„¸ì œ"]
}

# 51. ìŠ¤í‚¨ì¼€ì–´ ì§€ì¶œ (ì‹¤ì œ ì§ˆë¬¸: "í•œ ë‹¬ ê¸°ì¤€ìœ¼ë¡œ ìŠ¤í‚¨ì¼€ì–´ ì œí’ˆì— í‰ê· ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì†Œë¹„í•˜ì‹œë‚˜ìš”?")
SKINCARE_SPENDING_QUESTION_KEYWORDS = {
    "ìŠ¤í‚¨ì¼€ì–´", "ì§€ì¶œ", "ì†Œë¹„"
}
SKINCARE_SPENDING_ANSWER_VALUES = {
    "3ë§Œì› ë¯¸ë§Œ": ["3ë§Œì› ë¯¸ë§Œ"],
    "3ë§Œì› ì´ìƒ ~ 5ë§Œì› ë¯¸ë§Œ": ["3ë§Œì›", "5ë§Œì›"],
    "5ë§Œì› ì´ìƒ ~ 10ë§Œì› ë¯¸ë§Œ": ["5ë§Œì›", "10ë§Œì›"],
    "10ë§Œì› ì´ìƒ ~ 15ë§Œì› ë¯¸ë§Œ": ["10ë§Œì›", "15ë§Œì›"],
    "15ë§Œì› ì´ìƒ": ["15ë§Œì› ì´ìƒ"]
}

# 52. ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” AI ì±—ë´‡ (ì‹¤ì œ ì§ˆë¬¸: "ì‚¬ìš©í•´ ë³¸ AI ì±—ë´‡ ì„œë¹„ìŠ¤ ì¤‘ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?")
AI_CHATBOT_PRIMARY_QUESTION_KEYWORDS = {
    "AI ì±—ë´‡", "ì£¼ë¡œ ì‚¬ìš©", "ì£¼ë¡œ"
}
AI_CHATBOT_PRIMARY_ANSWER_VALUES = {
    "ChatGPT": ["chatgpt", "ì±—gpt"],
    "Gemini (êµ¬ê¸€)": ["gemini", "ì œë¯¸ë‚˜ì´"],
    "HyperCLOVER X (ë„¤ì´ë²„)": ["hyperclover", "í•˜ì´í¼í´ë¡œë°”"],
    "Copilot (ë§ˆì´í¬ë¡œì†Œí”„íŠ¸)": ["copilot", "ì½”íŒŒì¼ëŸ¿"],
    "ë”¥ì‹œí¬": ["ë”¥ì‹œí¬"],
    "Claude (Anthropic)": ["claude", "í´ë¡œë“œ"]
}

# 53. ìŠ¤í‚¨ì¼€ì–´ êµ¬ë§¤ ê¸°ì¤€ (ì‹¤ì œ ì§ˆë¬¸: "ìŠ¤í‚¨ì¼€ì–´ ì œí’ˆì„ êµ¬ë§¤í•  ë•Œ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ê³ ë ¤í•˜ëŠ” ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€ìš”?")
SKINCARE_PRIORITY_QUESTION_KEYWORDS = {
    "ìŠ¤í‚¨ì¼€ì–´", "êµ¬ë§¤", "ê³ ë ¤"
}
SKINCARE_PRIORITY_ANSWER_VALUES = {
    "ì„±ë¶„ ë° íš¨ê³¼": ["ì„±ë¶„", "íš¨ê³¼"],
    "ê°€ê²©": ["ê°€ê²©"],
    "ì œí’ˆ ë¦¬ë·° ë° ì‚¬ìš© í›„ê¸°": ["ë¦¬ë·°", "í›„ê¸°"],
    "ì¹œí™˜ê²½/ë¹„ê±´ ì œí’ˆ ì—¬ë¶€": ["ì¹œí™˜ê²½", "ë¹„ê±´"],
    "ë¸Œëœë“œ ëª…ì„±": ["ë¸Œëœë“œ"],
    "íŒ¨í‚¤ì§€ ë””ìì¸": ["íŒ¨í‚¤ì§€", "ë””ìì¸"]
}

# 54. ì•¼ì‹ ë°©ë²• (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì€ ì•¼ì‹ì„ ë¨¹ì„ ë•Œ ë³´í†µ ì–´ë–¤ ë°©ë²•ìœ¼ë¡œ ë“œì‹œë‚˜ìš”?")
LATE_NIGHT_SNACK_QUESTION_KEYWORDS = {
    "ì•¼ì‹", "ë¨¹ì„ ë•Œ"
}
LATE_NIGHT_SNACK_ANSWER_VALUES = {
    "ë°°ë‹¬ ì£¼ë¬¸í•´ì„œ ë¨¹ëŠ”ë‹¤": ["ë°°ë‹¬"],
    "ì•¼ì‹ì„ ê±°ì˜ ë¨¹ì§€ ì•ŠëŠ”ë‹¤": ["ë¨¹ì§€ ì•ŠëŠ”ë‹¤"],
    "ì§ì ‘ ì‚¬ì™€ì„œ ë¨¹ëŠ”ë‹¤": ["ì§ì ‘ ì‚¬"],
    "ì§‘ì—ì„œ ì§ì ‘ ë§Œë“¤ì–´ ë¨¹ëŠ”ë‹¤": ["ì§ì ‘ ë§Œë“¤"],
    "ì™¸ì¶œí•´ì„œ ì‹ë‹¹ì´ë‚˜ í¬ì¥ë§ˆì°¨ ë“±ì—ì„œ ë¨¹ëŠ”ë‹¤": ["ì™¸ì¶œ", "ì‹ë‹¹"]
}

# 55. ìµœê·¼ ì§€ì¶œ ì¹´í…Œê³ ë¦¬ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì€ ìµœê·¼ ê°€ì¥ ì§€ì¶œì„ ë§ì´ í•œ ê³³ì€ ì–´ë””ì…ë‹ˆê¹Œ?")
RECENT_SPENDING_QUESTION_KEYWORDS = {
    "ìµœê·¼", "ì§€ì¶œ", "ë§ì´"
}
RECENT_SPENDING_ANSWER_VALUES = {
    "ì™¸ì‹ë¹„": ["ì™¸ì‹"],
    "ì˜·/ì‡¼í•‘": ["ì˜·", "ì‡¼í•‘"],
    "ë°°ë‹¬ë¹„": ["ë°°ë‹¬"],
    "ì½˜ì„œíŠ¸, ì „ì‹œ ë“± ë¬¸í™”ìƒí™œ": ["ì½˜ì„œíŠ¸", "ì „ì‹œ", "ë¬¸í™”"]
}

# 56. í˜¼ë°¥ ë¹ˆë„ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì€ ì™¸ë¶€ ì‹ë‹¹ì—ì„œ í˜¼ì ì‹ì‚¬í•˜ëŠ” ë¹ˆë„ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”?")
SOLO_DINING_QUESTION_KEYWORDS = {
    "í˜¼ì", "ì‹ì‚¬", "ë¹ˆë„"
}
SOLO_DINING_ANSWER_VALUES = {
    "ê±°ì˜ í•˜ì§€ ì•Šê±°ë‚˜ í•œ ë²ˆë„ í•´ë³¸ ì  ì—†ë‹¤": ["ê±°ì˜ í•˜ì§€ ì•Š", "ì—†ë‹¤"],
    "ì›” 1~2íšŒ ì •ë„": ["ì›” 1", "ì›” 2"],
    "ì£¼ 1íšŒ ì •ë„": ["ì£¼ 1"],
    "ì£¼ 2~3íšŒ ì •ë„": ["ì£¼ 2", "ì£¼ 3"],
    "ê±°ì˜ ë§¤ì¼": ["ë§¤ì¼"]
}

# 57. ë‹¤ì´ì–´íŠ¸ ë°©ë²• (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì´ ì§€ê¸ˆê¹Œì§€ í•´ë³¸ ë‹¤ì´ì–´íŠ¸ ì¤‘ ê°€ì¥ íš¨ê³¼ ìˆì—ˆë˜ ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?")
DIET_METHOD_QUESTION_KEYWORDS = {
    "ë‹¤ì´ì–´íŠ¸", "íš¨ê³¼", "ë°©ë²•"
}
DIET_METHOD_ANSWER_VALUES = {
    "ê¾¸ì¤€í•œ ìœ ì‚°ì†Œ ìš´ë™": ["ìœ ì‚°ì†Œ"],
    "í•˜ë£¨ ì„¸ ë¼ë¥¼ ê·œì¹™ì ìœ¼ë¡œ ì†Œì‹í•˜ê¸°": ["ì†Œì‹", "ê·œì¹™ì "],
    "ê°„í—ì  ë‹¨ì‹(ì˜ˆ: 16ì‹œê°„ ê³µë³µ)": ["ê°„í—ì ", "ë‹¨ì‹"],
    "í—¬ìŠ¤ì¥ ë˜ëŠ” í™ˆíŠ¸ë ˆì´ë‹": ["í—¬ìŠ¤", "í™ˆíŠ¸"],
    "ì €íƒ„ê³ ì§€/ë‹¨ë°±ì§ˆ ìœ„ì£¼ ì‹ë‹¨": ["ì €íƒ„ê³ ì§€", "ë‹¨ë°±ì§ˆ"],
    "ì‹ìš• ì–µì œì œ ë˜ëŠ” ë‹¤ì´ì–´íŠ¸ ë³´ì¡°ì œ ì„­ì·¨": ["ì–µì œì œ", "ë³´ì¡°ì œ"]
}

# 58. ì•ŒëŒ ìŠ¤íƒ€ì¼ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì€ ì•„ì¹¨ì— ê¸°ìƒí•˜ê¸° ìœ„í•´ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì•ŒëŒì„ ì„¤ì •í•´ë‘ì‹œë‚˜ìš”?")
ALARM_STYLE_QUESTION_KEYWORDS = {
    "ì•ŒëŒ", "ê¸°ìƒ", "ì„¤ì •"
}
ALARM_STYLE_ANSWER_VALUES = {
    "í•œ ê°œë§Œ ì„¤ì •í•´ë†“ê³  ë°”ë¡œ ì¼ì–´ë‚œë‹¤": ["í•œ ê°œ", "ë°”ë¡œ"],
    "ì—¬ëŸ¬ ê°œì˜ ì•ŒëŒì„ ì§§ì€ ê°„ê²©ìœ¼ë¡œ ì„¤ì •í•´ë‘”ë‹¤": ["ì—¬ëŸ¬", "ì§§ì€ ê°„ê²©"]
}

# 59. ì—¬ë¦„ ê±±ì • (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì€ ë‹¤ê°€ì˜¤ëŠ” ì—¬ë¦„ì²  ê°€ì¥ ê±±ì •ë˜ëŠ” ì ì´ ë¬´ì—‡ì¸ê°€ìš”?")
SUMMER_CONCERN_QUESTION_KEYWORDS = {
    "ì—¬ë¦„", "ê±±ì •"
}
SUMMER_CONCERN_ANSWER_VALUES = {
    "ë”ìœ„ì™€ ë•€": ["ë”ìœ„", "ë•€"],
    "ì „ê¸°ìš”ê¸ˆ ë¶€ë‹´": ["ì „ê¸°ìš”ê¸ˆ"],
    "ì²´ë ¥ ì €í•˜": ["ì²´ë ¥"],
    "í”¼ë¶€ íŠ¸ëŸ¬ë¸”": ["í”¼ë¶€"],
    "ëƒ‰ë°©ë³‘": ["ëƒ‰ë°©ë³‘"],
    "íœ´ê°€ ê³„íš ìŠ¤íŠ¸ë ˆìŠ¤": ["íœ´ê°€"]
}

# 60. ì—¬ë¦„ ê°„ì‹ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì˜ ì—¬ë¦„ì²  ìµœì•  ê°„ì‹ì€ ë¬´ì—‡ì¸ê°€ìš”?")
SUMMER_SNACK_QUESTION_KEYWORDS = {
    "ì—¬ë¦„", "ê°„ì‹", "ìµœì• "
}
SUMMER_SNACK_ANSWER_VALUES = {
    "ì œì² ê³¼ì¼(ìˆ˜ë°•, ì°¸ì™¸ ë“±)": ["ìˆ˜ë°•", "ì°¸ì™¸", "ê³¼ì¼"],
    "ì•„ì´ìŠ¤í¬ë¦¼": ["ì•„ì´ìŠ¤í¬ë¦¼"],
    "ëƒ‰ë©´": ["ëƒ‰ë©´"],
    "ë¹™ìˆ˜": ["ë¹™ìˆ˜"]
}

# 61. ë•€ ë¶ˆí¸í•¨ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ë¦„ì²  ë•€ ë•Œë¬¸ì— ê²ªëŠ” ë¶ˆí¸í•¨ì€ ì–´ë–¤ ê²ƒì´ ìˆëŠ”ì§€ ëª¨ë‘ ì„ íƒí•´ì£¼ì„¸ìš”.")
SWEAT_CONCERN_QUESTION_KEYWORDS = {
    "ë•€", "ë¶ˆí¸", "ì—¬ë¦„"
}
SWEAT_CONCERN_ANSWER_VALUES = {
    "ë•€ ëƒ„ìƒˆê°€ ê±±ì •ëœë‹¤": ["ëƒ„ìƒˆ"],
    "ì˜·ì´ ì –ê±°ë‚˜ ì–¼ë£©ì§€ëŠ” ê²ƒì´ ì‹ ê²½ì“°ì¸ë‹¤": ["ì˜·", "ì–¼ë£©"],
    "ë‹¤ë¥¸ ì‚¬ëŒì˜ ë•€ ëƒ„ìƒˆê°€ ë¶ˆì¾Œí•˜ë‹¤": ["ë‹¤ë¥¸ ì‚¬ëŒ", "ë¶ˆì¾Œ"],
    "ë¨¸ë¦¬ë‚˜ ë‘í”¼ê°€ ê¸ˆë°© ê¸°ë¦„ì§„ë‹¤": ["ë‘í”¼", "ê¸°ë¦„"],
    "í”¼ë¶€ íŠ¸ëŸ¬ë¸”ì´ ìƒê¸´ë‹¤": ["íŠ¸ëŸ¬ë¸”"],
    "ë©”ì´í¬ì—…ì´ ë¬´ë„ˆì§„ë‹¤": ["ë©”ì´í¬ì—…"]
}

# 62. í–‰ë³µí•œ ë…¸ë…„ ì¡°ê±´ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì´ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•˜ëŠ” í–‰ë³µí•œ ë…¸ë…„ì˜ ì¡°ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?")
HAPPY_AGING_QUESTION_KEYWORDS = {
    "í–‰ë³µí•œ ë…¸ë…„", "ë…¸ë…„", "ì¡°ê±´", "ì¤‘ìš”"
}
HAPPY_AGING_ANSWER_VALUES = {
    "ê±´ê°•í•œ ëª¸ê³¼ ë§ˆìŒ": ["ê±´ê°•", "ëª¸", "ë§ˆìŒ"],
    "ì•ˆì •ì ì¸ ê²½ì œë ¥": ["ê²½ì œë ¥", "ì•ˆì •"],
    "ì—¬ê°€ê³¼ ì·¨ë¯¸ë¥¼ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ì‹œê°„ê³¼ ì—¬ìœ ": ["ì—¬ê°€", "ì·¨ë¯¸", "ì‹œê°„", "ì—¬ìœ "],
    "ê°€ì¡± ë˜ëŠ” ì¹œêµ¬ì™€ì˜ ì¹œë°€í•œ ê´€ê³„": ["ê°€ì¡±", "ì¹œêµ¬", "ê´€ê³„"],
    "ì‚¬íšŒì™€ì˜ ì ì ˆí•œ ì—°ê²°ê°": ["ì‚¬íšŒ", "ì—°ê²°ê°"]
}

# 63. ì—¬í–‰ ìŠ¤íƒ€ì¼ (ì‹¤ì œ ì§ˆë¬¸: "ì–´ë ¤ë¶„ì€ ì—¬í–‰ê°ˆ ë•Œ ì–´ë–¤ ìŠ¤íƒ€ì¼ì— ë” ê°€ê¹Œìš°ì‹ ê°€ìš”?")
TRAVEL_STYLE_QUESTION_KEYWORDS = {
    "ì—¬í–‰", "ìŠ¤íƒ€ì¼", "ê°€ê¹Œìš°"
}
TRAVEL_STYLE_ANSWER_VALUES = {
    "ê³„íší˜•(ì—¬í–‰ ì „ë¶€í„° ë™ì„ , ë§›ì§‘, ìˆ™ì†Œê¹Œì§€ ê¼¼ê¼¼íˆ ì¤€ë¹„)": ["ê³„íší˜•", "ê³„íš", "ê¼¼ê¼¼"],
    "ë°˜ë°˜í˜•(í° í‹€ë§Œ ì •í•˜ê³  ì„¸ë¶€ ì¼ì •ì€ í˜„ì§€ì—ì„œ ì •í•¨)": ["ë°˜ë°˜í˜•", "ë°˜ë°˜", "í° í‹€"],
    "ì¦‰í¥í˜•(ê°€ì„œ ë³´ê³  ëŠë¼ëŠ” ëŒ€ë¡œ ì›€ì§ì´ëŠ” ê±¸ ì„ í˜¸)": ["ì¦‰í¥í˜•", "ì¦‰í¥", "ëŠë¼ëŠ” ëŒ€ë¡œ"],
    "ì˜ ëª¨ë¥´ê² ë‹¤": ["ëª¨ë¥´ê² ë‹¤"]
}

# 64. ë¹„ë‹ë´‰íˆ¬ ì‚¬ìš© ì¤„ì´ê¸° (ì‹¤ì œ ì§ˆë¬¸: "í‰ì†Œ ì¼íšŒìš© ë¹„ë‹ë´‰íˆ¬ ì‚¬ìš©ì„ ì¤„ì´ê¸° ìœ„í•´ ì–´ë–¤ ë…¸ë ¥ì„ í•˜ê³  ê³„ì‹ ê°€ìš”?")
PLASTIC_BAG_REDUCTION_QUESTION_KEYWORDS = {
    "ë¹„ë‹ë´‰íˆ¬", "ì¼íšŒìš©", "ì¤„ì´ê¸°", "ë…¸ë ¥"
}
PLASTIC_BAG_REDUCTION_ANSWER_VALUES = {
    "ì¥ë°”êµ¬ë‹ˆë‚˜ ì—ì½”ë°±ì„ ì±™ê¸´ë‹¤": ["ì¥ë°”êµ¬ë‹ˆ", "ì—ì½”ë°±"],
    "ë¹„ë‹ ëŒ€ì‹  ì¢…ì´ë´‰íˆ¬ë‚˜ ë°•ìŠ¤ë¥¼ í™œìš©í•œë‹¤": ["ì¢…ì´ë´‰íˆ¬", "ë°•ìŠ¤"],
    "ì•„ì˜ˆ ì‡¼í•‘í•  ë•Œ ë´‰íˆ¬ë¥¼ ë°›ì§€ ì•ŠëŠ”ë‹¤": ["ë°›ì§€ ì•ŠëŠ”ë‹¤", "ì•„ì˜ˆ"],
    "í¸ì˜ì ì´ë‚˜ ë§ˆíŠ¸ì—ì„œ ìœ ë£Œ ë´‰íˆ¬ë¥¼ ì•„ê¹ë”ë¼ë„ ì‚°ë‹¤": ["ìœ ë£Œ ë´‰íˆ¬", "ì‚°ë‹¤"],
    "ë”°ë¡œ ë…¸ë ¥í•˜ê³  ìˆì§€ ì•Šë‹¤": ["ë…¸ë ¥í•˜ê³  ìˆì§€ ì•Šë‹¤"],
    "ê¸°íƒ€": ["ê¸°íƒ€"]
}

# 65. í¬ì¸íŠ¸ ì ë¦½ ê´€ì‹¬ë„ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì€ í• ì¸, ìºì‹œë°±, ë©¤ë²„ì‹­ ë“± í¬ì¸íŠ¸ ì ë¦½ í˜œíƒì„ ì–¼ë§ˆë‚˜ ì‹ ê²½ ì“°ì‹œë‚˜ìš”?")
REWARDS_ATTENTION_QUESTION_KEYWORDS = {
    "í• ì¸", "ìºì‹œë°±", "ë©¤ë²„ì‹­", "í¬ì¸íŠ¸", "ì ë¦½", "ì‹ ê²½"
}
REWARDS_ATTENTION_ANSWER_VALUES = {
    "ìì£¼ ì“°ëŠ” ê³³ë§Œ ì±™ê¸´ë‹¤": ["ìì£¼ ì“°ëŠ” ê³³"],
    "ë§¤ìš° ê¼¼ê¼¼í•˜ê²Œ ì±™ê¸´ë‹¤": ["ë§¤ìš° ê¼¼ê¼¼", "ê¼¼ê¼¼í•˜ê²Œ"],
    "ê°€ë” ìƒê°ë‚  ë•Œë§Œ ì±™ê¸´ë‹¤": ["ê°€ë”", "ìƒê°ë‚  ë•Œ"],
    "ê±°ì˜ ì‹ ê²½ì“°ì§€ ì•ŠëŠ”ë‹¤": ["ê±°ì˜ ì‹ ê²½ì“°ì§€"],
    "ì „í˜€ ê´€ì‹¬ ì—†ë‹¤": ["ì „í˜€ ê´€ì‹¬"]
}

# 66. ì´ˆì½œë¦¿ ì„­ì·¨ ì‹œì  (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì€ ì´ˆì½œë¦¿ì„ ì£¼ë¡œ ì–¸ì œ ë“œì‹œë‚˜ìš”?")
CHOCOLATE_TIMING_QUESTION_KEYWORDS = {
    "ì´ˆì½œë¦¿", "ì–¸ì œ", "ë“œì‹œë‚˜ìš”"
}
CHOCOLATE_TIMING_ANSWER_VALUES = {
    "ê±°ì˜ ë¨¹ì§€ ì•ŠëŠ”ë‹¤": ["ê±°ì˜ ë¨¹ì§€ ì•ŠëŠ”ë‹¤"],
    "ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ì„ ë•Œ": ["ìŠ¤íŠ¸ë ˆìŠ¤"],
    "ì„ ë¬¼ë¡œ ë°›ì•˜ì„ ë•Œ": ["ì„ ë¬¼"],
    "ê°„ì‹ìœ¼ë¡œ ìŠµê´€ì²˜ëŸ¼": ["ê°„ì‹", "ìŠµê´€"],
    "íŠ¹ë³„í•œ ë‚ (ìƒì¼, ë°œë Œíƒ€ì¸ë°ì´ ë“±)": ["íŠ¹ë³„í•œ ë‚ ", "ìƒì¼", "ë°œë Œíƒ€ì¸"],
    "ê¸°ë¶„ì´ ì¢‹ì„ ë•Œ": ["ê¸°ë¶„ì´ ì¢‹ì„ ë•Œ"],
    "ê¸°íƒ€": ["ê¸°íƒ€"]
}

# 67. ê°œì¸ì •ë³´ë³´í˜¸ ìŠµê´€ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì€ í‰ì†Œ ê°œì¸ì •ë³´ë³´í˜¸ë¥¼ ìœ„í•´ ì–´ë–¤ ìŠµê´€ì´ ìˆìœ¼ì‹ ê°€ìš”?")
PRIVACY_HABIT_QUESTION_KEYWORDS = {
    "ê°œì¸ì •ë³´", "ë³´í˜¸", "ìŠµê´€", "í‰ì†Œ"
}
PRIVACY_HABIT_ANSWER_VALUES = {
    "ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë§í¬/ì•±ì€ í´ë¦­í•˜ì§€ ì•ŠëŠ”ë‹¤": ["ë§í¬", "ì•±", "í´ë¦­í•˜ì§€ ì•ŠëŠ”ë‹¤"],
    "ì´ì¤‘ ì¸ì¦(OTP ë“±)ì„ ì„¤ì •í•œë‹¤": ["ì´ì¤‘ ì¸ì¦", "OTP"],
    "ê°œì¸ì •ë³´ ì œê³µ ë™ì˜ ì‹œ ê¼¼ê¼¼íˆ ì½ëŠ”ë‹¤": ["ë™ì˜", "ê¼¼ê¼¼íˆ"],
    "ê³µê³µ ì™€ì´íŒŒì´ ì‚¬ìš©ì„ ìì œí•œë‹¤": ["ì™€ì´íŒŒì´", "ìì œ"],
    "ë¹„ë°€ë²ˆí˜¸ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ë°”ê¾¼ë‹¤": ["ë¹„ë°€ë²ˆí˜¸", "ë°”ê¾¼ë‹¤"],
    "ë”°ë¡œ ì‹¤ì²œí•˜ëŠ” ê²Œ ì—†ë‹¤": ["ì‹¤ì²œí•˜ëŠ” ê²Œ ì—†ë‹¤"],
    "ê¸°íƒ€": ["ê¸°íƒ€"]
}

# 68. ì—¬ë¦„ íŒ¨ì…˜ í•„ìˆ˜í…œ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì´ ì ˆëŒ€ í¬ê¸°í•  ìˆ˜ ì—†ëŠ” ì—¬ë¦„ íŒ¨ì…˜ í•„ìˆ˜í…œì€ ë¬´ì—‡ì¸ê°€ìš”?")
SUMMER_FASHION_QUESTION_KEYWORDS = {
    "ì—¬ë¦„", "íŒ¨ì…˜", "í•„ìˆ˜í…œ", "í¬ê¸°í•  ìˆ˜ ì—†ëŠ”"
}
SUMMER_FASHION_ANSWER_VALUES = {
    "ë°˜ë°”ì§€": ["ë°˜ë°”ì§€"],
    "ìƒŒë“¤/ìŠ¬ë¦¬í¼": ["ìƒŒë“¤", "ìŠ¬ë¦¬í¼"],
    "ì„ ê¸€ë¼ìŠ¤": ["ì„ ê¸€ë¼ìŠ¤"],
    "ì–‡ì€ ê¸´íŒ” ì…”ì¸ ": ["ì–‡ì€ ê¸´íŒ”", "ì…”ì¸ "],
    "ì¿¨í† ì‹œ/ì¿¨ìŠ¤ì¹´í”„": ["ì¿¨í† ì‹œ", "ì¿¨ìŠ¤ì¹´í”„"],
    "ë¦°ë„¨ì…”ì¸ ": ["ë¦°ë„¨ì…”ì¸ ", "ë¦°ë„¨"],
    "ë¯¼ì†Œë§¤": ["ë¯¼ì†Œë§¤"],
    "ê¸°íƒ€": ["ê¸°íƒ€"]
}

# 69. ê°¤ëŸ¬ë¦¬ ì‚¬ì§„ ìœ í˜• (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì˜ íœ´ëŒ€í° ê°¤ëŸ¬ë¦¬ì— ê°€ì¥ ë§ì´ ì €ì¥ë˜ì–´ì ¸ ìˆëŠ” ì‚¬ì§„ì€ ë¬´ì—‡ì¸ê°€ìš”?")
GALLERY_PHOTO_QUESTION_KEYWORDS = {
    "íœ´ëŒ€í°", "ê°¤ëŸ¬ë¦¬", "ì‚¬ì§„", "ì €ì¥"
}
GALLERY_PHOTO_ANSWER_VALUES = {
    "ì¹œêµ¬/ê°€ì¡±ê³¼ì˜ ë‹¨ì²´ ì‚¬ì§„": ["ì¹œêµ¬", "ê°€ì¡±", "ë‹¨ì²´ ì‚¬ì§„"],
    "í’ê²½/ì—¬í–‰ ì‚¬ì§„": ["í’ê²½", "ì—¬í–‰ ì‚¬ì§„"],
    "ì…€ì¹´/ì¸ë¬¼ ì‚¬ì§„": ["ì…€ì¹´", "ì¸ë¬¼"],
    "ë©”ëª¨ìš© ìº¡ì²˜/ìŠ¤í¬ë¦°ìƒ·": ["ìº¡ì²˜", "ìŠ¤í¬ë¦°ìƒ·"],
    "ì—…ë¬´/í•™ì—… ê´€ë ¨ ì‚¬ì§„(ìë£Œ, í•„ê¸° ë“±)": ["ì—…ë¬´", "í•™ì—…", "í•„ê¸°"],
    "SNS/ì¸í„°ë„·ì—ì„œ ì €ì¥í•œ ì´ë¯¸ì§€": ["SNS", "ì¸í„°ë„·"],
    "ìŒì‹ ì‚¬ì§„": ["ìŒì‹ ì‚¬ì§„"],
    "ë°˜ë ¤ë™ë¬¼ ì‚¬ì§„": ["ë°˜ë ¤ë™ë¬¼"],
    "ê¸°íƒ€": ["ê¸°íƒ€"]
}

# 70. ìš°ì‚° ì—†ì„ ë•Œ í–‰ë™ (ì‹¤ì œ ì§ˆë¬¸: "ê°‘ì‘ìŠ¤ëŸ° ë¹„ë¡œ ìš°ì‚°ì´ ì—†ì„ ë•Œ ì—¬ëŸ¬ë¶„ì€ ì–´ë–»ê²Œ í•˜ì‹œë‚˜ìš”?")
RAIN_WITHOUT_UMBRELLA_QUESTION_KEYWORDS = {
    "ë¹„", "ìš°ì‚°", "ì—†ì„ ë•Œ", "ê°‘ì‘ìŠ¤ëŸ°"
}
RAIN_WITHOUT_UMBRELLA_ANSWER_VALUES = {
    "ê·¼ì²˜ ë¹„ë¥¼ í”¼í•  ìˆ˜ ìˆëŠ” ê³³ìœ¼ë¡œ ë›°ì–´ê°„ë‹¤": ["ë¹„ë¥¼ í”¼í• ", "ë›°ì–´ê°„ë‹¤"],
    "í¸ì˜ì ì—ì„œ ìš°ì‚°ì„ ì‚°ë‹¤": ["í¸ì˜ì ", "ìš°ì‚°ì„ ì‚°ë‹¤"],
    "ê·¸ëƒ¥ ë¹„ë¥¼ ë§ê³  ê°„ë‹¤": ["ë¹„ë¥¼ ë§ê³ "],
    "ê°€ì¡±/ì¹œêµ¬ ë“± ì£¼ë³€ì§€ì¸ì—ê²Œ ì—°ë½í•œë‹¤": ["ì£¼ë³€ì§€ì¸", "ì—°ë½"],
    "ê¸°íƒ€": ["ê¸°íƒ€"]
}

# 71. ë¬¼ë†€ì´ ì¥ì†Œ ì„ í˜¸ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì´ ì—¬ë¦„ì²  ë¬¼ë†€ì´ ì¥ì†Œë¡œ ê°€ì¥ ì„ í˜¸í•˜ëŠ” ê³³ì€ ì–´ë””ì…ë‹ˆê¹Œ?")
WATER_ACTIVITY_LOCATION_QUESTION_KEYWORDS = {
    "ë¬¼ë†€ì´", "ì¥ì†Œ", "ì„ í˜¸", "ì—¬ë¦„ì² "
}
WATER_ACTIVITY_LOCATION_ANSWER_VALUES = {
    "ê³„ê³¡": ["ê³„ê³¡"],
    "í•´ë³€": ["í•´ë³€"],
    "ì›Œí„°íŒŒí¬": ["ì›Œí„°íŒŒí¬"],
    "ë¬¼ë†€ì´ë¥¼ ì¢‹ì•„í•˜ì§€ ì•ŠëŠ”ë‹¤": ["ì¢‹ì•„í•˜ì§€ ì•ŠëŠ”ë‹¤"],
    "ê¸°íƒ€": ["ê¸°íƒ€"]
}

# 72. ë°˜ë ¤ë™ë¬¼ ê²½í—˜ ìƒíƒœ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì€ ë°˜ë ¤ë™ë¬¼ì„ í‚¤ìš°ëŠ” ì¤‘ì´ì‹œê±°ë‚˜ í˜¹ì€ í‚¤ì›Œë³´ì‹  ì ì´ ìˆìœ¼ì‹ ê°€ìš”?")
PET_EXPERIENCE_QUESTION_KEYWORDS = {
    "ë°˜ë ¤ë™ë¬¼", "í‚¤ìš°ëŠ”", "í‚¤ì›Œë³¸", "ì "
}
PET_EXPERIENCE_ANSWER_VALUES = {
    "ë°˜ë ¤ë™ë¬¼ì„ í‚¤ìš°ëŠ” ì¤‘ì´ë‹¤": ["í‚¤ìš°ëŠ” ì¤‘"],
    "ë°˜ë ¤ë™ë¬¼ì„ í‚¤ì›Œë³¸ ì ì´ ìˆë‹¤": ["í‚¤ì›Œë³¸ ì "],
    "ë°˜ë ¤ë™ë¬¼ì„ í‚¤ì›Œë³¸ ì ì´ ì—†ë‹¤": ["í‚¤ì›Œë³¸ ì ì´ ì—†ë‹¤", "ì—†ë‹¤"]
}

# 73. ì „í†µì‹œì¥ ë°©ë¬¸ ë¹ˆë„ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì€ ì „í†µì‹œì¥ì„ ì–¼ë§ˆë‚˜ ìì£¼ ë°©ë¬¸í•˜ì‹œë‚˜ìš”?")
TRADITIONAL_MARKET_FREQUENCY_QUESTION_KEYWORDS = {
    "ì „í†µì‹œì¥", "ì–¼ë§ˆë‚˜", "ìì£¼", "ë°©ë¬¸"
}
TRADITIONAL_MARKET_FREQUENCY_ANSWER_VALUES = {
    "ì¼ì£¼ì¼ì— 1íšŒ ì´ìƒ": ["ì¼ì£¼ì¼", "1íšŒ"],
    "2ì£¼ì— 1íšŒ ì´ìƒ": ["2ì£¼", "1íšŒ"],
    "í•œë‹¬ì— 1íšŒ ì´ìƒ": ["í•œë‹¬", "1íšŒ"],
    "3ê°œì›”ì— 1íšŒ ì´ìƒ": ["3ê°œì›”", "1íšŒ"],
    "6ê°œì›”ì— 1íšŒ ì´ìƒ": ["6ê°œì›”", "1íšŒ"],
    "1ë…„ì— 1íšŒ ì´ìƒ": ["1ë…„", "1íšŒ"],
    "ì „í˜€ ë°©ë¬¸í•˜ì§€ ì•ŠìŒ": ["ì „í˜€", "ë°©ë¬¸í•˜ì§€ ì•ŠìŒ"]
}

# 74. ìŠ¤íŠ¸ë ˆìŠ¤ ì›ì¸ (ì‹¤ì œ ì§ˆë¬¸: "ë‹¤ìŒ ì¤‘ ê°€ì¥ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë§ì´ ëŠë¼ëŠ” ìƒí™©ì€ ë¬´ì—‡ì¸ê°€ìš”?")
STRESS_SOURCE_QUESTION_KEYWORDS = {
    "ìŠ¤íŠ¸ë ˆìŠ¤", "ëŠë¼ëŠ”", "ìƒí™©", "ê°€ì¥"
}
STRESS_SOURCE_ANSWER_VALUES = {
    "ê²½ì œì  ë¬¸ì œ": ["ê²½ì œì ", "ëˆ"],
    "ì¸ê°„ê´€ê³„ (ê°€ì¡±, ì¹œêµ¬, ì§ì¥ ë“±)": ["ì¸ê°„ê´€ê³„", "ê´€ê³„", "ê°€ì¡±", "ì¹œêµ¬", "ì§ì¥"],
    "ê±´ê°• ë¬¸ì œ": ["ê±´ê°•"],
    "ì—…ë¬´ / í•™ì—…": ["ì—…ë¬´", "í•™ì—…", "ì¼", "ê³µë¶€"],
    "ì¶œí‡´ê·¼": ["ì¶œí‡´ê·¼", "í†µê·¼"],
    "ê¸°íƒ€": ["ê¸°íƒ€"]
}

# 75. ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì•± (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì€ ìš”ì¦˜ ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì•±ì€ ë¬´ì—‡ì¸ê°€ìš”?")
MOST_USED_APP_QUESTION_KEYWORDS = {
    "ì•±", "ê°€ì¥ ë§ì´", "ì‚¬ìš©", "ìš”ì¦˜"
}
MOST_USED_APP_ANSWER_VALUES = {
    "ë©”ì‹ ì € ì•± (ì¹´ì¹´ì˜¤í†¡, ë¬¸ì ë“±)": ["ë©”ì‹ ì €", "ì¹´ì¹´ì˜¤í†¡", "ë¬¸ì"],
    "ë™ì˜ìƒ ìŠ¤íŠ¸ë¦¬ë° ì•± (ìœ íŠœë¸Œ, ë„·í”Œë¦­ìŠ¤ ë“±)": ["ë™ì˜ìƒ", "ìŠ¤íŠ¸ë¦¬ë°", "ìœ íŠœë¸Œ", "ë„·í”Œë¦­ìŠ¤"],
    "ê¸ˆìœµ ì•±": ["ê¸ˆìœµ", "ì€í–‰"],
    "SNS ì•± (ì¸ìŠ¤íƒ€ê·¸ë¨, í˜ì´ìŠ¤ë¶, í‹±í†¡ ë“±)": ["SNS", "ì¸ìŠ¤íƒ€", "í˜ì´ìŠ¤ë¶", "í‹±í†¡"],
    "ìš´ë™/ê±´ê°• ì•±": ["ìš´ë™", "ê±´ê°•", "í”¼íŠ¸ë‹ˆìŠ¤"],
    "ì‡¼í•‘/ë°°ë‹¬ ì•± (ì¿ íŒ¡, ë°°ë‹¬ì˜ë¯¼ì¡±, ë¬´ì‹ ì‚¬ ë“±)": ["ì‡¼í•‘", "ë°°ë‹¬", "ì¿ íŒ¡"],
    "ê²Œì„ ì•±": ["ê²Œì„"],
    "ê¸°íƒ€": ["ê¸°íƒ€"]
}

# 76. ì²´ë ¥ ê´€ë¦¬ í™œë™ ì¢…ë¥˜ (ì‹¤ì œ ì§ˆë¬¸: "ì—¬ëŸ¬ë¶„ì€ í‰ì†Œ ì²´ë ¥ ê´€ë¦¬ë¥¼ ìœ„í•´ ì–´ë–¤ í™œë™ì„ í•˜ê³  ê³„ì‹ ê°€ìš”?")
EXERCISE_TYPE_QUESTION_KEYWORDS = {
    "ì²´ë ¥ ê´€ë¦¬", "ìš´ë™", "í™œë™", "í‰ì†Œ"
}
EXERCISE_TYPE_ANSWER_VALUES = {
    "ë‹¬ë¦¬ê¸°/ê±·ê¸°": ["ë‹¬ë¦¬ê¸°", "ê±·ê¸°", "ëŸ¬ë‹", "ì›Œí‚¹"],
    "í™ˆíŠ¸ë ˆì´ë‹": ["í™ˆíŠ¸", "í™ˆíŠ¸ë ˆì´ë‹"],
    "í—¬ìŠ¤": ["í—¬ìŠ¤", "ì›¨ì´íŠ¸"],
    "ë“±ì‚°": ["ë“±ì‚°", "ì‚°"],
    "ìì „ê±° íƒ€ê¸°": ["ìì „ê±°", "ì‚¬ì´í´"],
    "ìš”ê°€/í•„ë¼í…ŒìŠ¤": ["ìš”ê°€", "í•„ë¼í…ŒìŠ¤"],
    "ìŠ¤í¬ì¸ (ì¶•êµ¬, ë°°ë“œë¯¼í„´ ë“±)": ["ìŠ¤í¬ì¸ ", "ì¶•êµ¬", "ë°°ë“œë¯¼í„´"],
    "ìˆ˜ì˜": ["ìˆ˜ì˜"],
    "ì²´ë ¥ê´€ë¦¬ë¥¼ ìœ„í•´ í•˜ê³  ìˆëŠ” í™œë™ì´ ì—†ë‹¤": ["í™œë™ì´ ì—†ë‹¤", "í•˜ê³  ìˆì§€ ì•Šë‹¤"],
    "ê¸°íƒ€": ["ê¸°íƒ€"]
}

# 77. ë¹ ë¥¸ ë°°ì†¡ìœ¼ë¡œ êµ¬ë§¤í•˜ëŠ” ì œí’ˆ (ì‹¤ì œ ì§ˆë¬¸: "ë¹ ë¥¸ ë°°ì†¡(ë‹¹ì¼Â·ìƒˆë²½Â·ì§ì§„ ë°°ì†¡) ì„œë¹„ìŠ¤ë¥¼ ì£¼ë¡œ ì–´ë–¤ ì œí’ˆì„ êµ¬ë§¤í•  ë•Œ ì´ìš©í•˜ì‹œë‚˜ìš”?")
FAST_DELIVERY_PRODUCT_QUESTION_KEYWORDS = {
    "ë¹ ë¥¸ ë°°ì†¡", "ë‹¹ì¼", "ìƒˆë²½", "ì œí’ˆ", "êµ¬ë§¤"
}
FAST_DELIVERY_PRODUCT_ANSWER_VALUES = {
    "ì‹ ì„ ì‹í’ˆ(ê³¼ì¼, ì±„ì†Œ, ìœ¡ë¥˜ ë“±)": ["ì‹ ì„ ì‹í’ˆ", "ê³¼ì¼", "ì±„ì†Œ", "ìœ¡ë¥˜"],
    "ìƒí™œìš©í’ˆ(ìƒí•„í’ˆ, ìœ„ìƒìš©í’ˆ ë“±)": ["ìƒí™œìš©í’ˆ", "ìƒí•„í’ˆ", "ìœ„ìƒìš©í’ˆ"],
    "íŒ¨ì…˜Â·ë·°í‹° ì œí’ˆ": ["íŒ¨ì…˜", "ë·°í‹°", "í™”ì¥í’ˆ"],
    "ì „ìê¸°ê¸° ë° ê°€ì „ì œí’ˆ": ["ì „ìê¸°ê¸°", "ê°€ì „"],
    "ë¹ ë¥¸ ë°°ì†¡ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•´ ë³¸ ì  ì—†ë‹¤": ["ì´ìš©í•´ ë³¸ ì  ì—†ë‹¤", "ì—†ë‹¤"],
    "ê¸°íƒ€": ["ê¸°íƒ€"]
}

# â­ ë²”ìš© Behavioral í‚¤ì›Œë“œ ë§¤í•‘ (í™•ì¥ ê°€ëŠ¥)
BEHAVIORAL_KEYWORD_MAP = {
    'smoker': {
        'question_keywords': SMOKER_QUESTION_KEYWORDS,
        'positive_keywords': SMOKER_POSITIVE_KEYWORDS,
        'negative_keywords': SMOKER_NEGATIVE_KEYWORDS
    },
    'has_vehicle': {
        'question_keywords': VEHICLE_QUESTION_KEYWORDS,
        'positive_keywords': BEHAVIOR_YES_TOKENS,
        'negative_keywords': BEHAVIOR_NO_TOKENS
    },
    'drinker': {
        'question_keywords': ALCOHOL_QUESTION_KEYWORDS,
        'positive_keywords': DRINKER_POSITIVE_KEYWORDS,
        'negative_keywords': NON_DRINKER_KEYWORDS
    },
    'ott_user': {
        'question_keywords': OTT_QUESTION_KEYWORDS,
        'positive_keywords': OTT_POSITIVE_KEYWORDS,
        'negative_keywords': OTT_NEGATIVE_KEYWORDS
    },
    'has_pet': {
        'question_keywords': PET_QUESTION_KEYWORDS,
        'positive_keywords': PET_POSITIVE_KEYWORDS,
        'negative_keywords': PET_NEGATIVE_KEYWORDS
    },
    'exercises': {
        'question_keywords': EXERCISE_QUESTION_KEYWORDS,
        'positive_keywords': EXERCISE_POSITIVE_KEYWORDS,
        'negative_keywords': EXERCISE_NEGATIVE_KEYWORDS
    },
    'uses_fast_delivery': {
        'question_keywords': FAST_DELIVERY_QUESTION_KEYWORDS,
        'positive_keywords': FAST_DELIVERY_POSITIVE_KEYWORDS,
        'negative_keywords': FAST_DELIVERY_NEGATIVE_KEYWORDS
    },
    'visits_traditional_market': {
        'question_keywords': TRADITIONAL_MARKET_QUESTION_KEYWORDS,
        'positive_keywords': TRADITIONAL_MARKET_POSITIVE_KEYWORDS,
        'negative_keywords': TRADITIONAL_MARKET_NEGATIVE_KEYWORDS
    },
    'has_stress': {
        'question_keywords': STRESS_QUESTION_KEYWORDS,
        'positive_keywords': STRESS_POSITIVE_KEYWORDS,
        'negative_keywords': STRESS_NEGATIVE_KEYWORDS
    },
    'travels': {
        'question_keywords': TRAVEL_QUESTION_KEYWORDS,
        'positive_keywords': TRAVEL_POSITIVE_KEYWORDS,
        'negative_keywords': TRAVEL_NEGATIVE_KEYWORDS
    },
    'drinks_coffee': {
        'question_keywords': COFFEE_QUESTION_KEYWORDS,
        'positive_keywords': COFFEE_POSITIVE_KEYWORDS,
        'negative_keywords': COFFEE_NEGATIVE_KEYWORDS
    },
    'has_subscription': {
        'question_keywords': SUBSCRIPTION_QUESTION_KEYWORDS,
        'positive_keywords': SUBSCRIPTION_POSITIVE_KEYWORDS,
        'negative_keywords': SUBSCRIPTION_NEGATIVE_KEYWORDS
    },
    'uses_social_media': {
        'question_keywords': SOCIAL_MEDIA_QUESTION_KEYWORDS,
        'positive_keywords': SOCIAL_MEDIA_POSITIVE_KEYWORDS,
        'negative_keywords': SOCIAL_MEDIA_NEGATIVE_KEYWORDS
    },
    'plays_games': {
        'question_keywords': GAMING_QUESTION_KEYWORDS,
        'positive_keywords': GAMING_POSITIVE_KEYWORDS,
        'negative_keywords': GAMING_NEGATIVE_KEYWORDS
    },
    'watches_movies_dramas': {
        'question_keywords': MOVIE_DRAMA_QUESTION_KEYWORDS,
        'positive_keywords': MOVIE_DRAMA_POSITIVE_KEYWORDS,
        'negative_keywords': MOVIE_DRAMA_NEGATIVE_KEYWORDS
    },
    'uses_financial_services': {
        'question_keywords': FINANCIAL_SERVICE_QUESTION_KEYWORDS,
        'positive_keywords': FINANCIAL_SERVICE_POSITIVE_KEYWORDS,
        'negative_keywords': FINANCIAL_SERVICE_NEGATIVE_KEYWORDS
    },
    'uses_beauty_products': {
        'question_keywords': BEAUTY_QUESTION_KEYWORDS,
        'positive_keywords': BEAUTY_POSITIVE_KEYWORDS,
        'negative_keywords': BEAUTY_NEGATIVE_KEYWORDS
    },
    'shops_fashion': {
        'question_keywords': FASHION_QUESTION_KEYWORDS,
        'positive_keywords': FASHION_POSITIVE_KEYWORDS,
        'negative_keywords': FASHION_NEGATIVE_KEYWORDS
    },
    'interested_in_home_appliances': {
        'question_keywords': HOME_APPLIANCE_QUESTION_KEYWORDS,
        'positive_keywords': HOME_APPLIANCE_POSITIVE_KEYWORDS,
        'negative_keywords': HOME_APPLIANCE_NEGATIVE_KEYWORDS
    },
    'uses_smart_devices': {
        'question_keywords': SMART_DEVICE_QUESTION_KEYWORDS,
        'positive_keywords': SMART_DEVICE_POSITIVE_KEYWORDS,
        'negative_keywords': SMART_DEVICE_NEGATIVE_KEYWORDS
    },
    'cares_about_environment': {
        'question_keywords': ENVIRONMENT_QUESTION_KEYWORDS,
        'positive_keywords': ENVIRONMENT_POSITIVE_KEYWORDS,
        'negative_keywords': ENVIRONMENT_NEGATIVE_KEYWORDS
    },
    'does_charity': {
        'question_keywords': CHARITY_QUESTION_KEYWORDS,
        'positive_keywords': CHARITY_POSITIVE_KEYWORDS,
        'negative_keywords': CHARITY_NEGATIVE_KEYWORDS
    },
    'interested_in_cars': {
        'question_keywords': CAR_INTEREST_QUESTION_KEYWORDS,
        'positive_keywords': CAR_INTEREST_POSITIVE_KEYWORDS,
        'negative_keywords': CAR_INTEREST_NEGATIVE_KEYWORDS
    },
    'uses_parcel_delivery': {
        'question_keywords': PARCEL_DELIVERY_QUESTION_KEYWORDS,
        'positive_keywords': PARCEL_DELIVERY_POSITIVE_KEYWORDS,
        'negative_keywords': PARCEL_DELIVERY_NEGATIVE_KEYWORDS
    },
    'dines_out': {
        'question_keywords': DINING_OUT_QUESTION_KEYWORDS,
        'positive_keywords': DINING_OUT_POSITIVE_KEYWORDS,
        'negative_keywords': DINING_OUT_NEGATIVE_KEYWORDS
    },
    'attends_drinking_gatherings': {
        'question_keywords': DRINKING_GATHERING_QUESTION_KEYWORDS,
        'positive_keywords': DRINKING_GATHERING_POSITIVE_KEYWORDS,
        'negative_keywords': DRINKING_GATHERING_NEGATIVE_KEYWORDS
    },
    # â­ ì‹ ê·œ Behavioral íŒ¨í„´ (ì„¤ë¬¸ ë°ì´í„° ë¶„ì„ ê¸°ë°˜)
    'cares_about_rewards': {
        'question_keywords': REWARDS_QUESTION_KEYWORDS,
        'positive_keywords': REWARDS_POSITIVE_KEYWORDS,
        'negative_keywords': REWARDS_NEGATIVE_KEYWORDS
    },
    'uses_secondhand_market': {
        'question_keywords': SECONDHAND_MARKET_QUESTION_KEYWORDS,
        'positive_keywords': SECONDHAND_MARKET_POSITIVE_KEYWORDS,
        'negative_keywords': SECONDHAND_MARKET_NEGATIVE_KEYWORDS
    },
    'lifestyle_minimalist': {
        'question_keywords': MINIMALIST_QUESTION_KEYWORDS,
        'positive_keywords': MINIMALIST_POSITIVE_KEYWORDS,
        'negative_keywords': MINIMALIST_NEGATIVE_KEYWORDS
    },
    'privacy_conscious': {
        'question_keywords': PRIVACY_QUESTION_KEYWORDS,
        'positive_keywords': PRIVACY_POSITIVE_KEYWORDS,
        'negative_keywords': PRIVACY_NEGATIVE_KEYWORDS
    },
    'stress_relief_method': {
        'question_keywords': STRESS_RELIEF_QUESTION_KEYWORDS,
        # stress_relief_methodëŠ” íŠ¹ë³„ ì²˜ë¦¬ í•„ìš” (ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜)
        'positive_keywords': (
            STRESS_RELIEF_ACTIVE_KEYWORDS |
            STRESS_RELIEF_ENTERTAINMENT_KEYWORDS |
            STRESS_RELIEF_SOCIAL_KEYWORDS |
            STRESS_RELIEF_RELAXATION_KEYWORDS |
            STRESS_RELIEF_SHOPPING_KEYWORDS
        ),
        'negative_keywords': STRESS_RELIEF_NEGATIVE_KEYWORDS
    },
    # â­ ì‹ ê·œ: ê²¨ìš¸ë°©í•™ ì¶”ì–µ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'winter_vacation_memory': {
        'question_text': 'ì´ˆë“±í•™ìƒ ì‹œì ˆ ê²¨ìš¸ë°©í•™ ë•Œ ê°€ì¥ ê¸°ì–µì— ë‚¨ëŠ” ì¼ì€ ë¬´ì—‡ì¸ê°€ìš”?',
        'question_keywords': WINTER_VACATION_QUESTION_KEYWORDS,
        'answer_values': WINTER_VACATION_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: í”¼ë¶€ ìƒíƒœ ë§Œì¡±ë„ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'skin_satisfaction': {
        'question_text': 'í˜„ì¬ ë³¸ì¸ì˜ í”¼ë¶€ ìƒíƒœì— ì–¼ë§ˆë‚˜ ë§Œì¡±í•˜ì‹œë‚˜ìš”?',
        'question_keywords': SKIN_SATISFACTION_QUESTION_KEYWORDS,
        'answer_values': SKIN_SATISFACTION_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: AI ì„œë¹„ìŠ¤ í™œìš© ë¶„ì•¼ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'ai_service_field': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì€ ìš”ì¦˜ ì–´ë–¤ ë¶„ì•¼ì—ì„œ AI ì„œë¹„ìŠ¤ë¥¼ í™œìš©í•˜ê³  ê³„ì‹ ê°€ìš”?',
        'question_keywords': AI_SERVICE_FIELD_QUESTION_KEYWORDS,
        'answer_values': AI_SERVICE_FIELD_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ê¸°ë¶„ ì¢‹ì€ ì†Œë¹„ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'happy_consumption': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì€ ë³¸ì¸ì„ ìœ„í•´ ì†Œë¹„í•˜ëŠ” ê²ƒ ì¤‘ ê°€ì¥ ê¸°ë¶„ ì¢‹ì•„ì§€ëŠ” ì†Œë¹„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?',
        'question_keywords': HAPPY_CONSUMPTION_QUESTION_KEYWORDS,
        'answer_values': HAPPY_CONSUMPTION_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: AI ì±—ë´‡ ì„œë¹„ìŠ¤ ì¢…ë¥˜ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'ai_chatbot_service': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì´ ì‚¬ìš©í•´ ë³¸ AI ì±—ë´‡ ì„œë¹„ìŠ¤ëŠ” ë¬´ì—‡ì¸ê°€ìš”?',
        'question_keywords': AI_CHATBOT_SERVICE_QUESTION_KEYWORDS,
        'answer_values': AI_CHATBOT_SERVICE_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: í•´ì™¸ì—¬í–‰ ì„ í˜¸ ì§€ì—­ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'overseas_travel_preference': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì€ ì˜¬í•´ í•´ì™¸ì—¬í–‰ì„ ê°„ë‹¤ë©´ ì–´ë””ë¡œ ê°€ê³  ì‹¶ë‚˜ìš”?',
        'question_keywords': OVERSEAS_TRAVEL_QUESTION_KEYWORDS,
        'answer_values': OVERSEAS_TRAVEL_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: OTT ì„œë¹„ìŠ¤ ê°œìˆ˜ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'ott_count': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì´ í˜„ì¬ ì´ìš© ì¤‘ì¸ OTT ì„œë¹„ìŠ¤ëŠ” ëª‡ ê°œì¸ê°€ìš”?',
        'question_keywords': OTT_COUNT_QUESTION_KEYWORDS,
        'answer_values': OTT_COUNT_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ë¬¼ê±´ ì²˜ë¶„ ë°©ë²• (ë¬¸ìì—´ ê°’ ì €ì¥)
    'disposal_method': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì€ ì“°ì§€ ì•ŠëŠ” ë¬¼ê±´ì„ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ì‹œë‚˜ìš”?',
        'question_keywords': DISPOSAL_METHOD_QUESTION_KEYWORDS,
        'answer_values': DISPOSAL_METHOD_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ì´ì‚¬ ì‹œ ìŠ¤íŠ¸ë ˆìŠ¤ í¬ì¸íŠ¸ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'moving_stress': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì€ ì´ì‚¬í•  ë•Œ ê°€ì¥ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ëŠ” ë¶€ë¶„ì€ ë¬´ì—‡ì¸ê°€ìš”?',
        'question_keywords': MOVING_STRESS_QUESTION_KEYWORDS,
        'answer_values': MOVING_STRESS_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ì„¤ë‚  ì„ ë¬¼ ì„ í˜¸ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'lunar_gift_preference': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì€ ì„¤ë‚  ì„ ë¬¼ë¡œ ë°›ê³  ì‹¶ì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?',
        'question_keywords': LUNAR_GIFT_QUESTION_KEYWORDS,
        'answer_values': LUNAR_GIFT_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: í”¼ë¶€ ê´€ë¦¬ ì§€ì¶œ ìˆ˜ì¤€ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'skincare_spending': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì€ í”¼ë¶€ ê´€ë¦¬ì— ì–¼ë§ˆë‚˜ ì§€ì¶œí•˜ì‹œë‚˜ìš”?',
        'question_keywords': SKINCARE_SPENDING_QUESTION_KEYWORDS,
        'answer_values': SKINCARE_SPENDING_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” AI ì±—ë´‡ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'ai_chatbot_primary': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì´ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” AI ì±—ë´‡ì€ ë¬´ì—‡ì¸ê°€ìš”?',
        'question_keywords': AI_CHATBOT_PRIMARY_QUESTION_KEYWORDS,
        'answer_values': AI_CHATBOT_PRIMARY_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ìŠ¤í‚¨ì¼€ì–´ ì œí’ˆ êµ¬ë§¤ ì‹œ ìš°ì„ ìˆœìœ„ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'skincare_priority': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì€ ìŠ¤í‚¨ì¼€ì–´ ì œí’ˆì„ êµ¬ë§¤í•  ë•Œ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?',
        'question_keywords': SKINCARE_PRIORITY_QUESTION_KEYWORDS,
        'answer_values': SKINCARE_PRIORITY_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ì•¼ì‹ ë¨¹ëŠ” ë°©ë²• (ë¬¸ìì—´ ê°’ ì €ì¥)
    'late_night_snack_method': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì€ ì•¼ì‹ì„ ë¨¹ì„ ë•Œ ì£¼ë¡œ ì–´ë–¤ ë°©ë²•ìœ¼ë¡œ ë¨¹ë‚˜ìš”?',
        'question_keywords': LATE_NIGHT_SNACK_QUESTION_KEYWORDS,
        'answer_values': LATE_NIGHT_SNACK_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ìµœê·¼ ì†Œë¹„ ì¹´í…Œê³ ë¦¬ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'recent_spending_category': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì´ ìµœê·¼ ê°€ì¥ ë§ì´ ì†Œë¹„í•œ ì¹´í…Œê³ ë¦¬ëŠ” ë¬´ì—‡ì¸ê°€ìš”?',
        'question_keywords': RECENT_SPENDING_QUESTION_KEYWORDS,
        'answer_values': RECENT_SPENDING_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: í˜¼ë°¥ ë¹ˆë„ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'solo_dining_frequency': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì€ ì–¼ë§ˆë‚˜ ìì£¼ í˜¼ì ì‹ì‚¬ë¥¼ í•˜ì‹œë‚˜ìš”?',
        'question_keywords': SOLO_DINING_QUESTION_KEYWORDS,
        'answer_values': SOLO_DINING_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: íš¨ê³¼ì ì¸ ë‹¤ì´ì–´íŠ¸ ë°©ë²• (ë¬¸ìì—´ ê°’ ì €ì¥)
    'diet_method': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì—ê²Œ ê°€ì¥ íš¨ê³¼ì ì¸ ë‹¤ì´ì–´íŠ¸ ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?',
        'question_keywords': DIET_METHOD_QUESTION_KEYWORDS,
        'answer_values': DIET_METHOD_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ì•ŒëŒ ì„¤ì • ìŠ¤íƒ€ì¼ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'alarm_style': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì€ ì•„ì¹¨ì— ì¼ì–´ë‚  ë•Œ ì•ŒëŒì„ ì–´ë–»ê²Œ ì„¤ì •í•˜ì‹œë‚˜ìš”?',
        'question_keywords': ALARM_STYLE_QUESTION_KEYWORDS,
        'answer_values': ALARM_STYLE_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ì—¬ë¦„ì²  ê³ ë¯¼ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'summer_concern': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì€ ì—¬ë¦„ì² ì— ê°€ì¥ ê³ ë¯¼ë˜ëŠ” ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?',
        'question_keywords': SUMMER_CONCERN_QUESTION_KEYWORDS,
        'answer_values': SUMMER_CONCERN_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ì—¬ë¦„ ê°„ì‹ ì„ í˜¸ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'summer_snack': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì´ ì—¬ë¦„ì— ì¦ê²¨ ë¨¹ëŠ” ê°„ì‹ì€ ë¬´ì—‡ì¸ê°€ìš”?',
        'question_keywords': SUMMER_SNACK_QUESTION_KEYWORDS,
        'answer_values': SUMMER_SNACK_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ë•€ ê³ ë¯¼ ë¶€ìœ„ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'sweat_concern': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì€ ë•€ ë•Œë¬¸ì— ê³ ë¯¼ì´ ë˜ëŠ” ë¶€ìœ„ê°€ ìˆë‚˜ìš”?',
        'question_keywords': SWEAT_CONCERN_QUESTION_KEYWORDS,
        'answer_values': SWEAT_CONCERN_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: í–‰ë³µí•œ ë…¸ë…„ ì¡°ê±´ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'happy_aging_condition': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì´ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•˜ëŠ” í–‰ë³µí•œ ë…¸ë…„ì˜ ì¡°ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?',
        'question_keywords': HAPPY_AGING_QUESTION_KEYWORDS,
        'answer_values': HAPPY_AGING_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ì—¬í–‰ ìŠ¤íƒ€ì¼ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'travel_style': {
        'question_text': 'ì–´ë ¤ë¶„ì€ ì—¬í–‰ê°ˆ ë•Œ ì–´ë–¤ ìŠ¤íƒ€ì¼ì— ë” ê°€ê¹Œìš°ì‹ ê°€ìš”?',
        'question_keywords': TRAVEL_STYLE_QUESTION_KEYWORDS,
        'answer_values': TRAVEL_STYLE_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ë¹„ë‹ë´‰íˆ¬ ì‚¬ìš© ì¤„ì´ê¸° (ë¬¸ìì—´ ê°’ ì €ì¥)
    'plastic_bag_reduction': {
        'question_text': 'í‰ì†Œ ì¼íšŒìš© ë¹„ë‹ë´‰íˆ¬ ì‚¬ìš©ì„ ì¤„ì´ê¸° ìœ„í•´ ì–´ë–¤ ë…¸ë ¥ì„ í•˜ê³  ê³„ì‹ ê°€ìš”?',
        'question_keywords': PLASTIC_BAG_REDUCTION_QUESTION_KEYWORDS,
        'answer_values': PLASTIC_BAG_REDUCTION_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: í¬ì¸íŠ¸ ì ë¦½ ê´€ì‹¬ë„ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'rewards_attention': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì€ í• ì¸, ìºì‹œë°±, ë©¤ë²„ì‹­ ë“± í¬ì¸íŠ¸ ì ë¦½ í˜œíƒì„ ì–¼ë§ˆë‚˜ ì‹ ê²½ ì“°ì‹œë‚˜ìš”?',
        'question_keywords': REWARDS_ATTENTION_QUESTION_KEYWORDS,
        'answer_values': REWARDS_ATTENTION_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ì´ˆì½œë¦¿ ì„­ì·¨ ì‹œì  (ë¬¸ìì—´ ê°’ ì €ì¥)
    'chocolate_timing': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì€ ì´ˆì½œë¦¿ì„ ì£¼ë¡œ ì–¸ì œ ë“œì‹œë‚˜ìš”?',
        'question_keywords': CHOCOLATE_TIMING_QUESTION_KEYWORDS,
        'answer_values': CHOCOLATE_TIMING_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ê°œì¸ì •ë³´ë³´í˜¸ ìŠµê´€ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'privacy_protection_habit': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì€ í‰ì†Œ ê°œì¸ì •ë³´ë³´í˜¸ë¥¼ ìœ„í•´ ì–´ë–¤ ìŠµê´€ì´ ìˆìœ¼ì‹ ê°€ìš”?',
        'question_keywords': PRIVACY_HABIT_QUESTION_KEYWORDS,
        'answer_values': PRIVACY_HABIT_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ì—¬ë¦„ íŒ¨ì…˜ í•„ìˆ˜í…œ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'summer_fashion_essential': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì´ ì ˆëŒ€ í¬ê¸°í•  ìˆ˜ ì—†ëŠ” ì—¬ë¦„ íŒ¨ì…˜ í•„ìˆ˜í…œì€ ë¬´ì—‡ì¸ê°€ìš”?',
        'question_keywords': SUMMER_FASHION_QUESTION_KEYWORDS,
        'answer_values': SUMMER_FASHION_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ê°¤ëŸ¬ë¦¬ ì‚¬ì§„ ìœ í˜• (ë¬¸ìì—´ ê°’ ì €ì¥)
    'gallery_photo_type': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì˜ íœ´ëŒ€í° ê°¤ëŸ¬ë¦¬ì— ê°€ì¥ ë§ì´ ì €ì¥ë˜ì–´ì ¸ ìˆëŠ” ì‚¬ì§„ì€ ë¬´ì—‡ì¸ê°€ìš”?',
        'question_keywords': GALLERY_PHOTO_QUESTION_KEYWORDS,
        'answer_values': GALLERY_PHOTO_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ìš°ì‚° ì—†ì„ ë•Œ í–‰ë™ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'rain_without_umbrella': {
        'question_text': 'ê°‘ì‘ìŠ¤ëŸ° ë¹„ë¡œ ìš°ì‚°ì´ ì—†ì„ ë•Œ ì—¬ëŸ¬ë¶„ì€ ì–´ë–»ê²Œ í•˜ì‹œë‚˜ìš”?',
        'question_keywords': RAIN_WITHOUT_UMBRELLA_QUESTION_KEYWORDS,
        'answer_values': RAIN_WITHOUT_UMBRELLA_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ë¬¼ë†€ì´ ì¥ì†Œ ì„ í˜¸ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'water_activity_location': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì´ ì—¬ë¦„ì²  ë¬¼ë†€ì´ ì¥ì†Œë¡œ ê°€ì¥ ì„ í˜¸í•˜ëŠ” ê³³ì€ ì–´ë””ì…ë‹ˆê¹Œ?',
        'question_keywords': WATER_ACTIVITY_LOCATION_QUESTION_KEYWORDS,
        'answer_values': WATER_ACTIVITY_LOCATION_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ë°˜ë ¤ë™ë¬¼ ê²½í—˜ ìƒíƒœ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'pet_experience': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì€ ë°˜ë ¤ë™ë¬¼ì„ í‚¤ìš°ëŠ” ì¤‘ì´ì‹œê±°ë‚˜ í˜¹ì€ í‚¤ì›Œë³´ì‹  ì ì´ ìˆìœ¼ì‹ ê°€ìš”?',
        'question_keywords': PET_EXPERIENCE_QUESTION_KEYWORDS,
        'answer_values': PET_EXPERIENCE_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ì „í†µì‹œì¥ ë°©ë¬¸ ë¹ˆë„ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'traditional_market_frequency': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì€ ì „í†µì‹œì¥ì„ ì–¼ë§ˆë‚˜ ìì£¼ ë°©ë¬¸í•˜ì‹œë‚˜ìš”?',
        'question_keywords': TRADITIONAL_MARKET_FREQUENCY_QUESTION_KEYWORDS,
        'answer_values': TRADITIONAL_MARKET_FREQUENCY_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ìŠ¤íŠ¸ë ˆìŠ¤ ì›ì¸ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'stress_source': {
        'question_text': 'ë‹¤ìŒ ì¤‘ ê°€ì¥ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë§ì´ ëŠë¼ëŠ” ìƒí™©ì€ ë¬´ì—‡ì¸ê°€ìš”?',
        'question_keywords': STRESS_SOURCE_QUESTION_KEYWORDS,
        'answer_values': STRESS_SOURCE_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì•± (ë¬¸ìì—´ ê°’ ì €ì¥)
    'most_used_app': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì€ ìš”ì¦˜ ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì•±ì€ ë¬´ì—‡ì¸ê°€ìš”?',
        'question_keywords': MOST_USED_APP_QUESTION_KEYWORDS,
        'answer_values': MOST_USED_APP_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ì²´ë ¥ ê´€ë¦¬ í™œë™ ì¢…ë¥˜ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'exercise_type': {
        'question_text': 'ì—¬ëŸ¬ë¶„ì€ í‰ì†Œ ì²´ë ¥ ê´€ë¦¬ë¥¼ ìœ„í•´ ì–´ë–¤ í™œë™ì„ í•˜ê³  ê³„ì‹ ê°€ìš”?',
        'question_keywords': EXERCISE_TYPE_QUESTION_KEYWORDS,
        'answer_values': EXERCISE_TYPE_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    },
    # â­ ì‹ ê·œ: ë¹ ë¥¸ ë°°ì†¡ìœ¼ë¡œ êµ¬ë§¤í•˜ëŠ” ì œí’ˆ (ë¬¸ìì—´ ê°’ ì €ì¥)
    'fast_delivery_product': {
        'question_text': 'ë¹ ë¥¸ ë°°ì†¡(ë‹¹ì¼Â·ìƒˆë²½Â·ì§ì§„ ë°°ì†¡) ì„œë¹„ìŠ¤ë¥¼ ì£¼ë¡œ ì–´ë–¤ ì œí’ˆì„ êµ¬ë§¤í•  ë•Œ ì´ìš©í•˜ì‹œë‚˜ìš”?',
        'question_keywords': FAST_DELIVERY_PRODUCT_QUESTION_KEYWORDS,
        'answer_values': FAST_DELIVERY_PRODUCT_ANSWER_VALUES,
        'positive_keywords': set(),
        'negative_keywords': set()
    }
}


def extract_all_behaviors_batch(qa_pairs: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    âš¡âš¡âš¡ ì´ˆê³ ì† ìµœì í™”: qa_pairsë¥¼ í•œ ë²ˆë§Œ ìˆœíšŒí•˜ì—¬ ëª¨ë“  behavioral íŒ¨í„´ì„ ë™ì‹œì— ì¶”ì¶œ

    ìµœì í™” ê¸°ë²•:
    1. SequenceMatcher ì œê±° (100ë°° ê°œì„ ) âš¡
    2. í•´ì‹œë§µ ì „ì²˜ë¦¬ O(1) ì¡°íšŒ (10ë°° ê°œì„ ) âš¡
    3. Early termination (2-3ë°° ê°œì„ ) âš¡

    Args:
        qa_pairs: list of dict (ê° dictëŠ” ì§ˆë¬¸/ë‹µë³€ ìŒ)

    Returns:
        Dict[behavior_key, value] - ëª¨ë“  behavioral íŒ¨í„´ì˜ ê°’
    """
    # âš¡ ìµœì í™” 1: í•´ì‹œë§µ ì „ì²˜ë¦¬ (question_text â†’ behavior_key)
    question_to_behavior = {
        config['question_text']: behavior_key
        for behavior_key, config in BEHAVIORAL_KEYWORD_MAP.items()
        if config.get('question_text')
    }

    # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
    behavioral_values = {}
    total_patterns = len(BEHAVIORAL_KEYWORD_MAP)

    # qa_pairsê°€ ë¹„ì–´ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
    if not qa_pairs:
        return behavioral_values

    # qa_pairs ìˆœíšŒ (ë‹¨ í•œ ë²ˆ!)
    for qa in qa_pairs:
        if not isinstance(qa, dict):
            continue

        q_text = str(qa.get("q_text", ""))
        q_text_lower = q_text.lower()
        answer = qa.get("answer") or qa.get("answer_text")

        if not answer:
            continue

        answer_text = str(answer).lower()

        # âš¡ ìµœì í™” 2: í•´ì‹œë§µìœ¼ë¡œ O(1) ì¡°íšŒ (question_textê°€ ìˆëŠ” ê²½ìš°)
        behavior_key = question_to_behavior.get(q_text)
        if behavior_key and behavioral_values.get(behavior_key) is None:
            config = BEHAVIORAL_KEYWORD_MAP[behavior_key]
            answer_values = config.get('answer_values')

            # ë‹µë³€ ê°’ ì¶”ì¶œ
            if answer_values:
                # String íŒ¨í„´
                matched_value = None
                max_match_count = 0

                for value_name, keywords in answer_values.items():
                    match_count = sum(1 for kw in keywords if kw.lower() in answer_text)
                    if match_count > max_match_count:
                        max_match_count = match_count
                        matched_value = value_name

                if matched_value:
                    behavioral_values[behavior_key] = matched_value
            else:
                # Boolean íŒ¨í„´
                positive_kw = config.get('positive_keywords', set())
                negative_kw = config.get('negative_keywords', set())

                if positive_kw and any(kw.lower() in answer_text for kw in positive_kw):
                    behavioral_values[behavior_key] = True
                elif negative_kw and any(kw.lower() in answer_text for kw in negative_kw):
                    behavioral_values[behavior_key] = False

        # Fallback: question_keywords ë§¤ì¹­ (question_text ì—†ëŠ” íŒ¨í„´ìš©)
        for behavior_key, config in BEHAVIORAL_KEYWORD_MAP.items():
            # ì´ë¯¸ ê°’ì´ ì¶”ì¶œëœ ê²½ìš° ìŠ¤í‚µ
            if behavioral_values.get(behavior_key) is not None:
                continue

            # question_textê°€ ìˆëŠ” íŒ¨í„´ì€ ì´ë¯¸ ì²˜ë¦¬ë¨
            if config.get('question_text'):
                continue

            question_keywords = config.get('question_keywords', set())
            answer_values = config.get('answer_values')

            # Question keywords ë§¤ì¹­
            is_matched = False
            if question_keywords:
                for kw in question_keywords:
                    if kw.lower() in q_text_lower:
                        is_matched = True
                        break

            if not is_matched:
                continue

            # ë‹µë³€ ê°’ ì¶”ì¶œ
            if answer_values:
                # String íŒ¨í„´
                matched_value = None
                max_match_count = 0

                for value_name, keywords in answer_values.items():
                    match_count = sum(1 for kw in keywords if kw.lower() in answer_text)
                    if match_count > max_match_count:
                        max_match_count = match_count
                        matched_value = value_name

                if matched_value:
                    behavioral_values[behavior_key] = matched_value
            else:
                # Boolean íŒ¨í„´
                positive_kw = config.get('positive_keywords', set())
                negative_kw = config.get('negative_keywords', set())

                if positive_kw and any(kw.lower() in answer_text for kw in positive_kw):
                    behavioral_values[behavior_key] = True
                elif negative_kw and any(kw.lower() in answer_text for kw in negative_kw):
                    behavioral_values[behavior_key] = False

        # âš¡ ìµœì í™” 3: Early termination (ëª¨ë“  íŒ¨í„´ ì°¾ìœ¼ë©´ ì¢…ë£Œ)
        filled_count = sum(1 for v in behavioral_values.values() if v is not None)
        if filled_count == total_patterns:
            break  # ë” ì´ìƒ ìˆœíšŒ ë¶ˆí•„ìš”!

    return behavioral_values


def extract_behavior_from_qa_pairs(
    qa_pairs: List[Dict[str, Any]],
    behavior_key: str,
    debug: bool = False
) -> Optional[Union[bool, str]]:
    """qa_pairsì—ì„œ íŠ¹ì • behavioral ì¡°ê±´ì„ ì¶”ì¶œ (ë²”ìš©)

    Args:
        qa_pairs: QA ìŒ ë¦¬ìŠ¤íŠ¸
        behavior_key: behavioral ì¡°ê±´ í‚¤
            - Boolean: 'ott_user', 'smoker', 'drinker', ...
            - String: 'winter_vacation_memory', 'skin_satisfaction', 'ai_service_field', ...
        debug: ë””ë²„ê¹… ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€

    Returns:
        True/False: boolean ì¡°ê±´ (ì˜ˆ: smoker=True)
        str: ë¬¸ìì—´ ê°’ (ì˜ˆ: winter_vacation_memory="ì¹œêµ¬ë“¤ê³¼ ë³´ë‚¸ ì¦ê±°ìš´ ì‹œê°„")
        None: ì •ë³´ ì—†ìŒ
    """
    keyword_config = BEHAVIORAL_KEYWORD_MAP.get(behavior_key)
    if not keyword_config:
        if debug:
            logger.warning(f"[Behavioral] {behavior_key}ëŠ” BEHAVIORAL_KEYWORD_MAPì— ì—†ìŒ")
        return None

    question_keywords = keyword_config['question_keywords']
    question_text = keyword_config.get('question_text', '')  # â­ Question text ê°€ì ¸ì˜¤ê¸°

    # â­ ë¬¸ìì—´ ê°’ ì €ì¥ íŒ¨í„´ ì²˜ë¦¬ (answer_valuesê°€ ìˆëŠ” ê²½ìš°)
    answer_values = keyword_config.get('answer_values')
    if answer_values:
        for qa in qa_pairs:
            if not isinstance(qa, dict):
                continue

            q_text = str(qa.get("q_text", ""))
            q_text_lower = q_text.lower()

            # ========================================
            # â­â­â­ Step 1: Question Text ì •í™• ë§¤ì¹­ (ìµœìš°ì„ !)
            # ========================================
            if question_text:
                # ì™„ì „ ì¼ì¹˜ í™•ì¸
                if q_text == question_text:
                    if debug:
                        logger.warning(f"[Behavioral] {behavior_key} âœ… ì •í™• ë§¤ì¹­ (question_text)")

                    # ë‹µë³€ ê°€ì ¸ì˜¤ê¸°
                    answer = qa.get("answer") or qa.get("answer_text")
                    if answer:
                        answer_text = str(answer).lower()

                        # ë‹µë³€ ê°’ ë§¤ì¹­
                        matched_value = None
                        max_match_count = 0

                        for value_name, keywords in answer_values.items():
                            match_count = sum(1 for kw in keywords if kw.lower() in answer_text)
                            if match_count > max_match_count:
                                max_match_count = match_count
                                matched_value = value_name

                        if matched_value:
                            if debug:
                                logger.warning(f"[Behavioral] {behavior_key} = '{matched_value}'")
                            return matched_value

                # ë†’ì€ ìœ ì‚¬ë„ (95% ì´ìƒ)
                from difflib import SequenceMatcher
                similarity = SequenceMatcher(None, question_text, q_text).ratio()
                if similarity > 0.95:
                    if debug:
                        logger.warning(f"[Behavioral] {behavior_key} âœ… ìœ ì‚¬ ë§¤ì¹­ (ìœ ì‚¬ë„: {similarity:.2%})")

                    # ë‹µë³€ ê°€ì ¸ì˜¤ê¸°
                    answer = qa.get("answer") or qa.get("answer_text")
                    if answer:
                        answer_text = str(answer).lower()

                        # ë‹µë³€ ê°’ ë§¤ì¹­
                        matched_value = None
                        max_match_count = 0

                        for value_name, keywords in answer_values.items():
                            match_count = sum(1 for kw in keywords if kw.lower() in answer_text)
                            if match_count > max_match_count:
                                max_match_count = match_count
                                matched_value = value_name

                        if matched_value:
                            if debug:
                                logger.warning(f"[Behavioral] {behavior_key} = '{matched_value}'")
                            return matched_value

            # ========================================
            # â­ Step 2: Fallback - Question Keywords ë§¤ì¹­
            # ========================================
            # Question text ë§¤ì¹­ ì‹¤íŒ¨ ì‹œì—ë§Œ í‚¤ì›Œë“œ ì‚¬ìš©
            matched_kw = None
            for kw in question_keywords:
                if kw.lower() in q_text_lower:
                    matched_kw = kw
                    break

            if not matched_kw:
                continue

            # ë‹µë³€ ê°€ì ¸ì˜¤ê¸°
            answer = qa.get("answer") or qa.get("answer_text")
            if not answer:
                if debug:
                    logger.warning(f"[Behavioral] {behavior_key} ì§ˆë¬¸ ë°œê²¬í–ˆìœ¼ë‚˜ ë‹µë³€ ì—†ìŒ: q={q_text_lower[:30]}")
                continue

            answer_text = str(answer).lower()

            if debug:
                logger.warning(f"[Behavioral] {behavior_key} ê²€ì‚¬ì¤‘ (Fallback): q={q_text_lower[:30]}, a={answer_text[:50]}")

            # ë‹µë³€ ê°’ ë§¤ì¹­ (ê°€ì¥ ê¸´ ë§¤ì¹­ ìš°ì„ )
            matched_value = None
            max_match_count = 0

            for value_name, keywords in answer_values.items():
                match_count = sum(1 for kw in keywords if kw.lower() in answer_text)
                if match_count > max_match_count:
                    max_match_count = match_count
                    matched_value = value_name

            if matched_value:
                if debug:
                    logger.warning(f"[Behavioral] {behavior_key} = '{matched_value}' (ë§¤ì¹­: {max_match_count}ê°œ)")
                return matched_value

        return None

    # â­ ê¸°ì¡´ boolean ì¡°ê±´ ì²˜ë¦¬
    positive_keywords = keyword_config['positive_keywords']
    negative_keywords = keyword_config['negative_keywords']

    # â­ OTT íŠ¹ìˆ˜ ì²˜ë¦¬: ë‹µë³€ ì¤‘ì‹¬ ë§¤ì¹­ (ì§ˆë¬¸ ê´€ê³„ì—†ì´)
    # "ë™ì˜ìƒ ìŠ¤íŠ¸ë¦¬ë° ì•±"ì²˜ëŸ¼ ëª…í™•í•œ ë‹µë³€ì´ ìˆìœ¼ë©´ ë°”ë¡œ True ë°˜í™˜
    if behavior_key == 'ott_user':
        ANSWER_ONLY_POSITIVE = {"ë™ì˜ìƒ ìŠ¤íŠ¸ë¦¬ë° ì•±", "ë™ì˜ìƒìŠ¤íŠ¸ë¦¬ë°ì•±"}
        ANSWER_ONLY_NEGATIVE = {"ì´ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤", "ì´ìš©í•˜ì§€ì•ŠëŠ”ë‹¤"}

        for qa in qa_pairs:
            if not isinstance(qa, dict):
                continue

            answer = qa.get("answer") or qa.get("answer_text")
            if not answer:
                continue

            answer_text = str(answer).lower()
            answer_compact = answer_text.replace(" ", "")

            # ë¶€ì • ë‹µë³€ ìš°ì„  ì²´í¬
            for neg_kw in ANSWER_ONLY_NEGATIVE:
                if neg_kw.replace(" ", "").lower() in answer_compact:
                    if debug:
                        logger.warning(f"[Behavioral] {behavior_key} = False (ë‹µë³€ í‚¤ì›Œë“œ '{neg_kw}' ë°œê²¬)")
                    return False

            # ê¸ì • ë‹µë³€ ì²´í¬
            for pos_kw in ANSWER_ONLY_POSITIVE:
                if pos_kw.replace(" ", "").lower() in answer_compact:
                    if debug:
                        logger.warning(f"[Behavioral] {behavior_key} = True (ë‹µë³€ í‚¤ì›Œë“œ '{pos_kw}' ë°œê²¬)")
                    return True

    # â­ ê¸°ì¡´ ë¡œì§: ì§ˆë¬¸ í‚¤ì›Œë“œ ë§¤ì¹­ â†’ ë‹µë³€ í™•ì¸
    matched_questions = []
    for qa in qa_pairs:
        if not isinstance(qa, dict):
            continue

        q_text = str(qa.get("q_text", "")).lower()

        # ì§ˆë¬¸ì— ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
        matched_kw = None
        for kw in question_keywords:
            if kw.lower() in q_text:
                matched_kw = kw
                break

        if not matched_kw:
            continue

        matched_questions.append(q_text)

        # ë‹µë³€ ê°€ì ¸ì˜¤ê¸°
        answer = qa.get("answer") or qa.get("answer_text")
        if not answer:
            if debug:
                logger.warning(f"[Behavioral] {behavior_key} ì§ˆë¬¸ ë°œê²¬í–ˆìœ¼ë‚˜ ë‹µë³€ ì—†ìŒ: q={q_text}")
            continue

        answer_text = str(answer).lower()
        answer_compact = answer_text.replace(" ", "")

        if debug:
            logger.warning(f"[Behavioral] {behavior_key} ê²€ì‚¬ì¤‘: q={q_text[:30]}, a={answer_text[:50]}")

        # ë¶€ì • í‚¤ì›Œë“œ ì²´í¬ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        for neg_kw in negative_keywords:
            neg_kw_lower = str(neg_kw).lower()
            neg_kw_compact = neg_kw_lower.replace(" ", "")
            if neg_kw_lower in answer_text or neg_kw_compact in answer_compact:
                if debug:
                    logger.warning(f"[Behavioral] {behavior_key} = False (ë¶€ì • í‚¤ì›Œë“œ '{neg_kw}' ë°œê²¬)")
                return False

        # ê¸ì • í‚¤ì›Œë“œ ì²´í¬
        for pos_kw in positive_keywords:
            pos_kw_lower = str(pos_kw).lower()
            pos_kw_compact = pos_kw_lower.replace(" ", "")
            if pos_kw_lower in answer_text or pos_kw_compact in answer_compact:
                if debug:
                    logger.warning(f"[Behavioral] {behavior_key} = True (ê¸ì • í‚¤ì›Œë“œ '{pos_kw}' ë°œê²¬)")
                return True

    if debug and matched_questions:
        logger.warning(f"[Behavioral] {behavior_key} ê´€ë ¨ ì§ˆë¬¸ {len(matched_questions)}ê°œ ë°œê²¬í–ˆìœ¼ë‚˜ ë§¤ì¹­ ì‹¤íŒ¨")

    return None


def validate_llm_extraction(
    query: str,
    conditions: Dict[str, Union[bool, str]]
) -> Dict[str, Union[bool, str]]:
    """LLM ì¶”ì¶œ ê²°ê³¼ ê²€ì¦ (í™˜ê° ì œê±°! ğŸš¨)

    ì „ëµ:
    1. Categorical (ë¬¸ìì—´): ê°’ í‚¤ì›Œë“œ í™•ì¸ (ì—„ê²©!)
    2. Boolean: ë„ë©”ì¸ í‚¤ì›Œë“œë§Œ í™•ì¸ (ëŠìŠ¨!)

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        conditions: LLMì´ ì¶”ì¶œí•œ behavioral ì¡°ê±´

    Returns:
        ê²€ì¦ í†µê³¼í•œ ì¡°ê±´ë§Œ í¬í•¨
    """
    validated = {}
    query_lower = query.lower()

    # â­ í™˜ê° ì˜ì‹¬: ì§§ì€ ì¿¼ë¦¬ì— ë„ˆë¬´ ë§ì€ íŒ¨í„´
    if len(query) < 20 and len(conditions) > 3:
        logger.warning(
            f"ğŸš¨ í™˜ê° ì˜ì‹¬: ì§§ì€ ì¿¼ë¦¬({len(query)}ì)ì— "
            f"ë„ˆë¬´ ë§ì€ íŒ¨í„´({len(conditions)}ê°œ) - ì „ì²´ ì œê±°"
        )
        return {}

    for behavior_key, value in conditions.items():
        keyword_config = BEHAVIORAL_KEYWORD_MAP.get(behavior_key, {})

        if not keyword_config:
            logger.warning(f"âš ï¸ ê²€ì¦ ì‹¤íŒ¨: {behavior_key} (ì •ì˜ë˜ì§€ ì•Šì€ íŒ¨í„´)")
            continue

        answer_values = keyword_config.get('answer_values')

        # ========================================
        # â­ Categorical: Value í‚¤ì›Œë“œ í™•ì¸ (ì—„ê²©!)
        # ========================================
        if answer_values and isinstance(value, str):
            value_keywords = answer_values.get(value, [])

            has_value_keyword = any(
                kw.lower() in query_lower
                for kw in value_keywords
            )

            if has_value_keyword:
                validated[behavior_key] = value
                logger.debug(f"  âœ… Categorical í†µê³¼: {behavior_key}='{value}'")
            else:
                logger.warning(
                    f"  âš ï¸ Categorical ì œê±°: {behavior_key}='{value}' "
                    f"(ê°’ í‚¤ì›Œë“œ ì—†ìŒ: {value_keywords})"
                )

        # ========================================
        # â­ Boolean: ë„ë©”ì¸ í‚¤ì›Œë“œë§Œ í™•ì¸ (ëŠìŠ¨!)
        # ========================================
        elif isinstance(value, bool):
            # â­â­â­ í•µì‹¬: question_keywordsë¡œ ë„ë©”ì¸ë§Œ í™•ì¸!
            domain_keywords = keyword_config.get('question_keywords', set())

            # ë„ë©”ì¸ í‚¤ì›Œë“œê°€ ì¿¼ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸
            # ì˜ˆ: "ott", "ìŠ¤íŠ¸ë¦¬ë°" ê°™ì€ ë„ë©”ì¸ ë‹¨ì–´
            has_domain_keyword = any(
                kw.lower() in query_lower
                for kw in domain_keywords
            )

            if has_domain_keyword:
                validated[behavior_key] = value
                logger.debug(f"  âœ… Boolean í†µê³¼: {behavior_key}={value}")
            else:
                logger.warning(
                    f"  âš ï¸ Boolean ì œê±°: {behavior_key}={value} "
                    f"(ë„ë©”ì¸ í‚¤ì›Œë“œ ì—†ìŒ: {list(domain_keywords)[:3]}...)"
                )

    if len(validated) < len(conditions):
        removed = set(conditions.keys()) - set(validated.keys())
        logger.info(f"ğŸ” ê²€ì¦ ì™„ë£Œ: {len(removed)}ê°œ ì œê±° - {removed}")

    return validated


def filter_redundant_patterns(
    conditions: Dict[str, Union[bool, str]]
) -> Dict[str, Union[bool, str]]:
    """ì¤‘ë³µ íŒ¨í„´ ì œê±° (êµ¬ì²´ì  > ì¼ë°˜ì )

    Args:
        conditions: LLMì´ ì¶”ì¶œí•œ behavioral ì¡°ê±´

    Returns:
        ì¤‘ë³µ ì œê±°ëœ ì¡°ê±´
    """
    filtered = conditions.copy()

    for specific, generics in PATTERN_HIERARCHY.items():
        if specific in filtered:
            for generic in generics:
                if generic in filtered:
                    logger.info(f"ğŸ”§ ì¤‘ë³µ ì œê±°: {specific} ìš°ì„ , {generic} ì œê±°")
                    del filtered[generic]

    return filtered


def extract_behavioral_conditions_llm(
    query: str,
    anthropic_client
) -> Dict[str, Union[bool, str]]:
    """LLMì„ ì‚¬ìš©í•œ í–‰ë™ ì¡°ê±´ ì¶”ì¶œ (ê³ ì •í™•ë„!)

    ì¥ì :
    - ë¬¸ë§¥ ì´í•´ (í‘œí˜„ì´ ë‹¬ë¼ë„ ë§¤ì¹­)
    - ìœ ì§€ë³´ìˆ˜ ì‰¬ì›€ (í”„ë¡¬í”„íŠ¸ë§Œ ìˆ˜ì •)
    - ì •í™•ë„ ë†’ìŒ (90-95%)

    ë‹¨ì :
    - ë¹„ìš© ($0.00006/ì¿¼ë¦¬, ìºì‹± ì ìš© ì‹œ)
    - ì†ë„ (0.3~0.5ì´ˆ, ìºì‹± ì‹œ 0.001ì´ˆ)

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        anthropic_client: Anthropic í´ë¼ì´ì–¸íŠ¸

    Returns:
        behavioral ì¡°ê±´ ë”•ì…”ë„ˆë¦¬
    """
    if not anthropic_client:
        return {}

    # â­ ìºì‹œ í™•ì¸ (ë™ì¼ ì¿¼ë¦¬ ì¬ì‚¬ìš©)
    cache_key = f"llm_behavioral:{query}"
    if cache_key in llm_query_cache:
        logger.info(f"ğŸ” LLM ì¶”ì¶œ ìºì‹œ íˆíŠ¸: {query}")
        return llm_query_cache[cache_key]

    # â­ í”„ë¡¬í”„íŠ¸ ìƒì„± (Boolean íŒ¨í„´ í¬í•¨!)
    pattern_descriptions = []

    for idx, (behavior_key, keyword_config) in enumerate(BEHAVIORAL_KEYWORD_MAP.items(), 1):
        question_text = keyword_config.get('question_text', behavior_key)
        answer_values = keyword_config.get('answer_values')

        if answer_values:
            # Categorical íŒ¨í„´
            values_str = ", ".join(answer_values.keys())
            pattern_descriptions.append(
                f"{idx}. {behavior_key} (ì§ˆë¬¸: {question_text})\n   ê°€ëŠ¥í•œ ë‹µë³€: {values_str}"
            )
        else:
            # â­ Boolean íŒ¨í„´ (ì¶”ê°€!)
            pattern_descriptions.append(
                f"{idx}. {behavior_key} (ì§ˆë¬¸: {question_text})\n   ê°€ëŠ¥í•œ ë‹µë³€: true/false"
            )

    patterns_text = "\n\n".join(pattern_descriptions)

    # System prompt (ê°•í™”ëœ ë²„ì „ - í™˜ê° ë°©ì§€!)
    system_prompt = f"""ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ê²€ìƒ‰ ì¿¼ë¦¬ì—ì„œ í–‰ë™ íŒ¨í„´ì„ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒì€ ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™ íŒ¨í„´ ëª©ë¡ì…ë‹ˆë‹¤:

{patterns_text}

**ğŸš¨ ì ˆëŒ€ì  ê·œì¹™**:
1. â­ **ì¿¼ë¦¬ì— ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ëœ ê²ƒë§Œ** ì¶”ì¶œí•˜ì„¸ìš”.
2. â­ **ì ˆëŒ€ë¡œ ì¶”ì¸¡í•˜ê±°ë‚˜ ì¶”ë¡ í•˜ì§€ ë§ˆì„¸ìš”.**
3. â­ **í†µê³„ì  ê²½í–¥ì„ ê°€ì •í•˜ì§€ ë§ˆì„¸ìš”.**
4. â­ **ë” êµ¬ì²´ì ì¸ íŒ¨í„´ì„ ìš°ì„ **í•˜ì„¸ìš” (êµ¬ì²´ì  íŒ¨í„´ì´ ìˆìœ¼ë©´ ì¼ë°˜ íŒ¨í„´ì€ ì œì™¸).
5. ì• ë§¤í•˜ê±°ë‚˜ ë¶ˆí™•ì‹¤í•œ ê²ƒì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

**í•™ìŠµ ì˜ˆì‹œ** (ë°˜ë“œì‹œ ë”°ë¼ì•¼ í•¨):

âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ:
- ì¿¼ë¦¬: "ìœ ëŸ½ ì—¬í–‰ ê°€ëŠ” ì‚¬ëŒ" â†’ {{"overseas_travel_preference": "ìœ ëŸ½"}}
- ì¿¼ë¦¬: "ChatGPT ì“°ëŠ” 30ëŒ€" â†’ {{"ai_chatbot_service": "ChatGPT"}}
- ì¿¼ë¦¬: "í¡ì—°ìì´ë©´ì„œ ìš´ë™í•˜ëŠ”" â†’ {{"smoker": true, "exercises": true}}

âŒ ì˜ëª»ëœ ì˜ˆì‹œ (ì ˆëŒ€ í•˜ì§€ ë§ ê²ƒ):
- ì¿¼ë¦¬: "20ëŒ€ ë‚¨ì„±" â†’ {{}}  (í–‰ë™ íŒ¨í„´ ì—†ìŒ! ë‚˜ì´/ì„±ë³„ì€ Demographics)
- ì¿¼ë¦¬: "ì§ì¥ì¸" â†’ {{}}  (í–‰ë™ íŒ¨í„´ ì—†ìŒ!)
- ì¿¼ë¦¬: "ëŒ€í•™ìƒ" â†’ {{}}  (í–‰ë™ íŒ¨í„´ ì—†ìŒ!)

âš ï¸ í™˜ê° ì˜ˆì‹œ (ì ˆëŒ€ ê¸ˆì§€):
- ì¿¼ë¦¬: "20ëŒ€" â†’ {{"ai_chatbot_service": "ChatGPT"}}  â† ì ˆëŒ€ ì•ˆë¨!
  ì´ìœ : "ChatGPT"ê°€ ì¿¼ë¦¬ì— ì—†ìŒ
- ì¿¼ë¦¬: "ë‚¨ì„±" â†’ {{"exercise_type": "í—¬ìŠ¤"}}  â† ì ˆëŒ€ ì•ˆë¨!
  ì´ìœ : "í—¬ìŠ¤"ê°€ ì¿¼ë¦¬ì— ì—†ìŒ

**ì¶œë ¥ í˜•ì‹**:
{{
  "behavior_key": "ê°’"
}}

ë§¤ì¹­ë˜ëŠ” íŒ¨í„´ì´ ì—†ìœ¼ë©´ ë°˜ë“œì‹œ: {{}}"""

    user_prompt = f'ê²€ìƒ‰ ì¿¼ë¦¬: "{query}"'

    # â­ LLM í˜¸ì¶œ
    try:
        response = anthropic_client.messages.create(
            model="claude-3-5-haiku-20241022",
            max_tokens=500,
            temperature=0,
            system=[
                {
                    "type": "text",
                    "text": system_prompt,
                    "cache_control": {"type": "ephemeral"}
                }
            ],
            messages=[
                {"role": "user", "content": user_prompt}
            ]
        )

        response_text = response.content[0].text.strip()

        # â­ JSON íŒŒì‹± (ë” ì•ˆì „í•˜ê²Œ!)
        # ``` ë¸”ë¡ ì œê±°
        if response_text.startswith("```"):
            response_text = response_text.strip("`")
            if response_text.startswith("json"):
                response_text = response_text[4:]
            response_text = response_text.strip()

        # JSON ì¶”ì¶œ (ì •ê·œì‹ìœ¼ë¡œ ë” ì•ˆì „í•˜ê²Œ)
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            response_text = json_match.group(0)

        conditions = json.loads(response_text)

        # â­â­â­ 1ë‹¨ê³„: ê²€ì¦ (í™˜ê° ì œê±°!)
        conditions = validate_llm_extraction(query, conditions)

        # â­â­â­ 2ë‹¨ê³„: ì¤‘ë³µ íŒ¨í„´ ì œê±°
        conditions = filter_redundant_patterns(conditions)

        # ìºì‹œ ì €ì¥
        llm_query_cache[cache_key] = conditions

        logger.info(f"âœ… LLM ì¶”ì¶œ (ê²€ì¦ ì™„ë£Œ): {len(conditions)}ê°œ íŒ¨í„´ - {conditions}")

        return conditions

    except json.JSONDecodeError as e:
        logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}\nì‘ë‹µ: {response_text}")
        return {}
    except Exception as e:
        logger.error(f"âŒ LLM ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return {}


def extract_behavioral_conditions_from_query(query: str) -> Dict[str, Union[bool, str]]:
    """ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì—ì„œ behavioral ì¡°ê±´ ìë™ ì¶”ì¶œ (í‚¤ì›Œë“œ ê¸°ë°˜ - Fallbackìš©)

    â­ BEHAVIORAL_KEYWORD_MAPì„ ìë™ìœ¼ë¡œ ìˆœíšŒí•˜ì—¬ ëª¨ë“  íŒ¨í„´ ê°ì§€!
    ìƒˆë¡œìš´ íŒ¨í„´ ì¶”ê°€ ì‹œ BEHAVIORAL_KEYWORD_MAPì—ë§Œ ì¶”ê°€í•˜ë©´ ë¨!

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬

    Returns:
        behavioral ì¡°ê±´ ë”•ì…”ë„ˆë¦¬
        - bool: {"drinker": True, "smoker": False, ...}
        - str: {"winter_vacation_memory": "ì¹œêµ¬ë“¤ê³¼ ë³´ë‚¸ ì¦ê±°ìš´ ì‹œê°„"}
    """
    query_lower = query.lower()
    query_normalized = query_lower.replace(" ", "")
    conditions = {}

    # â­ BEHAVIORAL_KEYWORD_MAPì˜ ëª¨ë“  í‚¤ë¥¼ ìë™ìœ¼ë¡œ ìˆœíšŒ
    for behavior_key, keyword_config in BEHAVIORAL_KEYWORD_MAP.items():
        question_keywords = keyword_config.get('question_keywords', set())
        answer_values = keyword_config.get('answer_values')

        # ì§ˆë¬¸ í‚¤ì›Œë“œê°€ ì¿¼ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸
        has_question_keyword = any(
            kw.lower().replace(" ", "") in query_normalized
            for kw in question_keywords
        )

        # â­ ë¬¸ìì—´ ê°’ íŒ¨í„´ (answer_valuesê°€ ìˆëŠ” ê²½ìš°)
        if answer_values:
            # ì§ˆë¬¸ í‚¤ì›Œë“œ ì—†ì–´ë„ ë‹µë³€ í‚¤ì›Œë“œë¡œ ë§¤ì¹­ ì‹œë„
            for value_name, value_keywords in answer_values.items():
                if any(kw.lower().replace(" ", "") in query_normalized for kw in value_keywords):
                    conditions[behavior_key] = value_name
                    break

        # â­ Boolean íŒ¨í„´ (positive/negative keywordsê°€ ìˆëŠ” ê²½ìš°)
        else:
            positive_keywords = keyword_config.get('positive_keywords', set())
            negative_keywords = keyword_config.get('negative_keywords', set())

            # ë¶€ì • í‚¤ì›Œë“œ ì²´í¬ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
            has_negative = any(
                kw.lower().replace(" ", "") in query_normalized
                for kw in negative_keywords
            )

            # ê¸ì • í‚¤ì›Œë“œ ì²´í¬
            has_positive = any(
                kw.lower().replace(" ", "") in query_normalized
                for kw in positive_keywords
            )

            if has_negative:
                conditions[behavior_key] = False
            elif has_positive:
                conditions[behavior_key] = True

    return conditions


def build_behavioral_filters(behavioral_conditions: Dict[str, Union[bool, str]]) -> List[Dict[str, Any]]:
    """behavioral_conditionsë¥¼ OpenSearch nested í•„í„°ë¡œ ë³€í™˜ (ë™ì  ì²˜ë¦¬)

    â­ BEHAVIORAL_KEYWORD_MAPì„ ì‚¬ìš©í•´ì„œ ëª¨ë“  ì¡°ê±´ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    Args:
        behavioral_conditions:
            - bool: {"smoker": True, "has_vehicle": False}
            - str: {"winter_vacation_memory": "ì¹œêµ¬ë“¤ê³¼ ë³´ë‚¸ ì¦ê±°ìš´ ì‹œê°„"}

    Returns:
        OpenSearch nested ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸

    Example:
        {"uses_smart_devices": True} â†’
        {
            "nested": {
                "path": "qa_pairs",
                "query": {
                    "bool": {
                        "must": [
                            {"bool": {"should": [ì§ˆë¬¸ ë§¤ì¹­]}},
                            {"bool": {"should": [ê¸ì • ë‹µë³€], "must_not": [ë¶€ì • ë‹µë³€]}}
                        ]
                    }
                }
            }
        }
    """
    filters = []

    for key, value in behavioral_conditions.items():
        if value is None:
            continue

        # â­ BEHAVIORAL_KEYWORD_MAPì—ì„œ í‚¤ì›Œë“œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        if key not in BEHAVIORAL_KEYWORD_MAP:
            logger.warning(f"âš ï¸ Behavioral condition '{key}' not found in BEHAVIORAL_KEYWORD_MAP, skipping")
            continue

        keyword_config = BEHAVIORAL_KEYWORD_MAP[key]
        question_keywords = keyword_config['question_keywords']

        # ì§ˆë¬¸ ë§¤ì¹­ ì¿¼ë¦¬ ìƒì„±
        question_should = [
            {"match": {"qa_pairs.q_text": q}}
            for q in question_keywords
        ]

        # â­ íŠ¹ë³„ ì²˜ë¦¬: winter_vacation_memory (ë¬¸ìì—´ ê°’ ë§¤ì¹­)
        if isinstance(value, str):
            # ë¬¸ìì—´ ê°’: answerì—ì„œ ì •í™•í•œ ê°’ ë§¤ì¹­
            answer_should = [
                {"match_phrase": {"qa_pairs.answer": value}}
            ]

            # ë‹µë³€ ê°’ ë§¤í•‘ì—ì„œ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
            answer_values = keyword_config.get('answer_values', {})
            if value in answer_values:
                for kw in answer_values[value]:
                    answer_should.append({"match": {"qa_pairs.answer": kw}})

        else:
            # Boolean ê°’: positive/negative keywords ì‚¬ìš©
            positive_keywords = keyword_config['positive_keywords']
            negative_keywords = keyword_config['negative_keywords']

            # â­ ë‹µë³€ ë§¤ì¹­ ì¿¼ë¦¬ ìƒì„± (positive keywordsë§Œ ì‚¬ìš©, negative ë¬´ì‹œ)
            # ì´ìœ : negative keywordsê°€ ë„ˆë¬´ ì¼ë°˜ì  (ì˜ˆ: "í•´ë‹¹ ì—†ìŒ", "ë³´ìœ í•˜ì§€ ì•ŠìŒ")
            if value:  # True: positive keywordsë§Œ ì°¾ê¸°
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}
                    for kw in positive_keywords
                ]
            else:  # False: negative keywordsë§Œ ì°¾ê¸°
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}
                    for kw in negative_keywords
                ]

        # OpenSearch nested í•„í„° ìƒì„± (must_not ì œê±°)
        filters.append({
            "nested": {
                "path": "qa_pairs",
                "query": {
                    "bool": {
                        "must": [
                            {
                                "bool": {
                                    "should": question_should,
                                    "minimum_should_match": 1
                                }
                            },
                            {
                                "bool": {
                                    "should": answer_should,
                                    "minimum_should_match": 1
                                }
                            }
                        ]
                    }
                }
            }
        })

    return filters


# â­ ì•„ë˜ëŠ” legacy í•˜ë“œì½”ë”©ëœ ì¡°ê±´ë“¤ (ì°¸ê³ ìš©ìœ¼ë¡œ ì£¼ì„ ì²˜ë¦¬)
# ì´ì œ ìœ„ì˜ ë™ì  ì²˜ë¦¬ ë¡œì§ì´ ëª¨ë“  ì¡°ê±´ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
"""
def build_behavioral_filters_OLD_HARDCODED(behavioral_conditions: Dict[str, bool]) -> List[Dict[str, Any]]:
    filters = []

    for key, value in behavioral_conditions.items():
        if value is None:
            continue

        if key == "smoker":
            # í¡ì—° í•„í„°
            question_should = [
                {"match": {"qa_pairs.q_text": q}}
                for q in SMOKER_QUESTION_KEYWORDS
            ]

            if value:  # í¡ì—°ì
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in SMOKER_POSITIVE_KEYWORDS
                ]
                answer_must_not = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in SMOKER_NEGATIVE_KEYWORDS
                ]
            else:  # ë¹„í¡ì—°ì
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in SMOKER_NEGATIVE_KEYWORDS
                ]
                answer_must_not = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in SMOKER_POSITIVE_KEYWORDS
                ]

            filters.append({
                "nested": {
                    "path": "qa_pairs",
                    "query": {
                        "bool": {
                            "must": [
                                {
                                    "bool": {
                                        "should": question_should,
                                        "minimum_should_match": 1
                                    }
                                },
                                {
                                    "bool": {
                                        "should": answer_should,
                                        "must_not": answer_must_not,
                                        "minimum_should_match": 1
                                    }
                                }
                            ]
                        }
                    }
                }
            })

        elif key == "has_vehicle":
            # ì°¨ëŸ‰ ë³´ìœ  í•„í„°
            question_should = [
                {"match": {"qa_pairs.q_text": q}}
                for q in VEHICLE_QUESTION_KEYWORDS
            ]

            if value:  # ì°¨ëŸ‰ ìˆìŒ
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in BEHAVIOR_YES_TOKENS
                ]
                answer_must_not = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in BEHAVIOR_NO_TOKENS
                ]
            else:  # ì°¨ëŸ‰ ì—†ìŒ
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in BEHAVIOR_NO_TOKENS
                ]
                answer_must_not = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in BEHAVIOR_YES_TOKENS
                ]

            filters.append({
                "nested": {
                    "path": "qa_pairs",
                    "query": {
                        "bool": {
                            "must": [
                                {
                                    "bool": {
                                        "should": question_should,
                                        "minimum_should_match": 1
                                    }
                                },
                                {
                                    "bool": {
                                        "should": answer_should,
                                        "must_not": answer_must_not,
                                        "minimum_should_match": 1
                                    }
                                }
                            ]
                        }
                    }
                }
            })

        elif key == "drinker":
            # â­ ìŒì£¼ ì—¬ë¶€ í•„í„°
            question_should = [
                {"match": {"qa_pairs.q_text": q}}
                for q in ALCOHOL_QUESTION_KEYWORDS
            ]

            if value:  # ìŒì£¼ì
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}
                    for kw in DRINKER_POSITIVE_KEYWORDS
                ]
                answer_must_not = [
                    {"match": {"qa_pairs.answer": kw}}
                    for kw in NON_DRINKER_KEYWORDS
                ]
            else:  # ë¹„ìŒì£¼ì
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}
                    for kw in NON_DRINKER_KEYWORDS
                ]
                answer_must_not = [
                    {"match": {"qa_pairs.answer": kw}}
                    for kw in DRINKER_POSITIVE_KEYWORDS
                ]

            filters.append({
                "nested": {
                    "path": "qa_pairs",
                    "query": {
                        "bool": {
                            "must": [
                                {
                                    "bool": {
                                        "should": question_should,
                                        "minimum_should_match": 1
                                    }
                                },
                                {
                                    "bool": {
                                        "should": answer_should,
                                        "must_not": answer_must_not,
                                        "minimum_should_match": 1
                                    }
                                }
                            ]
                        }
                    }
                }
            })

        elif key == "drinks_beer":
            # ë§¥ì£¼ ìŒìš© í•„í„°
            if value:
                question_should = [
                    {"match": {"qa_pairs.q_text": q}}
                    for q in ALCOHOL_QUESTION_KEYWORDS
                ]
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in BEER_KEYWORDS
                ]

                filters.append({
                    "nested": {
                        "path": "qa_pairs",
                        "query": {
                            "bool": {
                                "must": [
                                    {"bool": {"should": question_should, "minimum_should_match": 1}},
                                    {"bool": {"should": answer_should, "minimum_should_match": 1}}
                                ]
                            }
                        }
                    }
                })

        elif key == "drinks_wine":
            # ì™€ì¸ ìŒìš© í•„í„°
            if value:
                question_should = [
                    {"match": {"qa_pairs.q_text": q}}
                    for q in ALCOHOL_QUESTION_KEYWORDS
                ]
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in WINE_KEYWORDS
                ]

                filters.append({
                    "nested": {
                        "path": "qa_pairs",
                        "query": {
                            "bool": {
                                "must": [
                                    {"bool": {"should": question_should, "minimum_should_match": 1}},
                                    {"bool": {"should": answer_should, "minimum_should_match": 1}}
                                ]
                            }
                        }
                    }
                })

        elif key == "drinks_soju":
            # ì†Œì£¼ ìŒìš© í•„í„°
            if value:
                question_should = [
                    {"match": {"qa_pairs.q_text": q}}
                    for q in ALCOHOL_QUESTION_KEYWORDS
                ]
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in SOJU_KEYWORDS
                ]

                filters.append({
                    "nested": {
                        "path": "qa_pairs",
                        "query": {
                            "bool": {
                                "must": [
                                    {"bool": {"should": question_should, "minimum_should_match": 1}},
                                    {"bool": {"should": answer_should, "minimum_should_match": 1}}
                                ]
                            }
                        }
                    }
                })

        elif key == "non_drinker":
            # ë¹„ìŒì£¼ì í•„í„°
            if value:
                question_should = [
                    {"match": {"qa_pairs.q_text": q}}
                    for q in ALCOHOL_QUESTION_KEYWORDS
                ]
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in NON_DRINKER_KEYWORDS
                ]

                filters.append({
                    "nested": {
                        "path": "qa_pairs",
                        "query": {
                            "bool": {
                                "must": [
                                    {"bool": {"should": question_should, "minimum_should_match": 1}},
                                    {"bool": {"should": answer_should, "minimum_should_match": 1}}
                                ]
                            }
                        }
                    }
                })

    return filters
"""


@router.get("/", summary="Search API ìƒíƒœ")
def search_root():
    """Search API ê¸°ë³¸ ì •ë³´"""
    return {
        "message": "Search API ì‹¤í–‰ ì¤‘",
        "version": "1.0",
        "endpoints": [
            "/search/nl"
        ]
    }


class NLSearchRequest(BaseModel):
    """ìì—°ì–´ ê¸°ë°˜ ê²€ìƒ‰ ìš”ì²­ (í•„í„°/size ìë™ ì¶”ì¶œ)"""
    query: str = Field(..., description="ìì—°ì–´ ì¿¼ë¦¬ (ì˜ˆ: '30ëŒ€ ì‚¬ë¬´ì§ 300ëª… ë°ì´í„° ë³´ì—¬ì¤˜')")
    index_name: str = Field(
        default="survey_responses_merged",
        description="ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ì´ë¦„ (ê¸°ë³¸ê°’: survey_responses_merged; ì™€ì¼ë“œì¹´ë“œ ì‚¬ìš© ê°€ëŠ¥)"
    )
    size: int = Field(default=30000, ge=1, le=50000, description="ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜ (ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œëœ ì¸ì› ìˆ˜ê°€ ì—†ì„ ë•Œ ì‚¬ìš©, ì „ì²´ ë°ì´í„° ì•½ 35000ê°œ)")
    use_vector_search: bool = Field(default=True, description="ë²¡í„° ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€")
    page: int = Field(default=1, ge=1, description="ìš”ì²­í•  í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)")
    use_claude_analyzer: Optional[bool] = Field(
        default=None,
        description="Claude ë¶„ì„ê¸° ì‚¬ìš© ì—¬ë¶€ (Noneì´ë©´ ì„œë²„ ì„¤ì •ê°’ì„ ë”°ë¦„)"
    )
    session_id: Optional[str] = Field(
        default=None,
        description="ëŒ€í™”/ì„¸ì…˜ ì‹ë³„ì (Redis ëŒ€í™” ë¡œê·¸ í‚¤)"
    )
    user_id: Optional[str] = Field(
        default=None,
        description="ìš”ì²­ ì‚¬ìš©ì ì‹ë³„ì (ê²€ìƒ‰ ì´ë ¥ í‚¤)"
    )
    request_id: Optional[str] = Field(
        default=None,
        description="ìš”ì²­ ì¶”ì ì„ ìœ„í•œ ID"
    )
    metadata: Optional[Dict[str, Any]] = Field(
        default=None,
        description="ì¶”ê°€ ìš”ì²­ ë©”íƒ€ë°ì´í„°"
    )
    log_conversation: bool = Field(
        default=True,
        description="Redis ëŒ€í™” ë¡œê·¸ ì €ì¥ ì—¬ë¶€"
    )
    log_search_history: bool = Field(
        default=True,
        description="Redis ê²€ìƒ‰ ì´ë ¥ ì €ì¥ ì—¬ë¶€"
    )
    request_llm_summary: bool = Field(
        default=False,
        description="LLM ìš”ì•½/ë¶„ì„ ìƒì„± ìš”ì²­ ì—¬ë¶€"
    )
    llm_summary_instructions: Optional[str] = Field(
        default=None,
        description="LLM ìš”ì•½ ì‹œ ì‚¬ìš©í•  ì¶”ê°€ ì§€ì¹¨"
    )


def convert_to_simple_response(
    search_response: SearchResponse,
    behavioral_conditions: Dict[str, Any],
    max_results: int = 100
) -> SimpleResponse:
    """
    SearchResponseë¥¼ SimpleResponseë¡œ ë³€í™˜ (í”„ë¡ íŠ¸ì—”ë“œ ì¹œí™”ì )

    Args:
        search_response: ê¸°ì¡´ ê²€ìƒ‰ ì‘ë‹µ
        behavioral_conditions: ì¿¼ë¦¬ì˜ behavioral ì¡°ê±´ {"smoker": True, "has_vehicle": True}
        max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ 100ê°œ)

    Returns:
        SimpleResponse: ê°„ì†Œí™”ëœ ì‘ë‹µ
    """
    try:
        simple_results = []

        for item in search_response.results[:max_results]:
            # Behavioral ì¡°ê±´ ë§¤ì¹­ QA ì¶”ì¶œ
            matched_conditions_data = extract_behavioral_qa_pairs(
                source={'qa_pairs': item.qa_pairs or []},
                behavioral_conditions=behavioral_conditions
            )

            # MatchedCondition ê°ì²´ë¡œ ë³€í™˜
            matched_objs = [
                MatchedCondition(
                    condition_type=mc['condition_type'],
                    condition_value=mc['condition_value'],
                    question=mc['q_text'],
                    answer=mc['answer'],
                    confidence=mc.get('confidence', 1.0)
                )
                for mc in matched_conditions_data
            ]

            # Demographics ì •ë³´ ì¶”ì¶œ
            demo_info = item.demographic_info or {}
            demographics = {
                'gender': demo_info.get('gender', 'N/A'),
                'age_group': demo_info.get('age_group', 'N/A'),
                'birth_year': str(demo_info.get('birth_year', 'N/A'))
            }

            simple_results.append(SimpleResult(
                user_id=item.user_id,
                score=item.score,
                demographics=demographics,
                matched_conditions=matched_objs
            ))

        # Query ë¶„ì„ ì •ë³´
        query_analysis = search_response.query_analysis or {}
        query_info = {
            'keywords': [
                *(query_analysis.get('must_terms', [])),
                *(query_analysis.get('should_terms', []))
            ],
            'filters_applied': bool(query_analysis.get('filters')),
            'behavioral_conditions': behavioral_conditions,
            'extracted_entities': query_analysis.get('extracted_entities')
        }

        return SimpleResponse(
            state="SUCCESS",
            message="ê²€ìƒ‰ ì„±ê³µ",
            query=search_response.query,
            total_hits=search_response.total_hits,
            results=simple_results,
            query_info=query_info,
            took_ms=search_response.took_ms
        )

    except Exception as e:
        logger.error(f"SimpleResponse ë³€í™˜ ì¤‘ ì—ëŸ¬: {e}", exc_info=True)
        # ì—ëŸ¬ ì‹œ ë¹ˆ ì‘ë‹µ ë°˜í™˜
        return SimpleResponse(
            state="ERROR",
            message=f"ì‘ë‹µ ë³€í™˜ ì‹¤íŒ¨: {str(e)}",
            query=search_response.query if search_response else "",
            total_hits=0,
            results=[],
            query_info=None,
            took_ms=0
        )


# ============================================
# â­ ì••ì¶• ì €ì¥/ë¡œë“œ í•¨ìˆ˜ (Redis ìºì‹œ ìµœì í™”)
# ============================================

def save_search_cache_compressed(
    cache_key: str,
    cache_ttl: int,
    # â­ cache_payload ëŒ€ì‹  ê°œë³„ íŒŒë¼ë¯¸í„°ë¡œ ë°›ê¸° (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë”•ì…”ë„ˆë¦¬ ìƒì„±)
    total_hits: int,
    max_score: float,
    stored_items: List[Dict],
    page_size: int,
    filters_for_response: Dict,
    extracted_entities_dict: Dict,
    behavioral_conditions: Dict,
    use_claude: bool,
    requested_count: int,
):
    """ì••ì¶•í•´ì„œ Redis ì €ì¥ (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰)"""
    try:
        cache_client = getattr(router, "redis_client", None)
        if not cache_client:
            return

        start = perf_counter()

        # â­ ë°±ê·¸ë¼ìš´ë“œì—ì„œ cache_payload ìƒì„± (ë©”ì¸ ìŠ¤ë ˆë“œ ì§€ì—° ì œê±°!)
        logger.info(f"ğŸ”§ [Background] cache_payload ìƒì„± ì‹œì‘ (items: {len(stored_items)}ê°œ)")
        payload_start = perf_counter()

        cache_payload = {
            "total_hits": total_hits,
            "max_score": max_score,
            "items": stored_items,
            "page_size": page_size,
            "filters": filters_for_response,
            "extracted_entities": extracted_entities_dict,
            "behavioral_conditions": behavioral_conditions,
            "use_claude": use_claude,
            "requested_count": requested_count,
        }

        payload_duration_ms = (perf_counter() - payload_start) * 1000
        logger.info(f"âœ… [Background] cache_payload ìƒì„± ì™„ë£Œ ({payload_duration_ms:.2f}ms)")

        # pickle + gzip (compresslevel=6: ì†ë„ì™€ ì••ì¶•ë¥  ê· í˜•)
        serialized = pickle.dumps(cache_payload, protocol=pickle.HIGHEST_PROTOCOL)
        compressed = gzip.compress(serialized, compresslevel=6)

        cache_client.setex(cache_key, cache_ttl, compressed)

        duration_ms = (perf_counter() - start) * 1000
        original_size_mb = len(serialized) / 1024**2
        compressed_size_mb = len(compressed) / 1024**2
        ratio = (1 - compressed_size_mb / original_size_mb) * 100 if original_size_mb > 0 else 0

        logger.info(
            f"ğŸ’¾ [Background] Redis ì••ì¶• ì €ì¥: "
            f"{compressed_size_mb:.2f}MB (ì›ë³¸: {original_size_mb:.2f}MB, ì••ì¶•ë¥ : {ratio:.1f}%), "
            f"{duration_ms:.2f}ms"
        )

    except Exception as e:
        logger.warning(f"âš ï¸ [Background] Redis ì €ì¥ ì‹¤íŒ¨: {e}")


def load_search_cache_compressed(cache_key: str) -> Optional[Dict[str, Any]]:
    """ì••ì¶•ëœ ìºì‹œ ë¡œë“œ"""
    try:
        cache_client = getattr(router, "redis_client", None)
        if not cache_client:
            return None

        start = perf_counter()

        compressed = cache_client.get(cache_key)
        if not compressed:
            return None

        serialized = gzip.decompress(compressed)
        payload = pickle.loads(serialized)

        duration_ms = (perf_counter() - start) * 1000
        logger.info(f"ğŸ” Redis ì••ì¶• ìºì‹œ íˆíŠ¸: {len(compressed)/1024**2:.2f}MB, {duration_ms:.2f}ms")

        return payload

    except Exception as e:
        logger.warning(f"âš ï¸ Redis ì••ì¶• ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


@router.post("/nl", response_model=SearchResponse, summary="ìì—°ì–´ ì¿¼ë¦¬: ìë™ ì¶”ì¶œ+ê²€ìƒ‰")
async def search_natural_language(
    request: NLSearchRequest,
    background_tasks: BackgroundTasks,  # â­ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì¶”ê°€
    os_client: OpenSearch = Depends(lambda: router.os_client),
    stream_callback: Optional[Any] = None,  # â­ SSE ìŠ¤íŠ¸ë¦¬ë°ìš© ì½œë°± (callable íƒ€ì…)
):
    """
    ìì—°ì–´ ì…ë ¥ì—ì„œ ì¸êµ¬í†µê³„(ì—°ë ¹/ì„±ë³„/ì§ì—…)ì™€ ìš”ì²­ ìˆ˜ëŸ‰ì„ ì¶”ì¶œí•˜ì—¬
    ê²€ìƒ‰ ì¿¼ë¦¬ì™€ sizeì— ë°˜ì˜í•œ ë’¤ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        stream_callback: ì„ íƒì  ì½œë°± í•¨ìˆ˜. (event_type, data) í˜•íƒœë¡œ í˜¸ì¶œë¨.
            - event_type: 'alpha' | 'before_filter' | 'after_filter'
            - data: ì´ë²¤íŠ¸ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    """
    try:
        logger.info("ğŸŸ¢ /search/nl ìš”ì²­ ì‹œì‘")

        if not os_client or not os_client.ping():
            raise HTTPException(status_code=503, detail="OpenSearch ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        _ensure_request_defaults(request)

        config = getattr(router, 'config', None)
        if config is None:
            from rag_query_analyzer.config import get_config
            config = get_config()
            router.config = config

        analyzer = getattr(router, 'analyzer', None)
        if analyzer is None:
            analyzer = AdvancedRAGQueryAnalyzer(config)
            router.analyzer = analyzer
        use_claude = request.use_claude_analyzer if request.use_claude_analyzer is not None else config.ENABLE_CLAUDE_ANALYZER
        analysis = analyzer.analyze_query(request.query, use_claude=use_claude)
        if analysis is None:
            raise RuntimeError("Query analysis returned None")
        query_analysis = analysis
        
        # â­ SSE ìŠ¤íŠ¸ë¦¬ë°: ì•ŒíŒŒê°’ ë° ì¿¼ë¦¬ ë¶„ì„ ì •ë³´ ì „ë‹¬
        if stream_callback:
            try:
                stream_callback('alpha', {'alpha': analysis.alpha})
                
                # Must terms, Should terms ì „ë‹¬
                stream_callback('query_analysis', {
                    'must_terms': analysis.must_terms or [],
                    'should_terms': analysis.should_terms or [],
                })
            except Exception as e:
                logger.warning(f"âš ï¸ stream_callback ì˜¤ë¥˜ (query_analysis): {e}")

        # â­ ìë™ìœ¼ë¡œ ì¿¼ë¦¬ì—ì„œ behavioral ì¡°ê±´ ì¶”ì¶œ (LLM ì‚¬ìš©!)
        anthropic_client = getattr(router, 'anthropic_client', None)
        auto_behavioral = extract_behavioral_conditions_llm(request.query, anthropic_client)

        # Fallback: LLM ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜
        if not auto_behavioral and not anthropic_client:
            auto_behavioral = extract_behavioral_conditions_from_query(request.query)

        if auto_behavioral:
            # ê¸°ì¡´ behavioral_conditionsì™€ ë³‘í•© (ìë™ ì¶”ì¶œì´ ìš°ì„ )
            if not analysis.behavioral_conditions:
                analysis.behavioral_conditions = {}
            for key, value in auto_behavioral.items():
                # â­ Noneì´ ì•„ë‹ˆë©´ ë®ì–´ì“°ê¸°! (LLM ì¶”ì¶œ ìš°ì„ )
                if value is not None:
                    analysis.behavioral_conditions[key] = value
            logger.info(f"âœ… ìë™ ì¶”ì¶œëœ behavioral ì¡°ê±´: {auto_behavioral}")
        
        # â­ SSE ìŠ¤íŠ¸ë¦¬ë°: Behavioral conditions (Trueì¸ ê²ƒë§Œ) ì „ë‹¬ (LLM ì¶”ì¶œ í›„)
        if stream_callback:
            try:
                behavioral_true = {}
                if analysis.behavioral_conditions:
                    for key, value in analysis.behavioral_conditions.items():
                        if value is True:
                            behavioral_true[key] = value
                if behavioral_true:
                    stream_callback('behavioral_conditions', {'behavioral_conditions': behavioral_true})
            except Exception as e:
                logger.warning(f"âš ï¸ stream_callback ì˜¤ë¥˜ (behavioral_conditions): {e}")

        embedding_model = getattr(router, 'embedding_model', None)
        if embedding_model is None and hasattr(router, 'embedding_model_factory'):
            embedding_model = router.embedding_model_factory()
            router.embedding_model = embedding_model
        data_fetcher = DataFetcher(
            opensearch_client=os_client,
            qdrant_client=getattr(router, 'qdrant_client', None),
            async_opensearch_client=getattr(router, 'async_os_client', None)
        )

        timings: Dict[str, float] = {}
        overall_start = perf_counter()

        # 1) ì¶”ì¶œ: filters + size
        extractor = DemographicExtractor()
       
        extracted_entities, requested_size = extractor.extract_with_size(
            request.query, 
            default_size=getattr(request, "size", 30000),  
            max_size=60000
        )

        # â­ Claudeì˜ demographicsë¥¼ extracted_entitiesì— ë³‘í•©
        if hasattr(analysis, 'demographic_entities') and analysis.demographic_entities:
            logger.warning(f"[MERGE] Claude demographics: {len(analysis.demographic_entities)}ê°œ")
            logger.warning(f"[MERGE] DemographicExtractor demographics: {len(extracted_entities.demographics)}ê°œ")

            # Claudeì˜ demographicsë¥¼ ìš°ì„  ì‚¬ìš© (ë” ì •í™•í•¨)
            extracted_entities.demographics = list(analysis.demographic_entities)

            logger.warning(f"[MERGE] ë³‘í•© í›„: {len(extracted_entities.demographics)}ê°œ")
            for demo in extracted_entities.demographics:
                logger.warning(f"  - {demo.demographic_type.value}: {demo.value}")
        
        # â­ SSE ìŠ¤íŠ¸ë¦¬ë°: Demographics ì „ë‹¬ (ì¶”ì¶œ í›„)
        if stream_callback:
            try:
                demographics_list = [d.raw_value for d in extracted_entities.demographics] if extracted_entities.demographics else []
                if demographics_list:
                    stream_callback('demographics', {'demographics': demographics_list})
            except Exception as e:
                logger.warning(f"âš ï¸ stream_callback ì˜¤ë¥˜ (demographics): {e}")

        filters: List[Dict[str, Any]] = []

        # Demographics í•„í„°
        for demo in extracted_entities.demographics:
            # â­ ì¸ë±ìŠ¤ë³„ í•„í„° ì „ëµ:
            # - survey_responses_merged: metadataì™€ qa_pairs ëª¨ë‘ ìˆìŒ
            # - survey_responses_merged: ì¼ë¶€ëŠ” metadata, ì¼ë¶€ëŠ” qa_pairsì—ë§Œ ìˆìŒ â†’ qa_fallback í™œì„±í™”
            # ì˜ˆ: regionì€ metadataì—, marital_statusëŠ” qa_pairsì—ë§Œ ìˆì„ ìˆ˜ ìˆìŒ
            is_survey_merged = request.index_name in ["survey_responses_merged", "s_survey*"]
            metadata_only = not is_survey_merged  # survey_responses_mergedëŠ” False
            include_nested_fallback = is_survey_merged  # survey_responses_mergedëŠ” True (qa_pairsë„ ê²€ìƒ‰)
            filter_clause = demo.to_opensearch_filter(
                metadata_only=metadata_only,
                include_qa_fallback=include_nested_fallback,
            )
            if filter_clause and filter_clause != {"match_all": {}}:
                filters.append(filter_clause)

        # â­ ê°œì„ : behavioral_conditionsë¥¼ OpenSearch í•„í„°ë¡œ ë³€í™˜
        if analysis.behavioral_conditions:
            behavioral_filters = build_behavioral_filters(analysis.behavioral_conditions)
            filters.extend(behavioral_filters)
            logger.info(f"âœ… Behavioral í•„í„° ì¶”ê°€: {analysis.behavioral_conditions} â†’ {len(behavioral_filters)}ê°œ í•„í„°")

        # â­ inner_hits ì œê±° (ì¤‘ë³µ key ì—ëŸ¬ ë°©ì§€)
        def remove_inner_hits_from_filter(query_dict):
            """ì¬ê·€ì ìœ¼ë¡œ inner_hits ì œê±°"""
            import copy
            cleaned = copy.deepcopy(query_dict)

            if isinstance(cleaned, dict):
                if 'nested' in cleaned:
                    if 'inner_hits' in cleaned['nested']:
                        del cleaned['nested']['inner_hits']
                    if 'query' in cleaned['nested']:
                        cleaned['nested']['query'] = remove_inner_hits_from_filter(cleaned['nested']['query'])

                if 'bool' in cleaned:
                    for key in ['must', 'should', 'must_not', 'filter']:
                        if key in cleaned['bool']:
                            if isinstance(cleaned['bool'][key], list):
                                cleaned['bool'][key] = [remove_inner_hits_from_filter(item) for item in cleaned['bool'][key]]
                            else:
                                cleaned['bool'][key] = remove_inner_hits_from_filter(cleaned['bool'][key])

            return cleaned

        # filtersì—ì„œ inner_hits ì œê±°
        logger.info(f"ğŸ” í•„í„° ìƒíƒœ before inner_hits removal: filters={len(filters) if filters else 0}ê°œ")
        if filters:
            filters = [remove_inner_hits_from_filter(f) for f in filters]
            logger.info(f"âœ… inner_hits ì œê±° ì™„ë£Œ: {len(filters)}ê°œ í•„í„°")
        else:
            logger.warning(f"âš ï¸ filtersê°€ ë¹„ì–´ìˆìŒ!")

        filters_for_response = list(filters)
        filters_signature = _normalize_filters_for_cache(filters_for_response)

        # â­ page_size ê²°ì •: 
        # 1. ì¿¼ë¦¬ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ì¸ì› ìˆ˜ë¥¼ ì¶”ì¶œí•œ ê²½ìš° (ì˜ˆ: "300ëª…") â†’ ì¶”ì¶œëœ ê°’ ì‚¬ìš©
        # 2. ì¿¼ë¦¬ì—ì„œ ì¸ì› ìˆ˜ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í•œ ê²½ìš° â†’ request.size ì‚¬ìš© (ê¸°ë³¸ê°’ 30000)
        # 
        # requested_sizeê°€ request.sizeì™€ ê°™ìœ¼ë©´ ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œí•˜ì§€ ëª»í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼
        request_size = getattr(request, "size", 30000)
        if requested_size is not None and requested_size > 0:
            # ì¿¼ë¦¬ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ì¶”ì¶œí•œ ê²½ìš° (request.sizeì™€ ë‹¤ë¦„)
            if requested_size != request_size:
                page_size = max(1, min(requested_size, 50000))
            else:
                # requested_sizeê°€ request.sizeì™€ ê°™ìœ¼ë©´ ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œí•˜ì§€ ëª»í•œ ê²ƒ
                # request.size ì‚¬ìš©
                page_size = max(1, min(request_size, 50000))
        else:
            # ì¿¼ë¦¬ì—ì„œ ì¸ì› ìˆ˜ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í•œ ê²½ìš°, request.size ì‚¬ìš© (ê¸°ë³¸ê°’ 10)
            page_size = max(1, min(request_size, 50000))
        page = max(1, request.page)
        requested_window = page_size * page
        cache_client = getattr(router, "redis_client", None)
        cache_ttl = getattr(router, "cache_ttl_seconds", 0)
        cache_limit = getattr(router, "cache_max_results", requested_window)
        cache_prefix = getattr(router, "cache_prefix", "search:results")
        cache_enabled = bool(cache_client) and cache_ttl > 0
        # â­ window_size: ë‚´ë¶€ ê²€ìƒ‰/í•„í„°ë§ìš© (ì¶©ë¶„í•œ í›„ë³´ í™•ë³´)
        # page_size: ì‚¬ìš©ìì—ê²Œ ë°˜í™˜í•  ìµœì¢… ê²°ê³¼ ê°œìˆ˜
        min_window_size = 10000  # ì „ì²´ ë°ì´í„° ì•½ 35000ê°œë¥¼ ê³ ë ¤í•˜ì—¬ ì¦ê°€ (ë‚´ë¶€ ì²˜ë¦¬ìš©)
        window_size = max(requested_window, min_window_size)
        if cache_limit and cache_limit > 0:
            window_size = min(window_size, cache_limit)
        size = window_size  # ë‚´ë¶€ ê²€ìƒ‰ í¬ê¸° (ì‘ë‹µ í¬ê¸°ì™€ ë¶„ë¦¬)
        cache_key = None
        cache_hit = False

        if cache_enabled:
            try:
                behavior_signature = ""
                if getattr(analysis, "behavioral_conditions", None):
                    try:
                        behavior_signature = json.dumps(analysis.behavioral_conditions, ensure_ascii=False, sort_keys=True)
                    except Exception:
                        behavior_signature = str(analysis.behavioral_conditions)

                cache_key = _make_cache_key(
                    prefix=cache_prefix,
                    query=request.query,
                    index_name=request.index_name,
                    page_size=page_size,
                    use_vector=request.use_vector_search,
                    use_claude=use_claude,
                    must_terms=analysis.must_terms or [],
                    should_terms=analysis.should_terms or [],
                    must_not_terms=getattr(analysis, "must_not_terms", []) or [],
                    filters_signature=filters_signature,
                    behavior_signature=behavior_signature,
                )

                # ========================================
                # â­ 1ì°¨: ë©”ëª¨ë¦¬ ìºì‹œ ì¡°íšŒ (0.001ì´ˆ)
                # ========================================
                if cache_key in memory_cache:
                    cache_payload = memory_cache[cache_key]
                    cache_hit = True
                    logger.info(f"ğŸ” ë©”ëª¨ë¦¬ ìºì‹œ íˆíŠ¸: key={cache_key[:50]}...")

                    extracted_entities_dict = cache_payload.get("extracted_entities")
                    if extracted_entities_dict is None:
                        extracted_entities_dict = extracted_entities.to_dict()

                    cached_response = _build_cached_response(
                        payload=cache_payload,
                        request=request,
                        analysis=analysis,
                        filters_for_response=filters_for_response,
                        overall_start=overall_start,
                        extracted_entities_dict=extracted_entities_dict,
                    )
                    return _finalize_search_response(
                        request=request,
                        response=cached_response,
                        analysis=analysis,
                        cache_hit=True,
                        timings=cached_response.query_analysis.get("timings_ms") if cached_response.query_analysis else None,
                    )

                # ========================================
                # â­ 2ì°¨: Redis ìºì‹œ ì¡°íšŒ (0.02ì´ˆ)
                # ========================================
                cache_payload = load_search_cache_compressed(cache_key)
                if cache_payload:
                    cache_hit = True
                    logger.info(f"ğŸ” Redis ìºì‹œ íˆíŠ¸: key={cache_key[:50]}...")

                    # â­ Redis â†’ ë©”ëª¨ë¦¬ ìºì‹œë¡œ ìŠ¹ê²©
                    memory_cache[cache_key] = cache_payload
                    logger.info(f"  âœ… Redis â†’ ë©”ëª¨ë¦¬ ìºì‹œ ìŠ¹ê²© ì™„ë£Œ")

                    extracted_entities_dict = cache_payload.get("extracted_entities")
                    if extracted_entities_dict is None:
                        extracted_entities_dict = extracted_entities.to_dict()

                    cached_response = _build_cached_response(
                        payload=cache_payload,
                        request=request,
                        analysis=analysis,
                        filters_for_response=filters_for_response,
                        overall_start=overall_start,
                        extracted_entities_dict=extracted_entities_dict,
                    )
                    return _finalize_search_response(
                        request=request,
                        response=cached_response,
                        analysis=analysis,
                        cache_hit=True,
                        timings=cached_response.query_analysis.get("timings_ms") if cached_response.query_analysis else None,
                    )
            except Exception as cache_exc:
                logger.warning(f"âš ï¸ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {cache_exc}")
                cache_key = None
                cache_enabled = False

        # â­ ë‹¨ìˆœí™”: í•„í„° ë¶„ë¥˜ ì—†ì´ ëª¨ë‘ ì‚¬ìš©
        has_demographic_filters = bool(filters_for_response)
        has_behavioral_conditions = bool(
            analysis.behavioral_conditions and
            any(v is not None for v in analysis.behavioral_conditions.values())
        )

        logger.info(f"ğŸ” í•„í„° ìƒíƒœ: {len(filters)}ê°œ (Demographics + Behavioral)")

        # 2) ì¿¼ë¦¬ ë¹Œë“œ
        # â­ í‚¤ì›Œë“œ ì •ì œëŠ” analyzerì—ì„œ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if analysis is None:
            raise RuntimeError("Query analysis not initialized")

        # ë¡œê¹…: ë¶„ì„ê¸°ì—ì„œ ì •ì œëœ ìµœì¢… í‚¤ì›Œë“œ í™•ì¸
        logger.info(f"ğŸ” [SearchAPI] ì¿¼ë¦¬ ë¶„ì„ ì™„ë£Œ:")
        logger.info(f"  âœ… Must terms: {analysis.must_terms}")
        logger.info(f"  âœ… Should terms: {analysis.should_terms}")
        logger.info(f"  âœ… Demographics: {[d.raw_value for d in extracted_entities.demographics]}")
        if hasattr(analysis, 'removed_demographic_terms') and analysis.removed_demographic_terms:
            logger.info(f"  â„¹ï¸ ì œê±°ëœ Demographics: {analysis.removed_demographic_terms}")
        if analysis.behavioral_conditions:
            logger.info(f"  âœ… Behavioral conditions: {analysis.behavioral_conditions}")

        query_builder = OpenSearchHybridQueryBuilder(config)
        query_vector = None
        if embedding_model:
            # ì™„ì „ ë™ì  ì„ë² ë”© ê¸°ë°˜ ë™ì˜ì–´ í™•ì¥ (ë„ë©”ì¸ ë¬´ê´€, ë²”ìš©)
            def _enrich_query_vector() -> Optional[list]:
                """ì„ì‹œ: ë™ì˜ì–´ í™•ì¥ ë¹„í™œì„±í™” (ì„±ëŠ¥ ìµœì í™”)"""
                try:
                    vec = embedding_model.encode(request.query).tolist()
                    logger.info("  âš ï¸ ë™ì˜ì–´ í™•ì¥ ë¹„í™œì„±í™” (ì„±ëŠ¥ ìµœì í™”)")
                    return vec
                except Exception:
                    return None

            query_vector = _enrich_query_vector()

        base_query = query_builder.build_query(
            analysis=analysis,
            query_vector=query_vector,
            size=size,
        )

        # ğŸ” Base Query ë¡œê¹… (user_id ë§ˆìŠ¤í‚¹) - DEBUG ë ˆë²¨ë¡œ ë³€ê²½
        logger.debug(f"ğŸ” [BASE QUERY] ìƒì„± ì™„ë£Œ")
        masked_base_query = _mask_user_ids_in_query(base_query)
        logger.debug(json.dumps(masked_base_query, ensure_ascii=False, indent=2))

        # 3) í•„í„° ì ìš© ì „ëµ: í•„í„°ëŠ” mustë¡œ, í‚¤ì›Œë“œëŠ” shouldë¡œ ì™„í™”
        # - í•„í„°(30ëŒ€, ì‚¬ë¬´ì§)ëŠ” ë°˜ë“œì‹œ ë§¤ì¹­ë˜ì–´ì•¼ í•¨
        # - í‚¤ì›Œë“œ ê²€ìƒ‰ì€ shouldë¡œ ì™„í™” (í•˜ë‚˜ë§Œ ë§¤ì¹­ë˜ì–´ë„ OK)
        final_query = base_query
        
        # â­ match_all/match_none/None ì œê±°: base_queryì—ì„œ match_all, match_none, Noneì´ ìˆìœ¼ë©´ ì œê±°
        existing_query = final_query.get('query', {"match_all": {}})
        if existing_query is None or existing_query == {"match_all": {}} or existing_query == {"match_none": {}}:
            # match_all/match_none/None ì œê±°
            removed_type = "None" if existing_query is None else ("match_all" if existing_query == {"match_all": {}} else "match_none")
            
            # â­ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í‚¤ì›Œë“œ ì¿¼ë¦¬ ìƒì„± (í•„í„°ë§Œ ìˆëŠ” ê²½ìš°ë¥¼ ìœ„í•´)
            if analysis.must_terms or analysis.should_terms:
                # í‚¤ì›Œë“œ ì¿¼ë¦¬ ì¬ìƒì„±
                keyword_queries = []
                if analysis.must_terms:
                    for term in analysis.must_terms:
                        keyword_queries.append({
                            "nested": {
                                "path": "qa_pairs",
                                "query": {"match": {"qa_pairs.answer_text": term}},
                                "score_mode": "max"
                            }
                        })
                
                if analysis.should_terms:
                    should_keywords = [{
                        "nested": {
                            "path": "qa_pairs",
                            "query": {"match": {"qa_pairs.answer_text": term}},
                            "score_mode": "max"
                        }
                    } for term in analysis.should_terms]
                    
                    if keyword_queries:
                        # mustì™€ should ëª¨ë‘ ìˆëŠ” ê²½ìš°
                        existing_query = {
                            "bool": {
                                "must": keyword_queries,
                                "should": should_keywords,
                                "minimum_should_match": 1
                            }
                        }
                    else:
                        # shouldë§Œ ìˆëŠ” ê²½ìš°
                        existing_query = {
                            "bool": {
                                "should": should_keywords,
                                "minimum_should_match": 1
                            }
                        }
                else:
                    # mustë§Œ ìˆëŠ” ê²½ìš°
                    if len(keyword_queries) == 1:
                        existing_query = keyword_queries[0]
                    else:
                        existing_query = {
                            "bool": {
                                "must": keyword_queries
                            }
                        }
                
                logger.info(f"âš ï¸ {removed_type} ì œê±°, í‚¤ì›Œë“œ ì¿¼ë¦¬ ì¬ìƒì„±: must={len(analysis.must_terms)}, should={len(analysis.should_terms)}")
            else:
                existing_query = None
                logger.info(f"âš ï¸ {removed_type} ì œê±°: í•„í„°ë§Œ ì‚¬ìš© (í‚¤ì›Œë“œ ì—†ìŒ)")
        
        # â­ inner_hits ì œê±° í•¨ìˆ˜ (ì¤‘ë³µ ë°©ì§€)
        def remove_inner_hits(query_dict):
            """ì¬ê·€ì ìœ¼ë¡œ inner_hits ì œê±° (í•„í„°ì—ì„œëŠ” ë§¤ì¹­ë§Œ í™•ì¸í•˜ë©´ ë˜ë¯€ë¡œ)"""
            import copy
            cleaned = copy.deepcopy(query_dict)
            
            if isinstance(cleaned, dict):
                # nested ì¿¼ë¦¬ì—ì„œ inner_hits ì œê±°
                if 'nested' in cleaned:
                    if 'inner_hits' in cleaned['nested']:
                        del cleaned['nested']['inner_hits']
                    # ì¬ê·€ì ìœ¼ë¡œ query ë‚´ë¶€ë„ ì •ì œ
                    if 'query' in cleaned['nested']:
                        cleaned['nested']['query'] = remove_inner_hits(cleaned['nested']['query'])
                
                # bool ì¿¼ë¦¬ ë‚´ë¶€ë„ ì¬ê·€ì ìœ¼ë¡œ ì •ì œ
                if 'bool' in cleaned:
                    for key in ['must', 'should', 'must_not', 'filter']:
                        if key in cleaned['bool']:
                            if isinstance(cleaned['bool'][key], list):
                                cleaned['bool'][key] = [remove_inner_hits(item) for item in cleaned['bool'][key]]
                            else:
                                cleaned['bool'][key] = remove_inner_hits(cleaned['bool'][key])
            
            return cleaned

        # â­ Behavioral í•„í„° ì¡´ì¬ ì—¬ë¶€ ì´ˆê¸°í™” (ê¸°ë³¸ê°’: False)
        has_behavioral_filters = False

        if filters:
            # â­ inner_hits ì œê±° (ì¤‘ë³µ ë°©ì§€)
            cleaned_filters = [remove_inner_hits(f) for f in filters]
            
            filter_by_type = {}
            for f in cleaned_filters:
                # í•„í„° íƒ€ì… ì¶”ì¶œ (ìƒˆë¡œìš´ bool ì¿¼ë¦¬ í˜•íƒœ ì§€ì›)
                filter_type = None
                
                # 1. bool ì¿¼ë¦¬ í˜•íƒœ (metadata OR qa_pairs)
                if 'bool' in f and 'should' in f['bool']:
                    should_clauses = f['bool']['should']
                    for clause in should_clauses:
                        # term í•„í„°ì—ì„œ íƒ€ì… ì¶”ì¶œ
                        if 'term' in clause:
                            term_key = list(clause['term'].keys())[0]
                            if 'age_group' in term_key:
                                filter_type = 'age'
                                break
                            elif 'gender' in term_key:
                                filter_type = 'gender'
                                break
                            elif 'occupation' in term_key:
                                filter_type = 'occupation'
                                break
                        # nested í•„í„°ì—ì„œ íƒ€ì… ì¶”ì¶œ
                        elif 'nested' in clause:
                            nested_q = clause['nested'].get('query', {}).get('bool', {}).get('must', [])
                            for nq in nested_q:
                                if isinstance(nq, dict) and 'bool' in nq and 'should' in nq['bool']:
                                    # q_text ë§¤ì¹­ í™•ì¸
                                    for sq in nq['bool']['should']:
                                        if 'match' in sq:
                                            match_key = list(sq['match'].keys())[0]
                                            if 'q_text' in match_key:
                                                q_text_val = sq['match'][match_key]
                                                if 'ì—°ë ¹' in str(q_text_val) or 'ë‚˜ì´' in str(q_text_val):
                                                    filter_type = 'age'
                                                    break
                                                elif 'ì„±ë³„' in str(q_text_val):
                                                    filter_type = 'gender'
                                                    break
                                                elif 'ì§ì—…' in str(q_text_val) or 'ì§ë¬´' in str(q_text_val):
                                                    filter_type = 'occupation'
                                                    break
                                elif 'match' in nq:
                                    match_key = list(nq['match'].keys())[0]
                                    if 'q_text' in match_key:
                                        q_text_val = nq['match'][match_key]
                                        if 'ì—°ë ¹' in str(q_text_val) or 'ë‚˜ì´' in str(q_text_val):
                                            filter_type = 'age'
                                            break
                                        elif 'ì„±ë³„' in str(q_text_val):
                                            filter_type = 'gender'
                                            break
                                        elif 'ì§ì—…' in str(q_text_val) or 'ì§ë¬´' in str(q_text_val):
                                            filter_type = 'occupation'
                                            break
                        if filter_type:
                            break
                
                # 2. ê¸°ì¡´ í˜•íƒœ (í•˜ìœ„ í˜¸í™˜ì„±)
                elif 'term' in f:
                    term_key = list(f['term'].keys())[0]
                    if 'age_group' in term_key:
                        filter_type = 'age'
                    elif 'gender' in term_key:
                        filter_type = 'gender'
                    elif 'occupation' in term_key:
                        filter_type = 'occupation'
                elif 'nested' in f:
                    nested_q = f['nested'].get('query', {}).get('bool', {}).get('must', [])
                    for nq in nested_q:
                        if 'match' in nq:
                            match_key = list(nq['match'].keys())[0]
                            if 'q_text' in match_key:
                                q_text_val = nq['match'][match_key]
                                if 'ì—°ë ¹' in str(q_text_val) or 'ë‚˜ì´' in str(q_text_val):
                                    filter_type = 'age'
                                elif 'ì„±ë³„' in str(q_text_val):
                                    filter_type = 'gender'
                                elif 'ì§ì—…' in str(q_text_val):
                                    filter_type = 'occupation'
                
                if filter_type:
                    if filter_type not in filter_by_type:
                        filter_by_type[filter_type] = []
                    filter_by_type[filter_type].append(f)
                else:
                    # íƒ€ì…ì„ ì•Œ ìˆ˜ ì—†ëŠ” í•„í„°ëŠ” ê·¸ëŒ€ë¡œ ì¶”ê°€
                    if 'unknown' not in filter_by_type:
                        filter_by_type['unknown'] = []
                    filter_by_type['unknown'].append(f)
            
            # â­ í•„í„°ë¥¼ should ì¡°ê±´ìœ¼ë¡œ ì „í™˜ (ì ìˆ˜ ë¶€ìŠ¤íŒ… í¬í•¨)
            # ê° íƒ€ì…ë³„ë¡œ OR, íƒ€ì… ê°„ì€ AND (shouldë¡œ ì™„í™”)
            should_filters = []
            for filter_type, type_filters in filter_by_type.items():
                if filter_type == 'unknown':
                    # â­ unknown íƒ€ì…ì€ ê°ê° ê°œë³„ì ìœ¼ë¡œ ì¶”ê°€ (AND ì²˜ë¦¬)
                    # region, marital_status ë“± ì„œë¡œ ë‹¤ë¥¸ í•„í„°ëŠ” ëª¨ë‘ ë§Œì¡±í•´ì•¼ í•¨
                    should_filters.extend(type_filters)
                elif len(type_filters) == 1:
                    # ë‹¨ì¼ í•„í„°: í•„í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì´ë¯¸ bool ì¿¼ë¦¬ í˜•íƒœ)
                    filter_item = type_filters[0]
                    should_filters.append(filter_item)
                else:
                    # ê°™ì€ íƒ€ì… í•„í„°ëŠ” OR (ì˜ˆ: 30ëŒ€ OR 40ëŒ€)
                    should_filters.append({
                        'bool': {
                            'should': type_filters,
                            "minimum_should_match": 1
                        }
                    })

            # â­â­â­ í•„í„°ë¥¼ ë‘ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¦¬:
            # 1) Demographics í•„í„° (ì—°ë ¹, ì„±ë³„, ì§ì—…): Python post-processing
            # 2) Behavioral í•„í„° (qa_pairsì˜ ë‹¤ë¥¸ ì§ˆë¬¸): OpenSearch ì¿¼ë¦¬ì— ì§ì ‘ í¬í•¨
            demographic_filters = []
            behavioral_filters = []

            logger.info(f"ğŸ” í•„í„° ë¶„ë¥˜ ì‹œì‘: should_filters={len(should_filters)}ê°œ")

            def is_demographic_filter(f):
                """Demographics í•„í„°ì¸ì§€ í™•ì¸ (ì—°ë ¹, ì„±ë³„, ì§€ì—­, ì§ì—… ë“± - Python post-processing ê°€ëŠ¥)

                â­ OCCUPATIONë„ í¬í•¨!
                - OCCUPATION í•„í„°ëŠ” metadata OR qa_pairs êµ¬ì¡°ë¡œ ìƒì„±ë¨
                - Python post-processingì—ì„œ qa_pairsë¥¼ í™•ì¸í•˜ì—¬ ì •í™•í•œ í•„í„°ë§ ìˆ˜í–‰
                - extract_from_qa_pairs_once() í•¨ìˆ˜ê°€ occupationì„ qa_pairsì—ì„œ ì¶”ì¶œí•¨
                """
                # â­ ëª¨ë“  Demographics ì²´í¬ (OCCUPATION í¬í•¨!)
                # OCCUPATIONë„ Python post-processingì—ì„œ qa_pairsë¥¼ í™•ì¸í•¨
                demo_keywords = ['ì—°ë ¹', 'ë‚˜ì´', 'ì„±ë³„',
                               'ì§€ì—­', 'ê±°ì£¼', 'ì£¼ì†Œ', 'region',
                               'ê²°í˜¼', 'í˜¼ì¸', 'ë°°ìš°ì',
                               'ì§ì—…', 'ì§ë¬´', 'occupation', 'ì§ì¢…']  # â­ OCCUPATION í‚¤ì›Œë“œ ì¶”ê°€!

                # Case 1: í•„í„°ì— nestedê°€ ì§ì ‘ ìˆëŠ” ê²½ìš°
                if 'nested' in f and 'path' in f['nested'] and f['nested']['path'] == 'qa_pairs':
                    nested_q = f['nested'].get('query', {}).get('bool', {})
                    must_list = nested_q.get('must', [])
                    for must_item in must_list:
                        if 'bool' in must_item and 'should' in must_item['bool']:
                            for should_item in must_item['bool']['should']:
                                if 'match' in should_item:
                                    for match_key, match_val in should_item['match'].items():
                                        if 'q_text' in match_key:
                                            if any(kw in str(match_val) for kw in demo_keywords):
                                                return True

                # Case 2: bool â†’ should ì•ˆì— nestedê°€ ìˆëŠ” ê²½ìš° (REGION í•„í„° êµ¬ì¡°)
                # ì˜ˆ: {"bool": {"should": [{metadata ë§¤ì¹­}, {"nested": {...}}]}}
                if 'bool' in f and 'should' in f['bool']:
                    for should_item in f['bool']['should']:
                        if 'nested' in should_item and 'path' in should_item['nested']:
                            if should_item['nested']['path'] == 'qa_pairs':
                                nested_q = should_item['nested'].get('query', {}).get('bool', {})
                                must_list = nested_q.get('must', [])
                                for must_item in must_list:
                                    if 'bool' in must_item and 'should' in must_item['bool']:
                                        for nested_should in must_item['bool']['should']:
                                            if 'match' in nested_should:
                                                for match_key, match_val in nested_should['match'].items():
                                                    if 'q_text' in match_key:
                                                        if any(kw in str(match_val) for kw in demo_keywords):
                                                            return True

                return False

            for f in should_filters:
                is_demo = is_demographic_filter(f)
                if is_demo:
                    demographic_filters.append(f)
                    # â­ ë””ë²„ê¹…: Demographicsë¡œ ë¶„ë¥˜ëœ í•„í„° ë¡œê·¸ ì¶œë ¥
                    logger.info(f"   âœ… Demographicsë¡œ ë¶„ë¥˜: {json.dumps(f, ensure_ascii=False)[:200]}")
                else:
                    behavioral_filters.append(f)
                    # â­ ë””ë²„ê¹…: Behavioralë¡œ ë¶„ë¥˜ëœ í•„í„° ë¡œê·¸ ì¶œë ¥
                    logger.info(f"   âš ï¸ Behavioralë¡œ ë¶„ë¥˜: {json.dumps(f, ensure_ascii=False)[:200]}")

            logger.info(f"ğŸ” í•„í„° ë¶„ë¦¬:")
            logger.info(f"   - Demographics í•„í„° (Python post-processing): {len(demographic_filters)}ê°œ")
            logger.info(f"   - Behavioral í•„í„° (OpenSearch ì§ì ‘ ì ìš©): {len(behavioral_filters)}ê°œ")

            # â­ should_filtersë¥¼ demographic_filtersë¡œ ëŒ€ì²´ (Python post-processingìš©)
            should_filters = demographic_filters

            # â­ Behavioral í•„í„° ì¡´ì¬ ì—¬ë¶€ (Qdrant ë¹„í™œì„±í™” íŒë‹¨ìš©)
            has_behavioral_filters = bool(behavioral_filters)

            # â­â­â­ Demographics í•„í„°ë§Œ Python post-processingìœ¼ë¡œ ì´ë™
            # ì´ìœ : ë¹„êµ¬ì¡°í™”ëœ ì„¤ë¬¸ ë°ì´í„°ëŠ” ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œë§Œ ì°¾ì„ ìˆ˜ ìˆìŒ
            logger.info(f"âœ… Demographics í•„í„°ë¥¼ Python post-processingìœ¼ë¡œ ì´ë™ ({len(demographic_filters)}ê°œ í•„í„°)")
            logger.info(f"   â†’ OpenSearchëŠ” í‚¤ì›Œë“œ + Behavioral ê²€ìƒ‰ ìˆ˜í–‰, QdrantëŠ” ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰")
            logger.info(f"   â†’ RRF í›„ Pythonì—ì„œ Demographics í•„í„° ì ìš©í•˜ì—¬ ì •í™•ë„ ìœ ì§€")

            # â­â­â­ Behavioral í•„í„°ëŠ” OpenSearch ì¿¼ë¦¬ì— ì§ì ‘ í¬í•¨
            # ì´ìœ : qa_pairsëŠ” OpenSearchì—ë§Œ ìˆìœ¼ë¯€ë¡œ ì§ì ‘ ê²€ìƒ‰í•´ì•¼ í•¨
            if behavioral_filters:
                logger.info(f"âœ… Behavioral í•„í„°ë¥¼ OpenSearch ì¿¼ë¦¬ì— ì§ì ‘ í¬í•¨ ({len(behavioral_filters)}ê°œ í•„í„°)")

                # í‚¤ì›Œë“œ ì¿¼ë¦¬ì™€ Behavioral í•„í„°ë¥¼ ê²°í•©
                if existing_query is None or existing_query == {"match_all": {}} or existing_query == {"match_none": {}}:
                    # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ Behavioral í•„í„°ë§Œ ì‚¬ìš©
                    final_query['query'] = {
                        'bool': {
                            'must': behavioral_filters
                        }
                    }
                    logger.info(f"   â†’ í‚¤ì›Œë“œ ì—†ìŒ: Behavioral í•„í„°ë§Œ ì‚¬ìš©")
                else:
                    # â­ Behavioral í•„í„°ëŠ” must (í•„ìˆ˜), í‚¤ì›Œë“œëŠ” should (ì ìˆ˜ ë¶€ìŠ¤íŒ…)
                    # ì´ìœ : nested ì¿¼ë¦¬ ê°„ ì¶©ëŒ ë°©ì§€ + í‚¤ì›Œë“œë¡œ ê²°ê³¼ ë­í‚¹ ê°œì„ 
                    final_query['query'] = {
                        'bool': {
                            'must': behavioral_filters,
                            'should': [existing_query],
                            'minimum_should_match': 0
                        }
                    }
                    logger.info(f"   â†’ Behavioral í•„í„° (í•„ìˆ˜) + í‚¤ì›Œë“œ ì¿¼ë¦¬ (ì ìˆ˜ ë¶€ìŠ¤íŒ…)")
            else:
                # Behavioral í•„í„°ê°€ ì—†ìœ¼ë©´ í‚¤ì›Œë“œ ì¿¼ë¦¬ë§Œ ì‚¬ìš©
                if existing_query is None or existing_query == {"match_all": {}} or existing_query == {"match_none": {}}:
                    final_query['query'] = {"match_all": {}}
                    logger.info(f"   â†’ í‚¤ì›Œë“œ ì—†ìŒ: match_all ì‚¬ìš©")
                else:
                    final_query['query'] = existing_query
                    logger.info(f"   â†’ í‚¤ì›Œë“œ ì¿¼ë¦¬ë§Œ ì ìš©")
        
        if 'size' not in final_query:
            final_query['size'] = size

        if filters:
            logger.debug(f"ğŸ” ì ìš©ëœ í•„í„° ({len(filters)}ê°œ):")
            for i, f in enumerate(filters, 1):
                masked_filter = _mask_user_ids_in_query(f)
                logger.debug(f"  í•„í„° {i}: {json.dumps(masked_filter, ensure_ascii=False, indent=2)}")
            logger.debug(f"ğŸ” ìµœì¢… ì¿¼ë¦¬ êµ¬ì¡°:")
            masked_final_query = _mask_user_ids_in_query(final_query)
            logger.debug(f"  {json.dumps(masked_final_query, ensure_ascii=False, indent=2)}")
        else:
            logger.debug(f"ğŸ” ìµœì¢… ì¿¼ë¦¬ êµ¬ì¡° (í•„í„° ì—†ìŒ):")
            masked_final_query = _mask_user_ids_in_query(final_query)
            logger.debug(f"  {json.dumps(masked_final_query, ensure_ascii=False, indent=2)}")

        # â­ Qdrant top-N ì œí•œ: í•„í„° ìœ ë¬´ì— ë”°ë¼ ë¶„ê¸°
        has_filters = bool(filters)
        rrf_k_used: Optional[int] = None
        rrf_reason: str = ""
        adaptive_threshold: Optional[float] = None
        threshold_reason: str = ""
        has_behavioral = bool(getattr(analysis, "behavioral_conditions", None))

        # â­â­â­ ê²€ìƒ‰ í¬ê¸° ì¦ê°€: Python post-filteringì„ ìœ„í•´ ì¶©ë¶„í•œ í›„ë³´ í™•ë³´
        # Demographics/Behavioral í•„í„°ë¥¼ OpenSearch ì¿¼ë¦¬ì—ì„œ ì œê±°í–ˆìœ¼ë¯€ë¡œ ë” ë§ì€ í›„ë³´ í•„ìš”
        if has_filters or has_behavioral:
            # Behavioral/Demographics í•„í„°ê°€ ìˆìœ¼ë©´ ì „ì²´ ë°ì´í„° ëŒ€ìƒìœ¼ë¡œ ê²€ìƒ‰
            # â­ ì „ì²´ ë°ì´í„°: 35000ê±´ â†’ ì¶©ë¶„í•œ ê²°ê³¼ë¥¼ ìœ„í•´ 35000ê±´ ì¡°íšŒ
            qdrant_limit = min(max(size * 10, 5000), 10000)   # QdrantëŠ” 10000ê°œë¡œ ì œí•œ (ì„±ëŠ¥)
            search_size = min(max(size * 20, 35000), 50000)   # â­ ìµœì†Œ 35000ê±´, ìµœëŒ€ 50000ê±´
            logger.info(f"ğŸ” í•„í„° ìˆìŒ (Python post-processing): OpenSearch size={search_size}, Qdrant limit={qdrant_limit}")
        else:
            # í•„í„°ê°€ ì—†ì–´ë„ ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•´ ì¶©ë¶„í•œ í›„ë³´ í™•ë³´
            qdrant_limit = min(max(size * 5, 500), 50000)     # ê¸°ë³¸ 500ê°œ, ìµœëŒ€ 50000ê°œ (ì „ì²´ ë°ì´í„° ì•½ 35000ê°œ)
            search_size = min(max(size * 10, 500), 50000)    # ê¸°ë³¸ 500ê°œ, ìµœëŒ€ 50000ê°œ (ì „ì²´ ë°ì´í„° ì•½ 35000ê°œ)
            logger.info(f"ğŸ” í•„í„° ì—†ìŒ: OpenSearch size={search_size}, Qdrant limit={qdrant_limit}")

        # 4) ì‹¤í–‰: í•˜ì´ë¸Œë¦¬ë“œ (OpenSearch + ì„ íƒì  Qdrant) with RRF
        # â­ survey_responses_merged ë‹¨ì¼ ì¸ë±ìŠ¤ë§Œ ê²€ìƒ‰
        
        # OpenSearch _source filtering: í•„ìš”í•œ í•„ë“œë§Œ ì¡°íšŒ
        # â­ qa_pairs í¬í•¨: marital_status, region ë“±ì´ qa_pairsì— ìˆì„ ìˆ˜ ìˆìŒ
        source_filter = {
            "includes": ["user_id", "metadata", "timestamp", "qa_pairs"],
            "excludes": []  # í•„ìš”ì‹œ ì œì™¸í•  í•„ë“œ ì¶”ê°€
        }
        
        # â­â­â­ survey_responses_merged ë‹¨ì¼ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©
        logger.info(f"ğŸ” ì¸ë±ìŠ¤ ê²€ìƒ‰: {request.index_name} (survey_responses_mergedë§Œ ì‚¬ìš©)")
        
        # survey_responses_merged ì¸ë±ìŠ¤ ê²€ìƒ‰
        keyword_results: List[Dict[str, Any]] = []
        vector_results: List[Dict[str, Any]] = []
        
        try:
            # â­â­â­ OpenSearch Scroll API ì‚¬ìš© (ì „ì²´ ë°ì´í„° ì¡°íšŒ)
            query_body = final_query.copy()
            if not isinstance(query_body.get('query'), dict):
                logger.warning("  âš ï¸ ì¿¼ë¦¬ê°€ ë¹„ì–´ ìˆì–´ match_allë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤")
                query_body['query'] = {"match_all": {}}

            # ğŸ” OpenSearch ì¿¼ë¦¬ ë¡œê¹… (ë””ë²„ê¹…ìš©) - DEBUG ë ˆë²¨ë¡œ ì¶•ì†Œ, user_id ë§ˆìŠ¤í‚¹
            logger.debug(f"ğŸ” OpenSearch Scroll ì¿¼ë¦¬:")
            masked_query_body = _mask_user_ids_in_query(query_body)
            logger.debug(json.dumps(masked_query_body, ensure_ascii=False, indent=2))

            # â­â­â­ ë©”ëª¨ë¦¬ ìºì‹œ ì‚¬ìš© (ì´ˆê³ ì†!) vs Scroll API (ëŠë¦¼)
            opensearch_start = perf_counter()

            if panel_cache.loaded:
                # â­ ë©”ëª¨ë¦¬ ìºì‹œì—ì„œ ì¦‰ì‹œ ì¡°íšŒ (0.01ì´ˆ ì´í•˜!)
                logger.info("  âš¡ ë©”ëª¨ë¦¬ ìºì‹œ ì‚¬ìš©: panel_cacheì—ì„œ ì „ì²´ ë°ì´í„° ì¡°íšŒ")

                # ì „ì²´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ ë©”ëª¨ë¦¬ì— ë¡œë“œë¨)
                all_user_ids = panel_cache.get_all_user_ids()

                # OpenSearch í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜ì„± ìœ ì§€)
                keyword_results = []
                for user_id in all_user_ids:
                    doc = panel_cache.user_map.get(user_id)
                    if doc:
                        keyword_results.append(doc)

                opensearch_duration_ms = (perf_counter() - opensearch_start) * 1000
                timings['memory_cache_ms'] = opensearch_duration_ms

                opensearch_total_hits = len(keyword_results)
                logger.info(f"  âœ… ë©”ëª¨ë¦¬ ìºì‹œ: {len(keyword_results)}ê±´ ({opensearch_duration_ms:.2f}ms) ğŸš€")
                
                # â­ SSE ìŠ¤íŠ¸ë¦¬ë°: OpenSearch ê²°ê³¼ ê°œìˆ˜ ì „ë‹¬
                if stream_callback:
                    try:
                        stream_callback('opensearch_results', {'count': opensearch_total_hits})
                    except Exception as e:
                        logger.warning(f"âš ï¸ stream_callback ì˜¤ë¥˜ (opensearch_results): {e}")

            else:
                # â­ Fallback: Scroll API (ë©”ëª¨ë¦¬ ìºì‹œ ì—†ì„ ë•Œ)
                logger.warning("  âš ï¸ ë©”ëª¨ë¦¬ ìºì‹œ ë¯¸ì‚¬ìš© â†’ Scroll API ì‚¬ìš© (ëŠë¦¼)")

                scroll_hits = await data_fetcher.scroll_search_async(
                    index_name=request.index_name,
                    query=query_body,
                    batch_size=1000,
                    scroll_time="5m",
                    num_slices=8,  # â­ 8ê°œë¡œ ì¦ê°€ (ë³‘ë ¬ì„± í–¥ìƒ)
                    source_filter=source_filter,
                    request_timeout=300,
                )
                opensearch_duration_ms = (perf_counter() - opensearch_start) * 1000
                timings['opensearch_scroll_ms'] = opensearch_duration_ms

                keyword_results = scroll_hits
                opensearch_total_hits = len(keyword_results)
                logger.info(f"  âœ… OpenSearch Scroll: {len(keyword_results)}ê±´ ({opensearch_duration_ms:.2f}ms)")
                
                # â­ SSE ìŠ¤íŠ¸ë¦¬ë°: OpenSearch ê²°ê³¼ ê°œìˆ˜ ì „ë‹¬
                if stream_callback:
                    try:
                        stream_callback('opensearch_results', {'count': opensearch_total_hits})
                    except Exception as e:
                        logger.warning(f"âš ï¸ stream_callback ì˜¤ë¥˜ (opensearch_results): {e}")

            # â­â­â­ Qdrant ë²¡í„° ê²€ìƒ‰ (survey_responses_merged í†µí•© ì»¬ë ‰ì…˜)
            # Behavioral í•„í„°ê°€ ìˆìœ¼ë©´ Qdrant ë¹„í™œì„±í™” (qa_pairsëŠ” OpenSearchì—ë§Œ ìˆìŒ)
            if has_behavioral_filters:
                logger.info(f"  âš ï¸ Behavioral í•„í„° ê°ì§€ â†’ Qdrant ë¹„í™œì„±í™” (OpenSearchë§Œ ì‚¬ìš©)")
                logger.info(f"     ì´ìœ : qa_pairsëŠ” OpenSearchì—ë§Œ ìˆì–´ì„œ ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ í•„í„°ë§ ë¶ˆê°€")
                # â­ SSE ìŠ¤íŠ¸ë¦¬ë°: Qdrant ë¹„í™œì„±í™” ì•Œë¦¼
                if stream_callback:
                    try:
                        logger.info(f"  ğŸ“¡ SSE: Qdrant ë¹„í™œì„±í™” â†’ count=0 ì „ì†¡")
                        stream_callback('qdrant_results', {'count': 0})
                    except Exception as e:
                        logger.warning(f"âš ï¸ stream_callback ì˜¤ë¥˜ (qdrant_results): {e}")
            elif request.use_vector_search and query_vector and hasattr(router, 'qdrant_client'):
                qdrant_client = router.qdrant_client
                try:
                    # â­ survey_responses_merged í†µí•© ì»¬ë ‰ì…˜ ì‚¬ìš© - ì‹œê°„ ì¸¡ì • ì‹œì‘
                    qdrant_start = perf_counter()
                    collection_name = request.index_name  # survey_responses_merged
                    logger.info(f"  ğŸ” Qdrant ì»¬ë ‰ì…˜: {collection_name} (í†µí•© ì»¬ë ‰ì…˜)")
                    try:
                        r = qdrant_client.search(
                            collection_name=collection_name,
                            query_vector=query_vector,
                            limit=qdrant_limit,
                            score_threshold=0.3,
                        )
                        for item in r:
                            vector_results.append({
                                '_id': str(item.id),
                                '_score': item.score,
                                '_source': item.payload,
                                '_index': collection_name,
                            })
                        qdrant_duration_ms = (perf_counter() - qdrant_start) * 1000
                        timings['qdrant_search_ms'] = qdrant_duration_ms
                        logger.info(f"  âœ… Qdrant ({collection_name}): {len(vector_results)}ê±´ ({qdrant_duration_ms:.2f}ms)")
                        
                        # â­ SSE ìŠ¤íŠ¸ë¦¬ë°: Qdrant ê²°ê³¼ ê°œìˆ˜ ì „ë‹¬
                        if stream_callback:
                            try:
                                stream_callback('qdrant_results', {'count': len(vector_results)})
                            except Exception as e:
                                logger.warning(f"âš ï¸ stream_callback ì˜¤ë¥˜ (qdrant_results): {e}")
                    except Exception as e:
                        logger.warning(f"  âš ï¸ Qdrant ì»¬ë ‰ì…˜ '{collection_name}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                except Exception as e:
                    logger.warning(f"  âš ï¸ Qdrant ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        except Exception as e:
            logger.warning(f"  âš ï¸ ì¸ë±ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

        # user_id ë° _id -> ì›ë³¸ ë¬¸ì„œ ë§¤í•‘ ìƒì„± - ì‹œê°„ ì¸¡ì • ì‹œì‘
        mapping_start = perf_counter()
        user_doc_map = {}
        id_doc_map = {}

        # â­ OpenSearch í‚¤ì›Œë“œ ê²°ê³¼ ë§¤í•‘
        for hit in keyword_results:
            source = hit.get('_source', {})
            user_id = source.get('user_id')
            doc_id = hit.get('_id')

            doc_info = {
                'source': source,
                'inner_hits': hit.get('inner_hits', {}),
                'highlight': hit.get('highlight'),
                'index': hit.get('_index', 'unknown')
            }

            if user_id:
                user_doc_map[user_id] = doc_info
            if doc_id:
                id_doc_map[doc_id] = doc_info

        mapping_duration_ms = (perf_counter() - mapping_start) * 1000
        timings['user_doc_mapping_ms'] = mapping_duration_ms
        logger.info(f"  âœ… user_doc_map ìƒì„± ì™„ë£Œ: {len(user_doc_map)}ê°œ ({mapping_duration_ms:.2f}ms)")

        # â­â­â­ Qdrant ë²¡í„° ê²°ê³¼ì—ì„œ user_id ìˆ˜ì§‘ ë° metadata ë³´ê°•
        # Qdrant payloadì—ëŠ” metadataê°€ ì—†ìœ¼ë¯€ë¡œ OpenSearchì—ì„œ ì „ì²´ ë¬¸ì„œ ì¡°íšŒ í•„ìš”
        if vector_results:
            qdrant_user_ids = set()
            for doc in vector_results:
                payload = doc.get('_source', {})
                user_id = payload.get('user_id')
                if user_id and user_id not in user_doc_map:
                    qdrant_user_ids.add(user_id)

            if qdrant_user_ids:
                logger.info(f"  ğŸ” Qdrant ê²°ê³¼ ì¤‘ metadata ì—†ëŠ” user_id: {len(qdrant_user_ids)}ê°œ")
                logger.info(f"     â†’ OpenSearchì—ì„œ ì „ì²´ ë¬¸ì„œ ì¡°íšŒ ì¤‘...")

                try:
                    # OpenSearchì—ì„œ user_idë¡œ ì „ì²´ ë¬¸ì„œ ì¡°íšŒ
                    # user_idëŠ” keyword íƒ€ì…ì´ë¯€ë¡œ terms ì¿¼ë¦¬ ì‚¬ìš©
                    user_id_list = list(qdrant_user_ids)
                    bulk_query = {
                        "query": {
                            "terms": {
                                "user_id": user_id_list
                            }
                        },
                        "size": len(qdrant_user_ids),
                        "_source": ["user_id", "metadata", "qa_pairs", "timestamp"]
                    }

                    # ğŸ” Bulk ì¿¼ë¦¬ ë¡œê¹… (user_id ë¦¬ìŠ¤íŠ¸ ë§ˆìŠ¤í‚¹)
                    logger.debug(f"     Bulk query: terms user_id (count={len(user_id_list)})")

                    bulk_response = data_fetcher.search_opensearch(
                        index_name=request.index_name,
                        query=bulk_query,
                        size=len(qdrant_user_ids),
                        source_filter=None,
                        request_timeout=DEFAULT_OS_TIMEOUT,
                    )

                    fetched_count = 0
                    for hit in bulk_response['hits']['hits']:
                        source = hit.get('_source', {})
                        user_id = source.get('user_id')
                        if user_id:
                            doc_info = {
                                'source': source,
                                'inner_hits': {},
                                'highlight': None,
                                'index': hit.get('_index', 'unknown')
                            }
                            user_doc_map[user_id] = doc_info
                            fetched_count += 1

                    logger.info(f"     âœ… {fetched_count}ê°œ user ë¬¸ì„œ ì¡°íšŒ ì™„ë£Œ (metadata í¬í•¨)")
                except Exception as e:
                    logger.warning(f"     âš ï¸ Bulk ì¡°íšŒ ì‹¤íŒ¨: {e}")
                    logger.warning(f"     â†’ Qdrant ê²°ê³¼ ì¤‘ ì¼ë¶€ëŠ” metadata ì—†ì´ í•„í„°ë§ë¨")

        # â­ RRF ê²°í•©
        logger.info(f"\n{'='*60}")
        logger.info("ğŸ“Š RRF ê²°í•©")
        logger.info(f"{'='*60}")

        # ë²¡í„° ê²°ê³¼ì— ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„° ë³´ê°•
        for doc in vector_results:
            if '_index' not in doc:
                doc['_index'] = doc.get('collection', doc.get('_source', {}).get('index', request.index_name))

        logger.info(f"  - í‚¤ì›Œë“œ ê²°ê³¼: {len(keyword_results)}ê±´")
        logger.info(f"  - ë²¡í„°   ê²°ê³¼: {len(vector_results)}ê±´")

        rrf_start = perf_counter()

        if request.use_vector_search and vector_results:
            combined_rrf, rrf_k_used, rrf_reason = calculate_rrf_score_adaptive(
                keyword_results=keyword_results,
                vector_results=vector_results,
                query_intent=getattr(analysis, "intent", None),
                has_filters=has_filters,
                use_vector_search=request.use_vector_search,
            )
        else:
            combined_rrf = keyword_results
            if request.use_vector_search:
                rrf_reason = "ë²¡í„° ê²°ê³¼ ì—†ìŒ â†’ í‚¤ì›Œë“œ ê²°ê³¼ ì‚¬ìš©"
            else:
                rrf_reason = "ë²¡í„° ê²€ìƒ‰ ë¹„í™œì„±í™” â†’ í‚¤ì›Œë“œ ê²°ê³¼ ì‚¬ìš©"
            rrf_k_used = 0

        user_rrf_variants: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        for doc in combined_rrf:
            user_id = get_user_id_from_doc(doc)
            if not user_id:
                continue
            user_rrf_variants[user_id].append(doc)
        
        user_rrf_map: Dict[str, List[Dict[str, Any]]] = {}
        final_rrf_results: List[Dict[str, Any]] = []
        for user_id, docs in user_rrf_variants.items():
            def _score(doc: Dict[str, Any]) -> float:
                score = doc.get('_score')
                if score is None:
                    score = doc.get('rrf_score', 0.0)
                return float(score or 0.0)

            best_doc_original = max(docs, key=_score)
            total_rrf_score = sum(_score(doc) for doc in docs)
            sources = [doc.get('_index', 'unknown') for doc in docs]

            best_doc = dict(best_doc_original)
            best_doc['_score'] = total_rrf_score
            best_doc['_rrf_details'] = {
                    'combined_score': total_rrf_score,
                'source_count': len(docs),
                'sources': sources,
                }

            # â­ RRF ì¬ì¡°í•© ì‹œì ì— OpenSearch metadata merge
            # best_docì´ Qdrant ë¬¸ì„œì¸ ê²½ìš° metadataê°€ ì—†ìœ¼ë¯€ë¡œ OpenSearchì—ì„œ ê°€ì ¸ì˜´
            if user_id and user_id in user_doc_map:
                opensearch_doc = user_doc_map[user_id]
                if opensearch_doc and isinstance(opensearch_doc, dict):
                    opensearch_source = opensearch_doc.get("source", {})
                    if isinstance(opensearch_source, dict):
                        # best_docì˜ _source ê°€ì ¸ì˜¤ê¸°
                        current_source = best_doc.get('_source', {})
                        if not isinstance(current_source, dict):
                            current_source = {}

                        # OpenSearch ë°ì´í„° ìš°ì„ ìœ¼ë¡œ merge (metadata, qa_pairs ë³´ì¡´)
                        merged_source = {}
                        merged_source.update(current_source)        # Qdrant: user_id, text
                        merged_source.update(opensearch_source)     # OpenSearch: metadata, qa_pairs
                        best_doc['_source'] = merged_source

            final_rrf_results.append(best_doc)
            # best_docë¥¼ ì²« ë²ˆì§¸ë¡œ ìœ ì§€í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” ì°¸ê³ ìš©ìœ¼ë¡œ ë³´ê´€
            others = [doc for doc in docs if doc is not best_doc_original]
            user_rrf_map[user_id] = [best_doc] + others
        
        final_rrf_results.sort(
            key=lambda d: d.get('_score', 0.0) or d.get('rrf_score', 0.0),
            reverse=True
        )
        
        rrf_results = final_rrf_results
        took_ms = 0  # ì—¬ëŸ¬ ê²€ìƒ‰ì˜ í•©ì´ë¯€ë¡œ ì •í™•í•œ ì‹œê°„ ì¸¡ì •ì€ ì–´ë ¤ì›€
        
        logger.info(f"  âœ… ë‹¨ì¼ RRF ê²°í•© ì™„ë£Œ: {len(rrf_results)}ê±´ (ê³ ìœ  user_id: {len(user_rrf_map)}ê°œ)")
        timings['rrf_recombination_ms'] = (perf_counter() - rrf_start) * 1000
        
        # â­ SSE ìŠ¤íŠ¸ë¦¬ë°: RRF Fusion ì´ë²¤íŠ¸ ì „ë‹¬
        if stream_callback:
            try:
                stream_callback('rrf_fusion', {
                    'opensearch_count': len(keyword_results),
                    'qdrant_count': len(vector_results),
                    'combined_count': len(rrf_results),
                    'alpha': analysis.alpha,
                    'rrf_k': rrf_k_used,
                    'rrf_reason': rrf_reason
                })
            except Exception as e:
                logger.warning(f"âš ï¸ stream_callback ì˜¤ë¥˜ (rrf_fusion): {e}")
        
        # â­ SSE ìŠ¤íŠ¸ë¦¬ë°: í•„í„°ë§ ì „ ê°œìˆ˜ ì „ë‹¬
        if stream_callback:
            try:
                stream_callback('before_filter', {'count': len(rrf_results)})
            except Exception as e:
                logger.warning(f"âš ï¸ stream_callback ì˜¤ë¥˜ (before_filter): {e}")

        # í›„ë³´ ë¬¸ì„œ ìˆ˜ ì œí•œ (í›„ì²˜ë¦¬ ë¶€ë‹´ ì™„í™”)
        fetch_size = window_size
        candidate_cap = max(
            fetch_size * 20,
            cache_limit if cache_limit else 0,
            40000  # ì „ì²´ ë°ì´í„° ì•½ 35000ê°œë¥¼ ê³ ë ¤í•˜ì—¬ ì¦ê°€
        )
        if candidate_cap and len(rrf_results) > candidate_cap:
            logger.info(
                f"  - í›„ë³´ ë¬¸ì„œ ì œí•œ ì ìš©: {len(rrf_results)} â†’ {candidate_cap} (size={fetch_size})"
            )
            rrf_results = rrf_results[:candidate_cap]
        elif len(rrf_results) < fetch_size:
            backup_cap = max(fetch_size * 6, fetch_size + 50)
            logger.info(
                f"  - í›„ë³´ ìˆ˜ê°€ sizeë³´ë‹¤ ì‘ì•„ ì¦ê°€ ì‹œë„: {len(rrf_results)} â†’ {min(len(final_rrf_results), backup_cap)}"
            )
            rrf_results = final_rrf_results[:backup_cap]
        
        # RRF ì ìˆ˜ ë””ë²„ê¹…: ìƒìœ„ 10ê°œ ì¶œë ¥
        if rrf_results:
            logger.info(f"  - RRF ì ìˆ˜ ìƒìœ„ 10ê°œ:")
            for i, doc in enumerate(rrf_results[:10], 1):
                rrf_score = doc.get('_score') or doc.get('rrf_score', 0.0)
                rrf_details = doc.get('_rrf_details', {})
                doc_index = doc.get('_index', 'unknown')
                logger.info(f"    {i}. doc_id={doc.get('_id', 'N/A')}, index={doc_index}, RRF={rrf_score:.6f}, "
                          f"keyword_rank={rrf_details.get('keyword_rank')}, vector_rank={rrf_details.get('vector_rank')}")
        
        # â­ ë””ë²„ê¹…: ë£¨í”„ ì „ extracted_entities.demographics í™•ì¸
        logger.warning(f"[EXTRACTED ENTITIES] demographics count: {len(extracted_entities.demographics)}")
        for demo in extracted_entities.demographics:
            logger.warning(f"  - {demo.demographic_type.value}: {demo.value}")

        demographic_filters: Dict[DemographicType, List["DemographicEntity"]] = defaultdict(list)
        for demo in extracted_entities.demographics:
            demographic_filters[demo.demographic_type].append(demo)

        # â­ ë””ë²„ê¹…: demographic_filters ë‚´ìš© í™•ì¸
        logger.warning(f"[DEMO FILTERS] demographic_filters keys: {[k.value for k in demographic_filters.keys()]}")
        for demo_type, demo_list in demographic_filters.items():
            logger.warning(f"  [{demo_type.value}]: {[d.value for d in demo_list]}")

        filtered_rrf_results: List[Dict[str, Any]] = rrf_results
        # â­ total_hitsëŠ” ë‚˜ì¤‘ì— ì‹¤ì œ ë°˜í™˜ëœ ê²°ê³¼ ìˆ˜ë¡œ ì„¤ì •ë¨ (len(results))
        # ì„ì‹œë¡œ rrf_results ê¸¸ì´ ì‚¬ìš©
        total_hits = len(rrf_results)

        occupation_display_map: Dict[str, str] = {}
        behavior_values_map: Dict[str, Dict[str, Optional[bool]]] = {}
        doc_user_map: Dict[int, str] = {}
        # â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ welcome_1st/welcome_2nd ë°°ì¹˜ ì œê±°
        synonym_cache: Dict[str, List[str]] = {}

        PLACEHOLDER_TOKENS: Set[str] = {
            "",
            "ë¯¸ì •",
            "ì—†ìŒ",
            "ë¬´ì‘ë‹µ",
            "í•´ë‹¹ì—†ìŒ",
            "n/a",
            "na",
            "null",
            "none",
            "unknown",
            "ë¯¸ì„ íƒ",
            "ë¯¸ê¸°ì¬",
        }
        PLACEHOLDER_TOKENS = {token.strip().lower() for token in PLACEHOLDER_TOKENS}

        def normalize_value(value: Any) -> str:
            if value is None:
                return ""
            if isinstance(value, bool):
                value_str = str(value)
            elif isinstance(value, (int, float)):
                try:
                    if value.is_integer():  # type: ignore[attr-defined]
                        value = int(value)
                except AttributeError:
                    pass
                value_str = str(value)
            else:
                value_str = str(value)

            cleaned = value_str.strip()
            lower = cleaned.lower()
            if lower in PLACEHOLDER_TOKENS:
                return ""
            return lower

        def build_expected_values(demo: "DemographicEntity") -> Set[str]:
            key = f"{demo.demographic_type.value}:{demo.raw_value}"
            expected: Set[str] = set()
            expected.add(demo.raw_value)
            expected.add(demo.value)
            expected.update(demo.synonyms or set())
            expected.update(synonym_cache.get(key, []))
            normalized_expected = {normalize_value(v) for v in expected if v}

            if demo.demographic_type == DemographicType.GENDER:
                male_aliases = {
                    normalize_value(v)
                    for v in {"m", "male", "man", "ë‚¨", "ë‚¨ì„±", "ë‚¨ì", "ë‚¨ì„±í˜•"}
                }
                female_aliases = {
                    normalize_value(v)
                    for v in {"f", "female", "woman", "ì—¬", "ì—¬ì„±", "ì—¬ì", "ì—¬ì„±í˜•"}
                }
                if normalized_expected & male_aliases:
                    normalized_expected.update(male_aliases)
                if normalized_expected & female_aliases:
                    normalized_expected.update(female_aliases)

            return normalized_expected

        def values_match(values: Set[str], expected: Set[str]) -> bool:
            """ê°’ ë§¤ì¹­ ê²€ì¦ (ì •í™•í•œ ë§¤ì¹­ë§Œ í—ˆìš©)"""
            if not values or not expected:
                return False

            # â­ ì •í™•í•œ ë§¤ì¹­ë§Œ í—ˆìš© (ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­ ì œê±°)
            # "ë‚¨ì„±" in "ì—¬ì„±" ê°™ì€ ì˜¤ë§¤ì¹­ ë°©ì§€
            for val in values:
                if not val:
                    continue
                # ì •ê·œí™”ëœ ê°’ë¼ë¦¬ ì •í™•íˆ ë¹„êµ
                if val in expected:
                    return True
            return False

        def expand_gender_aliases(values: Set[str]) -> None:
            male_aliases = {"m", "ë‚¨", "ë‚¨ì„±", "male", "man", "ë‚¨ì"}
            female_aliases = {"f", "ì—¬", "ì—¬ì„±", "female", "woman", "ì—¬ì"}
            if values & male_aliases:
                values.update(male_aliases)
            if values & female_aliases:
                values.update(female_aliases)

        def add_age_decade(values: Set[str], age_value: Any) -> None:
            if age_value in (None, ""):
                return
            try:
                age_int = int(age_value)
                decade = (age_int // 10) * 10
                for candidate in (f"{decade}ëŒ€", f"{decade}s", str(age_int)):
                    normalized_candidate = normalize_value(candidate)
                    if normalized_candidate:
                        values.add(normalized_candidate)
            except (ValueError, TypeError):
                pass

        filter_start = perf_counter()
        if 'has_filter_constraints' not in locals():
            has_filter_constraints = has_demographic_filters or has_behavioral_conditions
        if has_filter_constraints:

            # â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ welcome_1st ê´€ë ¨ ë¡œì§ ì œê±°
            gender_dsl_handled = bool(demographic_filters.get(DemographicType.GENDER))
            age_dsl_handled = bool(demographic_filters.get(DemographicType.AGE))
            occupation_dsl_handled = bool(demographic_filters.get(DemographicType.OCCUPATION))

            # â­ ëª¨ë“  demographic_filtersë¥¼ ê²€ì¦ (REGION, MARITAL_STATUS, OCCUPATION í¬í•¨!)
            # OCCUPATIONì€ demographic_filtersë¡œ ë¶„ë¥˜ë˜ì–´ Python post-processingì—ì„œ ì²˜ë¦¬ë¨
            filters_to_validate: List[DemographicType] = list(demographic_filters.keys())
            logger.info(f"  âœ… í›„ì²˜ë¦¬ ê²€ì¦ ëŒ€ìƒ: {[f.value for f in filters_to_validate]}")

            for demo in extracted_entities.demographics:
                cache_key = f"{demo.demographic_type.value}:{demo.raw_value}"
                if demo.demographic_type in {DemographicType.GENDER, DemographicType.OCCUPATION}:
                    try:
                        from rag_query_analyzer.utils.synonym_expander import get_synonym_expander
                        expander = get_synonym_expander()
                        synonym_cache[cache_key] = expander.expand(demo.raw_value)
                    except Exception:
                        synonyms = [demo.raw_value]
                        synonyms.extend([syn for syn in demo.synonyms if syn])
                        synonym_cache[cache_key] = synonyms
                else:
                    synonym_cache[cache_key] = [demo.raw_value]
            
            user_ids_to_fetch: Set[str] = set()
            doc_user_map.clear()
            
            logger.info(f"ğŸ” user_id ìˆ˜ì§‘ ì¤‘: RRF ê²°ê³¼ {len(rrf_results)}ê±´...")
            for doc in rrf_results:
                source = doc.get('_source', {})
                if not source and 'doc' in doc:
                    source = doc.get('doc', {}).get('_source', {})
                if not source or not isinstance(source, dict):
                    if 'payload' in doc:
                        payload = doc.get('payload', {})
                        if isinstance(payload, dict) and payload:
                            source = payload
                    elif isinstance(source, dict) and 'payload' in source:
                        payload = source.get('payload', {})
                        if isinstance(payload, dict) and payload:
                            source = payload
                user_id = None
                if isinstance(source, dict):
                    user_id = source.get('user_id')
                if not user_id:
                    user_id = doc.get('_id', '')
                if not user_id and 'payload' in doc:
                    payload = doc.get('payload', {})
                    if isinstance(payload, dict):
                        user_id = payload.get('user_id')
                if user_id:
                    user_ids_to_fetch.add(user_id)
                    doc_user_map[id(doc)] = user_id
            
            logger.info(f"  âœ… ìˆ˜ì§‘ëœ user_id: {len(user_ids_to_fetch)}ê±´")
            # â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ welcome_1st/welcome_2nd ë°°ì¹˜ ì¡°íšŒ ì œê±°

            # â­â­â­ Demographics í•„í„° ë˜ëŠ” Behavioral ì¡°ê±´ì´ ìˆìœ¼ë©´ Python post-processing ì‹¤í–‰
            has_behavioral_conditions = bool(analysis.behavioral_conditions and any(v is not None for v in analysis.behavioral_conditions.values()))

            if not filters_to_validate and not has_behavioral_conditions:
                timings["post_filter_ms"] = (perf_counter() - filter_start) * 1000
                filtered_rrf_results = rrf_results
                logger.info("  âœ… í•„í„° ì—†ìŒ: Python í›„ì²˜ë¦¬ ìƒëµ")
            else:
                def collect_doc_values(
                    user_id: str,
                    source: Dict[str, Any],
                    metadata: Dict[str, Any],
                    _unused: Dict[str, Any],  # í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€í•˜ì§€ë§Œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                ) -> Tuple[Dict[DemographicType, Set[str]], Dict[DemographicType, bool], Dict[str, Optional[bool]]]:
                    doc_values: Dict[DemographicType, Set[str]] = {
                        DemographicType.GENDER: set(),
                        DemographicType.AGE: set(),
                        DemographicType.OCCUPATION: set(),
                    }
                    metadata_presence: Dict[DemographicType, bool] = {
                        DemographicType.GENDER: False,
                        DemographicType.AGE: False,
                        DemographicType.OCCUPATION: False,
                    }
                    # â­ ë™ì  behavior_values ìƒì„± (Claudeê°€ ì¶”ì¶œí•œ ì¡°ê±´ ê¸°ë°˜)
                    behavior_values: Dict[str, Optional[bool]] = {}
                    if analysis.behavioral_conditions:
                        for key in analysis.behavioral_conditions.keys():
                            behavior_values[key] = None

                    def record_behavior(key: str, value: Optional[bool]) -> None:
                        if value is None:
                            return
                        if behavior_values.get(key) is None:
                            behavior_values[key] = value

                    def parse_yes_no(text: Optional[str]) -> Optional[bool]:
                        if not text:
                            return None
                        normalized = text.lower()
                        if any(keyword in normalized for keyword in BEHAVIOR_NO_TOKENS):
                            return False
                        if any(keyword in normalized for keyword in BEHAVIOR_YES_TOKENS):
                            return True
                        return None

                    def parse_smoker_answer(raw: Optional[Any]) -> Optional[bool]:
                        if raw is None:
                            return None
                        if isinstance(raw, (list, tuple, set)):
                            for item in raw:
                                decision = parse_smoker_answer(item)
                                if decision is not None:
                                    return decision
                            return None
                        text = str(raw).strip()
                        if not text:
                            return None
                        normalized = text.lower()
                        compact = normalized.replace(" ", "")
                        for keyword in SMOKER_NEGATIVE_KEYWORDS:
                            keyword_compact = keyword.replace(" ", "")
                            if keyword in normalized or keyword_compact in compact:
                                return False
                        for keyword in SMOKER_POSITIVE_KEYWORDS:
                            keyword_compact = keyword.replace(" ", "")
                            if keyword in normalized or keyword_compact in compact:
                                return True
                        if (
                            "ë‹´ë°°" in normalized
                            and not any(token in normalized for token in ("ì—†", "ì•ˆ", "ì•Š", "no", "ë¬´", "ë¯¸í¡ì—°"))
                        ):
                            return True
                        return parse_yes_no(text)

                    def parse_drinker_answer(raw: Optional[Any]) -> Optional[bool]:
                        """ìŒì£¼ ì—¬ë¶€ íŒŒì‹±: ìˆ  ì¢…ë¥˜ê°€ ìˆìœ¼ë©´ True, 'ìˆ ì„ ë§ˆì‹œì§€ ì•ŠìŒ'ì´ ìˆìœ¼ë©´ False"""
                        if raw is None:
                            return None
                        if isinstance(raw, (list, tuple, set)):
                            for item in raw:
                                decision = parse_drinker_answer(item)
                                if decision is not None:
                                    return decision
                            return None
                        text = str(raw).strip()
                        if not text:
                            return None
                        normalized = text.lower()
                        compact = normalized.replace(" ", "")

                        # ë„¤ê±°í‹°ë¸Œ í‚¤ì›Œë“œ ìš°ì„  ì²´í¬
                        for keyword in NON_DRINKER_KEYWORDS:
                            keyword_compact = keyword.replace(" ", "")
                            if keyword in normalized or keyword_compact in compact:
                                return False

                        # í¬ì§€í‹°ë¸Œ í‚¤ì›Œë“œ ì²´í¬
                        for keyword in DRINKER_POSITIVE_KEYWORDS:
                            keyword_compact = keyword.replace(" ", "")
                            if keyword in normalized or keyword_compact in compact:
                                return True

                        # "ìˆ " ì–¸ê¸‰ì´ ìˆì§€ë§Œ ë¶€ì • í‘œí˜„ì´ ì—†ìœ¼ë©´ ìŒì£¼ìë¡œ ê°„ì£¼
                        if (
                            "ìˆ " in normalized
                            and not any(token in normalized for token in ("ì—†", "ì•ˆ", "ì•Š", "no", "ë¬´", "ëª»"))
                        ):
                            return True

                        return None

                    metadata_candidates = [
                        metadata,  # ì´ì œ ë‹¨ì¼ metadataë§Œ ì‚¬ìš©
                        source.get("metadata", {}) if isinstance(source, dict) else {},
                    ]

                    payload = {}
                    if isinstance(source, dict):
                        payload_candidate = source.get("payload")
                        if isinstance(payload_candidate, dict):
                            payload = payload_candidate
                    if not payload and isinstance(source, dict) and "doc" in source:
                        doc_payload = source.get("doc", {}).get("payload")
                        if isinstance(doc_payload, dict):
                            payload = doc_payload
                    if not payload and isinstance(source, dict):
                        payload = source

                    if isinstance(payload, dict):
                        metadata_candidates.append(payload.get("metadata", {}))

                    for candidate in metadata_candidates:
                        if not isinstance(candidate, dict):
                            continue

                        candidate_sources: List[Dict[str, Any]] = [candidate]
                        nested_meta = candidate.get("metadata")
                        if isinstance(nested_meta, dict):
                            candidate_sources.append(nested_meta)

                        for meta_source in candidate_sources:
                            gender_val = meta_source.get("gender") or meta_source.get("gender_code")
                            if gender_val:
                                normalized_gender = normalize_value(gender_val)
                                if normalized_gender:
                                    doc_values[DemographicType.GENDER].add(normalized_gender)
                                    metadata_presence[DemographicType.GENDER] = True

                            age_group_val = meta_source.get("age_group")
                            if age_group_val:
                                normalized_age_group = normalize_value(age_group_val)
                                if normalized_age_group:
                                    doc_values[DemographicType.AGE].add(normalized_age_group)
                                    metadata_presence[DemographicType.AGE] = True

                            age_val = meta_source.get("age")
                            if age_val:
                                pre_count = len(doc_values[DemographicType.AGE])
                                add_age_decade(doc_values[DemographicType.AGE], age_val)
                                if len(doc_values[DemographicType.AGE]) > pre_count:
                                    metadata_presence[DemographicType.AGE] = True

                            birth_year_val = meta_source.get("birth_year")
                            if birth_year_val:
                                normalized_birth_year = normalize_value(birth_year_val)
                                if normalized_birth_year:
                                    doc_values[DemographicType.AGE].add(normalized_birth_year)
                                    metadata_presence[DemographicType.AGE] = True

                            occupation_val = meta_source.get("occupation") or meta_source.get("job")
                            if occupation_val:
                                normalized_occupation = normalize_value(occupation_val)
                                if normalized_occupation:
                                    doc_values[DemographicType.OCCUPATION].add(normalized_occupation)
                                    metadata_presence[DemographicType.OCCUPATION] = True

                            vehicle_val = meta_source.get("has_vehicle")
                            if vehicle_val:
                                normalized_vehicle = normalize_value(vehicle_val)
                                record_behavior("has_vehicle", parse_yes_no(normalized_vehicle))

                    if user_id:
                        qa_sources: List[List[Dict[str, Any]]] = []
                        if isinstance(source, dict):
                            qa_sources.append(source.get("qa_pairs", []) or [])
                        # â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ welcome_1st/welcome_2nd ë°°ì¹˜ ì œê±°

                        for qa_pairs in qa_sources:
                            for qa in qa_pairs:
                                if not isinstance(qa, dict):
                                    continue
                                q_text_raw = str(qa.get("q_text", "")).lower()
                                q_text = normalize_value(qa.get("q_text"))
                                answer_candidates = [
                                    qa.get("answer"),
                                    qa.get("answer_text"),
                                    qa.get("value"),
                                ]
                                answers: List[str] = []
                                for candidate in answer_candidates:
                                    if candidate is None:
                                        continue
                                    if isinstance(candidate, list):
                                        answers.extend(str(item) for item in candidate if item)
                                    else:
                                        answers.append(str(candidate))
                                normalized_answers = {normalize_value(ans) for ans in answers if ans}

                                if q_text and normalized_answers:
                                    if any(keyword in q_text_raw for keyword in ("ì§ì—…", "ì§ë¬´", "occupation")):
                                        # â­ occupation ì •ê·œí™”: ê´„í˜¸ ì´ì „ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                                        # ì˜ˆ: "ì „ë¬¸ì§ (ì˜ì‚¬, ê°„í˜¸ì‚¬...)" â†’ "ì „ë¬¸ì§"
                                        cleaned_occupations = set()
                                        for ans in normalized_answers:
                                            # ê´„í˜¸ê°€ ìˆìœ¼ë©´ ê´„í˜¸ ì´ì „ ë¶€ë¶„ë§Œ ì‚¬ìš©
                                            if '(' in ans:
                                                cleaned = ans.split('(')[0].strip()
                                                if cleaned:
                                                    cleaned_occupations.add(cleaned)
                                            else:
                                                cleaned_occupations.add(ans)
                                        doc_values[DemographicType.OCCUPATION].update(cleaned_occupations)

                                    if (
                                        not metadata_presence[DemographicType.GENDER]
                                        and any(keyword in q_text_raw for keyword in ("ì„±ë³„", "gender"))
                                    ):
                                        doc_values[DemographicType.GENDER].update(normalized_answers)
                                        metadata_presence[DemographicType.GENDER] = True

                        # â­ ë²”ìš© behavioral ì¶”ì¶œ (ë™ì  - Claudeê°€ ìš”êµ¬í•œ ì¡°ê±´ë§Œ ì¶”ì¶œ)
                        if analysis.behavioral_conditions:
                            qa_pairs_list = source.get("qa_pairs", []) or []
                            for behavior_key in behavior_values.keys():
                                if behavior_values.get(behavior_key) is None:
                                    extracted_value = extract_behavior_from_qa_pairs(qa_pairs_list, behavior_key, debug=False)
                                    if extracted_value is not None:
                                        behavior_values[behavior_key] = extracted_value

                    return doc_values, metadata_presence, behavior_values

                # â­â­â­ ì„±ëŠ¥ ìµœì í™”: QA pairsë¥¼ 1ë²ˆë§Œ ìˆœíšŒí•´ì„œ occupationê³¼ marital_status ì¶”ì¶œ
                def extract_from_qa_pairs_once(
                    source: Dict[str, Any],
                    needs_occupation: bool,
                    needs_marital: bool,
                    occupation_expected: Set[str],
                    marital_expected: Set[str]
                ) -> Tuple[Optional[str], Optional[str]]:
                    """QA pairsë¥¼ 1ë²ˆë§Œ ìˆœíšŒí•´ì„œ occupation displayì™€ marital_status ì¶”ì¶œ"""
                    display_occupation = None
                    marital_val = None

                    qa_sources: List[List[Dict[str, Any]]] = []
                    if isinstance(source, dict):
                        qa_sources.append(source.get("qa_pairs", []) or [])

                    for qa_pairs in qa_sources:
                        for qa in qa_pairs:
                            if not isinstance(qa, dict):
                                continue
                            q_text = str(qa.get("q_text", "")).lower()

                            # Occupation ì°¾ê¸°
                            if needs_occupation and not display_occupation:
                                if any(keyword in q_text for keyword in ("ì§ì—…", "ì§ë¬´", "occupation", "ì§ì¢…")):
                                    answer = qa.get("answer")
                                    if answer is None:
                                        answer = qa.get("answer_text")
                                    if answer:
                                        candidate_value = str(answer)
                                        normalized_candidate = normalize_value(candidate_value)

                                        # â­â­â­ ë¶€ë¶„ ë§¤ì¹­: "ì˜ì—…ì§" â†’ "ì˜ì—…" í¬í•¨ í™•ì¸
                                        # ì˜ˆ: "ë¬´ì—­â€¢ì˜ì—…â€¢íŒë§¤â€¢ë§¤ì¥ê´€ë¦¬"ì— "ì˜ì—…" í¬í•¨
                                        matched = False
                                        for expected in occupation_expected:
                                            # "ì§" ì œê±°í•˜ì—¬ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
                                            keyword = expected.replace('ì§', '')
                                            if keyword and keyword in normalized_candidate:
                                                matched = True
                                                break

                                        if matched:
                                            display_occupation = candidate_value

                            # Marital status ì°¾ê¸°
                            if needs_marital and not marital_val:
                                if any(keyword in q_text for keyword in ("ê²°í˜¼", "í˜¼ì¸", "marital")):
                                    answer = qa.get("answer") or qa.get("answer_text")
                                    if answer:
                                        marital_val = str(answer)

                            # ë‘˜ ë‹¤ ì°¾ì•˜ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
                            if (not needs_occupation or display_occupation) and (not needs_marital or marital_val):
                                break

                        # ë‘˜ ë‹¤ ì°¾ì•˜ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
                        if (not needs_occupation or display_occupation) and (not needs_marital or marital_val):
                            break

                    return display_occupation, marital_val

                # â­â­â­â­â­ ë©”ëª¨ë¦¬ ìºì‹œ ê¸°ë°˜ ì´ˆê³ ì† í•„í„°ë§! (Stage 1+2 í†µí•©)
                # ë³€ìˆ˜ ì´ˆê¸°í™” (fallback ê²½ë¡œì—ì„œ ì‚¬ìš©)
                expected_values_cache = {}
                stage1_duration_ms = 0
                stage2_duration_ms = 0

                if panel_cache.loaded:
                    logger.info(f"\n{'='*60}")
                    logger.info(f"âš¡ ë©”ëª¨ë¦¬ ìºì‹œ ê¸°ë°˜ í†µí•© í•„í„°ë§ (Stage 1+2 í•œë°©!)")
                    logger.info(f"{'='*60}")

                    filter_start = perf_counter()

                    # â­ Demographics í•„í„° ì¶”ì¶œ
                    gender_filter = None
                    age_filter = None
                    region_filter = None
                    sub_region_filter = None
                    occupation_filter = None
                    marital_filter = None

                    if DemographicType.GENDER in demographic_filters:
                        values = [d.value for d in demographic_filters[DemographicType.GENDER]]
                        gender_filter = values[0] if values else None  # ì²«ë²ˆì§¸ ê°’ ì‚¬ìš©

                    if DemographicType.AGE in demographic_filters:
                        values = [d.value for d in demographic_filters[DemographicType.AGE]]
                        age_filter = values[0] if values else None

                    if DemographicType.REGION in demographic_filters:
                        values = [d.value for d in demographic_filters[DemographicType.REGION]]
                        region_filter = values[0] if values else None

                    if DemographicType.SUB_REGION in demographic_filters:
                        values = [d.value for d in demographic_filters[DemographicType.SUB_REGION]]
                        sub_region_filter = values[0] if values else None

                    if DemographicType.OCCUPATION in demographic_filters:
                        values = [d.value for d in demographic_filters[DemographicType.OCCUPATION]]
                        occupation_filter = values[0] if values else None

                    if DemographicType.MARITAL_STATUS in demographic_filters:
                        values = [d.value for d in demographic_filters[DemographicType.MARITAL_STATUS]]
                        marital_filter = values[0] if values else None

                    # â­â­â­ í•œë°© í•„í„°ë§! (Pandas ë²¡í„°í™” - 0.1ì´ˆ ì´í•˜!)
                    filtered_df = panel_cache.filter_all(
                        gender=gender_filter,
                        age_group=age_filter,
                        region=region_filter,
                        sub_region=sub_region_filter,
                        occupation=occupation_filter,
                        marital_status=marital_filter,
                        behavioral_conditions=analysis.behavioral_conditions
                    )

                    filter_duration_ms = (perf_counter() - filter_start) * 1000
                    timings["post_filter_ms"] = filter_duration_ms

                    logger.info(f"  âœ… ë©”ëª¨ë¦¬ ìºì‹œ í•„í„°ë§: {panel_cache.total_count}ê±´ â†’ {len(filtered_df)}ê±´ ({filter_duration_ms:.2f}ms) ğŸš€")
                    logger.info(f"{'='*60}\n")

                    # â­ í•„í„°ë§ëœ user_idë¡œ full document ê°€ì ¸ì˜¤ê¸°
                    filtered_user_ids = filtered_df['user_id'].tolist()
                    filtered_list = panel_cache.get_user_docs(filtered_user_ids)
                    filtered_rrf_results = filtered_list

                    logger.info(f"ğŸ“Š í•„í„°ë§ í†µê³„:")
                    logger.info(f"  - ë©”ëª¨ë¦¬ ìºì‹œ í†µí•© í•„í„°ë§: {filter_duration_ms:.2f}ms")
                    logger.info(f"  - âœ… ìµœì¢… ê²°ê³¼: {len(filtered_list)}ê±´")
                    
                    # â­ SSE ìŠ¤íŠ¸ë¦¬ë°: Filter Breakdown ì´ë²¤íŠ¸ ì „ë‹¬
                    if stream_callback:
                        try:
                            breakdown_steps = []
                            before_count = len(rrf_results)
                            after_count = len(filtered_list)
                            
                            # Demographics í•„í„° ë‹¨ê³„
                            if demographic_filters:
                                demo_desc = ", ".join([
                                    f"{k.value}={v[0].value}" 
                                    for k, v in demographic_filters.items() 
                                    if v
                                ])
                                breakdown_steps.append({
                                    'filter': f'demographics ({demo_desc})',
                                    'removed': before_count - after_count,
                                    'remaining': after_count
                                })
                            
                            # Behavioral í•„í„° ë‹¨ê³„
                            if analysis.behavioral_conditions:
                                active_behavioral = {
                                    k: v for k, v in analysis.behavioral_conditions.items() 
                                    if v is not None
                                }
                                if active_behavioral:
                                    behav_desc = ", ".join([
                                        f"{k}={v}" for k, v in active_behavioral.items()
                                    ])
                                    breakdown_steps.append({
                                        'filter': f'behavioral ({behav_desc})',
                                        'removed': 0,  # ë©”ëª¨ë¦¬ ìºì‹œëŠ” í†µí•© í•„í„°ë§ì´ë¯€ë¡œ ê°œë³„ ë‹¨ê³„ ì¶”ì  ë¶ˆê°€
                                        'remaining': after_count
                                    })
                            
                            if breakdown_steps:
                                stream_callback('filter_breakdown', {
                                    'steps': breakdown_steps,
                                    'total_removed': before_count - after_count,
                                    'final_count': after_count
                                })
                        except Exception as e:
                            logger.warning(f"âš ï¸ stream_callback ì˜¤ë¥˜ (filter_breakdown): {e}")
                    
                    # â­ SSE ìŠ¤íŠ¸ë¦¬ë°: í•„í„°ë§ í›„ ê°œìˆ˜ ì „ë‹¬
                    if stream_callback:
                        try:
                            stream_callback('after_filter', {'count': len(filtered_list)})
                        except Exception as e:
                            logger.warning(f"âš ï¸ stream_callback ì˜¤ë¥˜ (after_filter): {e}")

                else:
                    # â­â­â­ Fallback: ê¸°ì¡´ Stage 1+2 ë¡œì§ (ë©”ëª¨ë¦¬ ìºì‹œ ì—†ì„ ë•Œ)
                    logger.warning("  âš ï¸ ë©”ëª¨ë¦¬ ìºì‹œ ë¯¸ì‚¬ìš© â†’ ê¸°ì¡´ Stage 1+2 ë¡œì§ ì‹¤í–‰ (ëŠë¦¼)")

                    # â­â­â­ ì„±ëŠ¥ ìµœì í™”: build_expected_valuesë¥¼ ë£¨í”„ ë°–ì—ì„œ 1ë²ˆë§Œ ê³„ì‚°
                    # ê° demographic íƒ€ì…ë³„ë¡œ expected values ì‚¬ì „ ê³„ì‚°
                    expected_values_cache = {}
                    for demo_type in filters_to_validate:
                        if demo_type in demographic_filters:
                            expected = set()
                            for demo in demographic_filters[demo_type]:
                                expected.update(build_expected_values(demo))
                            expected_values_cache[demo_type] = expected
                            logger.debug(f"  âœ… {demo_type.value} expected values ì‚¬ì „ ê³„ì‚°: {expected}")

                    # â­â­â­ STAGE 1: Pandas ë²¡í„°í™” í•„í„°ë§ (metadataë§Œ - ì´ˆê³ ì†!)
                    logger.info(f"\n{'='*60}")
                    logger.info(f"âš¡ STAGE 1: Pandas metadata í•„í„°ë§ ì‹œì‘")
                    logger.info(f"{'='*60}")

                    stage1_start = perf_counter()
                    metadata_list = []
                    doc_id_to_doc = {}

                    debug_sample_count = 0
                    for doc in rrf_results:
                        user_id = doc_user_map.get(id(doc))
                        if not user_id:
                            continue

                        # â­ docì—ì„œ ì§ì ‘ _source ê°€ì ¸ì˜¤ê¸°
                        source = doc.get('_source', {})

                        if not isinstance(source, dict):
                            source = {}

                        # ë””ë²„ê¹…: ì²˜ìŒ 3ê°œ ìƒ˜í”Œì˜ _source êµ¬ì¡° í™•ì¸
                        if debug_sample_count < 3:
                            logger.info(f"  [DEBUG {debug_sample_count+1}] user_id={user_id}")
                            logger.info(f"     doc keys: {list(doc.keys())}")
                            logger.info(f"     _source keys: {list(source.keys()) if source else 'Empty'}")
                            logger.info(f"     _source content preview: {str(source)[:200]}...")

                        metadata = source.get("metadata", {}) if isinstance(source.get("metadata"), dict) else {}

                        # ë””ë²„ê¹…: ì²˜ìŒ 3ê°œ ìƒ˜í”Œ ë¡œê¹…
                        if debug_sample_count < 3:
                            logger.info(f"     metadata keys: {list(metadata.keys()) if metadata else 'None'}")
                            logger.info(f"     gender: {metadata.get('gender')}, region: {metadata.get('region')}")
                            debug_sample_count += 1

                        # metadata ì •ë³´ ì €ì¥
                        metadata_list.append({
                            'doc_id': id(doc),
                            'user_id': user_id,
                            'gender': metadata.get('gender') or metadata.get('gender_code'),
                            'age_group': metadata.get('age_group') or metadata.get('age'),
                            'region': metadata.get('region'),
                            'sub_region': metadata.get('sub_region'),
                        })
                        doc_id_to_doc[id(doc)] = (doc, source, metadata)

                    # Pandas DataFrame ìƒì„±
                    df = pd.DataFrame(metadata_list)
                    logger.info(f"  ğŸ“Š ì „ì²´ ë¬¸ì„œ: {len(df)}ê±´")

                    # Pandas ë²¡í„°í™” í•„í„°ë§ (metadataë§Œ)
                    mask = pd.Series([True] * len(df))

                    # GENDER í•„í„°
                    if DemographicType.GENDER in filters_to_validate:
                        expected_genders = expected_values_cache.get(DemographicType.GENDER, set())
                        # normalize + expand aliases
                        expanded_expected = set()
                        for g in expected_genders:
                            expanded_expected.add(g)
                            # gender aliases
                            if g == 'ë‚¨ì„±': expanded_expected.update(['ë‚¨ì', 'ë‚¨', 'male', 'm', 'man', 'ë‚¨ì„±í˜•'])
                            if g == 'ì—¬ì„±': expanded_expected.update(['ì—¬ì', 'ì—¬', 'female', 'f', 'woman', 'ì—¬ì„±í˜•'])

                        # DataFrame ê°’ ì •ê·œí™” ë° í•„í„°ë§ (None ì œì™¸)
                        df['gender_normalized'] = df['gender'].apply(lambda x: normalize_value(x) if x else None)
                        gender_mask = df['gender_normalized'].notna() & df['gender_normalized'].isin(expanded_expected)
                        mask &= gender_mask
                        logger.info(f"  âœ… GENDER í•„í„°: {expected_genders} â†’ {mask.sum()}ê±´ í†µê³¼")

                    # AGE í•„í„°
                    if DemographicType.AGE in filters_to_validate:
                        expected_ages = expected_values_cache.get(DemographicType.AGE, set())
                        df['age_normalized'] = df['age_group'].apply(lambda x: normalize_value(x) if x else None)
                        age_mask = df['age_normalized'].notna() & df['age_normalized'].isin(expected_ages)
                        mask &= age_mask
                        logger.info(f"  âœ… AGE í•„í„°: {expected_ages} â†’ {mask.sum()}ê±´ í†µê³¼")

                    # REGION í•„í„°
                    if DemographicType.REGION in filters_to_validate:
                        expected_regions = expected_values_cache.get(DemographicType.REGION, set())
                        df['region_normalized'] = df['region'].apply(lambda x: normalize_value(x) if x else None)
                        region_mask = df['region_normalized'].notna() & df['region_normalized'].isin(expected_regions)
                        mask &= region_mask
                        logger.info(f"  âœ… REGION í•„í„°: {expected_regions} â†’ {mask.sum()}ê±´ í†µê³¼ (region ê°’ ìƒ˜í”Œ: {df['region'].value_counts().head(5).to_dict()})")

                    # SUB_REGION í•„í„°
                    if DemographicType.SUB_REGION in filters_to_validate:
                        expected_sub_regions = expected_values_cache.get(DemographicType.SUB_REGION, set())
                        df['sub_region_normalized'] = df['sub_region'].apply(lambda x: normalize_value(x) if x else None)
                        sub_region_mask = df['sub_region_normalized'].notna() & df['sub_region_normalized'].isin(expected_sub_regions)
                        mask &= sub_region_mask
                        logger.info(f"  âœ… SUB_REGION í•„í„°: {expected_sub_regions} â†’ {mask.sum()}ê±´ í†µê³¼")

                    # í•„í„°ë§ëœ ê²°ê³¼
                    candidate_df = df[mask]
                    stage1_duration_ms = (perf_counter() - stage1_start) * 1000
                    logger.info(f"\nâš¡ STAGE 1 ì™„ë£Œ: {len(df)}ê±´ â†’ {len(candidate_df)}ê±´ ({stage1_duration_ms:.2f}ms)")
                    logger.info(f"{'='*60}\n")

                    # â­â­â­ STAGE 2: qa_pairs/behavioral ì²´í¬ (í•„í„°ë§ëœ ë¬¸ì„œë§Œ)
                    logger.info(f"âš¡ STAGE 2: qa_pairs/behavioral í•„í„°ë§ ì‹œì‘ ({len(candidate_df)}ê±´)")

                    stage2_start = perf_counter()
                    filtered_list = []

                    # ì¹´ìš´í„° ì´ˆê¸°í™”
                    occupation_filter_failed = 0
                    occupation_metadata_missing = 0
                    marital_status_filter_failed = 0
                    marital_status_metadata_missing = 0
                    behavior_filter_failed = 0
                    behavior_metadata_missing = 0
                    debug_counter = 0

                    for _, row in candidate_df.iterrows():
                        doc, source, metadata = doc_id_to_doc[row['doc_id']]
                        user_id = row['user_id']

                        # â­ collect_doc_valuesë¡œ qa_pairsì™€ behavior_values ìˆ˜ì§‘
                        doc_values, metadata_presence, behavior_values = collect_doc_values(user_id, source, metadata, {})
                        behavior_values_map[user_id] = dict(behavior_values)

                        # â­â­â­ QA pairsì—ì„œ occupationê³¼ marital_status 1ë²ˆë§Œ ìˆœíšŒ ì¶”ì¶œ
                        needs_occupation = DemographicType.OCCUPATION in filters_to_validate
                        needs_marital = DemographicType.MARITAL_STATUS in filters_to_validate
                        qa_occupation = None
                        qa_marital = None

                        if needs_occupation or needs_marital:
                            occupation_expected = expected_values_cache.get(DemographicType.OCCUPATION, set()) if needs_occupation else set()
                            marital_expected = expected_values_cache.get(DemographicType.MARITAL_STATUS, set()) if needs_marital else set()
                            qa_occupation, qa_marital = extract_from_qa_pairs_once(source, needs_occupation, needs_marital, occupation_expected, marital_expected)

                        # â­ Stage 1ì—ì„œ ì´ë¯¸ gender, age, region, sub_region í•„í„° í†µê³¼í–ˆìœ¼ë¯€ë¡œ
                        # Stage 2ì—ì„œëŠ” occupation, marital_status, behavioralë§Œ ì²´í¬

                        occupation_pass = True
                        marital_status_pass = True
                        behavior_pass = True

                        # â­â­â­ OCCUPATION í•„í„° (ë¶€ë¶„ ë§¤ì¹­)
                        if needs_occupation:
                            expected = expected_values_cache.get(DemographicType.OCCUPATION, set())
                            actual_occupations = doc_values[DemographicType.OCCUPATION]

                            # â­ ë¶€ë¶„ ë§¤ì¹­: "ì˜ì—…ì§" â†’ "ì˜ì—…" í¬í•¨ í™•ì¸
                            occupation_pass = False
                            for actual in actual_occupations:
                                for expected_val in expected:
                                    # "ì§" ì œê±°í•˜ì—¬ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
                                    keyword = expected_val.replace('ì§', '')
                                    if keyword and keyword in actual:
                                        occupation_pass = True
                                        break
                                if occupation_pass:
                                    break

                            if not occupation_pass:
                                occupation_filter_failed += 1
                            else:
                                # Display occupation ì €ì¥
                                if qa_occupation:
                                    occupation_display_map[user_id] = qa_occupation

                        # â­â­â­ MARITAL_STATUS í•„í„°
                        if needs_marital:
                            expected = expected_values_cache.get(DemographicType.MARITAL_STATUS, set())
                            marital_val = metadata.get("marital_status") or qa_marital

                            if marital_val:
                                normalized_marital = normalize_value(marital_val)
                                if normalized_marital:
                                    marital_status_pass = values_match({normalized_marital}, expected)
                                    if not marital_status_pass:
                                        marital_status_filter_failed += 1
                                else:
                                    marital_status_pass = False
                                    marital_status_filter_failed += 1
                            else:
                                marital_status_metadata_missing += 1
                                marital_status_pass = False

                        # â­â­â­ BEHAVIORAL í•„í„°
                        if analysis.behavioral_conditions:
                            for condition_key, expected_value in analysis.behavioral_conditions.items():
                                if expected_value is None:
                                    continue

                                actual_value = behavior_values.get(condition_key)
                                if actual_value is None:
                                    behavior_metadata_missing += 1
                                    behavior_pass = False
                                    break
                                if actual_value != expected_value:
                                    behavior_filter_failed += 1
                                    behavior_pass = False
                                    break

                        # â­ Stage 2 í•„í„° ê²€ì¦ ì™„ë£Œ
                        all_pass = occupation_pass and marital_status_pass and behavior_pass

                        if all_pass:
                            filtered_list.append(doc)

                            # â­â­â­ Early Termination (ì œê±°ë¨ - ì •í™•í•œ ì „ì²´ ì¹´ìš´íŠ¸ë¥¼ ìœ„í•´)
                            # if len(filtered_list) >= size * 3:
                            #     logger.info(f"  âš¡ Early termination: {len(filtered_list)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
                            #     break

                    # Stage 2 ì™„ë£Œ
                    stage2_duration_ms = (perf_counter() - stage2_start) * 1000
                    logger.info(f"\nâš¡ STAGE 2 ì™„ë£Œ: {len(candidate_df)}ê±´ â†’ {len(filtered_list)}ê±´ ({stage2_duration_ms:.2f}ms)")
                    logger.info(f"{'='*60}\n")

                    # ì „ì²´ í•„í„°ë§ ì‹œê°„
                    total_filter_duration_ms = stage1_duration_ms + stage2_duration_ms
                    timings["post_filter_ms"] = total_filter_duration_ms
                    filtered_rrf_results = filtered_list

                    # í†µê³„ ë¡œê·¸
                    logger.info(f"ğŸ“Š í•„í„°ë§ í†µê³„:")
                    logger.info(f"  - Stage 1 (Pandas metadata): {stage1_duration_ms:.2f}ms")
                    logger.info(f"  - Stage 2 (qa_pairs/behavioral): {stage2_duration_ms:.2f}ms")
                    logger.info(f"  - ì „ì²´ í•„í„°ë§ ì‹œê°„: {total_filter_duration_ms:.2f}ms")
                    
                    # â­ SSE ìŠ¤íŠ¸ë¦¬ë°: Filter Breakdown ì´ë²¤íŠ¸ ì „ë‹¬ (Fallback ê²½ë¡œ)
                    if stream_callback:
                        try:
                            breakdown_steps = []
                            before_count = len(rrf_results)
                            after_stage1 = len(candidate_df)
                            after_stage2 = len(filtered_list)
                            
                            # Stage 1 (Demographics) í•„í„° ë‹¨ê³„
                            if before_count != after_stage1:
                                demo_desc = ", ".join([
                                    f"{k.value}={v[0].value}" 
                                    for k, v in demographic_filters.items() 
                                    if v and k in filters_to_validate
                                ])
                                breakdown_steps.append({
                                    'filter': f'demographics ({demo_desc})',
                                    'removed': before_count - after_stage1,
                                    'remaining': after_stage1
                                })
                            
                            # Stage 2 (Behavioral/Occupation/Marital) í•„í„° ë‹¨ê³„
                            if after_stage1 != after_stage2:
                                stage2_filters = []
                                if DemographicType.OCCUPATION in filters_to_validate:
                                    stage2_filters.append('occupation')
                                if DemographicType.MARITAL_STATUS in filters_to_validate:
                                    stage2_filters.append('marital_status')
                                if analysis.behavioral_conditions:
                                    active_behavioral = {
                                        k: v for k, v in analysis.behavioral_conditions.items() 
                                        if v is not None
                                    }
                                    if active_behavioral:
                                        stage2_filters.append('behavioral')
                                
                                filter_desc = ", ".join(stage2_filters) if stage2_filters else 'qa_pairs'
                                breakdown_steps.append({
                                    'filter': f'{filter_desc}',
                                    'removed': after_stage1 - after_stage2,
                                    'remaining': after_stage2
                                })
                            
                            if breakdown_steps:
                                stream_callback('filter_breakdown', {
                                    'steps': breakdown_steps,
                                    'total_removed': before_count - after_stage2,
                                    'final_count': after_stage2
                                })
                        except Exception as e:
                            logger.warning(f"âš ï¸ stream_callback ì˜¤ë¥˜ (filter_breakdown): {e}")
                    
                    # â­ SSE ìŠ¤íŠ¸ë¦¬ë°: í•„í„°ë§ í›„ ê°œìˆ˜ ì „ë‹¬
                    if stream_callback:
                        try:
                            stream_callback('after_filter', {'count': len(filtered_list)})
                        except Exception as e:
                            logger.warning(f"âš ï¸ stream_callback ì˜¤ë¥˜ (after_filter): {e}")

                    if DemographicType.OCCUPATION in filters_to_validate:
                        logger.info(f"  - OCCUPATION ë¯¸ì¶©ì¡±: {occupation_filter_failed}ê±´")
                    if DemographicType.MARITAL_STATUS in filters_to_validate:
                        logger.info(f"  - MARITAL_STATUS ë¯¸ì¶©ì¡±: {marital_status_filter_failed}ê±´")
                    if analysis.behavioral_conditions:
                        logger.info(f"  - BEHAVIORAL ë¯¸ì¶©ì¡±: {behavior_filter_failed}ê±´")

                logger.info(f"  âœ… ìµœì¢… ê²°ê³¼: {len(filtered_rrf_results)}ê±´")
        else:
            timings.setdefault('post_filter_ms', timings.get('post_filter_ms', 0.0))

            def collect_doc_values(
                user_id: str,
                source: Dict[str, Any],
                metadata: Dict[str, Any],
                _unused: Dict[str, Any],  # í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€í•˜ì§€ë§Œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
            ) -> Tuple[Dict[DemographicType, Set[str]], Dict[DemographicType, bool], Dict[str, Optional[bool]]]:
                doc_values: Dict[DemographicType, Set[str]] = {
                    DemographicType.GENDER: set(),
                    DemographicType.AGE: set(),
                    DemographicType.OCCUPATION: set(),
                }
                metadata_presence: Dict[DemographicType, bool] = {
                    DemographicType.GENDER: False,
                    DemographicType.AGE: False,
                    DemographicType.OCCUPATION: False,
                }
                # â­ ë™ì  behavior_values ìƒì„± (Claudeê°€ ì¶”ì¶œí•œ ì¡°ê±´ ê¸°ë°˜)
                behavior_values: Dict[str, Optional[bool]] = {}
                if analysis.behavioral_conditions:
                    for key in analysis.behavioral_conditions.keys():
                        behavior_values[key] = None

                def record_behavior(key: str, value: Optional[bool]) -> None:
                    if value is None:
                        return
                    if behavior_values.get(key) is None:
                        behavior_values[key] = value

                def parse_yes_no(text: Optional[str]) -> Optional[bool]:
                    if not text:
                        return None
                    normalized = text.lower()
                    if any(keyword in normalized for keyword in BEHAVIOR_NO_TOKENS):
                        return False
                    if any(keyword in normalized for keyword in BEHAVIOR_YES_TOKENS):
                        return True
                    return None

                def parse_smoker_answer(raw: Optional[Any]) -> Optional[bool]:
                    if raw is None:
                        return None
                    if isinstance(raw, (list, tuple, set)):
                        for item in raw:
                            decision = parse_smoker_answer(item)
                            if decision is not None:
                                return decision
                        return None
                    text = str(raw).strip()
                    if not text:
                        return None
                    normalized = text.lower()
                    compact = normalized.replace(" ", "")
                    for keyword in SMOKER_NEGATIVE_KEYWORDS:
                        keyword_compact = keyword.replace(" ", "")
                        if keyword in normalized or keyword_compact in compact:
                            return False
                    for keyword in SMOKER_POSITIVE_KEYWORDS:
                        keyword_compact = keyword.replace(" ", "")
                        if keyword in normalized or keyword_compact in compact:
                            return True
                    if (
                        "ë‹´ë°°" in normalized
                        and not any(token in normalized for token in ("ì—†", "ì•ˆ", "ì•Š", "no", "ë¬´", "ë¯¸í¡ì—°"))
                    ):
                        return True
                    return parse_yes_no(text)

                metadata_candidates = [
                    metadata,  # ì´ì œ ë‹¨ì¼ metadataë§Œ ì‚¬ìš©
                    source.get("metadata", {}) if isinstance(source, dict) else {},
                ]

                payload = {}
                if isinstance(source, dict):
                    payload_candidate = source.get("payload")
                    if isinstance(payload_candidate, dict):
                        payload = payload_candidate
                if not payload and isinstance(source, dict) and "doc" in source:
                    doc_payload = source.get("doc", {}).get("payload")
                    if isinstance(doc_payload, dict):
                        payload = doc_payload
                if not payload and isinstance(source, dict):
                    payload = source

                if isinstance(payload, dict):
                    metadata_candidates.append(payload.get("metadata", {}))

                for candidate in metadata_candidates:
                    if not isinstance(candidate, dict):
                        continue

                    gender_val = candidate.get("gender") or candidate.get("gender_code")
                    if gender_val:
                        normalized_gender = normalize_value(gender_val)
                        if normalized_gender:
                            doc_values[DemographicType.GENDER].add(normalized_gender)
                            metadata_presence[DemographicType.GENDER] = True

                    age_group_val = candidate.get("age_group")
                    if age_group_val:
                        normalized_age_group = normalize_value(age_group_val)
                        if normalized_age_group:
                            doc_values[DemographicType.AGE].add(normalized_age_group)
                            metadata_presence[DemographicType.AGE] = True

                    age_val = candidate.get("age")
                    if age_val:
                        pre_count = len(doc_values[DemographicType.AGE])
                        add_age_decade(doc_values[DemographicType.AGE], age_val)
                        if len(doc_values[DemographicType.AGE]) > pre_count:
                            metadata_presence[DemographicType.AGE] = True

                    birth_year_val = candidate.get("birth_year")
                    if birth_year_val:
                        normalized_birth_year = normalize_value(birth_year_val)
                        if normalized_birth_year:
                            doc_values[DemographicType.AGE].add(normalized_birth_year)
                            metadata_presence[DemographicType.AGE] = True

                    occupation_val = candidate.get("occupation") or candidate.get("job")
                    if occupation_val:
                        normalized_occupation = normalize_value(occupation_val)
                        if normalized_occupation:
                            doc_values[DemographicType.OCCUPATION].add(normalized_occupation)
                            metadata_presence[DemographicType.OCCUPATION] = True

                    job_group_val = candidate.get("job_group") or candidate.get("occupation_group")
                    if job_group_val:
                        normalized_job_group = normalize_value(job_group_val)
                        if normalized_job_group:
                            doc_values[DemographicType.OCCUPATION].add(normalized_job_group)
                            metadata_presence[DemographicType.OCCUPATION] = True

                # QA ê¸°ë°˜ ë³´ì™„ (ì§ì—…) - ë©”íƒ€ë°ì´í„°ê°€ ë¹„ì—ˆì„ ë•Œë§Œ ì‚¬ìš©
                if not metadata_presence[DemographicType.OCCUPATION]:
                    qa_sources: List[List[Dict[str, Any]]] = []
                    if isinstance(source, dict):
                        qa_sources.append(source.get("qa_pairs", []) or [])
                    # â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ welcome_2nd_batch ì œê±°

                    for qa_pairs in qa_sources:
                        for qa in qa_pairs:
                            if not isinstance(qa, dict):
                                continue
                            q_text = str(qa.get("q_text", "")).lower()
                            answer_text = qa.get("answer") or qa.get("answer_text")
                            if not answer_text:
                                continue
                            if any(keyword in q_text for keyword in ("ì§ì—…", "ì§ë¬´", "occupation", "ì§ì¢…")):
                                normalized_answer = normalize_value(answer_text)
                                if normalized_answer:
                                    doc_values[DemographicType.OCCUPATION].add(normalized_answer)

                # â­ ë²”ìš© behavioral ì¶”ì¶œ (ë™ì  - Claudeê°€ ìš”êµ¬í•œ ì¡°ê±´ë§Œ ì¶”ì¶œ)
                if analysis.behavioral_conditions:
                    qa_pairs_list = source.get("qa_pairs", []) or []
                    for behavior_key in behavior_values.keys():
                        if behavior_values.get(behavior_key) is None:
                            extracted_value = extract_behavior_from_qa_pairs(qa_pairs_list, behavior_key)
                            if extracted_value is not None:
                                behavior_values[behavior_key] = extracted_value

                # Normalize
                for demo_type, values in doc_values.items():
                    normalized = {normalize_value(v) for v in values if v}
                    if demo_type == DemographicType.GENDER:
                        expand_gender_aliases(normalized)
                    doc_values[demo_type] = normalized

                return doc_values, metadata_presence, behavior_values

            filtered_list: List[Dict[str, Any]] = []
            source_not_found_count = 0
            gender_filter_failed = 0
            age_filter_failed = 0
            occupation_filter_failed = 0
            gender_metadata_missing = 0
            age_metadata_missing = 0
            occupation_metadata_missing = 0
            behavior_filter_failed = 0
            behavior_metadata_missing = 0
            for doc in rrf_results:
                source = doc.get("_source")
                if not source and "doc" in doc:
                    source = doc.get("doc", {}).get("_source")
                if not source and "payload" in doc:
                    source = doc.get("payload")

                if not isinstance(source, dict):
                    source_not_found_count += 1
                    continue

                user_id = source.get("user_id") or doc.get("_id") or doc.get("id")
                if not user_id and "payload" in doc and isinstance(doc["payload"], dict):
                    user_id = doc["payload"].get("user_id")

                if not user_id:
                    source_not_found_count += 1
                    continue

                # â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ welcome_1st/welcome_2nd ë°°ì¹˜ ì œê±°
                # sourceì—ì„œ ì§ì ‘ metadata ê°€ì ¸ì˜¤ê¸°
                metadata = source.get("metadata", {}) if isinstance(source.get("metadata"), dict) else {}

                doc_values, metadata_presence, behavior_values = collect_doc_values(user_id, source, metadata, {})
                behavior_values_map[user_id] = dict(behavior_values)

                gender_pass = True
                age_pass = True
                occupation_pass = True

                if demographic_filters.get(DemographicType.GENDER):
                    expected = set()
                    for demo in demographic_filters[DemographicType.GENDER]:
                        expected.update(build_expected_values(demo))

                    # â­ Metadata ìš°ì„ : metadataê°€ ìˆìœ¼ë©´ metadataë§Œ í™•ì¸
                    if metadata_presence[DemographicType.GENDER]:
                        # metadataë¡œ ìˆ˜ì§‘ëœ ê°’ë§Œ ì‚¬ìš©
                        gender_from_metadata = set()
                        for meta_source in [metadata, source.get("metadata", {})]:
                            if isinstance(meta_source, dict):
                                gender_val = meta_source.get("gender") or meta_source.get("gender_code")
                                if gender_val:
                                    normalized_gender = normalize_value(gender_val)
                                    if normalized_gender:
                                        gender_from_metadata.add(normalized_gender)

                        if gender_from_metadata:
                            expand_gender_aliases(gender_from_metadata)
                            gender_pass = values_match(gender_from_metadata, expected)
                        else:
                            gender_metadata_missing += 1
                            gender_pass = False
                    else:
                        # metadata ì—†ìœ¼ë©´ qa_pairs ì‚¬ìš©
                        gender_metadata_missing += 1
                        gender_pass = values_match(doc_values[DemographicType.GENDER], expected)

                    if not gender_pass:
                        gender_filter_failed += 1

                if gender_pass and demographic_filters.get(DemographicType.AGE):
                    expected = set()
                    for demo in demographic_filters[DemographicType.AGE]:
                        expected.update(build_expected_values(demo))
                    if not metadata_presence[DemographicType.AGE]:
                        age_metadata_missing += 1
                    age_pass = values_match(doc_values[DemographicType.AGE], expected)
                    if not age_pass:
                        age_filter_failed += 1

                if gender_pass and age_pass and demographic_filters.get(DemographicType.OCCUPATION):
                    expected = set()
                    for demo in demographic_filters[DemographicType.OCCUPATION]:
                        expected.update(build_expected_values(demo))
                    if not metadata_presence[DemographicType.OCCUPATION]:
                        occupation_metadata_missing += 1
                    occupation_pass = values_match(doc_values[DemographicType.OCCUPATION], expected)
                    if not occupation_pass:
                        occupation_filter_failed += 1
                    else:
                        display_occupation = None
                        occupation_candidates = [
                            metadata.get("occupation") if isinstance(metadata, dict) else None,
                            metadata.get("job") if isinstance(metadata, dict) else None,
                            metadata.get("occupation_group") if isinstance(metadata, dict) else None,
                        ]
                        for candidate in occupation_candidates:
                            normalized_candidate = normalize_value(candidate)
                            if normalized_candidate and values_match({normalized_candidate}, expected):
                                display_occupation = str(candidate)
                                break
                        if not display_occupation:
                            qa_sources: List[List[Dict[str, Any]]] = []
                            if isinstance(source, dict):
                                qa_sources.append(source.get("qa_pairs", []) or [])
                            # â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ welcome_2nd_doc_full ì œê±°
                            for qa_pairs in qa_sources:
                                for qa in qa_pairs:
                                    if not isinstance(qa, dict):
                                        continue
                                    q_text = str(qa.get("q_text", "")).lower()
                                    if not any(keyword in q_text for keyword in ("ì§ì—…", "ì§ë¬´", "occupation", "ì§ì¢…")):
                                        continue
                                    answer = qa.get("answer")
                                    if answer is None:
                                        answer = qa.get("answer_text")
                                    if answer is None:
                                        continue
                                    candidate = str(answer)
                                    normalized_candidate = normalize_value(candidate)
                                    if normalized_candidate and values_match({normalized_candidate}, expected):
                                        display_occupation = candidate
                                        break
                                if display_occupation:
                                    break
                        if display_occupation:
                            occupation_display_map[user_id] = display_occupation

                # â­ Behavioral ê²€ì¦: OpenSearchëŠ” í›„ë³´ë¥¼ ë„“ê²Œ ê°€ì ¸ì˜¤ê³ , Pythonì—ì„œ ì •í™•íˆ ê²€ì¦
                behavior_pass = True
                if analysis.behavioral_conditions:
                    for condition_key, expected_value in analysis.behavioral_conditions.items():
                        # â­ expected_value=Noneì€ "ì´ ì¡°ê±´ì„ ì²´í¬í•˜ì§€ ì•ŠìŒ"ì„ ì˜ë¯¸ â†’ ìŠ¤í‚µ
                        if expected_value is None:
                            continue

                        actual_value = behavior_values.get(condition_key)
                        if actual_value is None:
                            behavior_metadata_missing += 1
                            behavior_pass = False
                            break
                        if actual_value != expected_value:
                            behavior_filter_failed += 1
                            behavior_pass = False
                            break

                if gender_pass and age_pass and occupation_pass and behavior_pass:
                    filtered_list.append(doc)

            filter_duration_ms = (perf_counter() - filter_start) * 1000
            timings["post_filter_ms"] = filter_duration_ms
            filtered_rrf_results = filtered_list

            logger.info(f"  - ì†ŒìŠ¤ ëˆ„ë½ ë¬¸ì„œ: {source_not_found_count}ê±´")
            if demographic_filters.get(DemographicType.GENDER):
                logger.info(f"  - ì„±ë³„ metadata ì—†ìŒ: {gender_metadata_missing}ê±´")
            logger.info(f"  - ì„±ë³„ í•„í„° ë¯¸ì¶©ì¡±: {gender_filter_failed}ê±´")
            if demographic_filters.get(DemographicType.AGE):
                logger.info(f"  - ì—°ë ¹ metadata ì—†ìŒ: {age_metadata_missing}ê±´")
            logger.info(f"  - ì—°ë ¹ í•„í„° ë¯¸ì¶©ì¡±: {age_filter_failed}ê±´")
            if demographic_filters.get(DemographicType.OCCUPATION):
                logger.info(f"  - ì§ì—… metadata ì—†ìŒ: {occupation_metadata_missing}ê±´")
            logger.info(f"  - ì§ì—… í•„í„° ë¯¸ì¶©ì¡±: {occupation_filter_failed}ê±´")
            logger.info(f"  - í•„í„° ì¡°ê±´ ì¶©ì¡± ë¬¸ì„œ: {len(filtered_rrf_results)}ê±´")
            if analysis.behavioral_conditions:
                logger.info(f"  âœ… í–‰ë™ í•„í„° ê²€ì¦ ì™„ë£Œ")
                logger.info(f"  - í–‰ë™ ì •ë³´ ì—†ìŒ: {behavior_metadata_missing}ê±´")
                logger.info(f"  - í–‰ë™ í•„í„° ë¯¸ì¶©ì¡±: {behavior_filter_failed}ê±´")

        lazy_join_start = perf_counter()
        # â­ ì‘ë‹µ ë‹¨ê³„: ì‚¬ìš©ìê°€ ìš”ì²­í•œ page_sizeë§Œí¼ë§Œ ë°˜í™˜
        # window_sizeëŠ” ë‚´ë¶€ ê²€ìƒ‰/í•„í„°ë§ìš©ìœ¼ë¡œë§Œ ì‚¬ìš© (ì¶©ë¶„í•œ í›„ë³´ í™•ë³´)
        final_hits = filtered_rrf_results[:page_size]
        results: List[SearchResult] = []
        inner_hits_map: Dict[str, List[Dict[str, Any]]] = {}

        for doc in final_hits:
            source = doc.get("_source")
            if not source and "doc" in doc:
                source = doc.get("doc", {}).get("_source")
            if not source and "payload" in doc:
                source = doc.get("payload")
            if not isinstance(source, dict):
                source = {}

            payload = {}
            payload_candidate = source.get("payload")
            if isinstance(payload_candidate, dict):
                payload = payload_candidate
            elif isinstance(doc.get("payload"), dict):
                payload = doc["payload"]

            user_id = (
                source.get("user_id")
                or payload.get("user_id")
                or doc.get("_id")
                or doc.get("id")
            )

            doc_info = None
            if user_id and user_id in user_doc_map:
                doc_info = user_doc_map[user_id]
            elif doc.get("_id") and doc.get("_id") in id_doc_map:
                doc_info = id_doc_map[doc.get("_id")]

            if doc_info:
                src_info = doc_info.get("source")
                if isinstance(src_info, dict):
                    # â­ ìˆœì„œ ë³€ê²½: Qdrant ë°ì´í„° ë¨¼ì €, OpenSearch ë°ì´í„°ë¡œ ë®ì–´ì“°ê¸°
                    # ì´ë ‡ê²Œ í•˜ë©´ metadata, qa_pairs ë“±ì´ ë³´ì¡´ë¨
                    merged_source = {}
                    merged_source.update(source)      # Qdrant: user_id, text
                    merged_source.update(src_info)    # OpenSearch: metadata, qa_pairs (ìš°ì„ )
                    source = merged_source
                    
                inner_hit_wrapper = {"inner_hits": doc_info.get("inner_hits", {})}
            else:
                inner_hit_wrapper = doc

            # â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ sourceì˜ metadataì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
            source_metadata = source.get("metadata", {}) if isinstance(source, dict) else {}
            if not source_metadata:
                # sourceê°€ ë¹„ì–´ìˆìœ¼ë©´ docì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
                doc_source = doc.get("_source") or {}
                if isinstance(doc_source, dict):
                    source_metadata = doc_source.get("metadata", {})
                # payloadì—ì„œë„ í™•ì¸
                if not source_metadata and isinstance(payload, dict):
                    source_metadata = payload.get("metadata", {})

            behavioral_values = behavior_values_map.get(user_id, {}) if user_id else {}
            behavioral_info: Dict[str, Any] = {}
            if behavioral_values.get("smoker") is not None:
                behavioral_info["smoker"] = behavioral_values.get("smoker")
            if behavioral_values.get("has_vehicle") is not None:
                behavioral_info["has_vehicle"] = behavioral_values.get("has_vehicle")
            if behavioral_values.get("drinker") is not None:
                behavioral_info["drinker"] = behavioral_values.get("drinker")

            demographic_info: Dict[str, Any] = {}
            if source_metadata:
                demographic_info["age_group"] = source_metadata.get("age_group")
                demographic_info["gender"] = source_metadata.get("gender")
                demographic_info["birth_year"] = source_metadata.get("birth_year")
                demographic_info["region"] = source_metadata.get("region")
                demographic_info["occupation"] = source_metadata.get("occupation")
                demographic_info["marital_status"] = source_metadata.get("marital_status")
                demographic_info["sub_region"] = source_metadata.get("sub_region")
                demographic_info["panel"] = source_metadata.get("panel")

            occupation_expected = set()
            for demo in demographic_filters.get(DemographicType.OCCUPATION, []):
                occupation_expected.update(build_expected_values(demo))

            if ("occupation" not in demographic_info or not demographic_info["occupation"]) and user_id:
                mapped_occupation = occupation_display_map.get(user_id) if has_demographic_filters else None
                if mapped_occupation:
                    demographic_info["occupation"] = mapped_occupation

            def occupation_matches(candidate: str) -> bool:
                normalized_candidate = normalize_value(candidate)
                if not normalized_candidate:
                    return False
                for expected in occupation_expected:
                    if not expected:
                        continue
                    if normalized_candidate == expected or normalized_candidate in expected or expected in normalized_candidate:
                        return True
                return False

            if ("occupation" not in demographic_info or not demographic_info["occupation"]) and isinstance(source, dict):
                qa_pairs_for_occ = source.get("qa_pairs", [])
                for qa in qa_pairs_for_occ:
                    if not isinstance(qa, dict):
                        continue
                    q_text = str(qa.get("q_text", "")).lower()
                    answer = qa.get("answer")
                    if answer is None:
                        answer = qa.get("answer_text")
                    if answer is None:
                        continue
                    answer_str = str(answer)
                    if any(keyword in q_text for keyword in ("ì§ì—…", "ì§ë¬´", "occupation", "ì§ì¢…")) and occupation_matches(answer_str):
                        demographic_info["occupation"] = answer_str
                        break

            # marital_statusë¥¼ qa_pairsì—ì„œ ì°¾ê¸°
            if ("marital_status" not in demographic_info or not demographic_info["marital_status"]) and isinstance(source, dict):
                qa_pairs_list = source.get("qa_pairs", [])
                for qa in qa_pairs_list:
                    if not isinstance(qa, dict):
                        continue
                    q_text = str(qa.get("q_text", "")).lower()
                    answer = qa.get("answer")
                    if answer is None:
                        answer = qa.get("answer_text")
                    if answer is None:
                        continue
                    answer_str = str(answer)
                    if any(keyword in q_text for keyword in ("ê²°í˜¼", "í˜¼ì¸")):
                        demographic_info["marital_status"] = answer_str
                        break

            # sub_regionì„ qa_pairsì—ì„œ ì°¾ê¸°
            if ("sub_region" not in demographic_info or not demographic_info["sub_region"]) and isinstance(source, dict):
                qa_pairs_list = source.get("qa_pairs", [])
                for qa in qa_pairs_list:
                    if not isinstance(qa, dict):
                        continue
                    q_text = str(qa.get("q_text", "")).lower()
                    answer = qa.get("answer")
                    if answer is None:
                        answer = qa.get("answer_text")
                    if answer is None:
                        continue
                    answer_str = str(answer)
                    if any(keyword in q_text for keyword in ("êµ¬", "êµ°", "ì„¸ë¶€ì§€ì—­", "ì–´ëŠ êµ¬")):
                        demographic_info["sub_region"] = answer_str
                        break

            matched_qa_pairs: List[Dict[str, Any]] = extract_inner_hit_matches(inner_hit_wrapper)
            if not matched_qa_pairs and analysis.must_terms:
                matched_qa_pairs = extract_matched_qa_pairs(source, analysis.must_terms)

            # â­ ê°œì„ : sourceì— qa_pairsê°€ ì—†ìœ¼ë©´ docì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
            qa_pairs_from_source = source.get("qa_pairs", []) if isinstance(source, dict) else []
            if not qa_pairs_from_source:
                # sourceê°€ ë¹„ì–´ìˆìœ¼ë©´ docì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
                doc_source = doc.get("_source") or {}
                if isinstance(doc_source, dict):
                    qa_pairs_from_source = doc_source.get("qa_pairs", [])
                # payloadì—ì„œë„ í™•ì¸
                if not qa_pairs_from_source and isinstance(payload, dict):
                    qa_pairs_from_source = payload.get("qa_pairs", [])

            qa_pairs_display = reorder_with_matches(
                qa_pairs_from_source if isinstance(qa_pairs_from_source, list) else [],
                matched_qa_pairs,
                limit=10
            )

            # survey_datetime ì¶”ì¶œ (metadataì—ì„œ)
            survey_datetime = None
            if source_metadata and isinstance(source_metadata, dict):
                survey_datetime = source_metadata.get("survey_datetime")
            elif isinstance(source, dict):
                metadata = source.get("metadata", {})
                if isinstance(metadata, dict):
                    survey_datetime = metadata.get("survey_datetime")

            results.append(
                SearchResult(
                    user_id=user_id,
                    score=doc.get("_score", 0.0),
                    timestamp=source.get("timestamp") if isinstance(source, dict) else None,
                    survey_datetime=survey_datetime,
                    demographic_info=demographic_info if demographic_info else None,
                    behavioral_info=behavioral_info if behavioral_info else None,
                    qa_pairs=qa_pairs_display[:5],
                    matched_qa_pairs=matched_qa_pairs,
                    highlights=doc.get("highlight"),
                )
            )
           

        timings["lazy_join_ms"] = (perf_counter() - lazy_join_start) * 1000
        timings.setdefault('post_filter_ms', timings.get('post_filter_ms', 0.0))
        timings.setdefault('rrf_recombination_ms', 0.0)
        timings.setdefault('qdrant_parallel_ms', 0.0)
        timings.setdefault(
            'opensearch_parallel_ms',
            timings.get('two_phase_stage1_ms', 0.0) + timings.get('two_phase_stage2_ms', 0.0)
        )

        total_duration_ms = (perf_counter() - overall_start) * 1000
        timings['total_ms'] = total_duration_ms
        timings['cache_hit'] = 1.0 if cache_hit else 0.0

        serialized_results = [_serialize_result(res) for res in results]
        stored_items = serialized_results
        if cache_enabled and cache_limit > 0:
            stored_items = serialized_results[:cache_limit]
        page_results, has_more_local = _slice_results(stored_items, page, page_size)
        # â­ total_hitsëŠ” ì´ë¯¸ ìœ„ì—ì„œ ê³„ì‚°ë¨ (len(results))
        has_more = has_more_local and ((page * page_size) < total_hits)
        max_score = results[0].score if results else 0.0
        response_took_ms = int(total_duration_ms)

        logger.info("ğŸ“ˆ ì„±ëŠ¥ ì¸¡ì • ìš”ì•½ (ms):")
        for key in sorted(timings.keys()):
            logger.info(f"  - {key}: {timings[key]:.2f}")

        summary_parts = [
            f"returned={len(page_results)}ê±´, í•„í„°ë§í›„={len(filtered_rrf_results)}ê±´, ì „ì²´={total_hits}ê±´",
            f"total_ms={response_took_ms}",
        ]
        if rrf_k_used is not None:
            summary_parts.append(f"rrf_k={rrf_k_used}")
        if adaptive_threshold is not None:
            summary_parts.append(f"qdrant_threshold={adaptive_threshold:.2f}")
        logger.info("âœ… ìµœì¢… ìš”ì•½: " + ", ".join(summary_parts))
        if rrf_reason:
            logger.info(f"   â€¢ RRF: {rrf_reason}")
        if threshold_reason:
            logger.info(f"   â€¢ Qdrant: {threshold_reason}")

        _log_final_summary(
            stage="search_nl",
            query=request.query,
            analysis=analysis,
            total_hits=total_hits,
            returned_count=len(page_results),
            page=page,
            page_size=page_size,
            cache_hit=cache_hit,
            timings=timings,
            took_ms=total_duration_ms,
            filters=filters_for_response,
            behavioral_conditions=getattr(analysis, "behavioral_conditions", {}),
            use_claude=use_claude,
        )

        # â­ total_hits: í•„í„°ë§ì„ í†µê³¼í•œ ì „ì²´ ê²°ê³¼ ìˆ˜
        total_hits = len(filtered_rrf_results) if filtered_rrf_results else 0

        # â­ requested_count ì„¤ì •:
        # - ì¿¼ë¦¬ì—ì„œ sizeê°€ ëª…ì‹œë˜ë©´ (ì˜ˆ: "ì „ë¬¸ì§ 100ëª…") â†’ requested_size ê°’
        #   ë‹¨, ì‹¤ì œ ë°˜í™˜ëœ ê²°ê³¼ ìˆ˜(total_hits)ë³´ë‹¤ í¬ë©´ total_hitsë¡œ ì œí•œ
        # - sizeê°€ ì—†ìœ¼ë©´ (ì˜ˆ: "ì „ë¬¸ì§") â†’ page_size ì‚¬ìš© (ê¸°ë³¸ê°’ 30000)
        if requested_size is not None and requested_size > 0:
            # ì‹¤ì œ ë°˜í™˜ëœ ê²°ê³¼ ìˆ˜ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ì œí•œ
            requested_count = min(requested_size, total_hits)
        else:
            # sizeê°€ ì—†ìœ¼ë©´ page_size ì‚¬ìš© (ê¸°ë³¸ê°’ 30000)
            requested_count = min(page_size, total_hits)
        
        if cache_enabled and cache_key and stored_items:
            # â­ 1ì°¨: ë©”ëª¨ë¦¬ ìºì‹œì— ì¦‰ì‹œ ì €ì¥ (ì „ì²´ ì •ë³´, ë‹¤ìŒ ìš”ì²­ë¶€í„° 0.001ì´ˆ!)
            cache_payload_for_memory = {
                "total_hits": total_hits,
                "max_score": max_score,
                "items": stored_items,  # âœ… qa_pairs í¬í•¨ (ì „ì²´)
                "page_size": page_size,
                "filters": filters_for_response,
                "extracted_entities": extracted_entities.to_dict(),
                "behavioral_conditions": getattr(analysis, "behavioral_conditions", {}),
                "use_claude": bool(use_claude),
                "requested_count": requested_count,
            }
            memory_cache[cache_key] = cache_payload_for_memory
            logger.info(f"âœ… ë©”ëª¨ë¦¬ ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(stored_items)}ê±´ (ì „ì²´ ì •ë³´)")

            # â­â­â­ ê²½ëŸ‰í™”: Redis ì €ì¥ìš© (qa_pairs, matched_qa_pairs, highlights ì œì™¸)
            lightweight_items = []
            for item in stored_items:
                lightweight_item = {k: v for k, v in item.items() if k not in ['qa_pairs', 'matched_qa_pairs', 'highlights']}
                lightweight_items.append(lightweight_item)

            # â­ 2ì°¨: ë°±ê·¸ë¼ìš´ë“œì—ì„œ Redis ì••ì¶• ì €ì¥ (ê²½ëŸ‰í™”, ì˜êµ¬ ë³´ì¡´)
            background_tasks.add_task(
                save_search_cache_compressed,
                cache_key,
                cache_ttl,
                total_hits,
                max_score,
                lightweight_items,  # â­ ê²½ëŸ‰í™”ëœ ë²„ì „!
                page_size,
                filters_for_response,
                extracted_entities.to_dict(),
                getattr(analysis, "behavioral_conditions", {}),
                bool(use_claude),
                requested_count,
            )
            logger.info(f"â³ Redis ìºì‹œ ì €ì¥ ì˜ˆì•½ (ë°±ê·¸ë¼ìš´ë“œ, ê²½ëŸ‰í™”): {len(lightweight_items)}ê±´")
        
        response = SearchResponse(
            requested_count=requested_count,
            query=request.query,
            session_id=getattr(request, "session_id", None),
            total_hits=total_hits,
            max_score=max_score,
            results=page_results,
            query_analysis={
                "intent": analysis.intent,
                "must_terms": analysis.must_terms,
                "should_terms": analysis.should_terms,
                "alpha": analysis.alpha,
                "confidence": analysis.confidence,
                "size": len(page_results),  # ì‹¤ì œ ë°˜í™˜ëœ ê²°ê³¼ ìˆ˜
                "timings_ms": timings,
                "extracted_entities": extracted_entities.to_dict(),
                "behavioral_conditions": getattr(analysis, "behavioral_conditions", {}),
                "use_claude_analyzer": bool(use_claude),
            },
            took_ms=response_took_ms,
            page=page,
            page_size=page_size,
            has_more=has_more,
        )
        return _finalize_search_response(
            request=request,
            response=response,
            analysis=analysis,
            cache_hit=cache_hit,
            timings=timings,
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[ERROR] ìì—°ì–´ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# -----------------------------
# í”„ë¡ íŠ¸ì—”ë“œ ì¹œí™”ì  ê°„ì†Œí™” ì—”ë“œí¬ì¸íŠ¸
# -----------------------------




class TestFiltersRequest(BaseModel):
    """í•„í„° í…ŒìŠ¤íŠ¸ ìš”ì²­"""
    filters: List[Dict[str, Any]] = Field(..., description="í…ŒìŠ¤íŠ¸í•  í•„í„° ë¦¬ìŠ¤íŠ¸")
    index_name: str = Field(default="*", description="ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ì´ë¦„")


@router.post("/debug/test-filters", summary="í•„í„° ê°œë³„ í…ŒìŠ¤íŠ¸ (ë””ë²„ê¹…ìš©)")
async def test_filters(
    request: TestFiltersRequest,
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """
    í•„í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ì—¬ ì–´ë–¤ ì¸ë±ìŠ¤ì—ì„œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
    
    - ê° í•„í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì‹¤í–‰
    - ì¸ë±ìŠ¤ë³„ ê²°ê³¼ ê°œìˆ˜ í™•ì¸
    
    ì‚¬ìš© ì˜ˆì‹œ:
    ```json
    {
      "filters": [
        {
          "bool": {
            "should": [
              {"term": {"metadata.age_group.keyword": "30ëŒ€"}}
            ],
            "minimum_should_match": 1
          }
        }
      ],
      "index_name": "*"
    }
    ```
    """
    try:
        if not os_client or not os_client.ping():
            raise HTTPException(status_code=503, detail="OpenSearch ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        filters = request.filters
        index_name = request.index_name
        
        results = []
        for i, filter_dict in enumerate(filters):
            # ê° í•„í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            query = {
                "query": {
                    "bool": {
                        "must": [filter_dict]
                    }
                },
                "size": 0,  # ê°œìˆ˜ë§Œ í™•ì¸
                "aggs": {
                    "by_index": {
                        "terms": {
                            "field": "_index",
                            "size": 20
                        }
                    }
                }
            }
            
            response = os_client.search(
                index=request.index_name,
                body=query
            )
            
            # ì¸ë±ìŠ¤ë³„ ê²°ê³¼ ê°œìˆ˜
            index_counts = {}
            if 'aggregations' in response and 'by_index' in response['aggregations']:
                for bucket in response['aggregations']['by_index']['buckets']:
                    index_counts[bucket['key']] = bucket['doc_count']
            
            results.append({
                "filter_index": i,
                "filter": filter_dict,
                "total_hits": response['hits']['total']['value'],
                "index_counts": index_counts
            })
        
        # ëª¨ë“  í•„í„°ë¥¼ ANDë¡œ ê²°í•©í•œ ê²°ê³¼ë„ í…ŒìŠ¤íŠ¸
        if len(filters) > 1:
            combined_query = {
                "query": {
                    "bool": {
                        "must": filters
                    }
                },
                "size": 0,
                "aggs": {
                    "by_index": {
                        "terms": {
                            "field": "_index",
                            "size": 20
                        }
                    }
                }
            }
            
            combined_response = os_client.search(
                index=request.index_name,
                body=combined_query
            )
            
            combined_index_counts = {}
            if 'aggregations' in combined_response and 'by_index' in combined_response['aggregations']:
                for bucket in combined_response['aggregations']['by_index']['buckets']:
                    combined_index_counts[bucket['key']] = bucket['doc_count']
            
            results.append({
                "filter_index": "combined",
                "filter": "ALL FILTERS (AND)",
                "total_hits": combined_response['hits']['total']['value'],
                "index_counts": combined_index_counts
            })
        
        return {
            "index_name": request.index_name,
            "results": results
        }
    
    except Exception as e:
        logger.error(f"[ERROR] í•„í„° í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get(
    "/nl/stream",
    summary="ìì—°ì–´ ê²€ìƒ‰ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (SSE)",
)
async def search_natural_language_stream(
    query: str = Query(..., description="ìì—°ì–´ ì¿¼ë¦¬"),
    index_name: str = Query(default="survey_responses_merged", description="ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ì´ë¦„"),
    size: int = Query(default=10, ge=1, le=50000, description="ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜"),
    use_vector_search: bool = Query(default=True, description="ë²¡í„° ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€"),
    page: int = Query(default=1, ge=1, description="í˜ì´ì§€ ë²ˆí˜¸"),
    session_id: Optional[str] = Query(default=None, description="ì„¸ì…˜ ID"),
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """
    ê²€ìƒ‰ ê³¼ì •ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•˜ëŠ” SSE ì—”ë“œí¬ì¸íŠ¸
    
    ì•ŒíŒŒê°’ê³¼ í•„í„°ë§ ì „í›„ ê°œìˆ˜ë§Œ ì‹¤ì‹œê°„ìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.
    """
    async def event_generator():
        try:
            import asyncio
            import time

            # â­ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ í
            event_queue = asyncio.Queue()

            # íƒ€ì´ë° ì¶”ì 
            timings = {}
            start_time = time.time()

            # ì½œë°± í•¨ìˆ˜ ì •ì˜ (ì¦‰ì‹œ íì— ë„£ìŒ!)
            def stream_callback(event_type: str, data: dict):
                """ìŠ¤íŠ¸ë¦¬ë° ì½œë°±: ì´ë²¤íŠ¸ë¥¼ ì¦‰ì‹œ íì— ì¶”ê°€"""
                try:
                    # ë¹„ë™ê¸° íì— ë„£ê¸° (thread-safe)
                    asyncio.create_task(event_queue.put((event_type, data)))
                except:
                    pass

            # â­â­â­ 1. ì‹œì‘ ì´ë²¤íŠ¸
            yield f"data: {json.dumps({'event': 'start', 'query': query, 'timestamp': int(time.time())}, ensure_ascii=False)}\n\n"
            yield f"data: {json.dumps({'event': 'progress', 'step': 1, 'total': 8, 'stage': 'ì´ˆê¸°í™”'}, ensure_ascii=False)}\n\n"

            # ê²€ìƒ‰ ìš”ì²­ ìƒì„±
            search_request = NLSearchRequest(
                query=query,
                index_name=index_name,
                size=size,
                use_vector_search=use_vector_search,
                page=page,
                session_id=session_id,
                log_conversation=False,
                log_search_history=False,
            )

            # â­â­â­ 2. ìºì‹œ í™•ì¸ (ì¼ë‹¨ falseë¡œ - ì‹¤ì œ êµ¬í˜„ì€ search í•¨ìˆ˜ì—ì„œ)
            cache_start = time.time()
            yield f"data: {json.dumps({'event': 'cache_check', 'cache_hit': False}, ensure_ascii=False)}\n\n"
            yield f"data: {json.dumps({'event': 'timing', 'stage': 'cache_check', 'ms': round((time.time() - cache_start) * 1000, 2)}, ensure_ascii=False)}\n\n"

            # â­â­â­ ê²€ìƒ‰ ì‹¤í–‰ì„ ë¹„ë™ê¸° íƒœìŠ¤í¬ë¡œ
            from fastapi import BackgroundTasks
            background_tasks = BackgroundTasks()

            search_task = asyncio.create_task(
                search_natural_language(
                    search_request,
                    background_tasks,
                    os_client,
                    stream_callback=stream_callback
                )
            )

            # ì´ë²¤íŠ¸ ì²˜ë¦¬ ë³€ìˆ˜
            step = 2
            query_analysis_sent = False
            filters_sent = False
            opensearch_sent = False
            qdrant_sent = False
            rrf_sent = False
            filter_before_sent = False
            filter_after_sent = False

            # â­â­â­ íì—ì„œ ì´ë²¤íŠ¸ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì²˜ë¦¬
            while True:
                try:
                    # ê²€ìƒ‰ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
                    if search_task.done():
                        # ë‚¨ì€ ì´ë²¤íŠ¸ ì²˜ë¦¬
                        while not event_queue.empty():
                            event_type, data = await asyncio.wait_for(event_queue.get(), timeout=0.1)

                            # ì´ë²¤íŠ¸ ì²˜ë¦¬ ë¡œì§ (ì•„ë˜ ì°¸ì¡°)
                            if event_type == 'query_analysis' and not query_analysis_sent:
                                yield f"data: {json.dumps({'event': 'progress', 'step': step, 'total': 8, 'stage': 'ì¿¼ë¦¬ ë¶„ì„'}, ensure_ascii=False)}\n\n"
                                step += 1
                                yield f"data: {json.dumps({'event': 'query_analysis', **data}, ensure_ascii=False)}\n\n"
                                query_analysis_sent = True

                            elif event_type in ['demographics', 'behavioral_conditions'] and not filters_sent:
                                if event_type == 'demographics':
                                    demo_data = data
                                else:
                                    behav_data = data

                                # ë‘˜ ë‹¤ ëª¨ì˜€ì„ ë•Œ
                                if 'demo_data' in locals() and 'behav_data' in locals():
                                    yield f"data: {json.dumps({'event': 'progress', 'step': step, 'total': 8, 'stage': 'í•„í„° ì¶”ì¶œ'}, ensure_ascii=False)}\n\n"
                                    step += 1
                                    yield f"data: {json.dumps({'event': 'filters_extracted', 'demographics': demo_data.get('demographics', []), 'behavioral': behav_data.get('behavioral_conditions', {})}, ensure_ascii=False)}\n\n"
                                    filters_sent = True

                            elif event_type == 'opensearch_results' and not opensearch_sent:
                                yield f"data: {json.dumps({'event': 'progress', 'step': step, 'total': 8, 'stage': 'í‚¤ì›Œë“œ ê²€ìƒ‰'}, ensure_ascii=False)}\n\n"
                                step += 1
                                yield f"data: {json.dumps({'event': 'opensearch_search_start'}, ensure_ascii=False)}\n\n"
                                yield f"data: {json.dumps({'event': 'opensearch_results', **data}, ensure_ascii=False)}\n\n"
                                opensearch_sent = True

                            elif event_type == 'qdrant_results' and not qdrant_sent:
                                yield f"data: {json.dumps({'event': 'progress', 'step': step, 'total': 8, 'stage': 'ì˜ë¯¸ ê²€ìƒ‰'}, ensure_ascii=False)}\n\n"
                                step += 1
                                yield f"data: {json.dumps({'event': 'qdrant_search_start'}, ensure_ascii=False)}\n\n"
                                yield f"data: {json.dumps({'event': 'qdrant_results', **data}, ensure_ascii=False)}\n\n"
                                qdrant_sent = True

                            elif event_type == 'rrf_fusion' and not rrf_sent:
                                yield f"data: {json.dumps({'event': 'progress', 'step': step, 'total': 8, 'stage': 'ê²°ê³¼ ê²°í•©'}, ensure_ascii=False)}\n\n"
                                step += 1
                                yield f"data: {json.dumps({'event': 'rrf_fusion', **data}, ensure_ascii=False)}\n\n"
                                rrf_sent = True

                            elif event_type == 'filter_breakdown':
                                yield f"data: {json.dumps({'event': 'filter_breakdown', **data}, ensure_ascii=False)}\n\n"
                            
                            elif event_type == 'before_filter' and not filter_before_sent:
                                yield f"data: {json.dumps({'event': 'progress', 'step': step, 'total': 8, 'stage': 'í•„í„°ë§'}, ensure_ascii=False)}\n\n"
                                step += 1
                                yield f"data: {json.dumps({'event': 'before_filter', **data}, ensure_ascii=False)}\n\n"
                                filter_before_sent = True

                            elif event_type == 'filter_breakdown':
                                yield f"data: {json.dumps({'event': 'filter_breakdown', **data}, ensure_ascii=False)}\n\n"

                            elif event_type == 'after_filter' and not filter_after_sent:
                                yield f"data: {json.dumps({'event': 'after_filter', **data}, ensure_ascii=False)}\n\n"
                                filter_after_sent = True

                        break

                    # íì—ì„œ ì´ë²¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒ 0.1ì´ˆ)
                    event_type, data = await asyncio.wait_for(event_queue.get(), timeout=0.1)

                    # â­ ì¦‰ì‹œ ì²˜ë¦¬!
                    if event_type == 'query_analysis' and not query_analysis_sent:
                        yield f"data: {json.dumps({'event': 'progress', 'step': step, 'total': 8, 'stage': 'ì¿¼ë¦¬ ë¶„ì„'}, ensure_ascii=False)}\n\n"
                        step += 1
                        stage_start = time.time()
                        yield f"data: {json.dumps({'event': 'query_analysis', **data}, ensure_ascii=False)}\n\n"
                        yield f"data: {json.dumps({'event': 'timing', 'stage': 'query_analysis', 'ms': round((time.time() - stage_start) * 1000, 2)}, ensure_ascii=False)}\n\n"
                        query_analysis_sent = True

                    elif event_type == 'opensearch_results' and not opensearch_sent:
                        yield f"data: {json.dumps({'event': 'progress', 'step': step, 'total': 8, 'stage': 'í‚¤ì›Œë“œ ê²€ìƒ‰'}, ensure_ascii=False)}\n\n"
                        step += 1
                        yield f"data: {json.dumps({'event': 'opensearch_search_start'}, ensure_ascii=False)}\n\n"
                        yield f"data: {json.dumps({'event': 'opensearch_results', **data}, ensure_ascii=False)}\n\n"
                        opensearch_sent = True

                    # ë‹¤ë¥¸ ì´ë²¤íŠ¸ë“¤ë„ ìœ ì‚¬í•˜ê²Œ ì²˜ë¦¬...

                except asyncio.TimeoutError:
                    continue

            # ê²€ìƒ‰ ì™„ë£Œ
            response = await search_task

            # â­ ì™„ë£Œ
            yield f"data: {json.dumps({'event': 'done'}, ensure_ascii=False)}\n\n"

        except Exception as e:
            logger.error(f"SSE ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}", exc_info=True)
            yield f"data: {json.dumps({'event': 'error', 'message': str(e)}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # nginx ë²„í¼ë§ ë¹„í™œì„±í™”
        }
    )


@router.get(
    "/logs/conversation/{session_id}",
    summary="ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ (Redis)",
)
async def get_conversation_logs_endpoint(
    session_id: str,
    limit: int = 50,
) -> Dict[str, Any]:
    client = getattr(router, "redis_client", None)
    if client is None:
        raise HTTPException(status_code=503, detail="Redis í´ë¼ì´ì–¸íŠ¸ê°€ êµ¬ì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if limit <= 0:
        raise HTTPException(status_code=400, detail="limit ê°’ì€ 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    key = _make_conversation_key(
        getattr(router, "conversation_history_prefix", None),
        session_id,
    )
    if not key:
        raise HTTPException(status_code=400, detail="session_id ë˜ëŠ” prefixê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    raw_items = client.lrange(key, -limit, -1)
    messages: List[ConversationMessage] = []
    for item in raw_items:
        parsed = _parse_conversation_record(item)
        if parsed is not None:
            messages.append(parsed)
    
    # ë©”ì‹œì§€ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (Pydantic ëª¨ë¸ ì§ë ¬í™”)
    messages_dict = []
    for msg in messages:
        msg_dict = msg.model_dump()
        # assistant ë©”ì‹œì§€ì˜ contentê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸í•˜ê³  ê·¸ëŒ€ë¡œ ìœ ì§€
        # (ì´ë¯¸ _parse_conversation_recordì—ì„œ ì²˜ë¦¬ë¨)
        messages_dict.append(msg_dict)

    return {
        "session_id": session_id,
        "count": len(messages_dict),
        "messages": messages_dict,
    }


@router.get(
    "/logs/search-history/{owner_id}",
    summary="ê²€ìƒ‰ ì´ë ¥ ì¡°íšŒ (Redis)",
)
async def get_search_history_endpoint(
    owner_id: str,
    limit: int = 50,
) -> Dict[str, Any]:
    client = getattr(router, "redis_client", None)
    if client is None:
        raise HTTPException(status_code=503, detail="Redis í´ë¼ì´ì–¸íŠ¸ê°€ êµ¬ì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if limit <= 0:
        raise HTTPException(status_code=400, detail="limit ê°’ì€ 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    key = _make_history_key(
        getattr(router, "search_history_prefix", None),
        owner_id,
    )
    if not key:
        raise HTTPException(status_code=400, detail="owner_id ë˜ëŠ” prefixê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    raw_items = client.lrange(key, -limit, -1)
    entries: List[SearchHistoryEntry] = []
    for item in raw_items:
        parsed = _parse_search_history_record(item)
        if parsed is not None:
            entries.append(parsed)

    return {
        "owner_id": owner_id,
        "count": len(entries),
        "history": [entry.model_dump() for entry in entries],
    }


@router.get(
    "/opensearch/{user_id}",
    summary="OpenSearchì—ì„œ user_idë¡œ ë¬¸ì„œ ê²€ìƒ‰ (DevTools ìŠ¤íƒ€ì¼)",
)
def search_by_user_id(
    user_id: str,
    index_name: str = Query(default="survey_responses_merged", description="ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ì´ë¦„"),
    os_client: OpenSearch = Depends(lambda: router.os_client),
) -> Dict[str, Any]:
    """
    OpenSearch DevToolsì²˜ëŸ¼ user_idë¡œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        user_id: ê²€ìƒ‰í•  ì‚¬ìš©ì ID
        index_name: ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ì´ë¦„ (ê¸°ë³¸ê°’: survey_responses_merged)
    
    Returns:
        OpenSearch ê²€ìƒ‰ ê²°ê³¼ (DevToolsì™€ ë™ì¼í•œ í˜•ì‹)
    """
    if not os_client or not os_client.ping():
        raise HTTPException(status_code=503, detail="OpenSearch ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    try:
        # OpenSearch term ì¿¼ë¦¬ë¡œ user_id ê²€ìƒ‰
        query = {
            "query": {
                "term": {
                    "user_id": user_id
                }
            },
            "size": 1  # user_idëŠ” ê³ ìœ í•˜ë¯€ë¡œ 1ê°œë§Œ ë°˜í™˜
        }
        
        logger.info(f"ğŸ” OpenSearch user_id ê²€ìƒ‰: {user_id} (ì¸ë±ìŠ¤: {index_name})")
        
        response = os_client.search(
            index=index_name,
            body=query
        )
        
        hits = response.get("hits", {})
        total = hits.get("total", {})
        total_value = total.get("value", 0) if isinstance(total, dict) else total
        
        if total_value == 0:
            return {
                "user_id": user_id,
                "found": False,
                "total": 0,
                "hits": []
            }
        
        # ì²« ë²ˆì§¸ ê²°ê³¼ ë°˜í™˜
        first_hit = hits.get("hits", [])[0] if hits.get("hits") else None
        
        if first_hit:
            # _sourceì—ì„œ timestamp ì œê±°
            source = first_hit.get("_source", {})
            if isinstance(source, dict):
                source = source.copy()  # ì›ë³¸ ìˆ˜ì • ë°©ì§€
                source.pop("timestamp", None)  # timestamp ì œê±°
            
            return {
                "user_id": user_id,
                "found": True,
                "total": total_value,
                "hits": [
                    {
                        "_id": first_hit.get("_id"),
                        "_score": first_hit.get("_score"),
                        "_source": source
                    }
                ]
            }
        else:
            return {
                "user_id": user_id,
                "found": False,
                "total": total_value,
                "hits": []
            }
            
    except Exception as e:
        logger.error(f"âŒ OpenSearch user_id ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


def _filter_to_string(filter_dict: Dict[str, Any]) -> str:
    """Helper: í•„í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜"""
    try:
        return json.dumps(filter_dict, ensure_ascii=False)
    except Exception:
        return str(filter_dict)


# â­ Two-phase search helper í•¨ìˆ˜ ì œê±°ë¨ (ë‹¨ìˆœí™”)
# - is_age_or_gender_filter
# - is_occupation_filter
# â†’ survey_responses_merged í†µí•© ì¸ë±ìŠ¤ ì‚¬ìš©ìœ¼ë¡œ ë¶ˆí•„ìš”


def get_user_id_from_doc(doc: Dict[str, Any]) -> Optional[str]:
    """ë¬¸ì„œì—ì„œ user_id ì¶”ì¶œ"""
    if not isinstance(doc, dict):
        return None
    source = doc.get('_source')
    if isinstance(source, dict):
        uid = source.get('user_id')
        if uid:
            return uid
    uid = doc.get('_id')
    if uid:
        return uid
    payload = doc.get('payload')
    if isinstance(payload, dict):
        return payload.get('user_id')
    return None
