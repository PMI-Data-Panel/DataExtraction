"""ê²€ìƒ‰ API ë¼ìš°í„° - Celery + Redis í†µí•©"""
import logging
from typing import Dict, Any, List, Optional
from fastapi import APIRouter, HTTPException, Depends, Query
from pydantic import BaseModel, Field
from opensearchpy import OpenSearch
import os
import redis
import json

from celery.result import AsyncResult
from redis_celery.celery_app import celery_app

logger = logging.getLogger(__name__)

router = APIRouter(
    prefix="/search",
    tags=["Search"]
)

# âš ï¸ íƒ€ì„ì•„ì›ƒ ì„¤ì •
DEFAULT_OS_TIMEOUT = 10


# ==================== Pydantic Models ====================

class NLSearchRequest(BaseModel):
    """ìì—°ì–´ ê¸°ë°˜ ê²€ìƒ‰ ìš”ì²­"""
    query: str = Field(..., description="ìì—°ì–´ ì¿¼ë¦¬ (ì˜ˆ: '30ëŒ€ ì‚¬ë¬´ì§ 300ëª… ë°ì´í„° ë³´ì—¬ì¤˜')")
    index_name: str = Field(default="*", description="ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ì´ë¦„ (ê¸°ë³¸ê°’: ì „ì²´ ì¸ë±ìŠ¤ '*')")
    use_vector_search: bool = Field(default=True, description="ë²¡í„° ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€")


class SearchResult(BaseModel):
    """ê²€ìƒ‰ ê²°ê³¼ í•­ëª©"""
    user_id: str
    score: float
    timestamp: Optional[str] = None
    demographic_info: Optional[Dict[str, Any]] = None
    qa_pairs: Optional[List[Dict[str, Any]]] = None
    matched_qa_pairs: Optional[List[Dict[str, Any]]] = None
    highlights: Optional[Dict[str, Any]] = None
    index: Optional[str] = None  # â­ ì¸ë±ìŠ¤ ì •ë³´ ì¶”ê°€


class SearchResponse(BaseModel):
    """ê²€ìƒ‰ ì‘ë‹µ"""
    query: str
    total_hits: int
    max_score: Optional[float]
    results: List[SearchResult]
    query_analysis: Optional[Dict[str, Any]] = None
    search_summary: Optional[Dict[str, Any]] = None  # â­ ì¸ë±ìŠ¤ë³„ í†µê³„ ì¶”ê°€
    took_ms: int


# ==================== Redis Helper ====================

def get_redis_client() -> redis.StrictRedis:
    """Redis ì—°ê²° ê°ì²´ ë°˜í™˜"""
    REDIS_HOST = os.getenv('REDIS_HOST', 'redis')
    REDIS_PORT = int(os.getenv('REDIS_PORT', '6379'))
    CACHE_DB = int(os.getenv('CACHE_DB', '2'))
    
    try:
        r = redis.StrictRedis(
            host=REDIS_HOST,
            port=REDIS_PORT,
            db=CACHE_DB,
            decode_responses=True
        )
        r.ping()
        return r
    except Exception as e:
        logger.error(f"Redis ì—°ê²° ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=503,
            detail="Redis ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        )


# ==================== API Endpoints ====================

@router.get("/", summary="Search API ìƒíƒœ")
def search_root():
    """Search API ê¸°ë³¸ ì •ë³´"""
    return {
        "message": "Search API ì‹¤í–‰ ì¤‘",
        "version": "2.0 (Celery + Redis)",
        "endpoints": [
            "/search/nl-async",
            "/search/status/{task_id}",
            "/search/scroll/{task_id}"
        ]
    }


@router.post("/nl-async", response_model=Dict[str, Any], summary="ìì—°ì–´ ê²€ìƒ‰ (ë¹„ë™ê¸°)")
async def search_natural_language_async(request: NLSearchRequest):
    """
    ğŸš€ ì „ì²´ ì¸ë±ìŠ¤ ë³‘ë ¬ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Celery Workerì— ìœ„ì„)
    
    - welcome_1st, welcome_2nd, survey_25_* (30ê°œ) ë™ì‹œ ê²€ìƒ‰
    - ê° ì¸ë±ìŠ¤ë§ˆë‹¤ OpenSearch + Qdrant ë³‘ë ¬ ì‹¤í–‰
    - RRF ê²°í•© í›„ Redis ìºì‹±
    
    Returns:
        task_id: Celery Task ID
        status_url: ì‘ì—… ìƒíƒœ ì¡°íšŒ URL
        scroll_url: ë¬´í•œ ìŠ¤í¬ë¡¤ URL
    """
    from redis_celery.tasks.search_tasks import parallel_hybrid_search_all
    
    try:
        # Celery íƒœìŠ¤í¬ ë¹„ë™ê¸° í˜¸ì¶œ
        task = parallel_hybrid_search_all.delay(
            query=request.query,
            index_name=request.index_name or "*",
            size=100,  # ì¶©ë¶„í•œ ê²°ê³¼ í™•ë³´ (Redisì— ìºì‹±ë¨)
            use_vector_search=request.use_vector_search,
        )
        
        logger.info(f"ğŸš€ ê²€ìƒ‰ ì‘ì—… ì‹œì‘: task_id={task.id}, query='{request.query}'")
        
        return {
            "message": "ë³‘ë ¬ ê²€ìƒ‰ ì‹œì‘",
            "task_id": task.id,
            "status_url": f"/search/status/{task.id}",
            "scroll_url": f"/search/scroll/{task.id}?offset=0&limit=20"
        }
        
    except Exception as e:
        logger.error(f"âŒ ê²€ìƒ‰ ì‘ì—… ì‹œì‘ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"ê²€ìƒ‰ ì‘ì—… ì‹œì‘ ì‹¤íŒ¨: {str(e)}"
        )


@router.get("/status/{task_id}", response_model=Dict[str, Any], summary="Celery ì‘ì—… ìƒíƒœ ì¡°íšŒ")
async def get_task_status(task_id: str):
    """
    í´ë¼ì´ì–¸íŠ¸ê°€ ì´ ì—”ë“œí¬ì¸íŠ¸ë¥¼ í´ë§í•˜ì—¬ ì‘ì—… ìƒíƒœ í™•ì¸
    
    ìƒíƒœ:
    - PENDING: ì‘ì—… ëŒ€ê¸° ì¤‘
    - STARTED: ì‘ì—… ì‹¤í–‰ ì¤‘
    - SUCCESS: ì‘ì—… ì™„ë£Œ
    - FAILURE: ì‘ì—… ì‹¤íŒ¨
    """
    try:
        task = AsyncResult(task_id, app=celery_app)
        
        response = {
            'task_id': task_id,
            'state': task.state,
            'ready': task.ready(),
            'successful': task.successful() if task.ready() else None,
        }
        
        if task.ready():
            if task.state == 'SUCCESS':
                result = task.result
                response['result'] = {
                    'status': result.get('status'),
                    'query': result.get('query'),
                    'total_hits': result.get('total_hits'),
                    'max_score': result.get('max_score'),
                    'took_ms': result.get('took_ms'),
                    'search_summary': result.get('search_summary'),
                    'scroll_url': f"/search/scroll/{task_id}?offset=0&limit=20"
                }
            elif task.state == 'FAILURE':
                error_info = task.result if isinstance(task.result, dict) else {'error': str(task.result)}
                response['result'] = {
                    'status': 'failed',
                    'error': error_info.get('error', str(task.result)),
                    'error_type': error_info.get('error_type', 'UnknownError')
                }
        else:
            response['message'] = f"ì‘ì—… ì§„í–‰ ì¤‘ ({task.state})"
        
        return response
        
    except Exception as e:
        logger.error(f"âŒ ì‘ì—… ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ì‘ì—… ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        )


@router.get("/scroll/{task_id}", response_model=SearchResponse, summary="ë¬´í•œ ìŠ¤í¬ë¡¤: RRF ê²°ê³¼ í˜ì´ì§• ì¡°íšŒ")
async def get_scrolled_results(
    task_id: str,
    offset: int = Query(0, ge=0, description="ì‹œì‘ ìœ„ì¹˜ (0ë¶€í„° ì‹œì‘)"),
    limit: int = Query(20, ge=1, le=100, description="ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜ (1~100)"),
    r: redis.StrictRedis = Depends(get_redis_client)
):
    """
    Redisì— ìºì‹±ëœ RRF ê²°ê³¼ë¥¼ offset/limit ë°©ì‹ìœ¼ë¡œ ì¡°íšŒ
    
    ì‚¬ìš© ì˜ˆì‹œ:
    - ì²« í˜ì´ì§€: GET /search/scroll/{task_id}?offset=0&limit=20
    - ë‘ ë²ˆì§¸ í˜ì´ì§€: GET /search/scroll/{task_id}?offset=20&limit=20
    - ì„¸ ë²ˆì§¸ í˜ì´ì§€: GET /search/scroll/{task_id}?offset=40&limit=20
    
    Returns:
        SearchResponse: ê²€ìƒ‰ ê²°ê³¼ (RRF ìˆœì„œ ìœ ì§€)
    """
    try:
        id_list_key = f"task:{task_id}:ids"
        
        # 1. Redisì—ì„œ ì „ì²´ ID ëª©ë¡ ê¸¸ì´ í™•ì¸
        total_hits = r.llen(id_list_key)
        
        if total_hits == 0:
            raise HTTPException(
                status_code=404,
                detail="ê²€ìƒ‰ ê²°ê³¼ê°€ ë§Œë£Œë˜ì—ˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê²€ìƒ‰ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
            )
        
        # 2. offset/limitìœ¼ë¡œ user_id ì¶”ì¶œ
        end_index = offset + limit - 1
        user_ids = r.lrange(id_list_key, offset, end_index)
        
        if not user_ids:
            # ëê¹Œì§€ ìŠ¤í¬ë¡¤í•œ ê²½ìš°
            return SearchResponse(
                query=f"Task {task_id}",
                total_hits=total_hits,
                max_score=0.0,
                results=[],
                took_ms=0
            )
        
        # 3. MGETìœ¼ë¡œ ìƒì„¸ ë°ì´í„° ì¼ê´„ ì¡°íšŒ
        data_keys = [f"task:{task_id}:data:{uid}" for uid in user_ids]
        detailed_results_json = r.mget(data_keys)
        
        results: List[SearchResult] = []
        max_score = 0.0
        
        for item_json in detailed_results_json:
            if item_json:
                try:
                    data = json.loads(item_json)
                    result = SearchResult(**data)
                    results.append(result)
                    max_score = max(max_score, result.score)
                except json.JSONDecodeError as e:
                    logger.error(f"Redis ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
        
        logger.info(f"ğŸ“„ í˜ì´ì§• ì¡°íšŒ: task_id={task_id}, offset={offset}, limit={limit}, returned={len(results)}/{total_hits}")
        
        # 4. ì‘ë‹µ êµ¬ì„±
        return SearchResponse(
            query=f"Task {task_id} (Offset: {offset}, Limit: {limit})",
            total_hits=total_hits,
            max_score=max_score,
            results=results,
            query_analysis={
                "note": "í˜ì´ì§• ë°ì´í„°ëŠ” ì „ì²´ ë¶„ì„ ê²°ê³¼ë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                "pagination": {
                    "offset": offset,
                    "limit": limit,
                    "has_more": (offset + limit) < total_hits
                }
            },
            took_ms=0
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ í˜ì´ì§• ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"í˜ì´ì§• ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        )


# ==================== ë””ë²„ê¹… ë° í†µê³„ ====================

@router.get("/stats/{index_name}", summary="ì¸ë±ìŠ¤ í†µê³„")
async def get_search_stats(
    index_name: str,
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """ì¸ë±ìŠ¤ ê²€ìƒ‰ í†µê³„ ì¡°íšŒ"""
    try:
        if not os_client.indices.exists(index=index_name):
            raise HTTPException(
                status_code=404,
                detail=f"ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {index_name}"
            )
        
        stats = os_client.indices.stats(index=index_name)
        count = os_client.count(index=index_name)
        
        return {
            "index_name": index_name,
            "doc_count": count['count'],
            "size_mb": round(stats['_all']['total']['store']['size_in_bytes'] / 1024 / 1024, 2),
            "search_total": stats['_all']['total']['search']['query_total'],
            "search_time_ms": stats['_all']['total']['search']['query_time_in_millis']
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[ERROR] í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/indices", summary="ëª¨ë“  survey ì¸ë±ìŠ¤ ëª©ë¡")
async def list_survey_indices(
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """OpenSearchì—ì„œ ëª¨ë“  survey ì¸ë±ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
    try:
        indices_response = os_client.cat.indices(format='json')
        
        survey_indices = [
            {
                'index': idx['index'],
                'doc_count': idx['docs.count'],
                'size': idx['store.size']
            }
            for idx in indices_response
            if idx['index'].startswith('s_welcome') or idx['index'].startswith('survey_25')
        ]
        
        survey_indices.sort(key=lambda x: x['index'])
        
        return {
            "total_indices": len(survey_indices),
            "indices": survey_indices
        }
        
    except Exception as e:
        logger.error(f"[ERROR] ì¸ë±ìŠ¤ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/cache/{task_id}", summary="Redis ìºì‹œ ì‚­ì œ")
async def delete_cache(
    task_id: str,
    r: redis.StrictRedis = Depends(get_redis_client)
):
    """íŠ¹ì • taskì˜ Redis ìºì‹œ ì‚­ì œ"""
    try:
        # ID ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        id_list_key = f"task:{task_id}:ids"
        user_ids = r.lrange(id_list_key, 0, -1)
        
        # ì‚­ì œí•  í‚¤ ëª©ë¡
        keys_to_delete = [id_list_key]
        keys_to_delete.extend([f"task:{task_id}:data:{uid}" for uid in user_ids])
        
        # ì¼ê´„ ì‚­ì œ
        deleted_count = 0
        if keys_to_delete:
            deleted_count = r.delete(*keys_to_delete)
        
        logger.info(f"ğŸ—‘ï¸ ìºì‹œ ì‚­ì œ: task_id={task_id}, deleted={deleted_count}ê°œ í‚¤")
        
        return {
            "task_id": task_id,
            "deleted_keys": deleted_count,
            "message": "ìºì‹œ ì‚­ì œ ì™„ë£Œ"
        }
        
    except Exception as e:
        logger.error(f"âŒ ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}"
        )


# ==================== Qdrant ë””ë²„ê¹… ====================

@router.get("/qdrant/collections", summary="Qdrant ì»¬ë ‰ì…˜ ëª©ë¡")
async def list_qdrant_collections():
    """Qdrant ì»¬ë ‰ì…˜ ëª©ë¡ ë° í†µê³„"""
    qdrant_client = getattr(router, 'qdrant_client', None)
    
    if not qdrant_client:
        raise HTTPException(
            status_code=503,
            detail="Qdrant í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        )
    
    try:
        cols = qdrant_client.get_collections()
        items = []
        
        for c in cols.collections:
            try:
                info = qdrant_client.get_collection(c.name)
                items.append({
                    "name": c.name,
                    "vectors_count": getattr(info, 'vectors_count', None),
                    "points_count": getattr(info, 'points_count', None),
                })
            except Exception as e:
                items.append({
                    "name": c.name,
                    "error": str(e)
                })
        
        return {
            "total_collections": len(items),
            "collections": items
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ==================== í—¬ìŠ¤ ì²´í¬ ====================

@router.get("/health", summary="ê²€ìƒ‰ ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬")
async def health_check(
    os_client: OpenSearch = Depends(lambda: router.os_client),
    r: redis.StrictRedis = Depends(get_redis_client)
):
    """ê²€ìƒ‰ ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    status = {
        "opensearch": "disconnected",
        "redis": "disconnected",
        "qdrant": "disconnected",
        "celery": "unknown"
    }
    
    # OpenSearch
    try:
        if os_client and os_client.ping():
            status["opensearch"] = "connected"
    except Exception as e:
        status["opensearch"] = f"error: {str(e)}"
    
    # Redis
    try:
        r.ping()
        status["redis"] = "connected"
    except Exception as e:
        status["redis"] = f"error: {str(e)}"
    
    # Qdrant
    try:
        qdrant_client = getattr(router, 'qdrant_client', None)
        if qdrant_client:
            qdrant_client.get_collections()
            status["qdrant"] = "connected"
    except Exception as e:
        status["qdrant"] = f"error: {str(e)}"
    
    # Celery
    try:
        inspect = celery_app.control.inspect()
        active_workers = inspect.active()
        if active_workers:
            status["celery"] = f"active ({len(active_workers)} workers)"
        else:
            status["celery"] = "no workers"
    except Exception as e:
        status["celery"] = f"error: {str(e)}"
    
    all_healthy = all(
        s == "connected" or "active" in s
        for s in [status["opensearch"], status["redis"], status["qdrant"], status["celery"]]
    )
    
    return {
        "status": "healthy" if all_healthy else "degraded",
        "components": status
    }