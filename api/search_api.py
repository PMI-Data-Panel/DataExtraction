"""ê²€ìƒ‰ API ë¼ìš°í„°"""
import asyncio
import json
import logging
import hashlib
import re
from collections import defaultdict, OrderedDict
from time import perf_counter
from datetime import datetime, timezone
from uuid import uuid4
from typing import List, Dict, Any, Optional, Set, Tuple, Literal
from fastapi import APIRouter, HTTPException, Depends, Query
from pydantic import BaseModel, Field
from opensearchpy import OpenSearch

# ë¶„ì„ê¸° ë° ì¿¼ë¦¬ ë¹Œë”
from rag_query_analyzer.analyzers.main_analyzer import AdvancedRAGQueryAnalyzer
from rag_query_analyzer.analyzers.demographic_extractor import DemographicExtractor
from rag_query_analyzer.models.entities import DemographicType, DemographicEntity
from connectors.hybrid_searcher import OpenSearchHybridQueryBuilder, calculate_rrf_score
from connectors.data_fetcher import DataFetcher
from connectors.qdrant_helper import search_qdrant_async, search_qdrant_collections_async

logger = logging.getLogger(__name__)
# main_api.pyì—ì„œ ì´ë¯¸ basicConfigë¡œ ë£¨íŠ¸ ë¡œê±°ê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë¯€ë¡œ
# propagateë§Œ Trueë¡œ ì„¤ì •í•˜ì—¬ ë£¨íŠ¸ ë¡œê±°ë¡œ ì „íŒŒë˜ë„ë¡ í•¨
logger.propagate = True

router = APIRouter(
    prefix="/search",
    tags=["Search"]
)

# OpenSearch ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ë³µì¡í•œ ì¿¼ë¦¬ë‚˜ ëŒ€ìš©ëŸ‰ ê²€ìƒ‰ì„ ìœ„í•´ 30ì´ˆë¡œ ì„¤ì •)
DEFAULT_OS_TIMEOUT = 30

# ëŸ°íƒ€ì„ ê³µìœ  ê°ì²´ (í•œ ë²ˆë§Œ ì´ˆê¸°í™” í›„ ì¬ì‚¬ìš©)
router.analyzer = None  # type: ignore[attr-defined]
router.embedding_model = None  # type: ignore[attr-defined]
router.config = None  # type: ignore[attr-defined]

# â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ welcome ì¸ë±ìŠ¤ ìºì‹œ ì œê±°

_SUMMARY_RESPONSE_TEMPLATE = (
    "{\n"
    '  "highlights": ["ìš”ì•½1", "ìš”ì•½2", "ìš”ì•½3"],\n'
    '  "demographic_summary": "ì£¼ìš” ì¸êµ¬í†µê³„ ì¸ì‚¬ì´íŠ¸",\n'
    '  "behavioral_summary": "ì£¼ìš” í–‰ë™/ìŠµê´€ ì¸ì‚¬ì´íŠ¸",\n'
    '  "data_signals": ["ì£¼ìš” ìˆ˜ì¹˜ë‚˜ íŒ¨í„´"],\n'
    '  "follow_up_questions": ["í›„ì†ìœ¼ë¡œ íƒìƒ‰í•˜ë©´ ì¢‹ì„ ì§ˆë¬¸"]\n'
    "}"
)

_DEFAULT_SUMMARY_INSTRUCTIONS = (
    "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ì œê³µí•˜ì„¸ìš”. "
    "ì •ëŸ‰ì  ì§€í‘œ(ì‘ë‹µì ìˆ˜, ë¹„ìœ¨ ë“±)ê°€ ìˆì„ ê²½ìš° ëª…ì‹œí•˜ê³ , ë°ì´í„°ì˜ í¸í–¥ì´ë‚˜ í•œê³„ë„ ì–¸ê¸‰í•˜ì„¸ìš”."
)


def _utc_now_iso() -> str:
    return datetime.now(timezone.utc).replace(tzinfo=timezone.utc).isoformat()


def _truncate_text(value: Any, max_length: int = 4000) -> str:
    text = value if isinstance(value, str) else json.dumps(value, ensure_ascii=False)
    if len(text) <= max_length:
        return text
    return text[: max_length - 3] + "..."


def _redis_list_append(
    client,
    key: str,
    payload: Dict[str, Any],
    max_length: Optional[int],
    ttl_seconds: Optional[int],
) -> None:
    if not client or not key:
        return
    try:
        serialized = json.dumps(payload, ensure_ascii=False, default=str)
        pipeline = client.pipeline()
        pipeline.rpush(key, serialized)
        if max_length and max_length > 0:
            pipeline.ltrim(key, -max_length, -1)
        if ttl_seconds and ttl_seconds > 0:
            pipeline.expire(key, ttl_seconds)
        pipeline.execute()
    except Exception as exc:
        logger.warning(f"âš ï¸ Redis ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: key={key}, error={exc}")


def _make_conversation_key(prefix: Optional[str], session_id: Optional[str]) -> Optional[str]:
    if not prefix or not session_id:
        return None
    return f"{prefix}:{session_id}"


def _make_history_key(prefix: Optional[str], owner_id: Optional[str]) -> Optional[str]:
    if not prefix or not owner_id:
        return None
    return f"{prefix}:{owner_id}"


def _extract_json_from_text(text: str) -> Optional[Dict[str, Any]]:
    if not text:
        return None
    try:
        fenced = re.search(r"```json\s*(.*?)\s*```", text, re.DOTALL)
        if fenced:
            return json.loads(fenced.group(1))
        fallback = re.search(r"\{.*\}", text, re.DOTALL)
        if fallback:
            return json.loads(fallback.group(0))
        return json.loads(text)
    except Exception:
        return None


def _ensure_request_defaults(request: Any) -> None:
    """ìš”ì²­ì— í•„ìˆ˜ ê¸°ë³¸ê°’ì„ ì±„ì›Œ ì‚¬ìš©ìê°€ queryë§Œ ë³´ë‚´ë„ ë™ì‘í•˜ë„ë¡ ë³´ì •."""
    session_id = getattr(request, "session_id", None)
    if not session_id:
        session_id = str(uuid4())
        request.session_id = session_id

    user_id = getattr(request, "user_id", None)
    if not user_id:
        request.user_id = session_id

    # ëª¨ë“  ë³´ì¡° ê¸°ëŠ¥ ê¸°ë³¸ í™œì„±í™”
    request.log_conversation = True
    request.log_search_history = True
    request.request_llm_summary = True

def _prepare_summary_results(
    results: List["SearchResult"],
    max_results: int,
    max_chars: int,
) -> List[Dict[str, Any]]:
    trimmed: List[Dict[str, Any]] = []
    total_chars = 0
    for idx, result in enumerate(results[:max_results], start=1):
        if hasattr(result, "model_dump"):
            item = result.model_dump()
        elif isinstance(result, dict):
            item = result
        else:
            continue

        item_copy = {
            "rank": idx,
            "user_id": item.get("user_id"),
            "score": item.get("score"),
            "timestamp": item.get("timestamp"),
            "demographic_info": item.get("demographic_info"),
            "behavioral_info": item.get("behavioral_info"),
            "qa_pairs": (item.get("qa_pairs") or [])[:3],
            "matched_qa_pairs": (item.get("matched_qa_pairs") or [])[:3],
            "highlights": item.get("highlights"),
        }

        serialized = json.dumps(item_copy, ensure_ascii=False)
        prospective_total = total_chars + len(serialized)
        if prospective_total > max_chars and trimmed:
            break
        trimmed.append(item_copy)
        total_chars = prospective_total
    return trimmed


def _maybe_generate_llm_summary(
    *,
    request,
    response: "SearchResponse",
    analysis,
) -> Optional[Dict[str, Any]]:
    if not getattr(request, "request_llm_summary", False):
        return None

    if not getattr(router, "enable_search_summary", False):
        logger.info("LLM ìš”ì•½ ë¹„í™œì„±í™” ì„¤ì •ìœ¼ë¡œ ì¸í•´ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    client = getattr(router, "anthropic_client", None)
    if client is None:
        logger.warning("Anthropic í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ LLM ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    config = getattr(router, "config", None)
    if config is None:
        logger.warning("Config ì„¤ì •ì´ ì—†ì–´ LLM ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    model_name = getattr(router, "search_summary_model", None) or config.CLAUDE_MODEL
    if not model_name:
        logger.warning("ìš”ì•½ì— ì‚¬ìš©í•  ëª¨ë¸ëª…ì´ ì„¤ì •ë˜ì§€ ì•Šì•„ LLM ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    max_results = getattr(router, "search_summary_max_results", 10)
    max_chars = getattr(router, "search_summary_max_chars", 16000)
    prepared_results = _prepare_summary_results(response.results, max_results, max_chars)
    if not prepared_results:
        logger.info("LLM ìš”ì•½ì„ ìœ„í•œ ê²°ê³¼ê°€ ì—†ì–´ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    instructions = getattr(request, "llm_summary_instructions", None) or _DEFAULT_SUMMARY_INSTRUCTIONS

    prompt = (
        "ë‹¹ì‹ ì€ ì„¤ë¬¸ì¡°ì‚¬ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
        "ì£¼ì–´ì§„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”.\n\n"
        f"ì‚¬ìš©ì ì§ˆì˜: {request.query}\n"
        f"ì˜ˆìƒ ê²€ìƒ‰ ì˜ë„: {getattr(analysis, 'intent', 'N/A')}\n"
        f"ì¶”ì¶œëœ must_terms: {getattr(analysis, 'must_terms', [])}\n"
        f"ì¶”ì¶œëœ should_terms: {getattr(analysis, 'should_terms', [])}\n"
        f"ì´ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {response.total_hits}\n"
        f"í˜„ì¬ ë°˜í™˜ëœ ê²°ê³¼ ìˆ˜: {len(response.results)}\n\n"
        f"ìš”ì•½ ì§€ì¹¨: {instructions}\n\n"
        "ê²€ìƒ‰ ê²°ê³¼(ìµœëŒ€ ì¼ë¶€) JSON:\n"
        f"{json.dumps(prepared_results, ensure_ascii=False, indent=2)}\n\n"
        "ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. í˜•ì‹ ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n"
        f"{_SUMMARY_RESPONSE_TEMPLATE}\n"
    )

    max_tokens = min(1200, getattr(config, "CLAUDE_MAX_TOKENS", 1500))
    temperature = getattr(config, "CLAUDE_TEMPERATURE", 0.1)

    try:
        message = client.messages.create(
            model=model_name,
            max_tokens=max_tokens,
            temperature=temperature,
            messages=[{"role": "user", "content": prompt}],
        )
        content = ""
        if message and getattr(message, "content", None):
            parts = getattr(message, "content", [])
            if parts:
                # Anthropics SDK returns list of blocks with .text
                first = parts[0]
                content = getattr(first, "text", "") or ""
        summary_json = _extract_json_from_text(content)
        if summary_json is None:
            logger.warning("LLM ìš”ì•½ ì‘ë‹µì—ì„œ JSONì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return {
                "model": model_name,
                "generated_at": _utc_now_iso(),
                "raw_text": content,
            }
        return {
            "model": model_name,
            "generated_at": _utc_now_iso(),
            "summary": summary_json,
        }
    except Exception as exc:
        logger.warning(f"âš ï¸ LLM ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {exc}")
        return None


def _extract_response_timings(response: "SearchResponse", fallback: Optional[Dict[str, Any]]) -> Dict[str, Any]:
    if response.query_analysis and isinstance(response.query_analysis, dict):
        timings = response.query_analysis.get("timings_ms")
        if isinstance(timings, dict):
            return timings
    return fallback or {}


def _persist_search_logs(
    *,
    request,
    response: "SearchResponse",
    analysis,
    cache_hit: bool,
    timings: Dict[str, Any],
) -> None:
    client = getattr(router, "redis_client", None)
    if client is None:
        return

    timestamp = _utc_now_iso()
    session_id = getattr(request, "session_id", None)
    user_id = getattr(request, "user_id", None)
    request_id = getattr(request, "request_id", None)
    request_metadata = getattr(request, "metadata", None)
    conversation_prefix = getattr(router, "conversation_history_prefix", None)
    conversation_ttl = getattr(router, "conversation_history_ttl_seconds", None)
    conversation_max = getattr(router, "conversation_history_max_messages", None)
    search_history_prefix = getattr(router, "search_history_prefix", None)
    search_history_ttl = getattr(router, "search_history_ttl_seconds", None)
    search_history_max = getattr(router, "search_history_max_entries", None)

    top_user_ids = [
        getattr(result, "user_id", None) for result in (response.results or [])[:5]
        if getattr(result, "user_id", None)
    ]

    if getattr(request, "log_conversation", True):
        conversation_key = _make_conversation_key(conversation_prefix, session_id)
        if conversation_key:
            user_entry = {
                "role": "user",
                "timestamp": timestamp,
                "content": _truncate_text(request.query, 4000),
                "session_id": session_id,
                "user_id": user_id,
                "request_id": request_id,
                "metadata": request_metadata,
            }
            _redis_list_append(client, conversation_key, user_entry, conversation_max, conversation_ttl)

            assistant_payload: Dict[str, Any] = {
                "total_hits": response.total_hits,
                "returned_count": len(response.results or []),
                "cache_hit": cache_hit,
                "top_user_ids": top_user_ids,
            }
            if response.llm_summary:
                assistant_payload["llm_summary"] = response.llm_summary

            assistant_entry = {
                "role": "assistant",
                "timestamp": timestamp,
                "content": _truncate_text(assistant_payload, 4000),
                "session_id": session_id,
                "user_id": user_id,
                "request_id": request_id,
            }
            _redis_list_append(client, conversation_key, assistant_entry, conversation_max, conversation_ttl)

    if getattr(request, "log_search_history", True):
        owner_id = user_id or session_id or "default"
        history_key = _make_history_key(search_history_prefix, owner_id)
        if history_key:
            history_entry = {
                "timestamp": timestamp,
                "user_id": user_id,
                "session_id": session_id,
                "request_id": request_id,
                "query": request.query,
                "intent": getattr(analysis, "intent", None),
                "must_terms": getattr(analysis, "must_terms", []),
                "should_terms": getattr(analysis, "should_terms", []),
                "page": response.page,
                "page_size": response.page_size,
                "total_hits": response.total_hits,
                "returned_count": len(response.results or []),
                "cache_hit": cache_hit,
                "timings": timings,
                "top_user_ids": top_user_ids,
                "llm_summary": response.llm_summary,
                "metadata": request_metadata,
            }
            _redis_list_append(client, history_key, history_entry, search_history_max, search_history_ttl)


class ConversationMessage(BaseModel):
    role: str
    timestamp: str
    session_id: Optional[str] = None
    user_id: Optional[str] = None
    request_id: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None
    content: Any


class SearchHistoryEntry(BaseModel):
    timestamp: str
    user_id: Optional[str] = None
    session_id: Optional[str] = None
    request_id: Optional[str] = None
    query: str
    intent: Optional[str] = None
    must_terms: List[str] = Field(default_factory=list)
    should_terms: List[str] = Field(default_factory=list)
    page: int
    page_size: int
    total_hits: int
    returned_count: int
    cache_hit: bool
    timings: Dict[str, Any] = Field(default_factory=dict)
    top_user_ids: List[str] = Field(default_factory=list)
    llm_summary: Optional[Dict[str, Any]] = None
    metadata: Optional[Dict[str, Any]] = None


def _parse_conversation_record(item: str) -> Optional[ConversationMessage]:
    if not item:
        return None
    try:
        payload = json.loads(item)
    except Exception as exc:
        logger.warning(f"âš ï¸ ëŒ€í™” ë¡œê·¸ JSON íŒŒì‹± ì‹¤íŒ¨: {exc}")
        return None

    content = payload.get("content")
    if payload.get("role") == "assistant" and isinstance(content, str):
        try:
            content = json.loads(content)
        except Exception:
            pass
    return ConversationMessage(
        role=payload.get("role"),
        timestamp=payload.get("timestamp"),
        session_id=payload.get("session_id"),
        user_id=payload.get("user_id"),
        request_id=payload.get("request_id"),
        metadata=payload.get("metadata"),
        content=content,
    )


def _parse_search_history_record(item: str) -> Optional[SearchHistoryEntry]:
    if not item:
        return None
    try:
        payload = json.loads(item)
    except Exception as exc:
        logger.warning(f"âš ï¸ ê²€ìƒ‰ ì´ë ¥ JSON íŒŒì‹± ì‹¤íŒ¨: {exc}")
        return None

    llm_summary = payload.get("llm_summary")
    if isinstance(llm_summary, str):
        try:
            llm_summary = json.loads(llm_summary)
        except Exception:
            pass

    return SearchHistoryEntry(
        timestamp=payload.get("timestamp"),
        user_id=payload.get("user_id"),
        session_id=payload.get("session_id"),
        request_id=payload.get("request_id"),
        query=payload.get("query", ""),
        intent=payload.get("intent"),
        must_terms=payload.get("must_terms") or [],
        should_terms=payload.get("should_terms") or [],
        page=payload.get("page", 1),
        page_size=payload.get("page_size", 10),
        total_hits=payload.get("total_hits", 0),
        returned_count=payload.get("returned_count", 0),
        cache_hit=bool(payload.get("cache_hit")),
        timings=payload.get("timings") or {},
        top_user_ids=payload.get("top_user_ids") or [],
        llm_summary=llm_summary,
        metadata=payload.get("metadata"),
    )


def _finalize_search_response(
    *,
    request,
    response: "SearchResponse",
    analysis,
    cache_hit: bool,
    timings: Optional[Dict[str, Any]] = None,
) -> "SearchResponse":
    summary_payload = _maybe_generate_llm_summary(
        request=request,
        response=response,
        analysis=analysis,
    )
    if summary_payload:
        response = response.model_copy(update={"llm_summary": summary_payload})

    effective_timings = _extract_response_timings(response, timings)
    _persist_search_logs(
        request=request,
        response=response,
        analysis=analysis,
        cache_hit=cache_hit,
        timings=effective_timings,
    )
    return response


# â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ welcome ì¸ë±ìŠ¤ ìºì‹œ í•¨ìˆ˜ ì œê±°


def calculate_rrf_score_adaptive(
    keyword_results: List[Dict[str, Any]],
    vector_results: List[Dict[str, Any]],
    query_intent: Optional[str],
    has_filters: bool,
    use_vector_search: bool,
) -> Tuple[List[Dict[str, Any]], int, str]:
    """ì¿¼ë¦¬ íŠ¹ì„±ì— ë”°ë¼ RRF k ê°’ì„ ì¡°ì •"""
    k = 60
    reason = "ê· í˜• ìœ ì§€ (k=60)"

    if has_filters:
        k = 40
        reason = "í•„í„° ì ìš© â†’ ì •í™•ë„ ì¤‘ì‹œ (k=40)"
    elif use_vector_search and query_intent and query_intent.lower() in {"semantic", "semantic_search"}:
        k = 80
        reason = f"ì˜ë„={query_intent} â†’ ë²¡í„° ê°€ì¤‘ (k=80)"

    combined = calculate_rrf_score(
        keyword_results=keyword_results,
        vector_results=vector_results,
        k=k,
    )
    return combined, k, reason


def _sort_dict_recursive(obj: Any) -> Any:
    """ë”•ì…”ë„ˆë¦¬/ë¦¬ìŠ¤íŠ¸ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì •ë ¬"""
    if isinstance(obj, dict):
        return {key: _sort_dict_recursive(obj[key]) for key in sorted(obj)}
    if isinstance(obj, list):
        normalized_items = [_sort_dict_recursive(item) for item in obj]
        try:
            return sorted(
                normalized_items,
                key=lambda item: json.dumps(item, ensure_ascii=False, sort_keys=True),
            )
        except TypeError:
            return normalized_items
    return obj


def _normalize_filters_for_cache(filters: List[Dict[str, Any]]) -> str:
    """í•„í„° ëª©ë¡ì„ ì•ˆì •ì ì¸ ë¬¸ìì—´ë¡œ ë³€í™˜"""
    if not filters:
        return ""

    normalized_strings = []
    for filter_item in filters:
        normalized = _sort_dict_recursive(filter_item)
        normalized_strings.append(json.dumps(normalized, ensure_ascii=False, sort_keys=True))

    normalized_strings.sort()
    return "|".join(normalized_strings)


def _make_cache_key(
    *,
    prefix: str,
    query: str,
    index_name: str,
    page_size: int,
    use_vector: bool,
    use_claude: bool,
    must_terms: List[str],
    should_terms: List[str],
    must_not_terms: List[str],
    filters_signature: Optional[str] = None,
    behavior_signature: Optional[str] = None,
) -> str:
    """ìƒì„±ëœ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©í•˜ê¸° ìœ„í•œ ìºì‹œ í‚¤ ìƒì„± (ì•ˆì •í™”)"""
    stable_must = sorted(must_terms) if must_terms else []
    stable_should = sorted(should_terms) if should_terms else []
    stable_must_not = sorted(must_not_terms) if must_not_terms else []

    payload = {
        "query": query.strip().lower(),
        "index": index_name,
        "page_size": page_size,
        "use_vector": use_vector,
        "use_claude": bool(use_claude),
        "must_terms": stable_must,
        "should_terms": stable_should,
        "must_not_terms": stable_must_not,
        "filters_signature": filters_signature or "",
        "behavior_signature": behavior_signature or "",
    }
    raw = json.dumps(payload, ensure_ascii=False, sort_keys=True)
    digest = hashlib.sha256(raw.encode("utf-8")).hexdigest()
    key = f"{prefix}:{digest}"

    logger.debug(f"ğŸ”‘ Cache key generated: {key}")
    logger.debug(f"   - must_terms: {stable_must}")
    logger.debug(f"   - should_terms: {stable_should}")
    logger.debug(f"   - filters_signature: {(filters_signature or '')[:100]}...")
    logger.debug(f"   - behavior_signature: {(behavior_signature or '')[:100]}...")
    logger.debug(f"   - use_claude: {bool(use_claude)}")

    return key


def _serialize_result(result: "SearchResult") -> Dict[str, Any]:
    """SearchResultë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ dictë¡œ ë³€í™˜"""
    return result.model_dump()


def _deserialize_result(payload: Dict[str, Any]) -> "SearchResult":
    """dictë¥¼ SearchResult ê°ì²´ë¡œ ì—­ì§ë ¬í™”"""
    return SearchResult(**payload)


def _slice_results(
    serialized_items: List[Dict[str, Any]],
    page: int,
    page_size: int,
) -> Tuple[List["SearchResult"], bool]:
    """í˜ì´ì§€ ì •ë³´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²°ê³¼ë¥¼ ìŠ¬ë¼ì´ì‹±"""
    if page <= 0:
        page = 1
    start = (page - 1) * page_size
    end = start + page_size

    if start >= len(serialized_items):
        return [], False

    page_items = serialized_items[start:end]
    results = [_deserialize_result(item) for item in page_items]
    has_more = end < len(serialized_items)
    return results, has_more


def _build_cached_response(
    *,
    payload: Dict[str, Any],
    request: "SearchRequest",
    analysis,
    filters_for_response: List[Dict[str, Any]],
    overall_start: float,
    extracted_entities_dict: Optional[Dict[str, Any]] = None,
) -> "SearchResponse":
    """Redis ìºì‹œì—ì„œ ë¶ˆëŸ¬ì˜¨ ê²°ê³¼ë¡œ SearchResponse êµ¬ì„±"""
    total_hits = payload.get("total_hits", 0)
    max_score = payload.get("max_score", 0.0)
    serialized_items = payload.get("items", [])

    cached_page_size = payload.get("page_size")
    request_page_size = getattr(request, "size", None)
    if request_page_size is None:
        request_page_size = getattr(request, "page_size", None)
    page_size = cached_page_size or request_page_size or max(len(serialized_items), 1)

    page_results, has_more_local = _slice_results(serialized_items, request.page, page_size)
    has_more = has_more_local and ((request.page * page_size) < total_hits)
    total_duration_ms = (perf_counter() - overall_start) * 1000

    timings = {
        "cache_hit": 1.0,
        "total_ms": total_duration_ms,
    }

    query_analysis = {
        "intent": analysis.intent,
        "must_terms": analysis.must_terms,
        "should_terms": analysis.should_terms,
        "alpha": analysis.alpha,
        "confidence": analysis.confidence,
        "filters": filters_for_response,
        "size": page_size,
        "timings_ms": timings,
        "behavioral_conditions": payload.get("behavioral_conditions", {}),
        "use_claude_analyzer": bool(payload.get("use_claude", False)),
    }
    if extracted_entities_dict is not None:
        query_analysis["extracted_entities"] = extracted_entities_dict

    return SearchResponse(
        query=request.query,
        total_hits=total_hits,
        max_score=max_score,
        results=page_results,
        query_analysis=query_analysis,
        took_ms=int(total_duration_ms),
        page=request.page,
        page_size=page_size,
        has_more=has_more,
    )


def _log_final_summary(
    *,
    stage: str,
    query: str,
    analysis,
    total_hits: int,
    returned_count: int,
    page: int,
    page_size: int,
    cache_hit: bool,
    timings: Dict[str, Any],
    took_ms: Optional[float],
    filters: Optional[List[Dict[str, Any]]],
    behavioral_conditions: Optional[Dict[str, Any]],
    use_claude: Optional[bool] = None,
) -> None:
    """ê²€ìƒ‰ ì¢…ë£Œ ì‹œ í•µì‹¬ ì •ë³´ë¥¼ í•œ ë²ˆ ë” ìš”ì•½ ì¶œë ¥."""
    intent = getattr(analysis, "intent", None)
    must_terms = getattr(analysis, "must_terms", [])
    should_terms = getattr(analysis, "should_terms", [])
    filter_count = len(filters or [])
    behavior_info = behavioral_conditions or {}
    important_timings = {k: round(v, 2) if isinstance(v, (int, float)) else v for k, v in (timings or {}).items()}

    lines = [
        "",
        "ğŸ”š ìµœì¢… ìš”ì•½ (í•µì‹¬)",
        f" â€¢ stage: {stage}",
        f" â€¢ query: {query}",
        f" â€¢ intent: {intent}",
        f" â€¢ must_terms: {must_terms}",
        f" â€¢ should_terms: {should_terms}",
        f" â€¢ behavioral_conditions: {behavior_info}",
        f" â€¢ filters: {filter_count}ê°œ",
        f" â€¢ returned/total: {returned_count}/{total_hits}",
        f" â€¢ page: {page} / page_size: {page_size}",
        f" â€¢ cache_hit: {cache_hit}",
        f" â€¢ timings: {important_timings}",
        f" â€¢ total_ms: {round(took_ms, 2) if took_ms is not None else 'N/A'}",
    ]
    if use_claude is not None:
        lines.append(f" â€¢ use_claude_analyzer: {use_claude}")

    logger.info("\n".join(lines))


def build_occupation_dsl_filter(occupation_entities: List["DemographicEntity"]) -> Dict[str, Any]:
    """ì§ì—… DemographicEntity ë¦¬ìŠ¤íŠ¸ë¥¼ OpenSearch DSL í•„í„°ë¡œ ë³€í™˜"""
    if not occupation_entities:
        return {"match_all": {}}

    occupation_values: Set[str] = set()
    for demo in occupation_entities:
        for candidate in (
            getattr(demo, "raw_value", None),
            getattr(demo, "value", None),
        ):
            if candidate:
                occupation_values.add(str(candidate))
        synonyms = getattr(demo, "synonyms", None)
        if synonyms:
            for syn in synonyms:
                if syn:
                    occupation_values.add(str(syn))

    occupation_values = {value.strip() for value in occupation_values if value and value.strip()}
    if not occupation_values:
        return {"match_all": {}}

    values_list = sorted(occupation_values)

    question_should = [
        {"match_phrase": {"qa_pairs.q_text": "ì§ì—…"}},
        {"match_phrase": {"qa_pairs.q_text": "ì§ë¬´"}},
        {"match_phrase": {"qa_pairs.q_text": "occupation"}},
    ]

    answer_should = [
        {"terms": {"qa_pairs.answer.keyword": values_list}},
        {"terms": {"qa_pairs.answer_text.keyword": values_list}},
    ]
    # â­ match ëŒ€ì‹  match_phrase ì‚¬ìš©: "ì „ë¬¸ì§"ê³¼ "ì‚¬ë¬´ì§"ì´ "ì§" í† í°ìœ¼ë¡œ ì˜ëª» ë§¤ì¹­ë˜ëŠ” ê²ƒ ë°©ì§€
    for value in values_list:
        answer_should.append({"match_phrase": {"qa_pairs.answer": value}})
        answer_should.append({"match_phrase": {"qa_pairs.answer_text": value}})

    nested_filter = {
        "nested": {
            "path": "qa_pairs",
            "query": {
                "bool": {
                    "must": [
                        {
                            "bool": {
                                "should": question_should,
                                "minimum_should_match": 1,
                            }
                        },
                        {
                            "bool": {
                                "should": answer_should,
                                "minimum_should_match": 1,
                            }
                        },
                    ]
                }
            }
        }
    }

    metadata_terms = {
        "terms": {
            "metadata.occupation.keyword": values_list,
        }
    }
    metadata_job_terms = {
        "terms": {
            "metadata.job.keyword": values_list,
        }
    }

    return {
        "bool": {
            "should": [
                metadata_terms,
                metadata_job_terms,
                nested_filter,
            ],
            "minimum_should_match": 1,
        }
    }


def get_adaptive_score_threshold(
    query: str,
    has_filters: bool,
    must_terms_count: int,
) -> Tuple[float, str]:
    """ì¿¼ë¦¬ íŠ¹ì„±ì— ë”°ë¼ Qdrant score_threshold ì¡°ì •"""
    threshold = 0.30
    reason = "ê¸°ë³¸ê°’ 0.30"

    if has_filters:
        threshold = 0.25
        reason = "í•„í„° ì ìš© â†’ í›„ë³´ í™•ë³´ (threshold=0.25)"
    elif must_terms_count >= 3:
        threshold = 0.35
        reason = "í‚¤ì›Œë“œ ë‹¤ìˆ˜ â†’ ì •í™•ë„ ì¤‘ì‹œ (threshold=0.35)"
    elif len(query.split()) <= 3:
        threshold = 0.35
        reason = "ì§§ì€ ì¿¼ë¦¬ â†’ ì •ë°€ë„ ì¤‘ì‹œ (threshold=0.35)"

    return threshold, reason


def _collect_text_from_doc(doc: Dict[str, Any]) -> str:
    text_fragments: List[str] = []

    source = doc.get("_source") or doc.get("source") or {}
    if not source and "doc" in doc:
        source = doc.get("doc", {}).get("_source", {})
    if not source and "payload" in doc:
        payload = doc["payload"]
        if isinstance(payload, dict):
            source = {
                "payload_text": payload.get("text"),
                "payload": payload,
            }

    if isinstance(source, dict):
        for key in ("qa_pairs", "qaPairs"):
            qa_pairs = source.get(key, [])
            if isinstance(qa_pairs, list):
                for qa in qa_pairs:
                    if not isinstance(qa, dict):
                        continue
                    q_text = qa.get("q_text") or qa.get("question")
                    if q_text:
                        text_fragments.append(str(q_text).lower())
                    answer = qa.get("answer") or qa.get("answer_text") or qa.get("value")
                    if answer:
                        if isinstance(answer, list):
                            text_fragments.extend(str(item).lower() for item in answer)
                        else:
                            text_fragments.append(str(answer).lower())

        for key in ("metadata", "demographic_info", "payload"):
            meta = source.get(key)
            if isinstance(meta, dict):
                for value in meta.values():
                    if value:
                        text_fragments.append(str(value).lower())

        for key in ("title", "text", "content", "payload_text"):
            value = source.get(key)
            if value:
                text_fragments.append(str(value).lower())

    if "payload" in doc and isinstance(doc["payload"], dict):
        payload = doc["payload"]
        for key in ("text", "keywords"):
            value = payload.get(key)
            if isinstance(value, list):
                text_fragments.extend(str(item).lower() for item in value)
            elif value:
                text_fragments.append(str(value).lower())

    return " ".join(text_fragments)


def contains_must_terms(doc: Dict[str, Any], must_terms: List[str]) -> bool:
    """
    âš ï¸ Deprecated: OpenSearch ì¿¼ë¦¬ì—ì„œ must ì¡°ê±´ ì²˜ë¦¬ë¡œ ëŒ€ì²´ë¨
    ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ Python ë ˆë²¨ ê²€ì¦ì€ ì œê±°ë¨ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìµœì í™”)

    ë ˆê±°ì‹œ ì½”ë“œ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€, ì‚¬ìš© ê¶Œì¥í•˜ì§€ ì•ŠìŒ
    """
    if not must_terms:
        return True

    combined_text = _collect_text_from_doc(doc)
    if not combined_text:
        return False

    for term in must_terms:
        normalized = term.lower().strip()
        if normalized and normalized not in combined_text:
            return False
    return True


def _qa_contains_terms(qa: Dict[str, Any], terms_lower: List[str]) -> bool:
    if not isinstance(qa, dict):
        return False

    text_candidates: List[str] = []
    q_text = qa.get("q_text") or qa.get("question")
    if q_text:
        text_candidates.append(str(q_text).lower())

    answer = qa.get("answer") or qa.get("answer_text") or qa.get("value")
    if answer:
        if isinstance(answer, list):
            text_candidates.extend(str(item).lower() for item in answer)
        else:
            text_candidates.append(str(answer).lower())

    if not text_candidates:
        return False

    combined = " ".join(text_candidates)
    return all(term in combined for term in terms_lower if term)


def extract_matched_qa_pairs(source: Dict[str, Any], must_terms: List[str], limit: int = 5) -> List[Dict[str, Any]]:
    if not must_terms:
        return []
    qa_pairs = source.get("qa_pairs")
    if not isinstance(qa_pairs, list):
        return []

    terms_lower = [term.lower().strip() for term in must_terms if term]
    matched: List[Dict[str, Any]] = []
    for qa in qa_pairs:
        if _qa_contains_terms(qa, terms_lower):
            matched.append(qa)
            if len(matched) >= limit:
                break
    return matched


def get_display_qa_pairs(source: Dict[str, Any], must_terms: List[str], limit: int = 5) -> List[Dict[str, Any]]:
    qa_pairs = source.get("qa_pairs")
    if not isinstance(qa_pairs, list):
        return []

    if not must_terms:
        return qa_pairs[:limit]

    terms_lower = [term.lower().strip() for term in must_terms if term]
    matched = []
    others = []
    for qa in qa_pairs:
        if _qa_contains_terms(qa, terms_lower):
            matched.append(qa)
        else:
            others.append(qa)

    ordered = matched + others
    return ordered[:limit]


def extract_inner_hit_matches(hit: Dict[str, Any]) -> List[Dict[str, Any]]:
    inner_hits = hit.get("inner_hits")
    if not isinstance(inner_hits, dict):
        return []

    collected: List[Dict[str, Any]] = []
    for inner_name, inner_data in inner_hits.items():
        hits_obj = inner_data.get("hits", {}) if isinstance(inner_data, dict) else {}
        for inner_hit in hits_obj.get("hits", []):
            inner_source = inner_hit.get("_source", {}) or {}
            if "qa_pairs" in inner_source and isinstance(inner_source["qa_pairs"], dict):
                qa_entry = inner_source["qa_pairs"].copy()
            else:
                qa_entry = inner_source.copy()

            if not isinstance(qa_entry, dict):
                continue

            if "_score" in inner_hit and "match_score" not in qa_entry:
                qa_entry["match_score"] = inner_hit["_score"]
            if "highlight" in inner_hit and "highlights" not in qa_entry:
                qa_entry["highlights"] = inner_hit["highlight"]

            qa_entry.setdefault("inner_hit_name", inner_name)
            collected.append(qa_entry)

    return collected


def extract_behavioral_qa_pairs(
    source: Dict[str, Any],
    behavioral_conditions: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """
    Behavioral ì¡°ê±´ì— ë§¤ì¹­ëœ qa_pairs ì¶”ì¶œ

    Args:
        source: OpenSearch ë¬¸ì„œ _source (ë˜ëŠ” qa_pairsë§Œ í¬í•¨ëœ dict)
        behavioral_conditions: {"smoker": True, "has_vehicle": True, ...}

    Returns:
        ë§¤ì¹­ëœ qa_pairs ë¦¬ìŠ¤íŠ¸
        [
            {
                "condition_type": "smoker",
                "condition_value": True,
                "q_text": "ê·€í•˜ëŠ” í¡ì—°ì„ í•˜ì‹­ë‹ˆê¹Œ?",
                "answer": "í¡ì—°í•¨",
                "confidence": 1.0
            },
            ...
        ]
    """
    qa_pairs = source.get('qa_pairs', [])
    if not qa_pairs or not behavioral_conditions:
        return []

    matched = []

    # ì¡°ê±´ë³„ í‚¤ì›Œë“œ ë§¤í•‘
    CONDITION_KEYWORDS = {
        'smoker': ['í¡ì—°', 'ë‹´ë°°', 'í”¼ìš°', 'í”¼ì›€'],
        'has_vehicle': ['ì°¨ëŸ‰', 'ì°¨', 'ìë™ì°¨', 'ë³´ìœ ì°¨ëŸ‰'],
        'alcohol_preference': ['ì£¼ë¥˜', 'ìŒì£¼', 'ìˆ ', 'ë§¥ì£¼', 'ì†Œì£¼', 'ì™€ì¸', 'ë§‰ê±¸ë¦¬'],
        'exercise_frequency': ['ìš´ë™', 'í—¬ìŠ¤', 'ì²´ìœ¡'],
        'pet_ownership': ['ë°˜ë ¤ë™ë¬¼', 'í«', 'ì• ì™„ë™ë¬¼', 'ê°•ì•„ì§€', 'ê³ ì–‘ì´'],
    }

    for condition_type, condition_value in behavioral_conditions.items():
        # ì´ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” í‚¤ì›Œë“œë“¤
        keywords = CONDITION_KEYWORDS.get(condition_type, [])
        if not keywords:
            continue

        # qa_pairsì—ì„œ ì´ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ì§ˆë¬¸ ì°¾ê¸°
        for qa in qa_pairs:
            q_text = qa.get('q_text', '').lower()
            answer = qa.get('answer', '')

            # í‚¤ì›Œë“œ ë§¤ì¹­
            if any(kw in q_text for kw in keywords):
                # Boolean ì¡°ê±´ (smoker, has_vehicle)
                if isinstance(condition_value, bool):
                    answer_lower = answer.lower()
                    is_positive = any(pos in answer_lower for pos in BEHAVIOR_YES_TOKENS)
                    is_negative = any(neg in answer_lower for neg in BEHAVIOR_NO_TOKENS)

                    # ì¡°ê±´ê°’ê³¼ ë‹µë³€ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                    if condition_value and is_positive:
                        matched.append({
                            'condition_type': condition_type,
                            'condition_value': condition_value,
                            'q_text': qa.get('q_text', ''),
                            'answer': answer,
                            'confidence': 1.0
                        })
                        break  # ì´ ì¡°ê±´ì— ëŒ€í•´ í•˜ë‚˜ë§Œ
                    elif not condition_value and is_negative:
                        matched.append({
                            'condition_type': condition_type,
                            'condition_value': condition_value,
                            'q_text': qa.get('q_text', ''),
                            'answer': answer,
                            'confidence': 1.0
                        })
                        break

                # String ì¡°ê±´ (alcohol_preference)
                else:
                    # ë‹µë³€ì— ì¡°ê±´ê°’ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ë§¤ì¹­
                    if str(condition_value).lower() in answer.lower():
                        matched.append({
                            'condition_type': condition_type,
                            'condition_value': condition_value,
                            'q_text': qa.get('q_text', ''),
                            'answer': answer,
                            'confidence': 0.8
                        })
                        break

    return matched


def reorder_with_matches(full_list: List[Dict[str, Any]], matched: List[Dict[str, Any]], limit: int) -> List[Dict[str, Any]]:
    if not isinstance(full_list, list):
        return []

    if not matched:
        return full_list[:limit]

    def _key(qa: Dict[str, Any]) -> tuple:
        return (
            qa.get("q_text") or qa.get("question") or "",
            str(qa.get("answer") or qa.get("answer_text") or qa.get("value") or "")
        )

    seen = set()
    ordered: List[Dict[str, Any]] = []

    for qa in matched:
        key = _key(qa)
        if key not in seen:
            ordered.append(qa)
            seen.add(key)

    for qa in full_list:
        key = _key(qa)
        if key not in seen:
            ordered.append(qa)
            seen.add(key)

    return ordered[:limit]


class SearchRequest(BaseModel):
    """ê²€ìƒ‰ ìš”ì²­"""
    query: str = Field(..., description="ê²€ìƒ‰ ì¿¼ë¦¬")
    index_name: str = Field(
        default="survey_responses_merged",
        description="ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ì´ë¦„ (ê¸°ë³¸ê°’: survey_responses_merged; ì™€ì¼ë“œì¹´ë“œ ì‚¬ìš© ê°€ëŠ¥)"
    )
    size: int = Field(default=10, ge=1, le=100, description="ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜")
    use_vector_search: bool = Field(default=True, description="ë²¡í„° ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€")
    page: int = Field(default=1, ge=1, description="ìš”ì²­í•  í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)")
    use_claude_analyzer: Optional[bool] = Field(
        default=None,
        description="Claude ë¶„ì„ê¸° ì‚¬ìš© ì—¬ë¶€ (Noneì´ë©´ ì„œë²„ ì„¤ì •ê°’ì„ ë”°ë¦„)"
    )
    session_id: Optional[str] = Field(
        default=None,
        description="ëŒ€í™”/ì„¸ì…˜ ì‹ë³„ì (Redis ëŒ€í™” ë¡œê·¸ í‚¤)"
    )
    user_id: Optional[str] = Field(
        default=None,
        description="ìš”ì²­ ì‚¬ìš©ì ì‹ë³„ì (ê²€ìƒ‰ ì´ë ¥ í‚¤)"
    )
    request_id: Optional[str] = Field(
        default=None,
        description="ìš”ì²­ ì¶”ì ì„ ìœ„í•œ ID"
    )
    metadata: Optional[Dict[str, Any]] = Field(
        default=None,
        description="ì¶”ê°€ ìš”ì²­ ë©”íƒ€ë°ì´í„°"
    )
    log_conversation: bool = Field(
        default=True,
        description="Redis ëŒ€í™” ë¡œê·¸ ì €ì¥ ì—¬ë¶€"
    )
    log_search_history: bool = Field(
        default=True,
        description="Redis ê²€ìƒ‰ ì´ë ¥ ì €ì¥ ì—¬ë¶€"
    )
    request_llm_summary: bool = Field(
        default=False,
        description="LLM ìš”ì•½/ë¶„ì„ ìƒì„± ìš”ì²­ ì—¬ë¶€"
    )
    llm_summary_instructions: Optional[str] = Field(
        default=None,
        description="LLM ìš”ì•½ ì‹œ ì‚¬ìš©í•  ì¶”ê°€ ì§€ì¹¨"
    )


class SearchResult(BaseModel):
    """ê²€ìƒ‰ ê²°ê³¼ í•­ëª©"""
    user_id: str
    score: float
    timestamp: Optional[str] = None
    demographic_info: Optional[Dict[str, Any]] = Field(default=None, description="ì¸êµ¬í†µê³„ ì •ë³´ (survey_responses_mergedì—ì„œ ì¡°íšŒ)")
    behavioral_info: Optional[Dict[str, Any]] = Field(default=None, description="í–‰ë™/ìŠµê´€ ì •ë³´ (ì˜ˆ: í¡ì—° ì—¬ë¶€, ì°¨ëŸ‰ ë³´ìœ  ì—¬ë¶€)")
    qa_pairs: Optional[List[Dict[str, Any]]] = None
    matched_qa_pairs: Optional[List[Dict[str, Any]]] = None
    highlights: Optional[Dict[str, Any]] = None


class SearchResponse(BaseModel):
    """ê²€ìƒ‰ ì‘ë‹µ"""
    query: str
    total_hits: int
    max_score: Optional[float]
    results: List[SearchResult]
    query_analysis: Optional[Dict[str, Any]] = None
    took_ms: int
    page: int = Field(default=1, description="í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸")
    page_size: int = Field(default=10, description="í˜ì´ì§€ ë‹¹ ê²°ê³¼ ìˆ˜")
    has_more: bool = Field(default=False, description="ì¶”ê°€ í˜ì´ì§€ ì¡´ì¬ ì—¬ë¶€")
    llm_summary: Optional[Dict[str, Any]] = Field(
        default=None,
        description="LLM ê¸°ë°˜ ë°ì´í„° ìš”ì•½/ë¶„ì„ ê²°ê³¼"
    )


# ===== ê°„ì†Œí™”ëœ ì‘ë‹µ ëª¨ë¸ (í”„ë¡ íŠ¸ì—”ë“œ ì¹œí™”ì ) =====

class MatchedCondition(BaseModel):
    """Behavioral ì¡°ê±´ ë§¤ì¹­ ì •ë³´"""
    condition_type: str = Field(..., description="ì¡°ê±´ íƒ€ì… (smoker, has_vehicle, alcohol_preference ë“±)")
    condition_value: Any = Field(..., description="ì¡°ê±´ ê°’ (True, False, 'ë§¥ì£¼' ë“±)")
    question: str = Field(..., description="ì‹¤ì œ ì§ˆë¬¸ í…ìŠ¤íŠ¸")
    answer: str = Field(..., description="ì‹¤ì œ ë‹µë³€ í…ìŠ¤íŠ¸")
    confidence: float = Field(default=1.0, description="ë§¤ì¹­ ì‹ ë¢°ë„ (0.0~1.0)")


class SimpleResult(BaseModel):
    """ê°„ì†Œí™”ëœ ê²€ìƒ‰ ê²°ê³¼ (í”„ë¡ íŠ¸ì—”ë“œìš©)"""
    user_id: str = Field(..., description="ì‚¬ìš©ì ID")
    score: float = Field(..., description="ê²€ìƒ‰ ì ìˆ˜")
    demographics: Dict[str, str] = Field(..., description="ì¸êµ¬í†µê³„ ì •ë³´ (gender, age_group, birth_year)")
    matched_conditions: List[MatchedCondition] = Field(
        default_factory=list,
        description="ë§¤ì¹­ëœ behavioral ì¡°ê±´ë“¤"
    )


class SimpleResponse(BaseModel):
    """í”„ë¡ íŠ¸ì—”ë“œ ì¹œí™”ì  ê°„ì†Œí™” ì‘ë‹µ"""
    state: Literal["SUCCESS", "ERROR"] = Field(..., description="ì‘ë‹µ ìƒíƒœ")
    message: str = Field(..., description="ì‘ë‹µ ë©”ì‹œì§€")
    query: str = Field(..., description="ê²€ìƒ‰ ì¿¼ë¦¬")
    total_hits: int = Field(..., description="ì´ ê²°ê³¼ ìˆ˜")
    results: List[SimpleResult] = Field(..., description="ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡")
    query_info: Optional[Dict[str, Any]] = Field(
        default=None,
        description="ì¿¼ë¦¬ ë¶„ì„ ì •ë³´ (keywords, filters_applied, behavioral_conditions)"
    )
    took_ms: int = Field(..., description="ê²€ìƒ‰ ì†Œìš” ì‹œê°„ (ë°€ë¦¬ì´ˆ)")


BEHAVIOR_YES_TOKENS = {
    "ìˆë‹¤", "ìˆìŒ", "ìˆì–´ìš”", "yes", "y", "ë³´ìœ ", "ë³´ìœ í•¨", "ë³´ìœ ì¤‘", "í•œë‹¤", "í•©ë‹ˆë‹¤", "í•´ìš”"
}
BEHAVIOR_NO_TOKENS = {
    "ì—†ë‹¤", "ì—†ìŒ", "ì—†ì–´ìš”", "no", "n", "ë¯¸ë³´ìœ ", "ì•ˆí•¨", "ì•ˆí•´ìš”", "í•˜ì§€ì•ŠëŠ”ë‹¤", "í•˜ì§€ ì•ŠëŠ”ë‹¤", "ì•ŠìŒ", "ì•ˆí•©ë‹ˆë‹¤"
}
SMOKER_NEGATIVE_KEYWORDS = {
    "í”¼ì›Œë³¸ ì ì´ ì—†ë‹¤", "í”¼ì›Œë³¸ì ì´ ì—†ë‹¤", "í”¼ì›Œë³¸ì  ì—†ë‹¤", "í”¼ìš°ì§€ ì•ŠëŠ”ë‹¤",
    "í¡ì—°í•˜ì§€ ì•ŠëŠ”ë‹¤", "ë¹„í¡ì—°", "ê¸ˆì—°", "ë‹´ë°°ë¥¼ í”¼ìš°ì§€ ì•ŠëŠ”ë‹¤", "ë‹´ë°°ë¥¼ í”¼ì›Œë³¸ì ì´ ì—†ë‹¤",
    "ë‹´ë°° ì•ˆ í”¼", "ë‹´ë°°ì•ˆí”¼", "í¡ì—° ì•ˆ í•¨", "í¡ì—° ì•ˆí•¨", "ë‹´ë°°ë¥¼ í”¼ìš°ì§€ ì•ŠìŒ", "í”¼ìš°ì§€ ì•ŠìŒ"
}
SMOKER_POSITIVE_KEYWORDS = {
    "í¡ì—°", "ë‹´ë°° í”¼", "ë‹´ë°°í”¼", "ë‹´ë°°ë¥¼ í”¼", "í¡ì—°ì¤‘", "í¡ì—°í•¨", "smoker",
    "í”¼ìš´ë‹¤", "í”¼ì›ë‹ˆë‹¤", "í”¼ì›€", "ì¼ë°˜ ë‹´ë°°", "ì¼ë°˜ë‹´ë°°", "ì „ì ë‹´ë°°",
    "ì „ìë‹´ë°°", "ê¶ë ¨í˜• ì „ìë‹´ë°°", "ê¶ë ¨í˜•ì „ìë‹´ë°°", "ê¶Œë ¨í˜• ì „ìë‹´ë°°",
    "ê¶Œë ¨í˜•ì „ìë‹´ë°°", "ì—°ì´ˆ", "ì‹œê°€í˜• ì „ìë‹´ë°°", "ë‹´ë°°", "ë‹´ë°°ë¥¼ í”¼ì›€",
    "í¡ì—° ê²½í—˜ ìˆìŒ", "í¡ì—°ê²½í—˜ ìˆìŒ"
}
SMOKER_QUESTION_KEYWORDS = {
    "í¡ì—°", "ë‹´ë°°", "í¡ì—°ê²½í—˜", "í¡ì—° ê²½í—˜", "í¡ì—°ê²½í—˜ ë‹´ë°°ë¸Œëœë“œ",
    "ê¶ë ¨í˜• ì „ìë‹´ë°°", "ê¶ë ¨í˜• ì „ìë‹´ë°°/ê°€ì—´ì‹ ì „ìë‹´ë°° ì´ìš©ê²½í—˜",
    "ê°€ì—´ì‹ ì „ìë‹´ë°°", "ì „ìë‹´ë°°"
}
VEHICLE_QUESTION_KEYWORDS = {
    "ë³´ìœ ì°¨ëŸ‰ì—¬ë¶€", "ë³´ìœ ì°¨ëŸ‰", "ì°¨ëŸ‰ì—¬ë¶€", "ì°¨ëŸ‰ ì—¬ë¶€", "ìë™ì°¨", "ì°¨ëŸ‰", "ì°¨ ë³´ìœ ", "ì°¨ëŸ‰ë³´ìœ "  # âœ… "ë³´ìœ ì°¨ëŸ‰ì—¬ë¶€" ì¶”ê°€
}

# ìŒì£¼ ê´€ë ¨ í‚¤ì›Œë“œ
ALCOHOL_QUESTION_KEYWORDS = {
    "ìŒìš©ê²½í—˜ ìˆ ", "ìŒìš©ê²½í—˜", "ìˆ ", "ìŒì£¼", "ìŒì£¼ê²½í—˜", "ì•Œì½œ", "ì•Œì½”ì˜¬"
}
BEER_KEYWORDS = {
    "ë§¥ì£¼", "beer"
}
WINE_KEYWORDS = {
    "ì™€ì¸", "wine"
}
SOJU_KEYWORDS = {
    "ì†Œì£¼", "soju"
}
NON_DRINKER_KEYWORDS = {
    "ìˆ ì„ ë§ˆì‹œì§€ ì•ŠìŒ", "ìˆ  ë§ˆì‹œì§€ ì•ŠìŒ", "ìˆ  ì•ˆ ë§ˆì‹¬", "ìˆ  ì•ˆë§ˆì‹¬", "ìˆ  ëª»ë§ˆì‹¬", "ìˆ  ëª» ë§ˆì‹¬",
    "ë¹„ìŒì£¼", "ê¸ˆì£¼", "ìµœê·¼ 1ë…„ ì´ë‚´ ìˆ ì„ ë§ˆì‹œì§€ ì•ŠìŒ", "ìŒì£¼ ê²½í—˜ ì—†ìŒ", "ìŒì£¼ê²½í—˜ ì—†ìŒ"
}
DRINKER_POSITIVE_KEYWORDS = {
    # ìˆ  ì¢…ë¥˜
    "ë§¥ì£¼", "beer", "ì†Œì£¼", "soju", "ë§‰ê±¸ë¦¬", "íƒì£¼", "ì™€ì¸", "wine",
    "ì–‘ì£¼", "ìœ„ìŠ¤í‚¤", "whiskey", "ë³´ë“œì¹´", "vodka", "ë°í‚¬ë¼", "tequila", "ì§„", "gin",
    "ì €ë„ì£¼", "ì²­ì£¼", "ë§¤ì‹¤ì£¼", "ë³µë¶„ìì£¼", "ê³¼ì¼ì¹µí…Œì¼ì£¼", "KGB", "í›„ì¹˜", "í¬ë£¨ì €",
    "ì¼ë³¸ì²­ì£¼", "ì‚¬ì¼€", "sake", "ì¹µí…Œì¼", "cocktail",
    # ìŒì£¼ ê¸ì • í‘œí˜„
    "ìˆ  ë§ˆì‹¬", "ìˆ  ë§ˆì…”", "ìˆ ë§ˆì‹¬", "ìˆ ë§ˆì…”", "ìŒì£¼í•¨", "ìŒì£¼ ê²½í—˜ ìˆìŒ", "ìŒì£¼ê²½í—˜ ìˆìŒ",
    "ê°€ë” ë§ˆì‹¬", "ìì£¼ ë§ˆì‹¬", "ì£¼ë§ì— ë§ˆì‹¬"
}


def extract_behavioral_conditions_from_query(query: str) -> Dict[str, bool]:
    """ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì—ì„œ behavioral ì¡°ê±´ ìë™ ì¶”ì¶œ

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬

    Returns:
        behavioral ì¡°ê±´ ë”•ì…”ë„ˆë¦¬ {"drinker": True, "smoker": False, ...}
    """
    query_lower = query.lower()
    query_normalized = query_lower.replace(" ", "")
    conditions = {}

    # â­ ìŒì£¼ ì—¬ë¶€ ê°ì§€
    drinker_positive = ["ìˆ ë§ˆ", "ìˆ ë„ë§ˆ", "ìŒì£¼", "ìˆ ë¨¹", "ìˆ ë§ˆì‹ ", "ìŒì£¼ê²½í—˜", "ìŒì£¼ ê²½í—˜", "ì£¼ë¥˜"]
    drinker_negative = ["ë¹„ìŒì£¼", "ê¸ˆì£¼", "ìˆ ì•ˆ", "ìˆ ì„ë§ˆì‹œì§€", "ìˆ ì„ì•ˆë§ˆì‹œ", "ìˆ ë„ì•ˆ"]

    has_drinker_negative = any(keyword in query_normalized for keyword in drinker_negative)
    has_drinker_positive = any(keyword in query_normalized for keyword in drinker_positive)

    if has_drinker_negative:
        conditions["drinker"] = False
    elif has_drinker_positive:
        conditions["drinker"] = True

    # â­ í¡ì—° ì—¬ë¶€ ê°ì§€
    smoker_positive = ["í¡ì—°ì", "ë‹´ë°°í”¼", "ë‹´ë°° í”¼", "ë‹´ë°°ë¥¼í”¼"]
    smoker_negative = ["ë¹„í¡ì—°", "ê¸ˆì—°", "ë‹´ë°°ì•ˆ", "ë‹´ë°°ë„ì•ˆ", "í¡ì—°ì•ˆ", "í¡ì—°ì„ì•ˆ", "ì•ˆí”¼ëŠ”", "ì•ˆ í”¼ëŠ”"]

    has_smoker_negative = any(keyword in query_normalized for keyword in smoker_negative)
    has_smoker_positive = any(keyword in query_normalized for keyword in smoker_positive)

    if has_smoker_negative:
        conditions["smoker"] = False
    elif has_smoker_positive:
        conditions["smoker"] = True

    # â­ ì°¨ëŸ‰ ë³´ìœ  ì—¬ë¶€ ê°ì§€
    vehicle_positive = ["ì°¨ëŸ‰", "ìë™ì°¨", "ì°¨ë³´ìœ ", "ì°¨ìˆëŠ”"]
    vehicle_negative = ["ì°¨ì—†ëŠ”", "ì°¨ëŸ‰ì—†ëŠ”", "ì°¨ê°€ì—†ëŠ”"]

    has_vehicle_negative = any(keyword in query_normalized for keyword in vehicle_negative)
    has_vehicle_positive = any(keyword in query_normalized for keyword in vehicle_positive)

    if has_vehicle_negative:
        conditions["has_vehicle"] = False
    elif has_vehicle_positive:
        conditions["has_vehicle"] = True

    return conditions


def build_behavioral_filters(behavioral_conditions: Dict[str, bool]) -> List[Dict[str, Any]]:
    """behavioral_conditionsë¥¼ OpenSearch nested í•„í„°ë¡œ ë³€í™˜

    Args:
        behavioral_conditions: {"smoker": True, "has_vehicle": False, ...}

    Returns:
        OpenSearch nested ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸

    Example:
        {"smoker": True} â†’
        {
            "nested": {
                "path": "qa_pairs",
                "query": {
                    "bool": {
                        "must": [
                            {"bool": {"should": [ì§ˆë¬¸ ë§¤ì¹­]}},
                            {"bool": {"should": [ê¸ì • ë‹µë³€], "must_not": [ë¶€ì • ë‹µë³€]}}
                        ]
                    }
                }
            }
        }
    """
    filters = []

    for key, value in behavioral_conditions.items():
        if value is None:
            continue

        if key == "smoker":
            # í¡ì—° í•„í„°
            question_should = [
                {"match": {"qa_pairs.q_text": q}}
                for q in SMOKER_QUESTION_KEYWORDS
            ]

            if value:  # í¡ì—°ì
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in SMOKER_POSITIVE_KEYWORDS
                ]
                answer_must_not = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in SMOKER_NEGATIVE_KEYWORDS
                ]
            else:  # ë¹„í¡ì—°ì
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in SMOKER_NEGATIVE_KEYWORDS
                ]
                answer_must_not = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in SMOKER_POSITIVE_KEYWORDS
                ]

            filters.append({
                "nested": {
                    "path": "qa_pairs",
                    "query": {
                        "bool": {
                            "must": [
                                {
                                    "bool": {
                                        "should": question_should,
                                        "minimum_should_match": 1
                                    }
                                },
                                {
                                    "bool": {
                                        "should": answer_should,
                                        "must_not": answer_must_not,
                                        "minimum_should_match": 1
                                    }
                                }
                            ]
                        }
                    }
                }
            })

        elif key == "has_vehicle":
            # ì°¨ëŸ‰ ë³´ìœ  í•„í„°
            question_should = [
                {"match": {"qa_pairs.q_text": q}}
                for q in VEHICLE_QUESTION_KEYWORDS
            ]

            if value:  # ì°¨ëŸ‰ ìˆìŒ
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in BEHAVIOR_YES_TOKENS
                ]
                answer_must_not = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in BEHAVIOR_NO_TOKENS
                ]
            else:  # ì°¨ëŸ‰ ì—†ìŒ
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in BEHAVIOR_NO_TOKENS
                ]
                answer_must_not = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in BEHAVIOR_YES_TOKENS
                ]

            filters.append({
                "nested": {
                    "path": "qa_pairs",
                    "query": {
                        "bool": {
                            "must": [
                                {
                                    "bool": {
                                        "should": question_should,
                                        "minimum_should_match": 1
                                    }
                                },
                                {
                                    "bool": {
                                        "should": answer_should,
                                        "must_not": answer_must_not,
                                        "minimum_should_match": 1
                                    }
                                }
                            ]
                        }
                    }
                }
            })

        elif key == "drinker":
            # â­ ìŒì£¼ ì—¬ë¶€ í•„í„°
            question_should = [
                {"match": {"qa_pairs.q_text": q}}
                for q in ALCOHOL_QUESTION_KEYWORDS
            ]

            if value:  # ìŒì£¼ì
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}
                    for kw in DRINKER_POSITIVE_KEYWORDS
                ]
                answer_must_not = [
                    {"match": {"qa_pairs.answer": kw}}
                    for kw in NON_DRINKER_KEYWORDS
                ]
            else:  # ë¹„ìŒì£¼ì
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}
                    for kw in NON_DRINKER_KEYWORDS
                ]
                answer_must_not = [
                    {"match": {"qa_pairs.answer": kw}}
                    for kw in DRINKER_POSITIVE_KEYWORDS
                ]

            filters.append({
                "nested": {
                    "path": "qa_pairs",
                    "query": {
                        "bool": {
                            "must": [
                                {
                                    "bool": {
                                        "should": question_should,
                                        "minimum_should_match": 1
                                    }
                                },
                                {
                                    "bool": {
                                        "should": answer_should,
                                        "must_not": answer_must_not,
                                        "minimum_should_match": 1
                                    }
                                }
                            ]
                        }
                    }
                }
            })

        elif key == "drinks_beer":
            # ë§¥ì£¼ ìŒìš© í•„í„°
            if value:
                question_should = [
                    {"match": {"qa_pairs.q_text": q}}
                    for q in ALCOHOL_QUESTION_KEYWORDS
                ]
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in BEER_KEYWORDS
                ]

                filters.append({
                    "nested": {
                        "path": "qa_pairs",
                        "query": {
                            "bool": {
                                "must": [
                                    {"bool": {"should": question_should, "minimum_should_match": 1}},
                                    {"bool": {"should": answer_should, "minimum_should_match": 1}}
                                ]
                            }
                        }
                    }
                })

        elif key == "drinks_wine":
            # ì™€ì¸ ìŒìš© í•„í„°
            if value:
                question_should = [
                    {"match": {"qa_pairs.q_text": q}}
                    for q in ALCOHOL_QUESTION_KEYWORDS
                ]
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in WINE_KEYWORDS
                ]

                filters.append({
                    "nested": {
                        "path": "qa_pairs",
                        "query": {
                            "bool": {
                                "must": [
                                    {"bool": {"should": question_should, "minimum_should_match": 1}},
                                    {"bool": {"should": answer_should, "minimum_should_match": 1}}
                                ]
                            }
                        }
                    }
                })

        elif key == "drinks_soju":
            # ì†Œì£¼ ìŒìš© í•„í„°
            if value:
                question_should = [
                    {"match": {"qa_pairs.q_text": q}}
                    for q in ALCOHOL_QUESTION_KEYWORDS
                ]
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in SOJU_KEYWORDS
                ]

                filters.append({
                    "nested": {
                        "path": "qa_pairs",
                        "query": {
                            "bool": {
                                "must": [
                                    {"bool": {"should": question_should, "minimum_should_match": 1}},
                                    {"bool": {"should": answer_should, "minimum_should_match": 1}}
                                ]
                            }
                        }
                    }
                })

        elif key == "non_drinker":
            # ë¹„ìŒì£¼ì í•„í„°
            if value:
                question_should = [
                    {"match": {"qa_pairs.q_text": q}}
                    for q in ALCOHOL_QUESTION_KEYWORDS
                ]
                answer_should = [
                    {"match": {"qa_pairs.answer": kw}}  # âœ… Changed to match (answer is text type)
                    for kw in NON_DRINKER_KEYWORDS
                ]

                filters.append({
                    "nested": {
                        "path": "qa_pairs",
                        "query": {
                            "bool": {
                                "must": [
                                    {"bool": {"should": question_should, "minimum_should_match": 1}},
                                    {"bool": {"should": answer_should, "minimum_should_match": 1}}
                                ]
                            }
                        }
                    }
                })

    return filters


@router.get("/", summary="Search API ìƒíƒœ")
def search_root():
    """Search API ê¸°ë³¸ ì •ë³´"""
    return {
        "message": "Search API ì‹¤í–‰ ì¤‘",
        "version": "1.0",
        "endpoints": [
            "/search/nl"
        ]
    }


class NLSearchRequest(BaseModel):
    """ìì—°ì–´ ê¸°ë°˜ ê²€ìƒ‰ ìš”ì²­ (í•„í„°/size ìë™ ì¶”ì¶œ)"""
    query: str = Field(..., description="ìì—°ì–´ ì¿¼ë¦¬ (ì˜ˆ: '30ëŒ€ ì‚¬ë¬´ì§ 300ëª… ë°ì´í„° ë³´ì—¬ì¤˜')")
    index_name: str = Field(
        default="survey_responses_merged",
        description="ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ì´ë¦„ (ê¸°ë³¸ê°’: survey_responses_merged; ì™€ì¼ë“œì¹´ë“œ ì‚¬ìš© ê°€ëŠ¥)"
    )
    use_vector_search: bool = Field(default=True, description="ë²¡í„° ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€")
    page: int = Field(default=1, ge=1, description="ìš”ì²­í•  í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)")
    use_claude_analyzer: Optional[bool] = Field(
        default=None,
        description="Claude ë¶„ì„ê¸° ì‚¬ìš© ì—¬ë¶€ (Noneì´ë©´ ì„œë²„ ì„¤ì •ê°’ì„ ë”°ë¦„)"
    )
    session_id: Optional[str] = Field(
        default=None,
        description="ëŒ€í™”/ì„¸ì…˜ ì‹ë³„ì (Redis ëŒ€í™” ë¡œê·¸ í‚¤)"
    )
    user_id: Optional[str] = Field(
        default=None,
        description="ìš”ì²­ ì‚¬ìš©ì ì‹ë³„ì (ê²€ìƒ‰ ì´ë ¥ í‚¤)"
    )
    request_id: Optional[str] = Field(
        default=None,
        description="ìš”ì²­ ì¶”ì ì„ ìœ„í•œ ID"
    )
    metadata: Optional[Dict[str, Any]] = Field(
        default=None,
        description="ì¶”ê°€ ìš”ì²­ ë©”íƒ€ë°ì´í„°"
    )
    log_conversation: bool = Field(
        default=True,
        description="Redis ëŒ€í™” ë¡œê·¸ ì €ì¥ ì—¬ë¶€"
    )
    log_search_history: bool = Field(
        default=True,
        description="Redis ê²€ìƒ‰ ì´ë ¥ ì €ì¥ ì—¬ë¶€"
    )
    request_llm_summary: bool = Field(
        default=False,
        description="LLM ìš”ì•½/ë¶„ì„ ìƒì„± ìš”ì²­ ì—¬ë¶€"
    )
    llm_summary_instructions: Optional[str] = Field(
        default=None,
        description="LLM ìš”ì•½ ì‹œ ì‚¬ìš©í•  ì¶”ê°€ ì§€ì¹¨"
    )


def convert_to_simple_response(
    search_response: SearchResponse,
    behavioral_conditions: Dict[str, Any],
    max_results: int = 100
) -> SimpleResponse:
    """
    SearchResponseë¥¼ SimpleResponseë¡œ ë³€í™˜ (í”„ë¡ íŠ¸ì—”ë“œ ì¹œí™”ì )

    Args:
        search_response: ê¸°ì¡´ ê²€ìƒ‰ ì‘ë‹µ
        behavioral_conditions: ì¿¼ë¦¬ì˜ behavioral ì¡°ê±´ {"smoker": True, "has_vehicle": True}
        max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ 100ê°œ)

    Returns:
        SimpleResponse: ê°„ì†Œí™”ëœ ì‘ë‹µ
    """
    try:
        simple_results = []

        for item in search_response.results[:max_results]:
            # Behavioral ì¡°ê±´ ë§¤ì¹­ QA ì¶”ì¶œ
            matched_conditions_data = extract_behavioral_qa_pairs(
                source={'qa_pairs': item.qa_pairs or []},
                behavioral_conditions=behavioral_conditions
            )

            # MatchedCondition ê°ì²´ë¡œ ë³€í™˜
            matched_objs = [
                MatchedCondition(
                    condition_type=mc['condition_type'],
                    condition_value=mc['condition_value'],
                    question=mc['q_text'],
                    answer=mc['answer'],
                    confidence=mc.get('confidence', 1.0)
                )
                for mc in matched_conditions_data
            ]

            # Demographics ì •ë³´ ì¶”ì¶œ
            demo_info = item.demographic_info or {}
            demographics = {
                'gender': demo_info.get('gender', 'N/A'),
                'age_group': demo_info.get('age_group', 'N/A'),
                'birth_year': str(demo_info.get('birth_year', 'N/A'))
            }

            simple_results.append(SimpleResult(
                user_id=item.user_id,
                score=item.score,
                demographics=demographics,
                matched_conditions=matched_objs
            ))

        # Query ë¶„ì„ ì •ë³´
        query_analysis = search_response.query_analysis or {}
        query_info = {
            'keywords': [
                *(query_analysis.get('must_terms', [])),
                *(query_analysis.get('should_terms', []))
            ],
            'filters_applied': bool(query_analysis.get('filters')),
            'behavioral_conditions': behavioral_conditions,
            'extracted_entities': query_analysis.get('extracted_entities')
        }

        return SimpleResponse(
            state="SUCCESS",
            message="ê²€ìƒ‰ ì„±ê³µ",
            query=search_response.query,
            total_hits=search_response.total_hits,
            results=simple_results,
            query_info=query_info,
            took_ms=search_response.took_ms
        )

    except Exception as e:
        logger.error(f"SimpleResponse ë³€í™˜ ì¤‘ ì—ëŸ¬: {e}", exc_info=True)
        # ì—ëŸ¬ ì‹œ ë¹ˆ ì‘ë‹µ ë°˜í™˜
        return SimpleResponse(
            state="ERROR",
            message=f"ì‘ë‹µ ë³€í™˜ ì‹¤íŒ¨: {str(e)}",
            query=search_response.query if search_response else "",
            total_hits=0,
            results=[],
            query_info=None,
            took_ms=0
        )


@router.post("/nl", response_model=SearchResponse, summary="ìì—°ì–´ ì¿¼ë¦¬: ìë™ ì¶”ì¶œ+ê²€ìƒ‰")
async def search_natural_language(
    request: NLSearchRequest,
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """
    ìì—°ì–´ ì…ë ¥ì—ì„œ ì¸êµ¬í†µê³„(ì—°ë ¹/ì„±ë³„/ì§ì—…)ì™€ ìš”ì²­ ìˆ˜ëŸ‰ì„ ì¶”ì¶œí•˜ì—¬
    ê²€ìƒ‰ ì¿¼ë¦¬ì™€ sizeì— ë°˜ì˜í•œ ë’¤ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        logger.info("ğŸŸ¢ /search/nl ìš”ì²­ ì‹œì‘")

        if not os_client or not os_client.ping():
            raise HTTPException(status_code=503, detail="OpenSearch ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        _ensure_request_defaults(request)

        config = getattr(router, 'config', None)
        if config is None:
            from rag_query_analyzer.config import get_config
            config = get_config()
            router.config = config

        analyzer = getattr(router, 'analyzer', None)
        if analyzer is None:
            analyzer = AdvancedRAGQueryAnalyzer(config)
            router.analyzer = analyzer
        use_claude = request.use_claude_analyzer if request.use_claude_analyzer is not None else config.ENABLE_CLAUDE_ANALYZER
        analysis = analyzer.analyze_query(request.query, use_claude=use_claude)
        if analysis is None:
            raise RuntimeError("Query analysis returned None")
        query_analysis = analysis

        # â­ ìë™ìœ¼ë¡œ ì¿¼ë¦¬ì—ì„œ behavioral ì¡°ê±´ ì¶”ì¶œ ë° ë³‘í•©
        auto_behavioral = extract_behavioral_conditions_from_query(request.query)
        if auto_behavioral:
            # ê¸°ì¡´ behavioral_conditionsì™€ ë³‘í•© (ìë™ ì¶”ì¶œì´ ìš°ì„ )
            if not analysis.behavioral_conditions:
                analysis.behavioral_conditions = {}
            for key, value in auto_behavioral.items():
                if key not in analysis.behavioral_conditions:
                    analysis.behavioral_conditions[key] = value
            logger.info(f"âœ… ìë™ ì¶”ì¶œëœ behavioral ì¡°ê±´: {auto_behavioral}")

        embedding_model = getattr(router, 'embedding_model', None)
        if embedding_model is None and hasattr(router, 'embedding_model_factory'):
            embedding_model = router.embedding_model_factory()
            router.embedding_model = embedding_model
        data_fetcher = DataFetcher(
            opensearch_client=os_client,
            qdrant_client=getattr(router, 'qdrant_client', None),
            async_opensearch_client=getattr(router, 'async_os_client', None)
        )

        timings: Dict[str, float] = {}
        overall_start = perf_counter()

        # 1) ì¶”ì¶œ: filters + size
        extractor = DemographicExtractor()
        extracted_entities, requested_size = extractor.extract_with_size(request.query)
        filters: List[Dict[str, Any]] = []

        # Demographics í•„í„°
        for demo in extracted_entities.demographics:
            # â­ ì¸ë±ìŠ¤ë³„ í•„í„° ì „ëµ:
            # - survey_responses_merged: metadataì™€ qa_pairs ëª¨ë‘ ìˆìŒ
            # - survey_responses_merged: ì¼ë¶€ëŠ” metadata, ì¼ë¶€ëŠ” qa_pairsì—ë§Œ ìˆìŒ â†’ qa_fallback í™œì„±í™”
            # ì˜ˆ: regionì€ metadataì—, marital_statusëŠ” qa_pairsì—ë§Œ ìˆì„ ìˆ˜ ìˆìŒ
            is_survey_merged = request.index_name in ["survey_responses_merged", "s_survey*"]
            metadata_only = not is_survey_merged  # survey_responses_mergedëŠ” False
            include_nested_fallback = is_survey_merged  # survey_responses_mergedëŠ” True (qa_pairsë„ ê²€ìƒ‰)
            filter_clause = demo.to_opensearch_filter(
                metadata_only=metadata_only,
                include_qa_fallback=include_nested_fallback,
            )
            if filter_clause and filter_clause != {"match_all": {}}:
                filters.append(filter_clause)

        # â­ ê°œì„ : behavioral_conditionsë¥¼ OpenSearch í•„í„°ë¡œ ë³€í™˜
        if analysis.behavioral_conditions:
            behavioral_filters = build_behavioral_filters(analysis.behavioral_conditions)
            filters.extend(behavioral_filters)
            logger.info(f"âœ… Behavioral í•„í„° ì¶”ê°€: {analysis.behavioral_conditions} â†’ {len(behavioral_filters)}ê°œ í•„í„°")

        # â­ inner_hits ì œê±° (ì¤‘ë³µ key ì—ëŸ¬ ë°©ì§€)
        def remove_inner_hits_from_filter(query_dict):
            """ì¬ê·€ì ìœ¼ë¡œ inner_hits ì œê±°"""
            import copy
            cleaned = copy.deepcopy(query_dict)

            if isinstance(cleaned, dict):
                if 'nested' in cleaned:
                    if 'inner_hits' in cleaned['nested']:
                        del cleaned['nested']['inner_hits']
                    if 'query' in cleaned['nested']:
                        cleaned['nested']['query'] = remove_inner_hits_from_filter(cleaned['nested']['query'])

                if 'bool' in cleaned:
                    for key in ['must', 'should', 'must_not', 'filter']:
                        if key in cleaned['bool']:
                            if isinstance(cleaned['bool'][key], list):
                                cleaned['bool'][key] = [remove_inner_hits_from_filter(item) for item in cleaned['bool'][key]]
                            else:
                                cleaned['bool'][key] = remove_inner_hits_from_filter(cleaned['bool'][key])

            return cleaned

        # filtersì—ì„œ inner_hits ì œê±°
        logger.info(f"ğŸ” í•„í„° ìƒíƒœ before inner_hits removal: filters={len(filters) if filters else 0}ê°œ")
        if filters:
            filters = [remove_inner_hits_from_filter(f) for f in filters]
            logger.info(f"âœ… inner_hits ì œê±° ì™„ë£Œ: {len(filters)}ê°œ í•„í„°")
        else:
            logger.warning(f"âš ï¸ filtersê°€ ë¹„ì–´ìˆìŒ!")

        filters_for_response = list(filters)
        filters_signature = _normalize_filters_for_cache(filters_for_response)

        # â­ page_size ì œí•œ ì™„í™”: 100 â†’ 5000 (ì „ì²´ ê²°ê³¼ í™•ì¸ ê°€ëŠ¥í•˜ë„ë¡)
        page_size = max(1, min(requested_size, 5000))
        page = max(1, request.page)
        requested_window = page_size * page
        cache_client = getattr(router, "redis_client", None)
        cache_ttl = getattr(router, "cache_ttl_seconds", 0)
        cache_limit = getattr(router, "cache_max_results", requested_window)
        cache_prefix = getattr(router, "cache_prefix", "search:results")
        cache_enabled = bool(cache_client) and cache_ttl > 0
        min_window_size = 2000
        window_size = max(requested_window, min_window_size)
        if cache_limit and cache_limit > 0:
            window_size = min(window_size, cache_limit)
        size = window_size
        cache_key = None
        cache_hit = False

        if cache_enabled:
            try:
                behavior_signature = ""
                if getattr(analysis, "behavioral_conditions", None):
                    try:
                        behavior_signature = json.dumps(analysis.behavioral_conditions, ensure_ascii=False, sort_keys=True)
                    except Exception:
                        behavior_signature = str(analysis.behavioral_conditions)

                cache_key = _make_cache_key(
                    prefix=cache_prefix,
                    query=request.query,
                    index_name=request.index_name,
                    page_size=page_size,
                    use_vector=request.use_vector_search,
                    use_claude=use_claude,
                    must_terms=analysis.must_terms or [],
                    should_terms=analysis.should_terms or [],
                    must_not_terms=getattr(analysis, "must_not_terms", []) or [],
                    filters_signature=filters_signature,
                    behavior_signature=behavior_signature,
                )
                cached_raw = cache_client.get(cache_key)
                if cached_raw:
                    cache_payload = json.loads(cached_raw)
                    cache_hit = True
                    logger.info(f"ğŸ” Redis ê²€ìƒ‰ ìºì‹œ íˆíŠ¸: key={cache_key}")
                    extracted_entities_dict = cache_payload.get("extracted_entities")
                    if extracted_entities_dict is None:
                        extracted_entities_dict = extracted_entities.to_dict()
                    cached_response = _build_cached_response(
                        payload=cache_payload,
                        request=request,
                        analysis=analysis,
                        filters_for_response=filters_for_response,
                        overall_start=overall_start,
                        extracted_entities_dict=extracted_entities_dict,
                    )
                    return _finalize_search_response(
                        request=request,
                        response=cached_response,
                        analysis=analysis,
                        cache_hit=True,
                        timings=cached_response.query_analysis.get("timings_ms") if cached_response.query_analysis else None,
                    )
            except Exception as cache_exc:
                logger.warning(f"âš ï¸ Redis ê²€ìƒ‰ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {cache_exc}")
                cache_key = None
                cache_enabled = False
        
        age_gender_filters = [f for f in filters if is_age_or_gender_filter(f)]
        occupation_filters = [f for f in filters if is_occupation_filter(f)]
        other_filters = [f for f in filters if f not in age_gender_filters and f not in occupation_filters]

        # â­ occupation_filtersë„ í¬í•¨ì‹œí‚¤ê¸° (ì´ì „ì—ëŠ” two-phase searchì—ë§Œ ì‚¬ìš©ë¨)
        filters_os = age_gender_filters + occupation_filters + other_filters
        filters = filters_os  # ìœ ì§€ë³´ìˆ˜: ê¸°ì¡´ ë¡œì§ê³¼ í˜¸í™˜ì„±ì„ ìœ„í•´
        has_demographic_filters = bool(filters_for_response)
        occupation_filter_handled = False

        logger.info("ğŸ” í•„í„° ìƒíƒœ ì²´í¬:")
        logger.info(f"  - age_gender_filters: {len(age_gender_filters)}ê°œ")
        logger.info(f"  - occupation_filters: {len(occupation_filters)}ê°œ")
        logger.info(f"  - other_filters: {len(other_filters)}ê°œ")
        logger.info(f"  - filters_os (í•©ê³„): {len(filters_os)}ê°œ")
        if occupation_filters:
            logger.info(f"  - occupation_filters ìƒ˜í”Œ: {json.dumps(occupation_filters[0] if occupation_filters else {}, ensure_ascii=False)[:500]}")

        two_phase_applicable = bool(age_gender_filters and occupation_filters)
        two_phase_response: Optional[SearchResponse] = None
        if two_phase_applicable:
            logger.info("âœ… 2ë‹¨ê³„ ê²€ìƒ‰ ì¡°ê±´ ì¶©ì¡± â€“ ë‘ ë‹¨ê³„ ê²€ìƒ‰ ì‹œë„")

            try:
                response = await run_two_phase_demographic_search(
                    request=request,
                    analysis=analysis,
                    extracted_entities=extracted_entities,
                    filters=filters_for_response,
                    size=size,
                    age_gender_filters=age_gender_filters,
                    occupation_filters=occupation_filters,
                    data_fetcher=data_fetcher,
                    timings=timings,
                    overall_start=overall_start,
                )

                if response is not None:
                    two_phase_response = response
                    logger.info("âœ… 2ë‹¨ê³„ ê²€ìƒ‰ ì„±ê³µ! ê²°ê³¼ ë°˜í™˜")
                    logger.info(f"ğŸ”µ /search/nl ìš”ì²­ ì™„ë£Œ: ê²°ê³¼ {len(response.results)}ê±´, took_ms={response.took_ms}")
            except Exception as e:
                logger.warning(f"âš ï¸ 2ë‹¨ê³„ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}, ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì§„í–‰")

        if two_phase_response is not None:
            return _finalize_search_response(
                request=request,
                response=two_phase_response,
                analysis=analysis,
                cache_hit=cache_hit,
                timings=two_phase_response.query_analysis.get("timings_ms") if two_phase_response.query_analysis else timings,
            )

        # 2) ì¿¼ë¦¬ ë¹Œë“œ
        # â­ í‚¤ì›Œë“œ ì •ì œëŠ” analyzerì—ì„œ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if analysis is None:
            raise RuntimeError("Query analysis not initialized")

        # ë¡œê¹…: ë¶„ì„ê¸°ì—ì„œ ì •ì œëœ ìµœì¢… í‚¤ì›Œë“œ í™•ì¸
        logger.info(f"ğŸ” [SearchAPI] ì¿¼ë¦¬ ë¶„ì„ ì™„ë£Œ:")
        logger.info(f"  âœ… Must terms: {analysis.must_terms}")
        logger.info(f"  âœ… Should terms: {analysis.should_terms}")
        logger.info(f"  âœ… Demographics: {[d.raw_value for d in extracted_entities.demographics]}")
        if hasattr(analysis, 'removed_demographic_terms') and analysis.removed_demographic_terms:
            logger.info(f"  â„¹ï¸ ì œê±°ëœ Demographics: {analysis.removed_demographic_terms}")
        if analysis.behavioral_conditions:
            logger.info(f"  âœ… Behavioral conditions: {analysis.behavioral_conditions}")

        query_builder = OpenSearchHybridQueryBuilder(config)
        query_vector = None
        if embedding_model:
            # ì™„ì „ ë™ì  ì„ë² ë”© ê¸°ë°˜ ë™ì˜ì–´ í™•ì¥ (ë„ë©”ì¸ ë¬´ê´€, ë²”ìš©)
            def _enrich_query_vector() -> Optional[list]:
                """ì„ì‹œ: ë™ì˜ì–´ í™•ì¥ ë¹„í™œì„±í™” (ì„±ëŠ¥ ìµœì í™”)"""
                try:
                    vec = embedding_model.encode(request.query).tolist()
                    logger.info("  âš ï¸ ë™ì˜ì–´ í™•ì¥ ë¹„í™œì„±í™” (ì„±ëŠ¥ ìµœì í™”)")
                    return vec
                except Exception:
                    return None

            query_vector = _enrich_query_vector()

        base_query = query_builder.build_query(
            analysis=analysis,
            query_vector=query_vector,
            size=size,
        )

        # ğŸ” Base Query ë¡œê¹…
        logger.info(f"ğŸ” [BASE QUERY] ìƒì„± ì™„ë£Œ")
        logger.info(json.dumps(base_query, ensure_ascii=False, indent=2))

        # 3) í•„í„° ì ìš© ì „ëµ: í•„í„°ëŠ” mustë¡œ, í‚¤ì›Œë“œëŠ” shouldë¡œ ì™„í™”
        # - í•„í„°(30ëŒ€, ì‚¬ë¬´ì§)ëŠ” ë°˜ë“œì‹œ ë§¤ì¹­ë˜ì–´ì•¼ í•¨
        # - í‚¤ì›Œë“œ ê²€ìƒ‰ì€ shouldë¡œ ì™„í™” (í•˜ë‚˜ë§Œ ë§¤ì¹­ë˜ì–´ë„ OK)
        final_query = base_query
        
        # â­ match_all/match_none/None ì œê±°: base_queryì—ì„œ match_all, match_none, Noneì´ ìˆìœ¼ë©´ ì œê±°
        existing_query = final_query.get('query', {"match_all": {}})
        if existing_query is None or existing_query == {"match_all": {}} or existing_query == {"match_none": {}}:
            # match_all/match_none/None ì œê±°
            removed_type = "None" if existing_query is None else ("match_all" if existing_query == {"match_all": {}} else "match_none")
            
            # â­ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í‚¤ì›Œë“œ ì¿¼ë¦¬ ìƒì„± (í•„í„°ë§Œ ìˆëŠ” ê²½ìš°ë¥¼ ìœ„í•´)
            if analysis.must_terms or analysis.should_terms:
                # í‚¤ì›Œë“œ ì¿¼ë¦¬ ì¬ìƒì„±
                keyword_queries = []
                if analysis.must_terms:
                    for term in analysis.must_terms:
                        keyword_queries.append({
                            "nested": {
                                "path": "qa_pairs",
                                "query": {"match": {"qa_pairs.answer_text": term}},
                                "score_mode": "max"
                            }
                        })
                
                if analysis.should_terms:
                    should_keywords = [{
                        "nested": {
                            "path": "qa_pairs",
                            "query": {"match": {"qa_pairs.answer_text": term}},
                            "score_mode": "max"
                        }
                    } for term in analysis.should_terms]
                    
                    if keyword_queries:
                        # mustì™€ should ëª¨ë‘ ìˆëŠ” ê²½ìš°
                        existing_query = {
                            "bool": {
                                "must": keyword_queries,
                                "should": should_keywords,
                                "minimum_should_match": 1
                            }
                        }
                    else:
                        # shouldë§Œ ìˆëŠ” ê²½ìš°
                        existing_query = {
                            "bool": {
                                "should": should_keywords,
                                "minimum_should_match": 1
                            }
                        }
                else:
                    # mustë§Œ ìˆëŠ” ê²½ìš°
                    if len(keyword_queries) == 1:
                        existing_query = keyword_queries[0]
                    else:
                        existing_query = {
                            "bool": {
                                "must": keyword_queries
                            }
                        }
                
                logger.info(f"âš ï¸ {removed_type} ì œê±°, í‚¤ì›Œë“œ ì¿¼ë¦¬ ì¬ìƒì„±: must={len(analysis.must_terms)}, should={len(analysis.should_terms)}")
            else:
                existing_query = None
                logger.info(f"âš ï¸ {removed_type} ì œê±°: í•„í„°ë§Œ ì‚¬ìš© (í‚¤ì›Œë“œ ì—†ìŒ)")
        
        # â­ inner_hits ì œê±° í•¨ìˆ˜ (ì¤‘ë³µ ë°©ì§€)
        def remove_inner_hits(query_dict):
            """ì¬ê·€ì ìœ¼ë¡œ inner_hits ì œê±° (í•„í„°ì—ì„œëŠ” ë§¤ì¹­ë§Œ í™•ì¸í•˜ë©´ ë˜ë¯€ë¡œ)"""
            import copy
            cleaned = copy.deepcopy(query_dict)
            
            if isinstance(cleaned, dict):
                # nested ì¿¼ë¦¬ì—ì„œ inner_hits ì œê±°
                if 'nested' in cleaned:
                    if 'inner_hits' in cleaned['nested']:
                        del cleaned['nested']['inner_hits']
                    # ì¬ê·€ì ìœ¼ë¡œ query ë‚´ë¶€ë„ ì •ì œ
                    if 'query' in cleaned['nested']:
                        cleaned['nested']['query'] = remove_inner_hits(cleaned['nested']['query'])
                
                # bool ì¿¼ë¦¬ ë‚´ë¶€ë„ ì¬ê·€ì ìœ¼ë¡œ ì •ì œ
                if 'bool' in cleaned:
                    for key in ['must', 'should', 'must_not', 'filter']:
                        if key in cleaned['bool']:
                            if isinstance(cleaned['bool'][key], list):
                                cleaned['bool'][key] = [remove_inner_hits(item) for item in cleaned['bool'][key]]
                            else:
                                cleaned['bool'][key] = remove_inner_hits(cleaned['bool'][key])
            
            return cleaned
        
        if filters_os:
            # â­ inner_hits ì œê±° (ì¤‘ë³µ ë°©ì§€)
            cleaned_filters = [remove_inner_hits(f) for f in filters_os]
            
            filter_by_type = {}
            for f in cleaned_filters:
                # í•„í„° íƒ€ì… ì¶”ì¶œ (ìƒˆë¡œìš´ bool ì¿¼ë¦¬ í˜•íƒœ ì§€ì›)
                filter_type = None
                
                # 1. bool ì¿¼ë¦¬ í˜•íƒœ (metadata OR qa_pairs)
                if 'bool' in f and 'should' in f['bool']:
                    should_clauses = f['bool']['should']
                    for clause in should_clauses:
                        # term í•„í„°ì—ì„œ íƒ€ì… ì¶”ì¶œ
                        if 'term' in clause:
                            term_key = list(clause['term'].keys())[0]
                            if 'age_group' in term_key:
                                filter_type = 'age'
                                break
                            elif 'gender' in term_key:
                                filter_type = 'gender'
                                break
                            elif 'occupation' in term_key:
                                filter_type = 'occupation'
                                break
                        # nested í•„í„°ì—ì„œ íƒ€ì… ì¶”ì¶œ
                        elif 'nested' in clause:
                            nested_q = clause['nested'].get('query', {}).get('bool', {}).get('must', [])
                            for nq in nested_q:
                                if isinstance(nq, dict) and 'bool' in nq and 'should' in nq['bool']:
                                    # q_text ë§¤ì¹­ í™•ì¸
                                    for sq in nq['bool']['should']:
                                        if 'match' in sq:
                                            match_key = list(sq['match'].keys())[0]
                                            if 'q_text' in match_key:
                                                q_text_val = sq['match'][match_key]
                                                if 'ì—°ë ¹' in str(q_text_val) or 'ë‚˜ì´' in str(q_text_val):
                                                    filter_type = 'age'
                                                    break
                                                elif 'ì„±ë³„' in str(q_text_val):
                                                    filter_type = 'gender'
                                                    break
                                                elif 'ì§ì—…' in str(q_text_val) or 'ì§ë¬´' in str(q_text_val):
                                                    filter_type = 'occupation'
                                                    break
                                elif 'match' in nq:
                                    match_key = list(nq['match'].keys())[0]
                                    if 'q_text' in match_key:
                                        q_text_val = nq['match'][match_key]
                                        if 'ì—°ë ¹' in str(q_text_val) or 'ë‚˜ì´' in str(q_text_val):
                                            filter_type = 'age'
                                            break
                                        elif 'ì„±ë³„' in str(q_text_val):
                                            filter_type = 'gender'
                                            break
                                        elif 'ì§ì—…' in str(q_text_val) or 'ì§ë¬´' in str(q_text_val):
                                            filter_type = 'occupation'
                                            break
                        if filter_type:
                            break
                
                # 2. ê¸°ì¡´ í˜•íƒœ (í•˜ìœ„ í˜¸í™˜ì„±)
                elif 'term' in f:
                    term_key = list(f['term'].keys())[0]
                    if 'age_group' in term_key:
                        filter_type = 'age'
                    elif 'gender' in term_key:
                        filter_type = 'gender'
                    elif 'occupation' in term_key:
                        filter_type = 'occupation'
                elif 'nested' in f:
                    nested_q = f['nested'].get('query', {}).get('bool', {}).get('must', [])
                    for nq in nested_q:
                        if 'match' in nq:
                            match_key = list(nq['match'].keys())[0]
                            if 'q_text' in match_key:
                                q_text_val = nq['match'][match_key]
                                if 'ì—°ë ¹' in str(q_text_val) or 'ë‚˜ì´' in str(q_text_val):
                                    filter_type = 'age'
                                elif 'ì„±ë³„' in str(q_text_val):
                                    filter_type = 'gender'
                                elif 'ì§ì—…' in str(q_text_val):
                                    filter_type = 'occupation'
                
                if filter_type:
                    if filter_type not in filter_by_type:
                        filter_by_type[filter_type] = []
                    filter_by_type[filter_type].append(f)
                else:
                    # íƒ€ì…ì„ ì•Œ ìˆ˜ ì—†ëŠ” í•„í„°ëŠ” ê·¸ëŒ€ë¡œ ì¶”ê°€
                    if 'unknown' not in filter_by_type:
                        filter_by_type['unknown'] = []
                    filter_by_type['unknown'].append(f)
            
            # â­ í•„í„°ë¥¼ should ì¡°ê±´ìœ¼ë¡œ ì „í™˜ (ì ìˆ˜ ë¶€ìŠ¤íŒ… í¬í•¨)
            # ê° íƒ€ì…ë³„ë¡œ OR, íƒ€ì… ê°„ì€ AND (shouldë¡œ ì™„í™”)
            should_filters = []
            for filter_type, type_filters in filter_by_type.items():
                if filter_type == 'unknown':
                    # â­ unknown íƒ€ì…ì€ ê°ê° ê°œë³„ì ìœ¼ë¡œ ì¶”ê°€ (AND ì²˜ë¦¬)
                    # region, marital_status ë“± ì„œë¡œ ë‹¤ë¥¸ í•„í„°ëŠ” ëª¨ë‘ ë§Œì¡±í•´ì•¼ í•¨
                    should_filters.extend(type_filters)
                elif len(type_filters) == 1:
                    # ë‹¨ì¼ í•„í„°: í•„í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì´ë¯¸ bool ì¿¼ë¦¬ í˜•íƒœ)
                    filter_item = type_filters[0]
                    should_filters.append(filter_item)
                else:
                    # ê°™ì€ íƒ€ì… í•„í„°ëŠ” OR (ì˜ˆ: 30ëŒ€ OR 40ëŒ€)
                    should_filters.append({
                        'bool': {
                            'should': type_filters,
                            "minimum_should_match": 1
                        }
                    })
            
            # â­ ê¸°ì¡´ ì¿¼ë¦¬ì™€ í•„í„° ê²°í•© (mustë¡œ ê²°í•©: ëª¨ë“  í•„í„°ë¥¼ ë§Œì¡±í•´ì•¼ í•¨)
            # survey_responses_merged: ëª¨ë“  ì¸êµ¬í†µê³„ ì •ë³´ í¬í•¨
            # ê° ì¸ë±ìŠ¤ì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì™€ì•¼ í•˜ë¯€ë¡œ mustë¡œ ê²°í•©
            if existing_query is None or existing_query == {"match_all": {}} or existing_query == {"match_none": {}}:
                # í‚¤ì›Œë“œ ì¿¼ë¦¬ê°€ ì—†ê±°ë‚˜ match_all/match_noneì¸ ê²½ìš°: í•„í„°ë¥¼ mustë¡œ ì‚¬ìš©
                final_query['query'] = {
                    'bool': {
                        'must': should_filters  # ëª¨ë“  í•„í„°ë¥¼ ë§Œì¡±í•´ì•¼ í•¨
                    }
                }
                logger.info(f"âœ… í•„í„°ë¥¼ mustë¡œ ì ìš© (ëª¨ë“  í•„í„° ë§Œì¡± í•„ìš”): {len(should_filters)}ê°œ í•„í„°")
            elif isinstance(existing_query, dict) and existing_query.get('bool'):
                # ê¸°ì¡´ bool ì¿¼ë¦¬ì— í•„í„°ë¥¼ mustë¡œ ì¶”ê°€
                if 'must' not in existing_query['bool']:
                    existing_query['bool']['must'] = []
                existing_query['bool']['must'].extend(should_filters)
                final_query['query'] = existing_query
                logger.info(f"âœ… í•„í„°ë¥¼ mustë¡œ ì¶”ê°€ (ëª¨ë“  í•„í„° ë§Œì¡± í•„ìš”): {len(should_filters)}ê°œ í•„í„°")
            else:
                # ê¸°ì¡´ ì¿¼ë¦¬ë¥¼ boolë¡œ ê°ì‹¸ê¸° (mustë¡œ ê²°í•©)
                final_query['query'] = {
                    'bool': {
                        'must': [existing_query] + should_filters
                    }
                }
                logger.info(f"âœ… í•„í„°ë¥¼ mustë¡œ ì¶”ê°€ (ëª¨ë“  í•„í„° ë§Œì¡± í•„ìš”): {len(should_filters)}ê°œ í•„í„°")
        
        if 'size' not in final_query:
            final_query['size'] = size

        if filters_os:
            logger.info(f"ğŸ” ì ìš©ëœ í•„í„° ({len(filters_os)}ê°œ):")
            for i, f in enumerate(filters_os, 1):
                logger.info(f"  í•„í„° {i}: {json.dumps(f, ensure_ascii=False, indent=2)}")
            logger.info(f"ğŸ” ìµœì¢… ì¿¼ë¦¬ êµ¬ì¡°:")
            logger.info(f"  {json.dumps(final_query, ensure_ascii=False, indent=2)}")
        else:
            logger.info(f"ğŸ” ìµœì¢… ì¿¼ë¦¬ êµ¬ì¡° (í•„í„° ì—†ìŒ):")
            logger.info(f"  {json.dumps(final_query, ensure_ascii=False, indent=2)}")

        # â­ Qdrant top-N ì œí•œ: í•„í„° ìœ ë¬´ì— ë”°ë¼ ë¶„ê¸°
        has_filters = bool(filters_os or occupation_filters)
        rrf_k_used: Optional[int] = None
        rrf_reason: str = ""
        adaptive_threshold: Optional[float] = None
        threshold_reason: str = ""
        has_behavioral = bool(getattr(analysis, "behavioral_conditions", None))

        # â­ ê²€ìƒ‰ í¬ê¸° ì„¤ì •: behavioral í•„í„°ê°€ ìˆìœ¼ë©´ ë” ë§ì€ ê²°ê³¼ í•„ìš”
        # â­ ìµœì†Œê°’ì„ 10ìœ¼ë¡œ ì„¤ì • (í…ŒìŠ¤íŠ¸/ë””ë²„ê¹…ìš©)
        if has_filters or has_behavioral:
            if has_behavioral:
                qdrant_limit = min(max(size * 5, 500), 5000)
                search_size = min(max(size * 10, 10), 10000)  # ìµœì†Œê°’ 1000 â†’ 10
            else:
                qdrant_limit = min(max(size * 3, 300), 5000)
                search_size = min(max(size * 5, 10), 10000)  # ìµœì†Œê°’ 500 â†’ 10
            logger.info(f"ğŸ” í•„í„° ì ìš©: OpenSearch size={search_size}, Qdrant limit={qdrant_limit} (behavioral={has_behavioral})")
        else:
            qdrant_limit = min(max(size, 60), 5000)
            search_size = min(max(size * 2, 10), 10000)  # ìµœì†Œê°’ 80 â†’ 10
            logger.info(f"ğŸ” í•„í„° ì—†ìŒ: OpenSearch size={search_size}, Qdrant limit={qdrant_limit}")

        # 4) ì‹¤í–‰: í•˜ì´ë¸Œë¦¬ë“œ (OpenSearch + ì„ íƒì  Qdrant) with RRF
        # â­ survey_responses_merged ë‹¨ì¼ ì¸ë±ìŠ¤ë§Œ ê²€ìƒ‰
        
        # OpenSearch _source filtering: í•„ìš”í•œ í•„ë“œë§Œ ì¡°íšŒ
        # â­ qa_pairs í¬í•¨: marital_status, region ë“±ì´ qa_pairsì— ìˆì„ ìˆ˜ ìˆìŒ
        source_filter = {
            "includes": ["user_id", "metadata", "timestamp", "qa_pairs"],
            "excludes": []  # í•„ìš”ì‹œ ì œì™¸í•  í•„ë“œ ì¶”ê°€
        }
        
        # â­â­â­ survey_responses_merged ë‹¨ì¼ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©
        logger.info(f"ğŸ” ì¸ë±ìŠ¤ ê²€ìƒ‰: {request.index_name} (survey_responses_mergedë§Œ ì‚¬ìš©)")
        
        # survey_responses_merged ì¸ë±ìŠ¤ ê²€ìƒ‰
        keyword_results: List[Dict[str, Any]] = []
        vector_results: List[Dict[str, Any]] = []
        
        try:
            # OpenSearch í‚¤ì›Œë“œ ê²€ìƒ‰
            query_body = final_query.copy()
            if not isinstance(query_body.get('query'), dict):
                logger.warning("  âš ï¸ ì¿¼ë¦¬ê°€ ë¹„ì–´ ìˆì–´ match_allë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤")
                query_body['query'] = {"match_all": {}}

            # ğŸ” OpenSearch ì¿¼ë¦¬ ë¡œê¹… (ë””ë²„ê¹…ìš©)
            logger.info(f"ğŸ” OpenSearch ì¿¼ë¦¬:")
            logger.info(json.dumps(query_body, ensure_ascii=False, indent=2))

            os_response = data_fetcher.search_opensearch(
                index_name=request.index_name,
                query=query_body,
                size=search_size,
                source_filter=source_filter,
                request_timeout=DEFAULT_OS_TIMEOUT,
            )
            keyword_results = os_response['hits']['hits']
            logger.info(f"  âœ… OpenSearch: {len(keyword_results)}ê±´")
            
            # Qdrant ë²¡í„° ê²€ìƒ‰
            if request.use_vector_search and query_vector and hasattr(router, 'qdrant_client'):
                qdrant_client = router.qdrant_client
                try:
                    # survey_responses_merged ì»¬ë ‰ì…˜ë§Œ ê²€ìƒ‰
                    collection_name = request.index_name  # survey_responses_merged
                    try:
                        r = qdrant_client.search(
                            collection_name=collection_name,
                            query_vector=query_vector,
                            limit=qdrant_limit,
                            score_threshold=0.3,
                        )
                        for item in r:
                            vector_results.append({
                                '_id': str(item.id),
                                '_score': item.score,
                                '_source': item.payload,
                                '_index': collection_name,
                            })
                        logger.info(f"  âœ… Qdrant: {len(vector_results)}ê±´")
                    except Exception as e:
                        logger.debug(f"  âš ï¸ Qdrant ì»¬ë ‰ì…˜ '{collection_name}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                except Exception as e:
                    logger.debug(f"  âš ï¸ Qdrant ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        except Exception as e:
            logger.warning(f"  âš ï¸ ì¸ë±ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # user_id ë° _id -> ì›ë³¸ ë¬¸ì„œ ë§¤í•‘ ìƒì„±
        user_doc_map = {}
        id_doc_map = {}
        
        # ê²€ìƒ‰ ê²°ê³¼ ë§¤í•‘
        for hit in keyword_results:
            source = hit.get('_source', {})
            user_id = source.get('user_id')
            doc_id = hit.get('_id')
            
            doc_info = {
                'source': source,
                'inner_hits': hit.get('inner_hits', {}),
                'highlight': hit.get('highlight'),
                'index': hit.get('_index', 'unknown')
            }
            
            if user_id:
                user_doc_map[user_id] = doc_info
            if doc_id:
                id_doc_map[doc_id] = doc_info

        # â­ RRF ê²°í•©
        logger.info(f"\n{'='*60}")
        logger.info("ğŸ“Š RRF ê²°í•©")
        logger.info(f"{'='*60}")

        # ë²¡í„° ê²°ê³¼ì— ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„° ë³´ê°•
        for doc in vector_results:
            if '_index' not in doc:
                doc['_index'] = doc.get('collection', doc.get('_source', {}).get('index', request.index_name))

        logger.info(f"  - í‚¤ì›Œë“œ ê²°ê³¼: {len(keyword_results)}ê±´")
        logger.info(f"  - ë²¡í„°   ê²°ê³¼: {len(vector_results)}ê±´")

        rrf_start = perf_counter()

        if request.use_vector_search and vector_results:
            combined_rrf, rrf_k_used, rrf_reason = calculate_rrf_score_adaptive(
                keyword_results=keyword_results,
                vector_results=vector_results,
                query_intent=getattr(analysis, "intent", None),
                has_filters=has_filters,
                use_vector_search=request.use_vector_search,
            )
        else:
            combined_rrf = keyword_results
            if request.use_vector_search:
                rrf_reason = "ë²¡í„° ê²°ê³¼ ì—†ìŒ â†’ í‚¤ì›Œë“œ ê²°ê³¼ ì‚¬ìš©"
            else:
                rrf_reason = "ë²¡í„° ê²€ìƒ‰ ë¹„í™œì„±í™” â†’ í‚¤ì›Œë“œ ê²°ê³¼ ì‚¬ìš©"
            rrf_k_used = 0

        user_rrf_variants: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        for doc in combined_rrf:
            user_id = get_user_id_from_doc(doc)
            if not user_id:
                continue
            user_rrf_variants[user_id].append(doc)
        
        user_rrf_map: Dict[str, List[Dict[str, Any]]] = {}
        final_rrf_results: List[Dict[str, Any]] = []
        for user_id, docs in user_rrf_variants.items():
            def _score(doc: Dict[str, Any]) -> float:
                score = doc.get('_score')
                if score is None:
                    score = doc.get('rrf_score', 0.0)
                return float(score or 0.0)

            best_doc_original = max(docs, key=_score)
            total_rrf_score = sum(_score(doc) for doc in docs)
            sources = [doc.get('_index', 'unknown') for doc in docs]

            best_doc = dict(best_doc_original)
            best_doc['_score'] = total_rrf_score
            best_doc['_rrf_details'] = {
                    'combined_score': total_rrf_score,
                'source_count': len(docs),
                'sources': sources,
                }

            final_rrf_results.append(best_doc)
            # best_docë¥¼ ì²« ë²ˆì§¸ë¡œ ìœ ì§€í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” ì°¸ê³ ìš©ìœ¼ë¡œ ë³´ê´€
            others = [doc for doc in docs if doc is not best_doc_original]
            user_rrf_map[user_id] = [best_doc] + others
        
        final_rrf_results.sort(
            key=lambda d: d.get('_score', 0.0) or d.get('rrf_score', 0.0),
            reverse=True
        )
        
        rrf_results = final_rrf_results
        took_ms = 0  # ì—¬ëŸ¬ ê²€ìƒ‰ì˜ í•©ì´ë¯€ë¡œ ì •í™•í•œ ì‹œê°„ ì¸¡ì •ì€ ì–´ë ¤ì›€
        
        logger.info(f"  âœ… ë‹¨ì¼ RRF ê²°í•© ì™„ë£Œ: {len(rrf_results)}ê±´ (ê³ ìœ  user_id: {len(user_rrf_map)}ê°œ)")
        timings['rrf_recombination_ms'] = (perf_counter() - rrf_start) * 1000

        # í›„ë³´ ë¬¸ì„œ ìˆ˜ ì œí•œ (í›„ì²˜ë¦¬ ë¶€ë‹´ ì™„í™”)
        fetch_size = window_size
        candidate_cap = max(
            fetch_size * 20,
            cache_limit if cache_limit else 0,
            2000
        )
        if candidate_cap and len(rrf_results) > candidate_cap:
            logger.info(
                f"  - í›„ë³´ ë¬¸ì„œ ì œí•œ ì ìš©: {len(rrf_results)} â†’ {candidate_cap} (size={fetch_size})"
            )
            rrf_results = rrf_results[:candidate_cap]
        elif len(rrf_results) < fetch_size:
            backup_cap = max(fetch_size * 6, fetch_size + 50)
            logger.info(
                f"  - í›„ë³´ ìˆ˜ê°€ sizeë³´ë‹¤ ì‘ì•„ ì¦ê°€ ì‹œë„: {len(rrf_results)} â†’ {min(len(final_rrf_results), backup_cap)}"
            )
            rrf_results = final_rrf_results[:backup_cap]
        
        # RRF ì ìˆ˜ ë””ë²„ê¹…: ìƒìœ„ 10ê°œ ì¶œë ¥
        if rrf_results:
            logger.info(f"  - RRF ì ìˆ˜ ìƒìœ„ 10ê°œ:")
            for i, doc in enumerate(rrf_results[:10], 1):
                rrf_score = doc.get('_score') or doc.get('rrf_score', 0.0)
                rrf_details = doc.get('_rrf_details', {})
                doc_index = doc.get('_index', 'unknown')
                logger.info(f"    {i}. doc_id={doc.get('_id', 'N/A')}, index={doc_index}, RRF={rrf_score:.6f}, "
                          f"keyword_rank={rrf_details.get('keyword_rank')}, vector_rank={rrf_details.get('vector_rank')}")
        
        demographic_filters: Dict[DemographicType, List["DemographicEntity"]] = defaultdict(list)
        for demo in extracted_entities.demographics:
            demographic_filters[demo.demographic_type].append(demo)

        filtered_rrf_results: List[Dict[str, Any]] = rrf_results
        total_hits = len(rrf_results)

        occupation_display_map: Dict[str, str] = {}
        behavior_values_map: Dict[str, Dict[str, Optional[bool]]] = {}
        doc_user_map: Dict[int, str] = {}
        # â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ welcome_1st/welcome_2nd ë°°ì¹˜ ì œê±°
        synonym_cache: Dict[str, List[str]] = {}

        PLACEHOLDER_TOKENS: Set[str] = {
            "",
            "ë¯¸ì •",
            "ì—†ìŒ",
            "ë¬´ì‘ë‹µ",
            "í•´ë‹¹ì—†ìŒ",
            "n/a",
            "na",
            "null",
            "none",
            "unknown",
            "ë¯¸ì„ íƒ",
            "ë¯¸ê¸°ì¬",
        }
        PLACEHOLDER_TOKENS = {token.strip().lower() for token in PLACEHOLDER_TOKENS}

        def normalize_value(value: Any) -> str:
            if value is None:
                return ""
            if isinstance(value, bool):
                value_str = str(value)
            elif isinstance(value, (int, float)):
                try:
                    if value.is_integer():  # type: ignore[attr-defined]
                        value = int(value)
                except AttributeError:
                    pass
                value_str = str(value)
            else:
                value_str = str(value)

            cleaned = value_str.strip()
            lower = cleaned.lower()
            if lower in PLACEHOLDER_TOKENS:
                return ""
            return lower

        def build_expected_values(demo: "DemographicEntity") -> Set[str]:
            key = f"{demo.demographic_type.value}:{demo.raw_value}"
            expected: Set[str] = set()
            expected.add(demo.raw_value)
            expected.add(demo.value)
            expected.update(demo.synonyms or set())
            expected.update(synonym_cache.get(key, []))
            normalized_expected = {normalize_value(v) for v in expected if v}

            if demo.demographic_type == DemographicType.GENDER:
                male_aliases = {
                    normalize_value(v)
                    for v in {"m", "male", "man", "ë‚¨", "ë‚¨ì„±", "ë‚¨ì", "ë‚¨ì„±í˜•"}
                }
                female_aliases = {
                    normalize_value(v)
                    for v in {"f", "female", "woman", "ì—¬", "ì—¬ì„±", "ì—¬ì", "ì—¬ì„±í˜•"}
                }
                if normalized_expected & male_aliases:
                    normalized_expected.update(male_aliases)
                if normalized_expected & female_aliases:
                    normalized_expected.update(female_aliases)

            return normalized_expected

        def values_match(values: Set[str], expected: Set[str]) -> bool:
            if not values or not expected:
                return False
            for val in values:
                if not val:
                    continue
                for exp in expected:
                    if not exp:
                        continue
                    if val == exp or val in exp or exp in val:
                        return True
            return False

        def expand_gender_aliases(values: Set[str]) -> None:
            male_aliases = {"m", "ë‚¨", "ë‚¨ì„±", "male", "man", "ë‚¨ì"}
            female_aliases = {"f", "ì—¬", "ì—¬ì„±", "female", "woman", "ì—¬ì"}
            if values & male_aliases:
                values.update(male_aliases)
            if values & female_aliases:
                values.update(female_aliases)

        def add_age_decade(values: Set[str], age_value: Any) -> None:
            if age_value in (None, ""):
                return
            try:
                age_int = int(age_value)
                decade = (age_int // 10) * 10
                for candidate in (f"{decade}ëŒ€", f"{decade}s", str(age_int)):
                    normalized_candidate = normalize_value(candidate)
                    if normalized_candidate:
                        values.add(normalized_candidate)
            except (ValueError, TypeError):
                pass

        filter_start = perf_counter()
        if 'has_filter_constraints' not in locals():
            has_filter_constraints = has_demographic_filters or has_behavioral_conditions
        if has_filter_constraints:

            # â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ welcome_1st ê´€ë ¨ ë¡œì§ ì œê±°
            gender_dsl_handled = bool(demographic_filters.get(DemographicType.GENDER))
            age_dsl_handled = bool(demographic_filters.get(DemographicType.AGE))
            occupation_dsl_handled = bool(demographic_filters.get(DemographicType.OCCUPATION)) and occupation_filter_handled

            # â­ ëª¨ë“  demographic_filtersë¥¼ ê²€ì¦ (REGION, MARITAL_STATUS í¬í•¨!)
            # â­ ë‹¨, OCCUPATIONê³¼ JOB_FUNCTIONì€ OpenSearchì—ì„œ qa_pairsë¡œ ê²€ìƒ‰í•˜ë¯€ë¡œ í›„ì²˜ë¦¬ ê²€ì¦ ìŠ¤í‚µ
            filters_to_validate: List[DemographicType] = [
                f for f in demographic_filters.keys()
                if f not in {DemographicType.OCCUPATION, DemographicType.JOB_FUNCTION}
            ]
            logger.info(f"  âœ… í›„ì²˜ë¦¬ ê²€ì¦ ëŒ€ìƒ: {[f.value for f in filters_to_validate]}")
            logger.info(f"  âš ï¸ í›„ì²˜ë¦¬ ê²€ì¦ ì œì™¸ (OpenSearch í•„í„°ë§Œ ì‚¬ìš©): {[f.value for f in demographic_filters.keys() if f in {DemographicType.OCCUPATION, DemographicType.JOB_FUNCTION}]}")

            for demo in extracted_entities.demographics:
                cache_key = f"{demo.demographic_type.value}:{demo.raw_value}"
                if demo.demographic_type in {DemographicType.GENDER, DemographicType.OCCUPATION}:
                    try:
                        from rag_query_analyzer.utils.synonym_expander import get_synonym_expander
                        expander = get_synonym_expander()
                        synonym_cache[cache_key] = expander.expand(demo.raw_value)
                    except Exception:
                        synonyms = [demo.raw_value]
                        synonyms.extend([syn for syn in demo.synonyms if syn])
                        synonym_cache[cache_key] = synonyms
                else:
                    synonym_cache[cache_key] = [demo.raw_value]
            
            user_ids_to_fetch: Set[str] = set()
            doc_user_map.clear()
            
            logger.info(f"ğŸ” user_id ìˆ˜ì§‘ ì¤‘: RRF ê²°ê³¼ {len(rrf_results)}ê±´...")
            for doc in rrf_results:
                source = doc.get('_source', {})
                if not source and 'doc' in doc:
                    source = doc.get('doc', {}).get('_source', {})
                if not source or not isinstance(source, dict):
                    if 'payload' in doc:
                        payload = doc.get('payload', {})
                        if isinstance(payload, dict) and payload:
                            source = payload
                    elif isinstance(source, dict) and 'payload' in source:
                        payload = source.get('payload', {})
                        if isinstance(payload, dict) and payload:
                            source = payload
                user_id = None
                if isinstance(source, dict):
                    user_id = source.get('user_id')
                if not user_id:
                    user_id = doc.get('_id', '')
                if not user_id and 'payload' in doc:
                    payload = doc.get('payload', {})
                    if isinstance(payload, dict):
                        user_id = payload.get('user_id')
                if user_id:
                    user_ids_to_fetch.add(user_id)
                    doc_user_map[id(doc)] = user_id
            
            logger.info(f"  âœ… ìˆ˜ì§‘ëœ user_id: {len(user_ids_to_fetch)}ê±´")
            # â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ welcome_1st/welcome_2nd ë°°ì¹˜ ì¡°íšŒ ì œê±°
            
            if not filters_to_validate:
                timings["post_filter_ms"] = (perf_counter() - filter_start) * 1000
                filtered_rrf_results = rrf_results
                logger.info("  âœ… Demographic í•„í„° ì—†ìŒ: Python í›„ì²˜ë¦¬ ìƒëµ")
            else:
                def collect_doc_values(
                    user_id: str,
                    source: Dict[str, Any],
                    metadata: Dict[str, Any],
                    _unused: Dict[str, Any],  # í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€í•˜ì§€ë§Œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                ) -> Tuple[Dict[DemographicType, Set[str]], Dict[DemographicType, bool], Dict[str, Optional[bool]]]:
                    doc_values: Dict[DemographicType, Set[str]] = {
                        DemographicType.GENDER: set(),
                        DemographicType.AGE: set(),
                        DemographicType.OCCUPATION: set(),
                    }
                    metadata_presence: Dict[DemographicType, bool] = {
                        DemographicType.GENDER: False,
                        DemographicType.AGE: False,
                        DemographicType.OCCUPATION: False,
                    }
                    behavior_values: Dict[str, Optional[bool]] = {
                        "smoker": None,
                        "has_vehicle": None,
                        "drinker": None,  # â­ ìŒì£¼ ì—¬ë¶€ ì¶”ê°€
                    }

                    def record_behavior(key: str, value: Optional[bool]) -> None:
                        if value is None:
                            return
                        if behavior_values.get(key) is None:
                            behavior_values[key] = value

                    def parse_yes_no(text: Optional[str]) -> Optional[bool]:
                        if not text:
                            return None
                        normalized = text.lower()
                        if any(keyword in normalized for keyword in BEHAVIOR_NO_TOKENS):
                            return False
                        if any(keyword in normalized for keyword in BEHAVIOR_YES_TOKENS):
                            return True
                        return None

                    def parse_smoker_answer(raw: Optional[Any]) -> Optional[bool]:
                        if raw is None:
                            return None
                        if isinstance(raw, (list, tuple, set)):
                            for item in raw:
                                decision = parse_smoker_answer(item)
                                if decision is not None:
                                    return decision
                            return None
                        text = str(raw).strip()
                        if not text:
                            return None
                        normalized = text.lower()
                        compact = normalized.replace(" ", "")
                        for keyword in SMOKER_NEGATIVE_KEYWORDS:
                            keyword_compact = keyword.replace(" ", "")
                            if keyword in normalized or keyword_compact in compact:
                                return False
                        for keyword in SMOKER_POSITIVE_KEYWORDS:
                            keyword_compact = keyword.replace(" ", "")
                            if keyword in normalized or keyword_compact in compact:
                                return True
                        if (
                            "ë‹´ë°°" in normalized
                            and not any(token in normalized for token in ("ì—†", "ì•ˆ", "ì•Š", "no", "ë¬´", "ë¯¸í¡ì—°"))
                        ):
                            return True
                        return parse_yes_no(text)

                    def parse_drinker_answer(raw: Optional[Any]) -> Optional[bool]:
                        """ìŒì£¼ ì—¬ë¶€ íŒŒì‹±: ìˆ  ì¢…ë¥˜ê°€ ìˆìœ¼ë©´ True, 'ìˆ ì„ ë§ˆì‹œì§€ ì•ŠìŒ'ì´ ìˆìœ¼ë©´ False"""
                        if raw is None:
                            return None
                        if isinstance(raw, (list, tuple, set)):
                            for item in raw:
                                decision = parse_drinker_answer(item)
                                if decision is not None:
                                    return decision
                            return None
                        text = str(raw).strip()
                        if not text:
                            return None
                        normalized = text.lower()
                        compact = normalized.replace(" ", "")

                        # ë„¤ê±°í‹°ë¸Œ í‚¤ì›Œë“œ ìš°ì„  ì²´í¬
                        for keyword in NON_DRINKER_KEYWORDS:
                            keyword_compact = keyword.replace(" ", "")
                            if keyword in normalized or keyword_compact in compact:
                                return False

                        # í¬ì§€í‹°ë¸Œ í‚¤ì›Œë“œ ì²´í¬
                        for keyword in DRINKER_POSITIVE_KEYWORDS:
                            keyword_compact = keyword.replace(" ", "")
                            if keyword in normalized or keyword_compact in compact:
                                return True

                        # "ìˆ " ì–¸ê¸‰ì´ ìˆì§€ë§Œ ë¶€ì • í‘œí˜„ì´ ì—†ìœ¼ë©´ ìŒì£¼ìë¡œ ê°„ì£¼
                        if (
                            "ìˆ " in normalized
                            and not any(token in normalized for token in ("ì—†", "ì•ˆ", "ì•Š", "no", "ë¬´", "ëª»"))
                        ):
                            return True

                        return None

                    metadata_candidates = [
                        metadata,  # ì´ì œ ë‹¨ì¼ metadataë§Œ ì‚¬ìš©
                        source.get("metadata", {}) if isinstance(source, dict) else {},
                    ]

                    payload = {}
                    if isinstance(source, dict):
                        payload_candidate = source.get("payload")
                        if isinstance(payload_candidate, dict):
                            payload = payload_candidate
                    if not payload and isinstance(source, dict) and "doc" in source:
                        doc_payload = source.get("doc", {}).get("payload")
                        if isinstance(doc_payload, dict):
                            payload = doc_payload
                    if not payload and isinstance(source, dict):
                        payload = source

                    if isinstance(payload, dict):
                        metadata_candidates.append(payload.get("metadata", {}))

                    for candidate in metadata_candidates:
                        if not isinstance(candidate, dict):
                            continue

                        candidate_sources: List[Dict[str, Any]] = [candidate]
                        nested_meta = candidate.get("metadata")
                        if isinstance(nested_meta, dict):
                            candidate_sources.append(nested_meta)

                        for meta_source in candidate_sources:
                            gender_val = meta_source.get("gender") or meta_source.get("gender_code")
                            if gender_val:
                                normalized_gender = normalize_value(gender_val)
                                if normalized_gender:
                                    doc_values[DemographicType.GENDER].add(normalized_gender)
                                    metadata_presence[DemographicType.GENDER] = True

                            age_group_val = meta_source.get("age_group")
                            if age_group_val:
                                normalized_age_group = normalize_value(age_group_val)
                                if normalized_age_group:
                                    doc_values[DemographicType.AGE].add(normalized_age_group)
                                    metadata_presence[DemographicType.AGE] = True

                            age_val = meta_source.get("age")
                            if age_val:
                                pre_count = len(doc_values[DemographicType.AGE])
                                add_age_decade(doc_values[DemographicType.AGE], age_val)
                                if len(doc_values[DemographicType.AGE]) > pre_count:
                                    metadata_presence[DemographicType.AGE] = True

                            birth_year_val = meta_source.get("birth_year")
                            if birth_year_val:
                                normalized_birth_year = normalize_value(birth_year_val)
                                if normalized_birth_year:
                                    doc_values[DemographicType.AGE].add(normalized_birth_year)
                                    metadata_presence[DemographicType.AGE] = True

                            occupation_val = meta_source.get("occupation") or meta_source.get("job")
                            if occupation_val:
                                normalized_occupation = normalize_value(occupation_val)
                                if normalized_occupation:
                                    doc_values[DemographicType.OCCUPATION].add(normalized_occupation)
                                    metadata_presence[DemographicType.OCCUPATION] = True

                            vehicle_val = meta_source.get("has_vehicle")
                            if vehicle_val:
                                normalized_vehicle = normalize_value(vehicle_val)
                                record_behavior("has_vehicle", parse_yes_no(normalized_vehicle))

                    if user_id:
                        qa_sources: List[List[Dict[str, Any]]] = []
                        if isinstance(source, dict):
                            qa_sources.append(source.get("qa_pairs", []) or [])
                        # â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ welcome_1st/welcome_2nd ë°°ì¹˜ ì œê±°

                        for qa_pairs in qa_sources:
                            for qa in qa_pairs:
                                if not isinstance(qa, dict):
                                    continue
                                q_text_raw = str(qa.get("q_text", "")).lower()
                                q_text = normalize_value(qa.get("q_text"))
                                answer_candidates = [
                                    qa.get("answer"),
                                    qa.get("answer_text"),
                                    qa.get("value"),
                                ]
                                answers: List[str] = []
                                for candidate in answer_candidates:
                                    if candidate is None:
                                        continue
                                    if isinstance(candidate, list):
                                        answers.extend(str(item) for item in candidate if item)
                                    else:
                                        answers.append(str(candidate))
                                normalized_answers = {normalize_value(ans) for ans in answers if ans}

                                if q_text and normalized_answers:
                                    if any(keyword in q_text_raw for keyword in ("ì§ì—…", "ì§ë¬´", "occupation")):
                                        doc_values[DemographicType.OCCUPATION].update(normalized_answers)

                                    if (
                                        not metadata_presence[DemographicType.GENDER]
                                        and any(keyword in q_text_raw for keyword in ("ì„±ë³„", "gender"))
                                    ):
                                        doc_values[DemographicType.GENDER].update(normalized_answers)
                                        metadata_presence[DemographicType.GENDER] = True

                                if behavior_values.get("smoker") is None and q_text_raw and any(keyword in q_text_raw for keyword in SMOKER_QUESTION_KEYWORDS):
                                    for ans in answers:
                                        smoker_decision = parse_smoker_answer(ans)
                                        if smoker_decision is not None:
                                            record_behavior("smoker", smoker_decision)
                                            if smoker_decision is False:
                                                break
                                    if behavior_values.get("smoker") is None:
                                        for ans in normalized_answers:
                                            smoker_decision = parse_smoker_answer(ans)
                                            if smoker_decision is not None:
                                                record_behavior("smoker", smoker_decision)
                                                break

                                if behavior_values.get("has_vehicle") is None and q_text_raw and any(keyword in q_text_raw for keyword in VEHICLE_QUESTION_KEYWORDS):
                                    # ğŸ” ë””ë²„ê¹…: ì‹¤ì œ ì°¨ëŸ‰ ë‹µë³€ ë¡œê·¸
                                    normalized_answers_sample = list(normalized_answers)[:3]

                                    for ans in normalized_answers:
                                        vehicle_decision = parse_yes_no(ans)

                                        if vehicle_decision is not None:
                                            record_behavior("has_vehicle", vehicle_decision)
                                            break

                                # â­ ìŒì£¼ ì—¬ë¶€ ì¶”ì¶œ
                                if behavior_values.get("drinker") is None and q_text_raw and any(keyword in q_text_raw for keyword in ALCOHOL_QUESTION_KEYWORDS):
                                    for ans in answers:
                                        drinker_decision = parse_drinker_answer(ans)
                                        if drinker_decision is not None:
                                            record_behavior("drinker", drinker_decision)
                                            if drinker_decision is False:
                                                break
                                    if behavior_values.get("drinker") is None:
                                        for ans in normalized_answers:
                                            drinker_decision = parse_drinker_answer(ans)
                                            if drinker_decision is not None:
                                                record_behavior("drinker", drinker_decision)
                                                break

                    return doc_values, metadata_presence, behavior_values

                filtered_list = []
                source_not_found_count = 0
                gender_filter_failed = 0
                age_filter_failed = 0
                occupation_filter_failed = 0
                gender_metadata_missing = 0
                age_metadata_missing = 0
                occupation_metadata_missing = 0
                region_filter_failed = 0  # â­ REGION í•„í„° ì¹´ìš´í„°
                region_metadata_missing = 0  # â­ REGION ë©”íƒ€ë°ì´í„° ëˆ„ë½ ì¹´ìš´í„°
                marital_status_filter_failed = 0  # â­ MARITAL_STATUS í•„í„° ì¹´ìš´í„°
                marital_status_metadata_missing = 0  # â­ MARITAL_STATUS ë©”íƒ€ë°ì´í„° ëˆ„ë½ ì¹´ìš´í„°
                sub_region_filter_failed = 0  # â­ SUB_REGION í•„í„° ì¹´ìš´í„°
                sub_region_metadata_missing = 0  # â­ SUB_REGION ë©”íƒ€ë°ì´í„° ëˆ„ë½ ì¹´ìš´í„°
                behavior_filter_failed = 0
                behavior_metadata_missing = 0

                for doc in rrf_results:
                    user_id = doc_user_map.get(id(doc))
                    if not user_id:
                        continue

                    source = user_rrf_map.get(user_id, [{}])[0].get('_source', {})
                    if not isinstance(source, dict):
                        source = {}

                    # â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ welcome_1st/welcome_2nd ë°°ì¹˜ ì œê±°
                    # sourceì—ì„œ ì§ì ‘ metadata ê°€ì ¸ì˜¤ê¸°
                    metadata = source.get("metadata", {}) if isinstance(source.get("metadata"), dict) else {}

                    doc_values, metadata_presence, behavior_values = collect_doc_values(user_id, source, metadata, {})
                    behavior_values_map[user_id] = dict(behavior_values)

                    gender_pass = True
                    age_pass = True
                    occupation_pass = True
                    region_pass = True  # â­ REGION í•„í„° ê²€ì¦ ì¶”ê°€
                    marital_status_pass = True  # â­ MARITAL_STATUS í•„í„° ê²€ì¦ ì¶”ê°€
                    sub_region_pass = True  # â­ SUB_REGION í•„í„° ê²€ì¦ ì¶”ê°€

                    if DemographicType.GENDER in filters_to_validate:
                        expected = set()
                        for demo in demographic_filters[DemographicType.GENDER]:
                            expected.update(build_expected_values(demo))

                        # â­ Metadata ìš°ì„ : metadataê°€ ìˆìœ¼ë©´ metadataë§Œ í™•ì¸
                        if metadata_presence[DemographicType.GENDER]:
                            # metadataë¡œ ìˆ˜ì§‘ëœ ê°’ë§Œ ì‚¬ìš© (qa_pairs ë¬´ì‹œ)
                            # doc_valuesì—ì„œ metadata ì†ŒìŠ¤ë§Œ í™•ì¸í•˜ê¸° ìœ„í•´ ë‹¤ì‹œ ìˆ˜ì§‘
                            gender_from_metadata = set()
                            for meta_source in [metadata, source.get("metadata", {})]:
                                if isinstance(meta_source, dict):
                                    gender_val = meta_source.get("gender") or meta_source.get("gender_code")
                                    if gender_val:
                                        normalized_gender = normalize_value(gender_val)
                                        if normalized_gender:
                                            gender_from_metadata.add(normalized_gender)

                            if gender_from_metadata:
                                expand_gender_aliases(gender_from_metadata)
                                gender_pass = values_match(gender_from_metadata, expected)
                            else:
                                # metadata_presenceê°€ Trueì¸ë° ê°’ì´ ì—†ìœ¼ë©´ ì˜¤ë¥˜
                                gender_metadata_missing += 1
                                gender_pass = False
                        else:
                            # metadata ì—†ìœ¼ë©´ qa_pairs ì‚¬ìš©
                            expand_gender_aliases(doc_values[DemographicType.GENDER])
                            gender_pass = values_match(doc_values[DemographicType.GENDER], expected)

                        if not gender_pass:
                            gender_filter_failed += 1

                    if gender_pass and DemographicType.AGE in filters_to_validate:
                        expected = set()
                        for demo in demographic_filters[DemographicType.AGE]:
                            expected.update(build_expected_values(demo))
                        age_pass = values_match(doc_values[DemographicType.AGE], expected)
                        if not age_pass:
                            age_filter_failed += 1

                    if gender_pass and age_pass and DemographicType.OCCUPATION in filters_to_validate:
                        expected = set()
                        for demo in demographic_filters[DemographicType.OCCUPATION]:
                            expected.update(build_expected_values(demo))
                        if not metadata_presence[DemographicType.OCCUPATION]:
                            occupation_metadata_missing += 1
                        occupation_pass = values_match(doc_values[DemographicType.OCCUPATION], expected)
                        if not occupation_pass:
                            occupation_filter_failed += 1
                        else:
                            display_occupation = None
                            occupation_candidates = [
                                metadata.get("occupation") if isinstance(metadata, dict) else None,
                                metadata.get("job") if isinstance(metadata, dict) else None,
                                metadata.get("occupation_group") if isinstance(metadata, dict) else None,
                            ]
                            for candidate in occupation_candidates:
                                normalized_candidate = normalize_value(candidate)
                                if normalized_candidate and values_match({normalized_candidate}, expected):
                                    display_occupation = str(candidate)
                                    break
                            if not display_occupation:
                                qa_sources: List[List[Dict[str, Any]]] = []
                                if isinstance(source, dict):
                                    qa_sources.append(source.get("qa_pairs", []) or [])
                                # â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ metadata_2nd_full ì œê±°
                                for qa_pairs in qa_sources:
                                    for qa in qa_pairs:
                                        if not isinstance(qa, dict):
                                            continue
                                        q_text = str(qa.get("q_text", "")).lower()
                                        if not any(keyword in q_text for keyword in ("ì§ì—…", "ì§ë¬´", "occupation", "ì§ì¢…")):
                                            continue
                                        answer = qa.get("answer")
                                        if answer is None:
                                            answer = qa.get("answer_text")
                                        if answer is None:
                                            continue
                                        candidate_value = str(answer)
                                        normalized_candidate = normalize_value(candidate_value)
                                        if normalized_candidate and values_match({normalized_candidate}, expected):
                                            display_occupation = candidate_value
                                            break
                                    if display_occupation:
                                        break
                            if display_occupation:
                                occupation_display_map[user_id] = display_occupation

                    # â­ REGION ê²€ì¦
                    if DemographicType.REGION in filters_to_validate:
                        expected = set()
                        for demo in demographic_filters[DemographicType.REGION]:
                            expected.update(build_expected_values(demo))

                        # metadataì—ì„œ region ê°€ì ¸ì˜¤ê¸°
                        region_val = metadata.get("region") if isinstance(metadata, dict) else None
                        if region_val:
                            normalized_region = normalize_value(region_val)
                            if normalized_region:
                                region_pass = values_match({normalized_region}, expected)
                                if not region_pass:
                                    region_filter_failed += 1
                            else:
                                region_pass = False
                                region_filter_failed += 1
                        else:
                            region_metadata_missing += 1
                            region_pass = False

                    # â­ MARITAL_STATUS ê²€ì¦
                    if DemographicType.MARITAL_STATUS in filters_to_validate:
                        expected = set()
                        for demo in demographic_filters[DemographicType.MARITAL_STATUS]:
                            expected.update(build_expected_values(demo))

                        # metadataì—ì„œ marital_status ê°€ì ¸ì˜¤ê¸°
                        marital_val = metadata.get("marital_status") if isinstance(metadata, dict) else None

                        if not marital_val:
                            # qa_pairsì—ì„œë„ ì°¾ì•„ë³´ê¸°
                            qa_sources: List[List[Dict[str, Any]]] = []
                            if isinstance(source, dict):
                                qa_sources.append(source.get("qa_pairs", []) or [])

                            for qa_pairs in qa_sources:
                                for qa in qa_pairs:
                                    if not isinstance(qa, dict):
                                        continue
                                    q_text = str(qa.get("q_text", "")).lower()
                                    if not any(keyword in q_text for keyword in ("ê²°í˜¼", "í˜¼ì¸", "marital")):
                                        continue
                                    answer = qa.get("answer") or qa.get("answer_text")
                                    if answer:
                                        marital_val = str(answer)
                                        break
                                if marital_val:
                                    break

                        if marital_val:
                            normalized_marital = normalize_value(marital_val)
                            if normalized_marital:
                                marital_status_pass = values_match({normalized_marital}, expected)
                                if not marital_status_pass:
                                    marital_status_filter_failed += 1
                            else:
                                marital_status_pass = False
                                marital_status_filter_failed += 1
                        else:
                            marital_status_metadata_missing += 1
                            marital_status_pass = False

                    # â­ SUB_REGION ê²€ì¦
                    if DemographicType.SUB_REGION in filters_to_validate:
                        expected = set()
                        for demo in demographic_filters[DemographicType.SUB_REGION]:
                            expected.update(build_expected_values(demo))

                        # metadataì—ì„œ sub_region ê°€ì ¸ì˜¤ê¸°
                        sub_region_val = metadata.get("sub_region") if isinstance(metadata, dict) else None
                        if sub_region_val:
                            normalized_sub_region = normalize_value(sub_region_val)
                            if normalized_sub_region:
                                sub_region_pass = values_match({normalized_sub_region}, expected)
                                if not sub_region_pass:
                                    sub_region_filter_failed += 1
                            else:
                                sub_region_pass = False
                                sub_region_filter_failed += 1
                        else:
                            sub_region_metadata_missing += 1
                            sub_region_pass = False

                    # â­ Behavioral ê²€ì¦: OpenSearchëŠ” í›„ë³´ë¥¼ ë„“ê²Œ ê°€ì ¸ì˜¤ê³ , Pythonì—ì„œ ì •í™•íˆ ê²€ì¦
                    behavior_pass = True
                    if analysis.behavioral_conditions:
                        for condition_key, expected_value in analysis.behavioral_conditions.items():
                            actual_value = behavior_values.get(condition_key)
                            if actual_value is None:
                                behavior_metadata_missing += 1
                                behavior_pass = False
                                break
                            if actual_value != expected_value:
                                behavior_filter_failed += 1
                                behavior_pass = False
                                break

                    # â­ ëª¨ë“  demographic í•„í„° ê²€ì¦ (REGION, MARITAL_STATUS, SUB_REGION í¬í•¨)
                    if gender_pass and age_pass and occupation_pass and region_pass and marital_status_pass and sub_region_pass and behavior_pass:
                        filtered_list.append(doc)

                filter_duration_ms = (perf_counter() - filter_start) * 1000
                timings["post_filter_ms"] = filter_duration_ms
                filtered_rrf_results = filtered_list
                total_hits = len(filtered_rrf_results)

                logger.info(f"  - ì†ŒìŠ¤ ëˆ„ë½ ë¬¸ì„œ: {source_not_found_count}ê±´")
                if DemographicType.GENDER in filters_to_validate:
                    logger.info(f"  - ì„±ë³„ metadata ì—†ìŒ: {gender_metadata_missing}ê±´")
                    logger.info(f"  - ì„±ë³„ í•„í„° ë¯¸ì¶©ì¡±: {gender_filter_failed}ê±´")
                if DemographicType.AGE in filters_to_validate:
                    logger.info(f"  - ì—°ë ¹ metadata ì—†ìŒ: {age_metadata_missing}ê±´")
                    logger.info(f"  - ì—°ë ¹ í•„í„° ë¯¸ì¶©ì¡±: {age_filter_failed}ê±´")
                if DemographicType.OCCUPATION in filters_to_validate:
                    logger.info(f"  - ì§ì—… metadata ì—†ìŒ: {occupation_metadata_missing}ê±´")
                    logger.info(f"  - ì§ì—… í•„í„° ë¯¸ì¶©ì¡±: {occupation_filter_failed}ê±´")
                if DemographicType.REGION in filters_to_validate:
                    logger.info(f"  - ì§€ì—­ metadata ì—†ìŒ: {region_metadata_missing}ê±´")
                    logger.info(f"  - ì§€ì—­ í•„í„° ë¯¸ì¶©ì¡±: {region_filter_failed}ê±´")
                if DemographicType.MARITAL_STATUS in filters_to_validate:
                    logger.info(f"  - ê²°í˜¼ì—¬ë¶€ metadata ì—†ìŒ: {marital_status_metadata_missing}ê±´")
                    logger.info(f"  - ê²°í˜¼ì—¬ë¶€ í•„í„° ë¯¸ì¶©ì¡±: {marital_status_filter_failed}ê±´")
                if DemographicType.SUB_REGION in filters_to_validate:
                    logger.info(f"  - ì„¸ë¶€ì§€ì—­ metadata ì—†ìŒ: {sub_region_metadata_missing}ê±´")
                    logger.info(f"  - ì„¸ë¶€ì§€ì—­ í•„í„° ë¯¸ì¶©ì¡±: {sub_region_filter_failed}ê±´")
                logger.info(f"  - í•„í„° ì¡°ê±´ ì¶©ì¡± ë¬¸ì„œ: {len(filtered_rrf_results)}ê±´")
                if analysis.behavioral_conditions:
                    logger.info(f"  âœ… í–‰ë™ í•„í„° ê²€ì¦ ì™„ë£Œ")
                    logger.info(f"  - í–‰ë™ ì •ë³´ ì—†ìŒ: {behavior_metadata_missing}ê±´")
                    logger.info(f"  - í–‰ë™ í•„í„° ë¯¸ì¶©ì¡±: {behavior_filter_failed}ê±´")
        else:
            timings.setdefault('post_filter_ms', timings.get('post_filter_ms', 0.0))

            def collect_doc_values(
                user_id: str,
                source: Dict[str, Any],
                metadata: Dict[str, Any],
                _unused: Dict[str, Any],  # í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€í•˜ì§€ë§Œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
            ) -> Tuple[Dict[DemographicType, Set[str]], Dict[DemographicType, bool], Dict[str, Optional[bool]]]:
                doc_values: Dict[DemographicType, Set[str]] = {
                    DemographicType.GENDER: set(),
                    DemographicType.AGE: set(),
                    DemographicType.OCCUPATION: set(),
                }
                metadata_presence: Dict[DemographicType, bool] = {
                    DemographicType.GENDER: False,
                    DemographicType.AGE: False,
                    DemographicType.OCCUPATION: False,
                }
                behavior_values: Dict[str, Optional[bool]] = {
                    "smoker": None,
                    "has_vehicle": None,
                }

                def record_behavior(key: str, value: Optional[bool]) -> None:
                    if value is None:
                        return
                    if behavior_values.get(key) is None:
                        behavior_values[key] = value

                def parse_yes_no(text: Optional[str]) -> Optional[bool]:
                    if not text:
                        return None
                    normalized = text.lower()
                    if any(keyword in normalized for keyword in BEHAVIOR_NO_TOKENS):
                        return False
                    if any(keyword in normalized for keyword in BEHAVIOR_YES_TOKENS):
                        return True
                    return None

                def parse_smoker_answer(raw: Optional[Any]) -> Optional[bool]:
                    if raw is None:
                        return None
                    if isinstance(raw, (list, tuple, set)):
                        for item in raw:
                            decision = parse_smoker_answer(item)
                            if decision is not None:
                                return decision
                        return None
                    text = str(raw).strip()
                    if not text:
                        return None
                    normalized = text.lower()
                    compact = normalized.replace(" ", "")
                    for keyword in SMOKER_NEGATIVE_KEYWORDS:
                        keyword_compact = keyword.replace(" ", "")
                        if keyword in normalized or keyword_compact in compact:
                            return False
                    for keyword in SMOKER_POSITIVE_KEYWORDS:
                        keyword_compact = keyword.replace(" ", "")
                        if keyword in normalized or keyword_compact in compact:
                            return True
                    if (
                        "ë‹´ë°°" in normalized
                        and not any(token in normalized for token in ("ì—†", "ì•ˆ", "ì•Š", "no", "ë¬´", "ë¯¸í¡ì—°"))
                    ):
                        return True
                    return parse_yes_no(text)

                metadata_candidates = [
                    metadata,  # ì´ì œ ë‹¨ì¼ metadataë§Œ ì‚¬ìš©
                    source.get("metadata", {}) if isinstance(source, dict) else {},
                ]

                payload = {}
                if isinstance(source, dict):
                    payload_candidate = source.get("payload")
                    if isinstance(payload_candidate, dict):
                        payload = payload_candidate
                if not payload and isinstance(source, dict) and "doc" in source:
                    doc_payload = source.get("doc", {}).get("payload")
                    if isinstance(doc_payload, dict):
                        payload = doc_payload
                if not payload and isinstance(source, dict):
                    payload = source

                if isinstance(payload, dict):
                    metadata_candidates.append(payload.get("metadata", {}))

                for candidate in metadata_candidates:
                    if not isinstance(candidate, dict):
                        continue

                    gender_val = candidate.get("gender") or candidate.get("gender_code")
                    if gender_val:
                        normalized_gender = normalize_value(gender_val)
                        if normalized_gender:
                            doc_values[DemographicType.GENDER].add(normalized_gender)
                            metadata_presence[DemographicType.GENDER] = True

                    age_group_val = candidate.get("age_group")
                    if age_group_val:
                        normalized_age_group = normalize_value(age_group_val)
                        if normalized_age_group:
                            doc_values[DemographicType.AGE].add(normalized_age_group)
                            metadata_presence[DemographicType.AGE] = True

                    age_val = candidate.get("age")
                    if age_val:
                        pre_count = len(doc_values[DemographicType.AGE])
                        add_age_decade(doc_values[DemographicType.AGE], age_val)
                        if len(doc_values[DemographicType.AGE]) > pre_count:
                            metadata_presence[DemographicType.AGE] = True

                    birth_year_val = candidate.get("birth_year")
                    if birth_year_val:
                        normalized_birth_year = normalize_value(birth_year_val)
                        if normalized_birth_year:
                            doc_values[DemographicType.AGE].add(normalized_birth_year)
                            metadata_presence[DemographicType.AGE] = True

                    occupation_val = candidate.get("occupation") or candidate.get("job")
                    if occupation_val:
                        normalized_occupation = normalize_value(occupation_val)
                        if normalized_occupation:
                            doc_values[DemographicType.OCCUPATION].add(normalized_occupation)
                            metadata_presence[DemographicType.OCCUPATION] = True

                    job_group_val = candidate.get("job_group") or candidate.get("occupation_group")
                    if job_group_val:
                        normalized_job_group = normalize_value(job_group_val)
                        if normalized_job_group:
                            doc_values[DemographicType.OCCUPATION].add(normalized_job_group)
                            metadata_presence[DemographicType.OCCUPATION] = True

                # QA ê¸°ë°˜ ë³´ì™„ (ì§ì—…) - ë©”íƒ€ë°ì´í„°ê°€ ë¹„ì—ˆì„ ë•Œë§Œ ì‚¬ìš©
                if not metadata_presence[DemographicType.OCCUPATION]:
                    qa_sources: List[List[Dict[str, Any]]] = []
                    if isinstance(source, dict):
                        qa_sources.append(source.get("qa_pairs", []) or [])
                    # â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ welcome_2nd_batch ì œê±°

                    for qa_pairs in qa_sources:
                        for qa in qa_pairs:
                            if not isinstance(qa, dict):
                                continue
                            q_text = str(qa.get("q_text", "")).lower()
                            answer_text = qa.get("answer") or qa.get("answer_text")
                            if not answer_text:
                                continue
                            if any(keyword in q_text for keyword in ("ì§ì—…", "ì§ë¬´", "occupation", "ì§ì¢…")):
                                normalized_answer = normalize_value(answer_text)
                                if normalized_answer:
                                    doc_values[DemographicType.OCCUPATION].add(normalized_answer)

                # Normalize
                for demo_type, values in doc_values.items():
                    normalized = {normalize_value(v) for v in values if v}
                    if demo_type == DemographicType.GENDER:
                        expand_gender_aliases(normalized)
                    doc_values[demo_type] = normalized

                return doc_values, metadata_presence, behavior_values

            filtered_list: List[Dict[str, Any]] = []
            source_not_found_count = 0
            gender_filter_failed = 0
            age_filter_failed = 0
            occupation_filter_failed = 0
            gender_metadata_missing = 0
            age_metadata_missing = 0
            occupation_metadata_missing = 0
            behavior_filter_failed = 0
            behavior_metadata_missing = 0
            for doc in rrf_results:
                source = doc.get("_source")
                if not source and "doc" in doc:
                    source = doc.get("doc", {}).get("_source")
                if not source and "payload" in doc:
                    source = doc.get("payload")

                if not isinstance(source, dict):
                    source_not_found_count += 1
                    continue

                user_id = source.get("user_id") or doc.get("_id") or doc.get("id")
                if not user_id and "payload" in doc and isinstance(doc["payload"], dict):
                    user_id = doc["payload"].get("user_id")

                if not user_id:
                    source_not_found_count += 1
                    continue

                # â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ welcome_1st/welcome_2nd ë°°ì¹˜ ì œê±°
                # sourceì—ì„œ ì§ì ‘ metadata ê°€ì ¸ì˜¤ê¸°
                metadata = source.get("metadata", {}) if isinstance(source.get("metadata"), dict) else {}

                doc_values, metadata_presence, behavior_values = collect_doc_values(user_id, source, metadata, {})
                behavior_values_map[user_id] = dict(behavior_values)

                gender_pass = True
                age_pass = True
                occupation_pass = True

                if demographic_filters.get(DemographicType.GENDER):
                    expected = set()
                    for demo in demographic_filters[DemographicType.GENDER]:
                        expected.update(build_expected_values(demo))

                    # â­ Metadata ìš°ì„ : metadataê°€ ìˆìœ¼ë©´ metadataë§Œ í™•ì¸
                    if metadata_presence[DemographicType.GENDER]:
                        # metadataë¡œ ìˆ˜ì§‘ëœ ê°’ë§Œ ì‚¬ìš©
                        gender_from_metadata = set()
                        for meta_source in [metadata, source.get("metadata", {})]:
                            if isinstance(meta_source, dict):
                                gender_val = meta_source.get("gender") or meta_source.get("gender_code")
                                if gender_val:
                                    normalized_gender = normalize_value(gender_val)
                                    if normalized_gender:
                                        gender_from_metadata.add(normalized_gender)

                        if gender_from_metadata:
                            expand_gender_aliases(gender_from_metadata)
                            gender_pass = values_match(gender_from_metadata, expected)
                        else:
                            gender_metadata_missing += 1
                            gender_pass = False
                    else:
                        # metadata ì—†ìœ¼ë©´ qa_pairs ì‚¬ìš©
                        gender_metadata_missing += 1
                        gender_pass = values_match(doc_values[DemographicType.GENDER], expected)

                    if not gender_pass:
                        gender_filter_failed += 1

                if gender_pass and demographic_filters.get(DemographicType.AGE):
                    expected = set()
                    for demo in demographic_filters[DemographicType.AGE]:
                        expected.update(build_expected_values(demo))
                    if not metadata_presence[DemographicType.AGE]:
                        age_metadata_missing += 1
                    age_pass = values_match(doc_values[DemographicType.AGE], expected)
                    if not age_pass:
                        age_filter_failed += 1

                if gender_pass and age_pass and demographic_filters.get(DemographicType.OCCUPATION):
                    expected = set()
                    for demo in demographic_filters[DemographicType.OCCUPATION]:
                        expected.update(build_expected_values(demo))
                    if not metadata_presence[DemographicType.OCCUPATION]:
                        occupation_metadata_missing += 1
                    occupation_pass = values_match(doc_values[DemographicType.OCCUPATION], expected)
                    if not occupation_pass:
                        occupation_filter_failed += 1
                    else:
                        display_occupation = None
                        occupation_candidates = [
                            metadata.get("occupation") if isinstance(metadata, dict) else None,
                            metadata.get("job") if isinstance(metadata, dict) else None,
                            metadata.get("occupation_group") if isinstance(metadata, dict) else None,
                        ]
                        for candidate in occupation_candidates:
                            normalized_candidate = normalize_value(candidate)
                            if normalized_candidate and values_match({normalized_candidate}, expected):
                                display_occupation = str(candidate)
                                break
                        if not display_occupation:
                            qa_sources: List[List[Dict[str, Any]]] = []
                            if isinstance(source, dict):
                                qa_sources.append(source.get("qa_pairs", []) or [])
                            # â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ welcome_2nd_doc_full ì œê±°
                            for qa_pairs in qa_sources:
                                for qa in qa_pairs:
                                    if not isinstance(qa, dict):
                                        continue
                                    q_text = str(qa.get("q_text", "")).lower()
                                    if not any(keyword in q_text for keyword in ("ì§ì—…", "ì§ë¬´", "occupation", "ì§ì¢…")):
                                        continue
                                    answer = qa.get("answer")
                                    if answer is None:
                                        answer = qa.get("answer_text")
                                    if answer is None:
                                        continue
                                    candidate = str(answer)
                                    normalized_candidate = normalize_value(candidate)
                                    if normalized_candidate and values_match({normalized_candidate}, expected):
                                        display_occupation = candidate
                                        break
                                if display_occupation:
                                    break
                        if display_occupation:
                            occupation_display_map[user_id] = display_occupation

                # â­ Behavioral ê²€ì¦: OpenSearchëŠ” í›„ë³´ë¥¼ ë„“ê²Œ ê°€ì ¸ì˜¤ê³ , Pythonì—ì„œ ì •í™•íˆ ê²€ì¦
                behavior_pass = True
                if analysis.behavioral_conditions:
                    for condition_key, expected_value in analysis.behavioral_conditions.items():
                        actual_value = behavior_values.get(condition_key)
                        if actual_value is None:
                            behavior_metadata_missing += 1
                            behavior_pass = False
                            break
                        if actual_value != expected_value:
                            behavior_filter_failed += 1
                            behavior_pass = False
                            break

                if gender_pass and age_pass and occupation_pass and behavior_pass:
                    filtered_list.append(doc)

            filter_duration_ms = (perf_counter() - filter_start) * 1000
            timings["post_filter_ms"] = filter_duration_ms
            filtered_rrf_results = filtered_list

            logger.info(f"  - ì†ŒìŠ¤ ëˆ„ë½ ë¬¸ì„œ: {source_not_found_count}ê±´")
            if demographic_filters.get(DemographicType.GENDER):
                logger.info(f"  - ì„±ë³„ metadata ì—†ìŒ: {gender_metadata_missing}ê±´")
            logger.info(f"  - ì„±ë³„ í•„í„° ë¯¸ì¶©ì¡±: {gender_filter_failed}ê±´")
            if demographic_filters.get(DemographicType.AGE):
                logger.info(f"  - ì—°ë ¹ metadata ì—†ìŒ: {age_metadata_missing}ê±´")
            logger.info(f"  - ì—°ë ¹ í•„í„° ë¯¸ì¶©ì¡±: {age_filter_failed}ê±´")
            if demographic_filters.get(DemographicType.OCCUPATION):
                logger.info(f"  - ì§ì—… metadata ì—†ìŒ: {occupation_metadata_missing}ê±´")
            logger.info(f"  - ì§ì—… í•„í„° ë¯¸ì¶©ì¡±: {occupation_filter_failed}ê±´")
            logger.info(f"  - í•„í„° ì¡°ê±´ ì¶©ì¡± ë¬¸ì„œ: {len(filtered_rrf_results)}ê±´")
            if analysis.behavioral_conditions:
                logger.info(f"  âœ… í–‰ë™ í•„í„° ê²€ì¦ ì™„ë£Œ")
                logger.info(f"  - í–‰ë™ ì •ë³´ ì—†ìŒ: {behavior_metadata_missing}ê±´")
                logger.info(f"  - í–‰ë™ í•„í„° ë¯¸ì¶©ì¡±: {behavior_filter_failed}ê±´")

        lazy_join_start = perf_counter()
        final_hits = filtered_rrf_results[:window_size]
        results: List[SearchResult] = []
        inner_hits_map: Dict[str, List[Dict[str, Any]]] = {}

        for doc in final_hits:
            source = doc.get("_source")
            if not source and "doc" in doc:
                source = doc.get("doc", {}).get("_source")
            if not source and "payload" in doc:
                source = doc.get("payload")
            if not isinstance(source, dict):
                source = {}

            payload = {}
            payload_candidate = source.get("payload")
            if isinstance(payload_candidate, dict):
                payload = payload_candidate
            elif isinstance(doc.get("payload"), dict):
                payload = doc["payload"]

            user_id = (
                source.get("user_id")
                or payload.get("user_id")
                or doc.get("_id")
                or doc.get("id")
            )

            doc_info = None
            if user_id and user_id in user_doc_map:
                doc_info = user_doc_map[user_id]
            elif doc.get("_id") and doc.get("_id") in id_doc_map:
                doc_info = id_doc_map[doc.get("_id")]

            if doc_info:
                src_info = doc_info.get("source")
                if isinstance(src_info, dict):
                    merged_source = {}
                    merged_source.update(src_info)
                    merged_source.update(source)
                    source = merged_source
                    
                inner_hit_wrapper = {"inner_hits": doc_info.get("inner_hits", {})}
            else:
                inner_hit_wrapper = doc

            # â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ sourceì˜ metadataì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
            source_metadata = source.get("metadata", {}) if isinstance(source, dict) else {}
            if not source_metadata:
                # sourceê°€ ë¹„ì–´ìˆìœ¼ë©´ docì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
                doc_source = doc.get("_source") or {}
                if isinstance(doc_source, dict):
                    source_metadata = doc_source.get("metadata", {})
                # payloadì—ì„œë„ í™•ì¸
                if not source_metadata and isinstance(payload, dict):
                    source_metadata = payload.get("metadata", {})

            behavioral_values = behavior_values_map.get(user_id, {}) if user_id else {}
            behavioral_info: Dict[str, Any] = {}
            if behavioral_values.get("smoker") is not None:
                behavioral_info["smoker"] = behavioral_values.get("smoker")
            if behavioral_values.get("has_vehicle") is not None:
                behavioral_info["has_vehicle"] = behavioral_values.get("has_vehicle")
            if behavioral_values.get("drinker") is not None:
                behavioral_info["drinker"] = behavioral_values.get("drinker")

            demographic_info: Dict[str, Any] = {}
            if source_metadata:
                demographic_info["age_group"] = source_metadata.get("age_group")
                demographic_info["gender"] = source_metadata.get("gender")
                demographic_info["birth_year"] = source_metadata.get("birth_year")
                demographic_info["region"] = source_metadata.get("region")
                demographic_info["occupation"] = source_metadata.get("occupation")
                demographic_info["marital_status"] = source_metadata.get("marital_status")
                demographic_info["sub_region"] = source_metadata.get("sub_region")

            occupation_expected = set()
            for demo in demographic_filters.get(DemographicType.OCCUPATION, []):
                occupation_expected.update(build_expected_values(demo))

            if ("occupation" not in demographic_info or not demographic_info["occupation"]) and user_id:
                mapped_occupation = occupation_display_map.get(user_id) if has_demographic_filters else None
                if mapped_occupation:
                    demographic_info["occupation"] = mapped_occupation

            def occupation_matches(candidate: str) -> bool:
                normalized_candidate = normalize_value(candidate)
                if not normalized_candidate:
                    return False
                for expected in occupation_expected:
                    if not expected:
                        continue
                    if normalized_candidate == expected or normalized_candidate in expected or expected in normalized_candidate:
                        return True
                return False

            if ("occupation" not in demographic_info or not demographic_info["occupation"]) and isinstance(source, dict):
                qa_pairs_for_occ = source.get("qa_pairs", [])
                for qa in qa_pairs_for_occ:
                    if not isinstance(qa, dict):
                        continue
                    q_text = str(qa.get("q_text", "")).lower()
                    answer = qa.get("answer")
                    if answer is None:
                        answer = qa.get("answer_text")
                    if answer is None:
                        continue
                    answer_str = str(answer)
                    if any(keyword in q_text for keyword in ("ì§ì—…", "ì§ë¬´", "occupation", "ì§ì¢…")) and occupation_matches(answer_str):
                        demographic_info["occupation"] = answer_str
                        break

            # marital_statusë¥¼ qa_pairsì—ì„œ ì°¾ê¸°
            if ("marital_status" not in demographic_info or not demographic_info["marital_status"]) and isinstance(source, dict):
                qa_pairs_list = source.get("qa_pairs", [])
                for qa in qa_pairs_list:
                    if not isinstance(qa, dict):
                        continue
                    q_text = str(qa.get("q_text", "")).lower()
                    answer = qa.get("answer")
                    if answer is None:
                        answer = qa.get("answer_text")
                    if answer is None:
                        continue
                    answer_str = str(answer)
                    if any(keyword in q_text for keyword in ("ê²°í˜¼", "í˜¼ì¸")):
                        demographic_info["marital_status"] = answer_str
                        break

            # sub_regionì„ qa_pairsì—ì„œ ì°¾ê¸°
            if ("sub_region" not in demographic_info or not demographic_info["sub_region"]) and isinstance(source, dict):
                qa_pairs_list = source.get("qa_pairs", [])
                for qa in qa_pairs_list:
                    if not isinstance(qa, dict):
                        continue
                    q_text = str(qa.get("q_text", "")).lower()
                    answer = qa.get("answer")
                    if answer is None:
                        answer = qa.get("answer_text")
                    if answer is None:
                        continue
                    answer_str = str(answer)
                    if any(keyword in q_text for keyword in ("êµ¬", "êµ°", "ì„¸ë¶€ì§€ì—­", "ì–´ëŠ êµ¬")):
                        demographic_info["sub_region"] = answer_str
                        break

            matched_qa_pairs: List[Dict[str, Any]] = extract_inner_hit_matches(inner_hit_wrapper)
            if not matched_qa_pairs and analysis.must_terms:
                matched_qa_pairs = extract_matched_qa_pairs(source, analysis.must_terms)

            # â­ ê°œì„ : sourceì— qa_pairsê°€ ì—†ìœ¼ë©´ docì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
            qa_pairs_from_source = source.get("qa_pairs", []) if isinstance(source, dict) else []
            if not qa_pairs_from_source:
                # sourceê°€ ë¹„ì–´ìˆìœ¼ë©´ docì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
                doc_source = doc.get("_source") or {}
                if isinstance(doc_source, dict):
                    qa_pairs_from_source = doc_source.get("qa_pairs", [])
                # payloadì—ì„œë„ í™•ì¸
                if not qa_pairs_from_source and isinstance(payload, dict):
                    qa_pairs_from_source = payload.get("qa_pairs", [])

            qa_pairs_display = reorder_with_matches(
                qa_pairs_from_source if isinstance(qa_pairs_from_source, list) else [],
                matched_qa_pairs,
                limit=10
            )

            results.append(
                SearchResult(
                    user_id=user_id,
                    score=doc.get("_score", 0.0),
                    timestamp=source.get("timestamp") if isinstance(source, dict) else None,
                    demographic_info=demographic_info if demographic_info else None,
                    behavioral_info=behavioral_info if behavioral_info else None,
                    qa_pairs=qa_pairs_display[:5],
                    matched_qa_pairs=matched_qa_pairs,
                    highlights=doc.get("highlight"),
                )
            )
           

        timings["lazy_join_ms"] = (perf_counter() - lazy_join_start) * 1000
        timings.setdefault('post_filter_ms', timings.get('post_filter_ms', 0.0))
        timings.setdefault('rrf_recombination_ms', 0.0)
        timings.setdefault('qdrant_parallel_ms', 0.0)
        timings.setdefault(
            'opensearch_parallel_ms',
            timings.get('two_phase_stage1_ms', 0.0) + timings.get('two_phase_stage2_ms', 0.0)
        )

        total_duration_ms = (perf_counter() - overall_start) * 1000
        timings['total_ms'] = total_duration_ms
        timings['cache_hit'] = 1.0 if cache_hit else 0.0

        serialized_results = [_serialize_result(res) for res in results]
        stored_items = serialized_results
        if cache_enabled and cache_limit > 0:
            stored_items = serialized_results[:cache_limit]
        page_results, has_more_local = _slice_results(stored_items, page, page_size)
        has_more = has_more_local and ((page * page_size) < total_hits)
        total_hits = len(filtered_rrf_results)
        max_score = results[0].score if results else 0.0
        response_took_ms = int(total_duration_ms)

        logger.info("ğŸ“ˆ ì„±ëŠ¥ ì¸¡ì • ìš”ì•½ (ms):")
        for key in sorted(timings.keys()):
            logger.info(f"  - {key}: {timings[key]:.2f}")

        summary_parts = [
            f"returned={len(page_results)}/{total_hits}",
            f"total_ms={response_took_ms}",
        ]
        if rrf_k_used is not None:
            summary_parts.append(f"rrf_k={rrf_k_used}")
        if adaptive_threshold is not None:
            summary_parts.append(f"qdrant_threshold={adaptive_threshold:.2f}")
        logger.info("âœ… ìµœì¢… ìš”ì•½: " + ", ".join(summary_parts))
        if rrf_reason:
            logger.info(f"   â€¢ RRF: {rrf_reason}")
        if threshold_reason:
            logger.info(f"   â€¢ Qdrant: {threshold_reason}")

        _log_final_summary(
            stage="search_nl",
            query=request.query,
            analysis=analysis,
            total_hits=total_hits,
            returned_count=len(page_results),
            page=page,
            page_size=page_size,
            cache_hit=cache_hit,
            timings=timings,
            took_ms=total_duration_ms,
            filters=filters_for_response,
            behavioral_conditions=getattr(analysis, "behavioral_conditions", {}),
            use_claude=use_claude,
        )

        if cache_enabled and cache_key and stored_items:
            cache_payload = {
                "total_hits": total_hits,
                "max_score": max_score,
                "items": stored_items,
                "page_size": page_size,
                "filters": filters_for_response,
                "extracted_entities": extracted_entities.to_dict(),
                "behavioral_conditions": getattr(analysis, "behavioral_conditions", {}),
                "use_claude": bool(use_claude),
            }
            try:
                cache_client.setex(
                    cache_key,
                    cache_ttl,
                    json.dumps(cache_payload, ensure_ascii=False),
                )
                logger.info(f"ğŸ’¾ Redis ê²€ìƒ‰ ìºì‹œ ì €ì¥: key={cache_key}, ttl={cache_ttl}s")
            except Exception as cache_exc:
                logger.warning(f"âš ï¸ Redis ê²€ìƒ‰ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {cache_exc}")

        response = SearchResponse(
            query=request.query,
            total_hits=total_hits,
            max_score=max_score,
            results=page_results,
            query_analysis={
                "intent": analysis.intent,
                "must_terms": analysis.must_terms,
                "should_terms": analysis.should_terms,
                "alpha": analysis.alpha,
                "confidence": analysis.confidence,
                "filters": filters_for_response,
                "size": page_size,
                "timings_ms": timings,
                "extracted_entities": extracted_entities.to_dict(),
                "behavioral_conditions": getattr(analysis, "behavioral_conditions", {}),
                "use_claude_analyzer": bool(use_claude),
            },
            took_ms=response_took_ms,
            page=page,
            page_size=page_size,
            has_more=has_more,
        )
        return _finalize_search_response(
            request=request,
            response=response,
            analysis=analysis,
            cache_hit=cache_hit,
            timings=timings,
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[ERROR] ìì—°ì–´ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# -----------------------------
# í”„ë¡ íŠ¸ì—”ë“œ ì¹œí™”ì  ê°„ì†Œí™” ì—”ë“œí¬ì¸íŠ¸
# -----------------------------




class TestFiltersRequest(BaseModel):
    """í•„í„° í…ŒìŠ¤íŠ¸ ìš”ì²­"""
    filters: List[Dict[str, Any]] = Field(..., description="í…ŒìŠ¤íŠ¸í•  í•„í„° ë¦¬ìŠ¤íŠ¸")
    index_name: str = Field(default="*", description="ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ì´ë¦„")


@router.post("/debug/test-filters", summary="í•„í„° ê°œë³„ í…ŒìŠ¤íŠ¸ (ë””ë²„ê¹…ìš©)")
async def test_filters(
    request: TestFiltersRequest,
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """
    í•„í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ì—¬ ì–´ë–¤ ì¸ë±ìŠ¤ì—ì„œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
    
    - ê° í•„í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì‹¤í–‰
    - ì¸ë±ìŠ¤ë³„ ê²°ê³¼ ê°œìˆ˜ í™•ì¸
    
    ì‚¬ìš© ì˜ˆì‹œ:
    ```json
    {
      "filters": [
        {
          "bool": {
            "should": [
              {"term": {"metadata.age_group.keyword": "30ëŒ€"}}
            ],
            "minimum_should_match": 1
          }
        }
      ],
      "index_name": "*"
    }
    ```
    """
    try:
        if not os_client or not os_client.ping():
            raise HTTPException(status_code=503, detail="OpenSearch ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        filters = request.filters
        index_name = request.index_name
        
        results = []
        for i, filter_dict in enumerate(filters):
            # ê° í•„í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            query = {
                "query": {
                    "bool": {
                        "must": [filter_dict]
                    }
                },
                "size": 0,  # ê°œìˆ˜ë§Œ í™•ì¸
                "aggs": {
                    "by_index": {
                        "terms": {
                            "field": "_index",
                            "size": 20
                        }
                    }
                }
            }
            
            response = os_client.search(
                index=request.index_name,
                body=query
            )
            
            # ì¸ë±ìŠ¤ë³„ ê²°ê³¼ ê°œìˆ˜
            index_counts = {}
            if 'aggregations' in response and 'by_index' in response['aggregations']:
                for bucket in response['aggregations']['by_index']['buckets']:
                    index_counts[bucket['key']] = bucket['doc_count']
            
            results.append({
                "filter_index": i,
                "filter": filter_dict,
                "total_hits": response['hits']['total']['value'],
                "index_counts": index_counts
            })
        
        # ëª¨ë“  í•„í„°ë¥¼ ANDë¡œ ê²°í•©í•œ ê²°ê³¼ë„ í…ŒìŠ¤íŠ¸
        if len(filters) > 1:
            combined_query = {
                "query": {
                    "bool": {
                        "must": filters
                    }
                },
                "size": 0,
                "aggs": {
                    "by_index": {
                        "terms": {
                            "field": "_index",
                            "size": 20
                        }
                    }
                }
            }
            
            combined_response = os_client.search(
                index=request.index_name,
                body=combined_query
            )
            
            combined_index_counts = {}
            if 'aggregations' in combined_response and 'by_index' in combined_response['aggregations']:
                for bucket in combined_response['aggregations']['by_index']['buckets']:
                    combined_index_counts[bucket['key']] = bucket['doc_count']
            
            results.append({
                "filter_index": "combined",
                "filter": "ALL FILTERS (AND)",
                "total_hits": combined_response['hits']['total']['value'],
                "index_counts": combined_index_counts
            })
        
        return {
            "index_name": request.index_name,
            "results": results
        }
    
    except Exception as e:
        logger.error(f"[ERROR] í•„í„° í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get(
    "/logs/conversation/{session_id}",
    summary="ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ (Redis)",
)
async def get_conversation_logs_endpoint(
    session_id: str,
    limit: int = 50,
) -> Dict[str, Any]:
    client = getattr(router, "redis_client", None)
    if client is None:
        raise HTTPException(status_code=503, detail="Redis í´ë¼ì´ì–¸íŠ¸ê°€ êµ¬ì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if limit <= 0:
        raise HTTPException(status_code=400, detail="limit ê°’ì€ 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    key = _make_conversation_key(
        getattr(router, "conversation_history_prefix", None),
        session_id,
    )
    if not key:
        raise HTTPException(status_code=400, detail="session_id ë˜ëŠ” prefixê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    raw_items = client.lrange(key, -limit, -1)
    messages: List[ConversationMessage] = []
    for item in raw_items:
        parsed = _parse_conversation_record(item)
        if parsed is not None:
            messages.append(parsed)

    return {
        "session_id": session_id,
        "count": len(messages),
        "messages": [msg.model_dump() for msg in messages],
    }


@router.get(
    "/logs/search-history/{owner_id}",
    summary="ê²€ìƒ‰ ì´ë ¥ ì¡°íšŒ (Redis)",
)
async def get_search_history_endpoint(
    owner_id: str,
    limit: int = 50,
) -> Dict[str, Any]:
    client = getattr(router, "redis_client", None)
    if client is None:
        raise HTTPException(status_code=503, detail="Redis í´ë¼ì´ì–¸íŠ¸ê°€ êµ¬ì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if limit <= 0:
        raise HTTPException(status_code=400, detail="limit ê°’ì€ 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    key = _make_history_key(
        getattr(router, "search_history_prefix", None),
        owner_id,
    )
    if not key:
        raise HTTPException(status_code=400, detail="owner_id ë˜ëŠ” prefixê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    raw_items = client.lrange(key, -limit, -1)
    entries: List[SearchHistoryEntry] = []
    for item in raw_items:
        parsed = _parse_search_history_record(item)
        if parsed is not None:
            entries.append(parsed)

    return {
        "owner_id": owner_id,
        "count": len(entries),
        "history": [entry.model_dump() for entry in entries],
    }


def _filter_to_string(filter_dict: Dict[str, Any]) -> str:
    try:
        return json.dumps(filter_dict, ensure_ascii=False)
    except Exception:
        return str(filter_dict)


AGE_GENDER_KEYWORDS = [
    "metadata.age_group", "metadata.gender", "birth_year", "ì—°ë ¹", "ë‚˜ì´", "ì„±ë³„"
]
OCCUPATION_KEYWORDS = [
    "metadata.occupation", "occupation", "ì§ì—…", "ì§ë¬´"
]


def is_age_or_gender_filter(filter_dict: Dict[str, Any]) -> bool:
    filter_str = _filter_to_string(filter_dict)
    return any(keyword in filter_str for keyword in AGE_GENDER_KEYWORDS)


def is_occupation_filter(filter_dict: Dict[str, Any]) -> bool:
    filter_str = _filter_to_string(filter_dict)
    return any(keyword in filter_str for keyword in OCCUPATION_KEYWORDS)


async def run_two_phase_demographic_search(
    request,
    analysis,
    extracted_entities,
    filters: List[Dict[str, Any]],
    size: int,
    age_gender_filters: List[Dict[str, Any]],
    occupation_filters: List[Dict[str, Any]],
    data_fetcher: "DataFetcher",
    timings: Dict[str, float],
    overall_start: float,
) -> Optional[SearchResponse]:
    """ë‘ ë‹¨ê³„ ê²€ìƒ‰ìœ¼ë¡œ user_idë¥¼ ë¨¼ì € ì¢íˆê³  ì •ë°€ ì¡°íšŒ"""
    logger.info("ğŸš€ ë‘ ë‹¨ê³„ ì¸êµ¬í†µê³„ ìµœì í™” ì‹¤í–‰")

    async_client = data_fetcher.os_async_client
    sync_client = data_fetcher.os_client
    if not (async_client or sync_client):
        logger.warning("âš ï¸ OpenSearch í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ 2ë‹¨ê³„ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤")
        return None

    stage1_start = perf_counter()
    stage1_query_size = min(max(size * 50, 2000), 10000)
    stage1_query = {
        "query": {
            "bool": {
                "must": age_gender_filters
            }
        },
        "size": stage1_query_size,
        "_source": ["user_id"],
        "track_total_hits": True
    }

    try:
        # â­ survey_responses_mergedë§Œ ì‚¬ìš©
        if async_client:
            response_1st = await data_fetcher.search_opensearch_async(
                index_name=request.index_name,  # survey_responses_merged
                query=stage1_query,
                size=stage1_query_size,
                source_filter=None,
                request_timeout=DEFAULT_OS_TIMEOUT,
            )
        else:
            response_1st = data_fetcher.search_opensearch(
                index_name=request.index_name,  # survey_responses_merged
                query=stage1_query,
                size=stage1_query_size,
                source_filter=None,
                request_timeout=DEFAULT_OS_TIMEOUT,
            )
    except Exception as e:
        logger.warning(f"âš ï¸ 2ë‹¨ê³„ ê²€ìƒ‰ Stage1 ì‹¤íŒ¨: {e}")
        return None

    timings['two_phase_stage1_ms'] = (perf_counter() - stage1_start) * 1000
    hits_1st = response_1st.get('hits', {}).get('hits', [])
    total_stage1 = response_1st.get('hits', {}).get('total', {}).get('value', len(hits_1st))

    if not hits_1st:
        logger.info("   âš ï¸ Stage1ì—ì„œ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” user_idê°€ ì—†ìŠµë‹ˆë‹¤")
        total_time = (perf_counter() - overall_start) * 1000
        timings['total_ms'] = total_time
        timings.setdefault('two_phase_stage2_ms', 0.0)
        timings.setdefault('two_phase_fetch_demographics_ms', 0.0)
        timings.setdefault('lazy_join_ms', 0.0)
        timings.setdefault('post_filter_ms', 0.0)
        timings.setdefault('rrf_recombination_ms', 0.0)
        timings.setdefault('qdrant_parallel_ms', 0.0)
        timings.setdefault('opensearch_parallel_ms', timings['two_phase_stage1_ms'])
        logger.info("ğŸ“ˆ ì„±ëŠ¥ ì¸¡ì • ìš”ì•½ (ms):")
        for key in sorted(timings.keys()):
            logger.info(f"  - {key}: {timings[key]:.2f}")
        return SearchResponse(
            query=request.query,
            total_hits=0,
            max_score=0.0,
            results=[],
            query_analysis={
                "intent": analysis.intent,
                "must_terms": analysis.must_terms,
                "should_terms": analysis.should_terms,
                "alpha": analysis.alpha,
                "confidence": analysis.confidence,
                "extracted_entities": extracted_entities.to_dict(),
                "filters": filters,
                "size": size,
                "timings_ms": timings,
            },
            took_ms=int(total_time)
        )

    user_ids_filtered = []
    for hit in hits_1st:
        src = hit.get('_source', {})
        uid = src.get('user_id') or hit.get('_id')
        if uid:
            user_ids_filtered.append(uid)
    user_ids_filtered = list(dict.fromkeys(user_ids_filtered))

    logger.info(f"   âœ… Stage1 user_id ì¶”ì¶œ: {len(user_ids_filtered)}/{total_stage1}ê±´")
    if total_stage1 > len(user_ids_filtered):
        logger.warning("   âš ï¸ Stage1 size ì œí•œìœ¼ë¡œ ì¼ë¶€ user_idê°€ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤")

    if not user_ids_filtered:
        total_time = (perf_counter() - overall_start) * 1000
        timings['two_phase_stage2_ms'] = 0.0
        timings['two_phase_fetch_demographics_ms'] = 0.0
        timings['lazy_join_ms'] = 0.0
        timings['post_filter_ms'] = 0.0
        timings['rrf_recombination_ms'] = 0.0
        timings.setdefault('opensearch_parallel_ms', timings['two_phase_stage1_ms'])
        timings['total_ms'] = total_time
        logger.info("ğŸ“ˆ ì„±ëŠ¥ ì¸¡ì • ìš”ì•½ (ms):")
        for key in sorted(timings.keys()):
            logger.info(f"  - {key}: {timings[key]:.2f}")
        return SearchResponse(
            query=request.query,
            total_hits=0,
            max_score=0.0,
            results=[],
            query_analysis={
                "intent": analysis.intent,
                "must_terms": analysis.must_terms,
                "should_terms": analysis.should_terms,
                "alpha": analysis.alpha,
                "confidence": analysis.confidence,
                "extracted_entities": extracted_entities.to_dict(),
                "filters": filters,
                "size": size,
                "timings_ms": timings,
            },
            took_ms=int(total_time)
        )

    max_terms = 10000
    if len(user_ids_filtered) > max_terms:
        logger.warning(f"   âš ï¸ user_idê°€ {len(user_ids_filtered)}ê±´ì…ë‹ˆë‹¤. ìƒìœ„ {max_terms}ê±´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤")
        user_ids_filtered = user_ids_filtered[:max_terms]

    detail_size = max(size * 2, min(len(user_ids_filtered), 500))
    stage2_query = {
        "query": {
            "bool": {
                "must": [
                    {"terms": {"_id": user_ids_filtered}},
                ]
            }
        },
        "size": detail_size,
        "_source": {
            "includes": ["user_id", "metadata", "qa_pairs", "timestamp"]
        },
        "track_total_hits": True
    }

    stage2_start = perf_counter()
    try:
        # â­ survey_responses_mergedë§Œ ì‚¬ìš©
        if async_client:
            response_2nd = await data_fetcher.search_opensearch_async(
                index_name=request.index_name,  # survey_responses_merged
                query=stage2_query,
                size=detail_size,
                source_filter=None,
                request_timeout=DEFAULT_OS_TIMEOUT,
            )
        else:
            response_2nd = data_fetcher.search_opensearch(
                index_name=request.index_name,  # survey_responses_merged
                query=stage2_query,
                size=detail_size,
                source_filter=None,
                request_timeout=DEFAULT_OS_TIMEOUT,
            )
    except Exception as e:
        logger.warning(f"âš ï¸ 2ë‹¨ê³„ ê²€ìƒ‰ Stage2 ì‹¤íŒ¨: {e}")
        return None

    timings['two_phase_stage2_ms'] = (perf_counter() - stage2_start) * 1000
    hits_2nd = response_2nd.get('hits', {}).get('hits', [])
    total_stage2 = response_2nd.get('hits', {}).get('total', {}).get('value', len(hits_2nd))
    logger.info(f"   âœ… Stage2 ê²°ê³¼: {len(hits_2nd)}ê±´ (ì´ {total_stage2}ê±´)")

    if not hits_2nd:
        total_time = (perf_counter() - overall_start) * 1000
        timings.setdefault('two_phase_fetch_demographics_ms', 0.0)
        timings['lazy_join_ms'] = 0.0
        timings['post_filter_ms'] = 0.0
        timings['rrf_recombination_ms'] = 0.0
        timings.setdefault('opensearch_parallel_ms', timings.get('two_phase_stage1_ms', 0.0))
        timings['total_ms'] = total_time
        logger.info("ğŸ“ˆ ì„±ëŠ¥ ì¸¡ì • ìš”ì•½ (ms):")
        for key in sorted(timings.keys()):
            logger.info(f"  - {key}: {timings[key]:.2f}")
        return SearchResponse(
            query=request.query,
            total_hits=0,
            max_score=0.0,
            results=[],
            query_analysis={
                "intent": analysis.intent,
                "must_terms": analysis.must_terms,
                "should_terms": analysis.should_terms,
                "alpha": analysis.alpha,
                "confidence": analysis.confidence,
                "extracted_entities": extracted_entities.to_dict(),
                "filters": filters,
                "size": size,
                "timings_ms": timings,
            },
            took_ms=int(total_time)
        )

    final_hits = hits_2nd[:size]
    final_user_ids = [hit.get('_id') or hit.get('_source', {}).get('user_id') for hit in final_hits]

    # â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ welcome_1st/welcome_2nd ì¡°íšŒ ì œê±°
    timings['two_phase_fetch_demographics_ms'] = 0.0

    results: List[SearchResult] = []
    lazy_join_start = perf_counter()
    final_hits = final_hits if 'final_hits' in locals() else []
    for doc in final_hits:
        source = doc.get('_source', {}) or {}
        user_id = source.get('user_id') or hit.get('_id', '')
        # â­ survey_responses_mergedë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ sourceì˜ metadataì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
        source_metadata = source.get('metadata', {}) if isinstance(source, dict) else {}

        behavioral_values = behavior_values_map.get(user_id, {}) if user_id else {}
        behavioral_info: Dict[str, Any] = {}
        if behavioral_values.get("smoker") is not None:
            behavioral_info["smoker"] = behavioral_values.get("smoker")
        if behavioral_values.get("has_vehicle") is not None:
            behavioral_info["has_vehicle"] = behavioral_values.get("has_vehicle")
        if behavioral_values.get("drinker") is not None:
            behavioral_info["drinker"] = behavioral_values.get("drinker")

        demographic_info: Dict[str, Any] = {}
        if source_metadata:
            demographic_info["age_group"] = source_metadata.get("age_group")
            demographic_info["gender"] = source_metadata.get("gender")
            demographic_info["birth_year"] = source_metadata.get("birth_year")
            demographic_info["region"] = source_metadata.get("region")
            demographic_info['occupation'] = source_metadata.get('occupation')
            demographic_info["marital_status"] = source_metadata.get("marital_status")
            demographic_info["sub_region"] = source_metadata.get("sub_region")

        if 'occupation' not in demographic_info or not demographic_info['occupation']:
            qa_pairs_for_occ = source.get('qa_pairs', []) if isinstance(source, dict) else []
            for qa in qa_pairs_for_occ:
                if isinstance(qa, dict):
                    q_text = qa.get('q_text', '')
                    answer = str(qa.get('answer', qa.get('answer_text', '')))
                    if 'ì§ì—…' in q_text or 'occupation' in q_text.lower() or 'ì§ë¬´' in q_text:
                        if answer:
                            demographic_info['occupation'] = answer
                        break

        # marital_statusë¥¼ qa_pairsì—ì„œ ì°¾ê¸°
        if 'marital_status' not in demographic_info or not demographic_info['marital_status']:
            qa_pairs_list = source.get('qa_pairs', []) if isinstance(source, dict) else []
            for qa in qa_pairs_list:
                if isinstance(qa, dict):
                    q_text = qa.get('q_text', '')
                    answer = str(qa.get('answer', qa.get('answer_text', '')))
                    if 'ê²°í˜¼' in q_text or 'í˜¼ì¸' in q_text:
                        if answer:
                            demographic_info['marital_status'] = answer
                        break

        # sub_regionì„ qa_pairsì—ì„œ ì°¾ê¸°
        if 'sub_region' not in demographic_info or not demographic_info['sub_region']:
            qa_pairs_list = source.get('qa_pairs', []) if isinstance(source, dict) else []
            for qa in qa_pairs_list:
                if isinstance(qa, dict):
                    q_text = qa.get('q_text', '')
                    answer = str(qa.get('answer', qa.get('answer_text', '')))
                    if 'êµ¬' in q_text or 'êµ°' in q_text or 'ì„¸ë¶€ì§€ì—­' in q_text or 'ì–´ëŠ êµ¬' in q_text:
                        if answer:
                            demographic_info['sub_region'] = answer
                        break

        matched_qa = []
        inner_hits = hit.get('inner_hits', {}).get('qa_pairs', {}).get('hits', {}).get('hits', [])
        for inner_hit in inner_hits:
            qa_data = inner_hit.get('_source', {}).copy()
            qa_data['match_score'] = inner_hit.get('_score')
            if 'highlight' in inner_hit:
                qa_data['highlights'] = inner_hit['highlight']
            matched_qa.append(qa_data)

        results.append(
            SearchResult(
                user_id=user_id,
                score=hit.get('_score', 0.0),
                timestamp=source.get('timestamp') if isinstance(source, dict) else None,
                demographic_info=demographic_info if demographic_info else None,
                behavioral_info=behavioral_info if behavioral_info else None,
                qa_pairs=source.get('qa_pairs', [])[:5] if isinstance(source, dict) else [],
                matched_qa_pairs=matched_qa,
                highlights=hit.get('highlight'),
            )
        )
    timings['lazy_join_ms'] = (perf_counter() - lazy_join_start) * 1000

    timings.setdefault('post_filter_ms', 0.0)
    timings.setdefault('rrf_recombination_ms', 0.0)
    timings.setdefault('qdrant_parallel_ms', 0.0)
    timings.setdefault('opensearch_parallel_ms', timings.get('two_phase_stage1_ms', 0.0) + timings.get('two_phase_stage2_ms', 0.0))

    total_duration_ms = (perf_counter() - overall_start) * 1000
    timings['total_ms'] = total_duration_ms

    logger.info("ğŸ“ˆ ì„±ëŠ¥ ì¸¡ì • ìš”ì•½ (ms):")
    for key in sorted(timings.keys()):
        logger.info(f"  - {key}: {timings[key]:.2f}")

    response_took_ms = int(total_duration_ms)
    total_hits = len(final_hits)
    max_score = final_hits[0].get('_score', 0.0) if final_hits else 0.0

    response_payload = SearchResponse(
        query=request.query,
        total_hits=total_hits,
        max_score=max_score,
        results=results,
        query_analysis={
            "intent": analysis.intent,
            "must_terms": analysis.must_terms,
            "should_terms": analysis.should_terms,
            "alpha": analysis.alpha,
            "confidence": analysis.confidence,
            "extracted_entities": extracted_entities.to_dict(),
            "filters": filters,
            "size": size,
            "timings_ms": timings,
        },
        took_ms=response_took_ms,
    )
    return response_payload

def get_user_id_from_doc(doc: Dict[str, Any]) -> Optional[str]:
    if not isinstance(doc, dict):
        return None
    source = doc.get('_source')
    if isinstance(source, dict):
        uid = source.get('user_id')
        if uid:
            return uid
        payload = source.get('payload')
        if isinstance(payload, dict):
            uid = payload.get('user_id')
            if uid:
                return uid
    uid = doc.get('_id')
    if uid:
        return uid
    payload = doc.get('payload')
    if isinstance(payload, dict):
        return payload.get('user_id')
    return None
