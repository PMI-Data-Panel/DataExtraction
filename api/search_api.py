"""ê²€ìƒ‰ API ë¼ìš°í„°"""
import logging
from typing import List, Dict, Any, Optional
from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel, Field
from opensearchpy import OpenSearch

# ë¶„ì„ê¸° ë° ì¿¼ë¦¬ ë¹Œë”
from rag_query_analyzer.analyzers.main_analyzer import AdvancedRAGQueryAnalyzer
from rag_query_analyzer.analyzers.demographic_extractor import DemographicExtractor
from rag_query_analyzer.models.entities import DemographicType
from connectors.hybrid_searcher import OpenSearchHybridQueryBuilder, calculate_rrf_score
from connectors.data_fetcher import DataFetcher

logger = logging.getLogger(__name__)

router = APIRouter(
    prefix="/search",
    tags=["Search"]
)


class SearchRequest(BaseModel):
    """ê²€ìƒ‰ ìš”ì²­"""
    query: str = Field(..., description="ê²€ìƒ‰ ì¿¼ë¦¬")
    index_name: str = Field(default="*", description="ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ì´ë¦„ (ì™€ì¼ë“œì¹´ë“œ ì§€ì›, ê¸°ë³¸ê°’: ì „ì²´ ì¸ë±ìŠ¤ '*')")
    size: int = Field(default=10, ge=1, le=100, description="ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜")
    use_vector_search: bool = Field(default=True, description="ë²¡í„° ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€")


class SearchResult(BaseModel):
    """ê²€ìƒ‰ ê²°ê³¼ í•­ëª©"""
    user_id: str
    score: float
    timestamp: Optional[str] = None
    demographic_info: Optional[Dict[str, Any]] = Field(default=None, description="ì¸êµ¬í†µê³„ ì •ë³´ (welcome_1st, welcome_2ndì—ì„œ ì¡°íšŒ)")
    qa_pairs: Optional[List[Dict[str, Any]]] = None
    matched_qa_pairs: Optional[List[Dict[str, Any]]] = None
    highlights: Optional[Dict[str, Any]] = None


class SearchResponse(BaseModel):
    """ê²€ìƒ‰ ì‘ë‹µ"""
    query: str
    total_hits: int
    max_score: Optional[float]
    results: List[SearchResult]
    query_analysis: Optional[Dict[str, Any]] = None
    took_ms: int


@router.get("/", summary="Search API ìƒíƒœ")
def search_root():
    """Search API ê¸°ë³¸ ì •ë³´"""
    return {
        "message": "Search API ì‹¤í–‰ ì¤‘",
        "version": "1.0",
        "endpoints": [
            "/search/query",
            "/search/similar"
        ]
    }


@router.post("/query", response_model=SearchResponse, summary="ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰")
async def search_query(
    request: SearchRequest,
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """
    ìì—°ì–´ ì¿¼ë¦¬ë¡œ ì„¤ë¬¸ ë°ì´í„° ê²€ìƒ‰

    - ì¿¼ë¦¬ ë¶„ì„ (ì˜ë„ íŒŒì•…, í‚¤ì›Œë“œ ì¶”ì¶œ)
    - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í‚¤ì›Œë“œ + ë²¡í„°)
    - ì¸êµ¬í†µê³„ í•„í„°ë§
    - ê²°ê³¼ ë­í‚¹ ë° í¬ë§¤íŒ…
    """
    try:
        # OpenSearch ì—°ê²° í™•ì¸
        if not os_client or not os_client.ping():
            raise HTTPException(
                status_code=503,
                detail="OpenSearch ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )

        # ì„ë² ë”© ëª¨ë¸ í™•ì¸
        embedding_model = getattr(router, 'embedding_model', None)
        config = getattr(router, 'config', None)

        logger.info(f"\n{'='*60}")
        logger.info(f"[SEARCH] ê²€ìƒ‰ ì¿¼ë¦¬: '{request.query}'")
        logger.info(f"{'='*60}")

        # 1ë‹¨ê³„: ì¿¼ë¦¬ ë¶„ì„
        logger.info("\n[1/3] ì¿¼ë¦¬ ë¶„ì„ ì¤‘...")
        analyzer = AdvancedRAGQueryAnalyzer(config)
        query_analysis = analyzer.analyze_query(request.query)

        logger.info(f"   - ì˜ë„: {query_analysis.intent}")
        logger.info(f"   - must_terms: {query_analysis.must_terms}")
        logger.info(f"   - should_terms: {query_analysis.should_terms}")
        logger.info(f"   - alpha: {query_analysis.alpha}")

        # 2ë‹¨ê³„: ì¿¼ë¦¬ ë¹Œë“œ
        logger.info("\n[2/3] ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ì¤‘...")
        query_builder = OpenSearchHybridQueryBuilder(config)

        # ì„ë² ë”© ë²¡í„° ìƒì„±
        query_vector = None
        if request.use_vector_search and embedding_model:
            query_vector = embedding_model.encode(request.query).tolist()
            logger.info(f"   - ì¿¼ë¦¬ ë²¡í„° ìƒì„± ì™„ë£Œ (dim: {len(query_vector)})")

        # OpenSearch ì¿¼ë¦¬ ìƒì„±
        os_query = query_builder.build_query(
            analysis=query_analysis,
            query_vector=query_vector,
            size=request.size
        )

        # 3ë‹¨ê³„: ê²€ìƒ‰ ì‹¤í–‰
        logger.info("\n[3/3] ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")

        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (OpenSearch + Qdrant + RRF)
        if request.use_vector_search and query_vector and hasattr(router, 'qdrant_client'):
            logger.info("   - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ëª¨ë“œ (OpenSearch + Qdrant + RRF)")

            # OpenSearch í‚¤ì›Œë“œ ê²€ìƒ‰
            logger.info("   - [1/3] OpenSearch í‚¤ì›Œë“œ ê²€ìƒ‰...")
            data_fetcher = DataFetcher(opensearch_client=os_client)
            # â­ í•„í„°ê°€ ìˆëŠ” ê²½ìš°, êµì§‘í•©ì„ ìœ„í•´ ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì•¼ í•¨
            has_filters = bool(os_query.get('query', {}).get('bool', {}).get('must'))
            
            # Qdrant top-N ì œí•œ: í•„í„° ìœ ë¬´ì— ë”°ë¼ ë¶„ê¸°
            if has_filters:
                # í•„í„° ìˆìŒ: 300~500ê°œ (êµì§‘í•© í™•ë³´ë¥¼ ìœ„í•´)
                qdrant_limit = min(500, max(300, request.size * 10))
                search_size = max(1000, min(request.size * 20, 5000))
                logger.info(f"ğŸ” í•„í„° ì ìš©: OpenSearch size={search_size}, Qdrant limit={qdrant_limit} (êµì§‘í•© í™•ë³´ë¥¼ ìœ„í•´)")
            else:
                # í•„í„° ì—†ìŒ: 100~200ê°œ
                qdrant_limit = min(200, max(100, request.size * 2))
                search_size = request.size * 2
                logger.info(f"ğŸ” í•„í„° ì—†ìŒ: OpenSearch size={search_size}, Qdrant limit={qdrant_limit}")
            
            # OpenSearch _source filtering: í•„ìš”í•œ í•„ë“œë§Œ ì¡°íšŒ
            source_filter = {
                "includes": ["user_id", "metadata", "qa_pairs", "timestamp"],
                "excludes": []  # í•„ìš”ì‹œ ì œì™¸í•  í•„ë“œ ì¶”ê°€
            }
            
            os_response = data_fetcher.search_opensearch(
                index_name=request.index_name,
                query=os_query,
                size=search_size,
                source_filter=source_filter
            )
            logger.info(f"      â†’ OpenSearch: {len(os_response['hits']['hits'])}ê±´")

            # Qdrant ë²¡í„° ê²€ìƒ‰ (ëª¨ë“  ì»¬ë ‰ì…˜)
            logger.info("   - [2/3] Qdrant ë²¡í„° ê²€ìƒ‰ (ëª¨ë“  ì»¬ë ‰ì…˜)...")
            qdrant_client = router.qdrant_client

            # ëª¨ë“  Qdrant ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
            try:
                collections = qdrant_client.get_collections()
                collection_names = [col.name for col in collections.collections]
                logger.info(f"      â†’ ê²€ìƒ‰í•  ì»¬ë ‰ì…˜: {collection_names}")
            except Exception as e:
                logger.warning(f"      â†’ Qdrant ì»¬ë ‰ì…˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
                collection_names = []

            # ê° ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰ í›„ ê²°í•©
            qdrant_results = []
            for collection_name in collection_names:
                try:
                    # qdrant_limit ì‚¬ìš© (í•„í„° ìœ ë¬´ì— ë”°ë¼ ë¶„ê¸°)
                    # HNSW íŠœë‹: ef=128ë¡œ ì„¤ì • (íƒìƒ‰ í’ˆì§ˆê³¼ ì†ë„ ê· í˜•)
                    results = qdrant_client.search(
                        collection_name=collection_name,
                        query_vector=query_vector,
                        limit=qdrant_limit,
                        score_threshold=0.3,  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
                        # ef íŒŒë¼ë¯¸í„°ëŠ” Qdrant clientì˜ search ë©”ì„œë“œì— ì§ì ‘ ì „ë‹¬ ë¶ˆê°€
                        # ëŒ€ì‹  limitë¡œ ì œí•œí•˜ì—¬ ì„±ëŠ¥ ìµœì í™”
                    )
                    qdrant_results.extend(results)
                    logger.info(f"      â†’ {collection_name}: {len(results)}ê±´ (limit={qdrant_limit})")
                except Exception as e:
                    logger.warning(f"      â†’ {collection_name} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            qdrant_results.sort(key=lambda x: x.score, reverse=True)
            qdrant_results = qdrant_results[:qdrant_limit]  # ìƒìœ„ Nê°œë§Œ
            logger.info(f"      â†’ ì´ Qdrant ê²°ê³¼: {len(qdrant_results)}ê±´ (limit={qdrant_limit})")

            # RRFë¡œ ê²°í•©
            logger.info("   - [3/3] RRF ê²°í•© ì¤‘...")
            keyword_results = os_response['hits']['hits']
            vector_results = [
                {
                    '_id': str(r.id),
                    '_score': r.score,
                    '_source': r.payload
                }
                for r in qdrant_results
            ]

            combined_results = calculate_rrf_score(
                keyword_results=keyword_results,
                vector_results=vector_results,
                k=60  # RRF ìƒìˆ˜
            )

            # ìƒìœ„ Nê°œë§Œ ì„ íƒ
            final_hits = combined_results[:request.size]
            logger.info(f"      â†’ RRF ê²°í•© ì™„ë£Œ: {len(final_hits)}ê±´")

            # ê²°ê³¼ í¬ë§¤íŒ… (RRF ìˆœì„œ ìœ ì§€)
            results = []
            for doc in final_hits:
                source = doc.get('_source', {})

                # Qdrant ê²°ê³¼ì¸ ê²½ìš° payloadì—ì„œ user_id ì¶”ì¶œ
                if 'payload' in source:
                    user_id = source['payload'].get('user_id', '')
                else:
                    user_id = source.get('user_id', '')

                result = SearchResult(
                    user_id=user_id,
                    score=doc.get('_score', 0.0),
                    timestamp=source.get('timestamp'),
                    qa_pairs=source.get('qa_pairs', [])[:5],
                    matched_qa_pairs=[],
                    highlights=None
                )
                results.append(result)

            total_hits = max(os_response['hits']['total']['value'], len(qdrant_results))
            max_score = final_hits[0].get('_score', 0.0) if final_hits else 0.0
            took_ms = os_response['took']

        else:
            # ê¸°ì¡´ OpenSearch ë‹¨ë… ê²€ìƒ‰
            logger.info("   - OpenSearch í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ ì‚¬ìš©")
            data_fetcher = DataFetcher(opensearch_client=os_client)
            search_response = data_fetcher.search_opensearch(
                index_name=request.index_name,
                query=os_query,
                size=request.size
            )

            # ê²°ê³¼ í¬ë§¤íŒ…
            results = []
            for hit in search_response['hits']['hits']:
                # inner_hitsì—ì„œ ë§¤ì¹­ëœ qa_pairs ì¶”ì¶œ
                matched_qa = []
                if 'inner_hits' in hit and 'qa_pairs' in hit['inner_hits']:
                    for inner_hit in hit['inner_hits']['qa_pairs']['hits']['hits']:
                        qa_data = inner_hit['_source'].copy()
                        qa_data['match_score'] = inner_hit['_score']
                        if 'highlight' in inner_hit:
                            qa_data['highlights'] = inner_hit['highlight']
                        matched_qa.append(qa_data)

                result = SearchResult(
                    user_id=hit['_source'].get('user_id', ''),
                    score=hit['_score'],
                    timestamp=hit['_source'].get('timestamp'),
                    qa_pairs=hit['_source'].get('qa_pairs', [])[:5],
                    matched_qa_pairs=matched_qa,
                    highlights=hit.get('highlight')
                )
                results.append(result)

            total_hits = search_response['hits']['total']['value']
            max_score = search_response['hits']['max_score']
            took_ms = search_response['took']

        logger.info(f"\n[OK] ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê±´ ë°˜í™˜")
        logger.info(f"{'='*60}\n")

        return SearchResponse(
            query=request.query,
            total_hits=total_hits,
            max_score=max_score,
            results=results,
            query_analysis={
                "intent": query_analysis.intent,
                "must_terms": query_analysis.must_terms,
                "should_terms": query_analysis.should_terms,
                "alpha": query_analysis.alpha,
                "confidence": query_analysis.confidence
            },
            took_ms=took_ms
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[ERROR] ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )


class NLSearchRequest(BaseModel):
    """ìì—°ì–´ ê¸°ë°˜ ê²€ìƒ‰ ìš”ì²­ (í•„í„°/size ìë™ ì¶”ì¶œ)"""
    query: str = Field(..., description="ìì—°ì–´ ì¿¼ë¦¬ (ì˜ˆ: '30ëŒ€ ì‚¬ë¬´ì§ 300ëª… ë°ì´í„° ë³´ì—¬ì¤˜')")
    index_name: str = Field(default="*", description="ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ì´ë¦„ (ê¸°ë³¸ê°’: ì „ì²´ ì¸ë±ìŠ¤ '*')")
    use_vector_search: bool = Field(default=True, description="ë²¡í„° ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€")


@router.post("/nl", response_model=SearchResponse, summary="ìì—°ì–´ ì¿¼ë¦¬: ìë™ ì¶”ì¶œ+ê²€ìƒ‰")
async def search_natural_language(
    request: NLSearchRequest,
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """
    ìì—°ì–´ ì…ë ¥ì—ì„œ ì¸êµ¬í†µê³„(ì—°ë ¹/ì„±ë³„/ì§ì—…)ì™€ ìš”ì²­ ìˆ˜ëŸ‰ì„ ì¶”ì¶œí•˜ì—¬
    ê²€ìƒ‰ ì¿¼ë¦¬ì™€ sizeì— ë°˜ì˜í•œ ë’¤ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        if not os_client or not os_client.ping():
            raise HTTPException(status_code=503, detail="OpenSearch ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        embedding_model = getattr(router, 'embedding_model', None)
        config = getattr(router, 'config', None)

        # 1) ì¶”ì¶œ: filters + size
        extractor = DemographicExtractor()
        extracted_entities, requested_size = extractor.extract_with_size(request.query)
        filters = extracted_entities.to_filters()
        size = max(1, min(requested_size, 100))
        
        # â­ í•„í„°ê°€ ìˆëŠ” ê²½ìš°, êµì§‘í•©ì„ ìœ„í•´ ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì•¼ í•¨
        # ì´ë¡ ìƒ êµì§‘í•©ì´ ìˆ˜ë°±~ìˆ˜ì²œ ëª…ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¶©ë¶„íˆ í° size ì‚¬ìš©
        # ì˜ˆ: welcome_1stì—ì„œ 5,192ëª…, welcome_2ndì—ì„œ 10,000ëª… ì´ìƒ
        # â†’ ê°ê° 1,000ê°œì”© ê°€ì ¸ì™€ë„ êµì§‘í•©ì´ ì¶©ë¶„íˆ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ
        search_size = size * 2  # ê¸°ë³¸ê°’
        if filters:
            # í•„í„°ê°€ ìˆìœ¼ë©´ ë” ë§ì´ ê°€ì ¸ì˜¤ê¸° (êµì§‘í•©ì„ ìœ„í•´)
            # ìµœì†Œ 1,000ê°œ, ìµœëŒ€ 5,000ê°œ (ì„±ëŠ¥ ê³ ë ¤)
            search_size = max(1000, min(size * 20, 5000))
            logger.info(f"ğŸ” í•„í„° ì ìš©: ê²€ìƒ‰ sizeë¥¼ {search_size}ë¡œ ì¦ê°€ (êµì§‘í•© í™•ë³´ë¥¼ ìœ„í•´)")

        # 2) ë¶„ì„ + ì¿¼ë¦¬ ë¹Œë“œ
        analyzer = AdvancedRAGQueryAnalyzer(config)
        analysis = analyzer.analyze_query(request.query)
        
        # â­ ìµœì¢… í‚¤ì›Œë“œ ì •ì œ: ë©”íƒ€ í‚¤ì›Œë“œ, ìˆ˜ëŸ‰ íŒ¨í„´, Demographics ì œê±°
        import re
        
        # ë©”íƒ€ í‚¤ì›Œë“œ ì •ì˜ (ê²€ìƒ‰ ì¡°ê±´ì—ì„œ ì œì™¸)
        meta_keywords = {
            'ì„¤ë¬¸ì¡°ì‚¬', 'ì„¤ë¬¸', 'ë°ì´í„°', 'ìë£Œ', 'ì •ë³´',
            'ë³´ì—¬ì¤˜', 'ë³´ì—¬ì£¼ì„¸ìš”', 'ì•Œë ¤ì¤˜', 'ì•Œë ¤ì£¼ì„¸ìš”',
            'ê²€ìƒ‰', 'ì°¾ì•„ì¤˜', 'ì°¾ì•„ì£¼ì„¸ìš”', 'ì¡°íšŒ',
            'ì„', 'ë¥¼', 'ì´', 'ê°€', 'ì˜', 'ì—', 'ì—ì„œ',
            'ì™€', 'ê³¼', 'ì—ê²Œ', 'í•œí…Œ', 'ëª…', 'ê°œ', 'ê±´',
            'ì‚¬ëŒ', 'ì¸', 'ë¶„', 'ì¤‘', 'ì¤‘ì—', 'ì¤‘ì—ì„œ'
        }
        
        # ìˆ˜ëŸ‰ íŒ¨í„´("ìˆ«ì+ëª…/ê±´") ì œê±°
        quantity_pattern = re.compile(r'\d+\s*(ëª…|ê±´)')
        
        # ì¶”ì¶œëœ Demographics í‚¤ì›Œë“œ ì§‘í•©
        extracted_keywords = set()
        for demo in extracted_entities.demographics:
            extracted_keywords.add(demo.raw_value)
            extracted_keywords.update(demo.synonyms)
        
        # ì •ì œ ì „ í‚¤ì›Œë“œ ì €ì¥ (ë¡œê¹…ìš©)
        original_must = analysis.must_terms.copy()
        original_should = analysis.should_terms.copy()
        
        # must_terms ì •ì œ
        analysis.must_terms = [
            t for t in analysis.must_terms
            if (t not in meta_keywords and
                t not in extracted_keywords and
                not quantity_pattern.search(t))
        ]
        
        # should_terms ì •ì œ
        analysis.should_terms = [
            t for t in analysis.should_terms
            if (t not in meta_keywords and
                t not in extracted_keywords and
                not quantity_pattern.search(t))
        ]
        
        # ì œê±°ëœ í‚¤ì›Œë“œ ì¶”ì 
        removed_meta = [t for t in (original_must + original_should) if t in meta_keywords]
        removed_demo = [t for t in (original_must + original_should) if t in extracted_keywords]
        removed_quantity = [t for t in (original_must + original_should) if quantity_pattern.search(t)]
        
        logger.info(f"ğŸ” ìµœì¢… í‚¤ì›Œë“œ ì •ì œ:")
        logger.info(f"  - Must terms: {analysis.must_terms} (ì›ë³¸: {original_must})")
        logger.info(f"  - Should terms: {analysis.should_terms} (ì›ë³¸: {original_should})")
        if removed_meta:
            logger.info(f"  - âŒ ì œê±°ëœ ë©”íƒ€ í‚¤ì›Œë“œ: {removed_meta}")
        if removed_demo:
            logger.info(f"  - âŒ ì œê±°ëœ Demographics: {removed_demo} (í•„í„°ë¡œë§Œ ì²˜ë¦¬)")
        if removed_quantity:
            logger.info(f"  - âŒ ì œê±°ëœ ìˆ˜ëŸ‰ íŒ¨í„´: {removed_quantity}")
        logger.info(f"  - âœ… Demographics í•„í„°: {[d.raw_value for d in extracted_entities.demographics]}")

        query_builder = OpenSearchHybridQueryBuilder(config)
        query_vector = None
        if embedding_model:
            # ì™„ì „ ë™ì  ì„ë² ë”© ê¸°ë°˜ ë™ì˜ì–´ í™•ì¥ (ë„ë©”ì¸ ë¬´ê´€, ë²”ìš©)
            def _enrich_query_vector() -> Optional[list]:
                """
                ExtractedEntitiesì˜ ëª¨ë“  ì—”í‹°í‹°(raw_value)ì— ëŒ€í•´
                Qdrantì—ì„œ ìœ ì‚¬ ë²¡í„°ë¥¼ ì°¾ì•„ ë™ì˜ì–´ë¥¼ ìë™ í™•ì¥
                ì •ì  ì‚¬ì „ ì—†ì´ ì™„ì „ ë™ì  ë°©ì‹
                """
                import re
                phrases = [request.query]  # ì›ë³¸ ì¿¼ë¦¬ í¬í•¨
                qdrant_client = getattr(router, 'qdrant_client', None)
                
                if not qdrant_client:
                    # Qdrant ì—†ìœ¼ë©´ ì›ë³¸ ì¿¼ë¦¬ë§Œ ì„ë² ë”©
                    try:
                        vec = embedding_model.encode(request.query).tolist()
                        return vec
                    except Exception:
                        return None
                
                # ëª¨ë“  ì¶”ì¶œëœ ì—”í‹°í‹°ì— ëŒ€í•´ ë™ì  í™•ì¥
                all_entity_values = []
                
                # Demographics: raw_value ìˆ˜ì§‘
                for demo in extracted_entities.demographics:
                    if demo.raw_value:
                        all_entity_values.append(demo.raw_value)
                
                # Topics: name + keywords ìˆ˜ì§‘
                for topic in extracted_entities.topics:
                    if topic.name:
                        all_entity_values.append(topic.name)
                    all_entity_values.extend(list(topic.keywords)[:3])  # ìƒìœ„ 3ê°œë§Œ
                
                # Questions: question_text ìˆ˜ì§‘
                for q in extracted_entities.questions:
                    if q.question_text:
                        all_entity_values.append(q.question_text)
                
                # ê° ì—”í‹°í‹° ê°’ì— ëŒ€í•´ Qdrantì—ì„œ ìœ ì‚¬ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
                syn_candidates = set()  # ì¤‘ë³µ ì œê±°ìš©
                collections = qdrant_client.get_collections()
                
                for entity_val in all_entity_values[:5]:  # ìµœëŒ€ 5ê°œ ì—”í‹°í‹°ë§Œ ì²˜ë¦¬ (ì„±ëŠ¥)
                    try:
                        base_vec = embedding_model.encode(entity_val).tolist()
                        for col in collections.collections:
                            try:
                                results = qdrant_client.search(
                                    collection_name=col.name,
                                    query_vector=base_vec,
                                    limit=10,  # ê° ì—”í‹°í‹°ë‹¹ 10ê°œ
                                    score_threshold=0.3  # ìµœì†Œ ìœ ì‚¬ë„
                                )
                                for r in results:
                                    payload = getattr(r, 'payload', {}) or {}
                                    txt = payload.get('answer_text') or payload.get('text') or payload.get('q_text')
                                    if isinstance(txt, str) and len(txt.strip()) > 0:
                                        # ê¸´ ë¬¸ì¥ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì„ë² ë”©ì´ ì˜ë¯¸ë¥¼ í¬ì°©)
                                        syn_candidates.add(txt.strip())
                            except Exception:
                                continue
                    except Exception:
                        continue
                
                # ìˆ˜ì§‘ëœ ìœ ì‚¬ í…ìŠ¤íŠ¸ë¥¼ phrasesì— ì¶”ê°€ (ìµœëŒ€ 10ê°œ)
                phrases.extend(list(syn_candidates)[:10])
                
                # ëª¨ë“  phrasesë¥¼ ì„ë² ë”©í•˜ì—¬ í‰ê· 
                try:
                    vecs = embedding_model.encode(phrases, convert_to_tensor=False)
                    if hasattr(vecs, 'tolist'):
                        vecs = vecs.tolist()
                    if isinstance(vecs, list) and vecs:
                        dim = len(vecs[0])
                        avg = [0.0] * dim
                        for v in vecs:
                            for i in range(dim):
                                avg[i] += v[i]
                        avg = [x / len(vecs) for x in avg]
                        return avg
                except Exception:
                    # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì¿¼ë¦¬ë§Œ
                    try:
                        return embedding_model.encode(request.query).tolist()
                    except Exception:
                        return None
                return None

            if request.use_vector_search:
                query_vector = _enrich_query_vector()

        base_query = query_builder.build_query(
            analysis=analysis,
            query_vector=query_vector,
            size=size,
        )

        # 3) í•„í„° ì ìš© ì „ëµ: í•„í„°ëŠ” mustë¡œ, í‚¤ì›Œë“œëŠ” shouldë¡œ ì™„í™”
        # - í•„í„°(30ëŒ€, ì‚¬ë¬´ì§)ëŠ” ë°˜ë“œì‹œ ë§¤ì¹­ë˜ì–´ì•¼ í•¨
        # - í‚¤ì›Œë“œ ê²€ìƒ‰ì€ shouldë¡œ ì™„í™” (í•˜ë‚˜ë§Œ ë§¤ì¹­ë˜ì–´ë„ OK)
        final_query = base_query
        
        # â­ match_all/match_none/None ì œê±°: base_queryì—ì„œ match_all, match_none, Noneì´ ìˆìœ¼ë©´ ì œê±°
        existing_query = final_query.get('query', {"match_all": {}})
        if existing_query is None or existing_query == {"match_all": {}} or existing_query == {"match_none": {}}:
            # match_all/match_none/None ì œê±°
            removed_type = "None" if existing_query is None else ("match_all" if existing_query == {"match_all": {}} else "match_none")
            
            # â­ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í‚¤ì›Œë“œ ì¿¼ë¦¬ ìƒì„± (í•„í„°ë§Œ ìˆëŠ” ê²½ìš°ë¥¼ ìœ„í•´)
            if analysis.must_terms or analysis.should_terms:
                # í‚¤ì›Œë“œ ì¿¼ë¦¬ ì¬ìƒì„±
                keyword_queries = []
                if analysis.must_terms:
                    for term in analysis.must_terms:
                        keyword_queries.append({
                            "nested": {
                                "path": "qa_pairs",
                                "query": {"match": {"qa_pairs.answer_text": term}},
                                "score_mode": "max"
                            }
                        })
                
                if analysis.should_terms:
                    should_keywords = [{
                        "nested": {
                            "path": "qa_pairs",
                            "query": {"match": {"qa_pairs.answer_text": term}},
                            "score_mode": "max"
                        }
                    } for term in analysis.should_terms]
                    
                    if keyword_queries:
                        # mustì™€ should ëª¨ë‘ ìˆëŠ” ê²½ìš°
                        existing_query = {
                            "bool": {
                                "must": keyword_queries,
                                "should": should_keywords,
                                "minimum_should_match": 1
                            }
                        }
                    else:
                        # shouldë§Œ ìˆëŠ” ê²½ìš°
                        existing_query = {
                            "bool": {
                                "should": should_keywords,
                                "minimum_should_match": 1
                            }
                        }
                else:
                    # mustë§Œ ìˆëŠ” ê²½ìš°
                    if len(keyword_queries) == 1:
                        existing_query = keyword_queries[0]
                    else:
                        existing_query = {
                            "bool": {
                                "must": keyword_queries
                            }
                        }
                
                logger.info(f"âš ï¸ {removed_type} ì œê±°, í‚¤ì›Œë“œ ì¿¼ë¦¬ ì¬ìƒì„±: must={len(analysis.must_terms)}, should={len(analysis.should_terms)}")
            else:
                existing_query = None
                logger.info(f"âš ï¸ {removed_type} ì œê±°: í•„í„°ë§Œ ì‚¬ìš© (í‚¤ì›Œë“œ ì—†ìŒ)")
        
        # â­ inner_hits ì œê±° í•¨ìˆ˜ (ì¤‘ë³µ ë°©ì§€)
        def remove_inner_hits(query_dict):
            """ì¬ê·€ì ìœ¼ë¡œ inner_hits ì œê±° (í•„í„°ì—ì„œëŠ” ë§¤ì¹­ë§Œ í™•ì¸í•˜ë©´ ë˜ë¯€ë¡œ)"""
            import copy
            cleaned = copy.deepcopy(query_dict)
            
            if isinstance(cleaned, dict):
                # nested ì¿¼ë¦¬ì—ì„œ inner_hits ì œê±°
                if 'nested' in cleaned:
                    if 'inner_hits' in cleaned['nested']:
                        del cleaned['nested']['inner_hits']
                    # ì¬ê·€ì ìœ¼ë¡œ query ë‚´ë¶€ë„ ì •ì œ
                    if 'query' in cleaned['nested']:
                        cleaned['nested']['query'] = remove_inner_hits(cleaned['nested']['query'])
                
                # bool ì¿¼ë¦¬ ë‚´ë¶€ë„ ì¬ê·€ì ìœ¼ë¡œ ì •ì œ
                if 'bool' in cleaned:
                    for key in ['must', 'should', 'must_not', 'filter']:
                        if key in cleaned['bool']:
                            if isinstance(cleaned['bool'][key], list):
                                cleaned['bool'][key] = [remove_inner_hits(item) for item in cleaned['bool'][key]]
                            else:
                                cleaned['bool'][key] = remove_inner_hits(cleaned['bool'][key])
            
            return cleaned
        
        if filters:
            # â­ inner_hits ì œê±° (ì¤‘ë³µ ë°©ì§€)
            cleaned_filters = [remove_inner_hits(f) for f in filters]
            
            filter_by_type = {}
            for f in cleaned_filters:
                # í•„í„° íƒ€ì… ì¶”ì¶œ (ìƒˆë¡œìš´ bool ì¿¼ë¦¬ í˜•íƒœ ì§€ì›)
                filter_type = None
                
                # 1. bool ì¿¼ë¦¬ í˜•íƒœ (metadata OR qa_pairs)
                if 'bool' in f and 'should' in f['bool']:
                    should_clauses = f['bool']['should']
                    for clause in should_clauses:
                        # term í•„í„°ì—ì„œ íƒ€ì… ì¶”ì¶œ
                        if 'term' in clause:
                            term_key = list(clause['term'].keys())[0]
                            if 'age_group' in term_key:
                                filter_type = 'age'
                                break
                            elif 'gender' in term_key:
                                filter_type = 'gender'
                                break
                            elif 'occupation' in term_key:
                                filter_type = 'occupation'
                                break
                        # nested í•„í„°ì—ì„œ íƒ€ì… ì¶”ì¶œ
                        elif 'nested' in clause:
                            nested_q = clause['nested'].get('query', {}).get('bool', {}).get('must', [])
                            for nq in nested_q:
                                if isinstance(nq, dict) and 'bool' in nq and 'should' in nq['bool']:
                                    # q_text ë§¤ì¹­ í™•ì¸
                                    for sq in nq['bool']['should']:
                                        if 'match' in sq:
                                            match_key = list(sq['match'].keys())[0]
                                            if 'q_text' in match_key:
                                                q_text_val = sq['match'][match_key]
                                                if 'ì—°ë ¹' in str(q_text_val) or 'ë‚˜ì´' in str(q_text_val):
                                                    filter_type = 'age'
                                                    break
                                                elif 'ì„±ë³„' in str(q_text_val):
                                                    filter_type = 'gender'
                                                    break
                                                elif 'ì§ì—…' in str(q_text_val):
                                                    filter_type = 'occupation'
                                                    break
                                elif 'match' in nq:
                                    match_key = list(nq['match'].keys())[0]
                                    if 'q_text' in match_key:
                                        q_text_val = nq['match'][match_key]
                                        if 'ì—°ë ¹' in str(q_text_val) or 'ë‚˜ì´' in str(q_text_val):
                                            filter_type = 'age'
                                            break
                                        elif 'ì„±ë³„' in str(q_text_val):
                                            filter_type = 'gender'
                                            break
                                        elif 'ì§ì—…' in str(q_text_val):
                                            filter_type = 'occupation'
                                            break
                        if filter_type:
                            break
                
                # 2. ê¸°ì¡´ í˜•íƒœ (í•˜ìœ„ í˜¸í™˜ì„±)
                elif 'term' in f:
                    term_key = list(f['term'].keys())[0]
                    if 'age_group' in term_key:
                        filter_type = 'age'
                    elif 'gender' in term_key:
                        filter_type = 'gender'
                    elif 'occupation' in term_key:
                        filter_type = 'occupation'
                elif 'nested' in f:
                    nested_q = f['nested'].get('query', {}).get('bool', {}).get('must', [])
                    for nq in nested_q:
                        if 'match' in nq:
                            match_key = list(nq['match'].keys())[0]
                            if 'q_text' in match_key:
                                q_text_val = nq['match'][match_key]
                                if 'ì—°ë ¹' in str(q_text_val) or 'ë‚˜ì´' in str(q_text_val):
                                    filter_type = 'age'
                                elif 'ì„±ë³„' in str(q_text_val):
                                    filter_type = 'gender'
                                elif 'ì§ì—…' in str(q_text_val):
                                    filter_type = 'occupation'
                
                if filter_type:
                    if filter_type not in filter_by_type:
                        filter_by_type[filter_type] = []
                    filter_by_type[filter_type].append(f)
                else:
                    # íƒ€ì…ì„ ì•Œ ìˆ˜ ì—†ëŠ” í•„í„°ëŠ” ê·¸ëŒ€ë¡œ ì¶”ê°€
                    if 'unknown' not in filter_by_type:
                        filter_by_type['unknown'] = []
                    filter_by_type['unknown'].append(f)
            
            # â­ í•„í„°ë¥¼ should ì¡°ê±´ìœ¼ë¡œ ì „í™˜ (ì ìˆ˜ ë¶€ìŠ¤íŒ… í¬í•¨)
            # ê° íƒ€ì…ë³„ë¡œ OR, íƒ€ì… ê°„ì€ AND (shouldë¡œ ì™„í™”)
            should_filters = []
            for filter_type, type_filters in filter_by_type.items():
                if len(type_filters) == 1:
                    # ë‹¨ì¼ í•„í„°: í•„í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì´ë¯¸ bool ì¿¼ë¦¬ í˜•íƒœ)
                    filter_item = type_filters[0]
                    should_filters.append(filter_item)
                else:
                    # ê°™ì€ íƒ€ì… í•„í„°ëŠ” OR
                    should_filters.append({
                        'bool': {
                            'should': type_filters,
                            'minimum_should_match': 1
                        }
                    })
            
            # â­ ê¸°ì¡´ ì¿¼ë¦¬ì™€ í•„í„° ê²°í•© (mustë¡œ ê²°í•©: ëª¨ë“  í•„í„°ë¥¼ ë§Œì¡±í•´ì•¼ í•¨)
            # welcome_1st: ì—°ë ¹/ì„±ë³„, welcome_2nd: ì§ì—… ì •ë³´
            # ê° ì¸ë±ìŠ¤ì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì™€ì•¼ í•˜ë¯€ë¡œ mustë¡œ ê²°í•©
            if existing_query is None or existing_query == {"match_all": {}} or existing_query == {"match_none": {}}:
                # í‚¤ì›Œë“œ ì¿¼ë¦¬ê°€ ì—†ê±°ë‚˜ match_all/match_noneì¸ ê²½ìš°: í•„í„°ë¥¼ mustë¡œ ì‚¬ìš©
                final_query['query'] = {
                    'bool': {
                        'must': should_filters  # ëª¨ë“  í•„í„°ë¥¼ ë§Œì¡±í•´ì•¼ í•¨
                    }
                }
                logger.info(f"âœ… í•„í„°ë¥¼ mustë¡œ ì ìš© (ëª¨ë“  í•„í„° ë§Œì¡± í•„ìš”): {len(should_filters)}ê°œ í•„í„°")
            elif isinstance(existing_query, dict) and existing_query.get('bool'):
                # ê¸°ì¡´ bool ì¿¼ë¦¬ì— í•„í„°ë¥¼ mustë¡œ ì¶”ê°€
                if 'must' not in existing_query['bool']:
                    existing_query['bool']['must'] = []
                existing_query['bool']['must'].extend(should_filters)
                final_query['query'] = existing_query
                logger.info(f"âœ… í•„í„°ë¥¼ mustë¡œ ì¶”ê°€ (ëª¨ë“  í•„í„° ë§Œì¡± í•„ìš”): {len(should_filters)}ê°œ í•„í„°")
            else:
                # ê¸°ì¡´ ì¿¼ë¦¬ë¥¼ boolë¡œ ê°ì‹¸ê¸° (mustë¡œ ê²°í•©)
                final_query['query'] = {
                    'bool': {
                        'must': [existing_query] + should_filters
                    }
                }
                logger.info(f"âœ… í•„í„°ë¥¼ mustë¡œ ì¶”ê°€ (ëª¨ë“  í•„í„° ë§Œì¡± í•„ìš”): {len(should_filters)}ê°œ í•„í„°")
        
        if 'size' not in final_query:
            final_query['size'] = size

        # â­ í•„í„° ì ìš© í™•ì¸ ë¡œê¹…
        if filters:
            import json
            logger.info(f"ğŸ” ì ìš©ëœ í•„í„° ({len(filters)}ê°œ):")
            for i, f in enumerate(filters, 1):
                logger.info(f"  í•„í„° {i}: {json.dumps(f, ensure_ascii=False, indent=2)}")
            logger.info(f"ğŸ” ìµœì¢… ì¿¼ë¦¬ êµ¬ì¡°:")
            logger.info(f"  {json.dumps(final_query, ensure_ascii=False, indent=2)}")

        # â­ Qdrant top-N ì œí•œ: í•„í„° ìœ ë¬´ì— ë”°ë¼ ë¶„ê¸°
        has_filters = bool(filters)
        if has_filters:
            # í•„í„° ìˆìŒ: 300~500ê°œ (êµì§‘í•© í™•ë³´ë¥¼ ìœ„í•´)
            qdrant_limit = min(500, max(300, size * 10))
            search_size = max(1000, min(size * 20, 5000))
            logger.info(f"ğŸ” í•„í„° ì ìš©: OpenSearch size={search_size}, Qdrant limit={qdrant_limit} (êµì§‘í•© í™•ë³´ë¥¼ ìœ„í•´)")
        else:
            # í•„í„° ì—†ìŒ: 100~200ê°œ
            qdrant_limit = min(200, max(100, size * 2))
            search_size = size * 2
            logger.info(f"ğŸ” í•„í„° ì—†ìŒ: OpenSearch size={search_size}, Qdrant limit={qdrant_limit}")

        # 4) ì‹¤í–‰: í•˜ì´ë¸Œë¦¬ë“œ (OpenSearch + ì„ íƒì  Qdrant) with RRF
        # â­ STEP 1: welcome_1stì™€ welcome_2ndë¥¼ ê°ê° ë³„ë„ë¡œ ê²€ìƒ‰
        data_fetcher = DataFetcher(opensearch_client=os_client)
        
        # OpenSearch _source filtering: í•„ìš”í•œ í•„ë“œë§Œ ì¡°íšŒ
        source_filter = {
            "includes": ["user_id", "metadata", "qa_pairs", "timestamp"],
            "excludes": []  # í•„ìš”ì‹œ ì œì™¸í•  í•„ë“œ ì¶”ê°€
        }
        
        # welcome_1stì™€ welcome_2ndë¥¼ ë³„ë„ë¡œ ê²€ìƒ‰í• ì§€ ê²°ì •
        # í•„í„°ì— ì—°ë ¹/ì„±ë³„ì´ ìˆìœ¼ë©´ welcome_1st ê²€ìƒ‰, ì§ì—…ì´ ìˆìœ¼ë©´ welcome_2nd ê²€ìƒ‰
        search_welcome_1st = False
        search_welcome_2nd = False
        search_other_indices = True
        
        if filters:
            for demo in extracted_entities.demographics:
                if demo.demographic_type == DemographicType.AGE or demo.demographic_type == DemographicType.GENDER:
                    search_welcome_1st = True
                elif demo.demographic_type == DemographicType.OCCUPATION:
                    search_welcome_2nd = True
        
        # í•„í„°ê°€ ì—†ê±°ë‚˜ ëª¨ë“  ì¸ë±ìŠ¤ë¥¼ ê²€ìƒ‰í•´ì•¼ í•˜ëŠ” ê²½ìš°
        if not filters or request.index_name == '*':
            search_welcome_1st = True
            search_welcome_2nd = True
        
        logger.info(f"ğŸ” ì¸ë±ìŠ¤ë³„ ê²€ìƒ‰ ì „ëµ:")
        logger.info(f"  - welcome_1st ê²€ìƒ‰: {search_welcome_1st}")
        logger.info(f"  - welcome_2nd ê²€ìƒ‰: {search_welcome_2nd}")
        logger.info(f"  - ê¸°íƒ€ ì¸ë±ìŠ¤ ê²€ìƒ‰: {search_other_indices}")
        
        # â­ ì¸ë±ìŠ¤ë³„ í•„í„° ë¶„ë¦¬: welcome_1stëŠ” ì—°ë ¹/ì„±ë³„ë§Œ, welcome_2ndëŠ” ì§ì—…ë§Œ
        welcome_1st_query = final_query.copy()
        welcome_2nd_query = final_query.copy()
        
        if filters and 'query' in final_query:
            logger.info(f"ğŸ” ì¸ë±ìŠ¤ë³„ í•„í„° ë¶„ë¦¬ ì¤‘...")
            # í•„í„°ë¥¼ íƒ€ì…ë³„ë¡œ ë¶„ë¦¬
            age_gender_filters = []
            occupation_filters = []
            
            for demo in extracted_entities.demographics:
                if demo.demographic_type in [DemographicType.AGE, DemographicType.GENDER]:
                    # welcome_1stìš© í•„í„°
                    demo_filter = demo.to_opensearch_filter()
                    if demo_filter:
                        age_gender_filters.append(demo_filter)
                elif demo.demographic_type == DemographicType.OCCUPATION:
                    # welcome_2ndìš© í•„í„°
                    demo_filter = demo.to_opensearch_filter()
                    if demo_filter:
                        occupation_filters.append(demo_filter)
            
            # welcome_1st ì¿¼ë¦¬: ì—°ë ¹/ì„±ë³„ í•„í„°ë§Œ ì ìš©
            if age_gender_filters:
                base_query = final_query['query'].get('bool', {}).get('must', [])
                # match_all/match_none ì œê±°
                base_query = [q for q in base_query if q not in [{"match_all": {}}, {"match_none": {}}] and q is not None]
                
                # â­ í‚¤ì›Œë“œ ì¿¼ë¦¬ì™€ í•„í„° ë¶„ë¦¬
                keyword_queries = []  # í‚¤ì›Œë“œ ê²€ìƒ‰ ì¿¼ë¦¬ (nested with match on answer_text)
                filtered_base = []    # ì—°ë ¹/ì„±ë³„ í•„í„°ë§Œ
                
                for q in base_query:
                    if isinstance(q, dict):
                        # í‚¤ì›Œë“œ ì¿¼ë¦¬ì¸ì§€ í™•ì¸ (nested + match on answer_text, í•„í„°ê°€ ì•„ë‹Œ ê²ƒ)
                        is_keyword_query = False
                        if 'nested' in q:
                            nested_query = q['nested'].get('query', {})
                            # match ì¿¼ë¦¬ì´ê³  answer_textë¥¼ ê²€ìƒ‰í•˜ëŠ” ê²½ìš° (í‚¤ì›Œë“œ ê²€ìƒ‰)
                            if 'match' in nested_query:
                                match_field = list(nested_query['match'].keys())[0]
                                if 'answer_text' in match_field:
                                    is_keyword_query = True
                            # bool ì¿¼ë¦¬ ë‚´ë¶€ì— matchê°€ ìˆëŠ” ê²½ìš°ë„ í‚¤ì›Œë“œ ì¿¼ë¦¬
                            elif 'bool' in nested_query:
                                for bool_type in ['must', 'should']:
                                    if bool_type in nested_query['bool']:
                                        for subq in nested_query['bool'][bool_type]:
                                            if isinstance(subq, dict) and 'match' in subq:
                                                match_field = list(subq['match'].keys())[0]
                                                if 'answer_text' in match_field:
                                                    is_keyword_query = True
                                                    break
                        
                        if is_keyword_query:
                            # í‚¤ì›Œë“œ ì¿¼ë¦¬ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                            keyword_queries.append(q)
                        else:
                            # í•„í„°ì¸ì§€ í™•ì¸ (term, nested with q_text ë“±)
                            is_age_gender = False
                            for f in age_gender_filters:
                                if q == f or (isinstance(q, dict) and isinstance(f, dict) and q.get('term') == f.get('term')):
                                    is_age_gender = True
                                    break
                            if is_age_gender:
                                filtered_base.append(q)
                    else:
                        # ê¸°íƒ€ ì¿¼ë¦¬ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                        keyword_queries.append(q)
                
                # â­ í‚¤ì›Œë“œ ì¿¼ë¦¬ì™€ í•„í„° ê²°í•©
                all_must_clauses = keyword_queries + filtered_base + age_gender_filters
                if all_must_clauses:
                    welcome_1st_query['query'] = {
                        'bool': {
                            'must': all_must_clauses
                        }
                    }
                    logger.info(f"  âœ… welcome_1st ì¿¼ë¦¬: í‚¤ì›Œë“œ {len(keyword_queries)}ê°œ + ì—°ë ¹/ì„±ë³„ í•„í„° {len(age_gender_filters)}ê°œ ì ìš©")
                elif age_gender_filters:
                    # í•„í„°ë§Œ ìˆëŠ” ê²½ìš°
                    welcome_1st_query['query'] = {
                        'bool': {
                            'must': age_gender_filters
                        }
                    }
                    logger.info(f"  âœ… welcome_1st í•„í„°: ì—°ë ¹/ì„±ë³„ {len(age_gender_filters)}ê°œë§Œ ì ìš©")
            
            # welcome_2nd ì¿¼ë¦¬: ì§ì—… í•„í„°ë§Œ ì ìš© (í‚¤ì›Œë“œ ì¿¼ë¦¬ë„ í¬í•¨)
            if occupation_filters:
                base_query = final_query['query'].get('bool', {}).get('must', [])
                base_query = [q for q in base_query if q not in [{"match_all": {}}, {"match_none": {}}] and q is not None]
                
                # â­ í‚¤ì›Œë“œ ì¿¼ë¦¬ì™€ í•„í„° ë¶„ë¦¬
                keyword_queries_2nd = []  # í‚¤ì›Œë“œ ê²€ìƒ‰ ì¿¼ë¦¬
                filtered_base_2nd = []    # ì§ì—… í•„í„°ë§Œ
                
                for q in base_query:
                    if isinstance(q, dict):
                        # í‚¤ì›Œë“œ ì¿¼ë¦¬ì¸ì§€ í™•ì¸
                        is_keyword_query = False
                        if 'nested' in q:
                            nested_query = q['nested'].get('query', {})
                            if 'match' in nested_query:
                                match_field = list(nested_query['match'].keys())[0]
                                if 'answer_text' in match_field:
                                    is_keyword_query = True
                            elif 'bool' in nested_query:
                                for bool_type in ['must', 'should']:
                                    if bool_type in nested_query['bool']:
                                        for subq in nested_query['bool'][bool_type]:
                                            if isinstance(subq, dict) and 'match' in subq:
                                                match_field = list(subq['match'].keys())[0]
                                                if 'answer_text' in match_field:
                                                    is_keyword_query = True
                                                    break
                        
                        if is_keyword_query:
                            keyword_queries_2nd.append(q)
                        else:
                            # í•„í„°ì¸ì§€ í™•ì¸
                            is_occupation = False
                            for f in occupation_filters:
                                if q == f or (isinstance(q, dict) and isinstance(f, dict) and q.get('term') == f.get('term')):
                                    is_occupation = True
                                    break
                            if is_occupation:
                                filtered_base_2nd.append(q)
                    else:
                        keyword_queries_2nd.append(q)
                
                # â­ í‚¤ì›Œë“œ ì¿¼ë¦¬ì™€ í•„í„° ê²°í•©
                all_must_clauses_2nd = keyword_queries_2nd + filtered_base_2nd + occupation_filters
                if all_must_clauses_2nd:
                    welcome_2nd_query['query'] = {
                        'bool': {
                            'must': all_must_clauses_2nd
                        }
                    }
                    logger.info(f"  âœ… welcome_2nd ì¿¼ë¦¬: í‚¤ì›Œë“œ {len(keyword_queries_2nd)}ê°œ + ì§ì—… í•„í„° {len(occupation_filters)}ê°œ ì ìš©")
                elif occupation_filters:
                    welcome_2nd_query['query'] = {
                        'bool': {
                            'must': occupation_filters
                        }
                    }
                    logger.info(f"  âœ… welcome_2nd í•„í„°: ì§ì—… {len(occupation_filters)}ê°œë§Œ ì ìš©")
        
        # welcome_1st ê²€ìƒ‰
        welcome_1st_keyword_results = []
        welcome_1st_vector_results = []
        if search_welcome_1st:
            logger.info(f"ğŸ“Š [1/3] welcome_1st ê²€ìƒ‰ ì¤‘...")
            try:
                os_response_1st = data_fetcher.search_opensearch(
                    index_name="s_welcome_1st",
                    query=welcome_1st_query,  # â­ ì—°ë ¹/ì„±ë³„ í•„í„°ë§Œ ì ìš©ëœ ì¿¼ë¦¬
                    size=search_size,
                    source_filter=source_filter
                )
                welcome_1st_keyword_results = os_response_1st['hits']['hits']
                logger.info(f"  âœ… OpenSearch: {len(welcome_1st_keyword_results)}ê±´")
                
                # Qdrant ë²¡í„° ê²€ìƒ‰
                if request.use_vector_search and query_vector and hasattr(router, 'qdrant_client'):
                    qdrant_client = router.qdrant_client
                    try:
                        r = qdrant_client.search(
                            collection_name="s_welcome_1st",
                            query_vector=query_vector,
                            limit=qdrant_limit,  # í•„í„° ìœ ë¬´ì— ë”°ë¼ ë¶„ê¸°ëœ limit ì‚¬ìš©
                            score_threshold=0.3,
                        )
                        for item in r:
                            welcome_1st_vector_results.append({
                                '_id': str(item.id),
                                '_score': item.score,
                                '_source': item.payload
                            })
                        logger.info(f"  âœ… Qdrant: {len(welcome_1st_vector_results)}ê±´")
                    except Exception as e:
                        logger.debug(f"  âš ï¸ Qdrant ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            except Exception as e:
                logger.warning(f"  âš ï¸ welcome_1st ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # welcome_2nd ê²€ìƒ‰
        welcome_2nd_keyword_results = []
        welcome_2nd_vector_results = []
        if search_welcome_2nd:
            logger.info(f"ğŸ“Š [2/3] welcome_2nd ê²€ìƒ‰ ì¤‘...")
            try:
                os_response_2nd = data_fetcher.search_opensearch(
                    index_name="s_welcome_2nd",
                    query=welcome_2nd_query,  # â­ ì§ì—… í•„í„°ë§Œ ì ìš©ëœ ì¿¼ë¦¬
                    size=search_size,
                    source_filter=source_filter
                )
                welcome_2nd_keyword_results = os_response_2nd['hits']['hits']
                logger.info(f"  âœ… OpenSearch: {len(welcome_2nd_keyword_results)}ê±´")
                
                # Qdrant ë²¡í„° ê²€ìƒ‰
                if request.use_vector_search and query_vector and hasattr(router, 'qdrant_client'):
                    qdrant_client = router.qdrant_client
                    try:
                        r = qdrant_client.search(
                            collection_name="s_welcome_2nd",
                            query_vector=query_vector,
                            limit=qdrant_limit,  # í•„í„° ìœ ë¬´ì— ë”°ë¼ ë¶„ê¸°ëœ limit ì‚¬ìš©
                            score_threshold=0.3,
                        )
                        for item in r:
                            welcome_2nd_vector_results.append({
                                '_id': str(item.id),
                                '_score': item.score,
                                '_source': item.payload
                            })
                        logger.info(f"  âœ… Qdrant: {len(welcome_2nd_vector_results)}ê±´")
                    except Exception as e:
                        logger.debug(f"  âš ï¸ Qdrant ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            except Exception as e:
                logger.warning(f"  âš ï¸ welcome_2nd ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # ê¸°íƒ€ ì¸ë±ìŠ¤ ê²€ìƒ‰ (survey_* ë“±)
        other_keyword_results = []
        other_vector_results = []
        if search_other_indices:
            logger.info(f"ğŸ“Š [3/3] ê¸°íƒ€ ì¸ë±ìŠ¤ ê²€ìƒ‰ ì¤‘...")
            # welcome_1st, welcome_2ndë¥¼ ì œì™¸í•œ ì¸ë±ìŠ¤ ê²€ìƒ‰
            other_index_pattern = request.index_name
            if request.index_name == '*':
                # survey_* íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰ (welcome_1st, welcome_2nd ì œì™¸)
                other_index_pattern = "survey_*"
            elif 's_welcome_1st' in request.index_name or 's_welcome_2nd' in request.index_name:
                # welcome ì¸ë±ìŠ¤ë¥¼ ì œì™¸í•œ íŒ¨í„´ ìƒì„±
                indices = [idx.strip() for idx in request.index_name.split(',')]
                other_indices = [idx for idx in indices if idx not in ['s_welcome_1st', 's_welcome_2nd']]
                if other_indices:
                    other_index_pattern = ','.join(other_indices)
                else:
                    search_other_indices = False
            
            if search_other_indices:
                try:
                    os_response_other = data_fetcher.search_opensearch(
                        index_name=other_index_pattern,
                        query=final_query,
                        size=search_size,
                        source_filter=source_filter
                    )
                    other_keyword_results = os_response_other['hits']['hits']
                    logger.info(f"  âœ… OpenSearch: {len(other_keyword_results)}ê±´")
                    
                    # Qdrant ë²¡í„° ê²€ìƒ‰ (ê¸°íƒ€ ì»¬ë ‰ì…˜)
                    if request.use_vector_search and query_vector and hasattr(router, 'qdrant_client'):
                        qdrant_client = router.qdrant_client
                        try:
                            collections = qdrant_client.get_collections()
                            for col in collections.collections:
                                if col.name not in ['s_welcome_1st', 's_welcome_2nd']:
                                    try:
                                        r = qdrant_client.search(
                                            collection_name=col.name,
                                            query_vector=query_vector,
                                            limit=qdrant_limit,  # í•„í„° ìœ ë¬´ì— ë”°ë¼ ë¶„ê¸°ëœ limit ì‚¬ìš©
                                            score_threshold=0.3,
                                        )
                                        for item in r:
                                            other_vector_results.append({
                                                '_id': str(item.id),
                                                '_score': item.score,
                                                '_source': item.payload
                                            })
                                    except Exception:
                                        continue
                            logger.info(f"  âœ… Qdrant: {len(other_vector_results)}ê±´")
                        except Exception as e:
                            logger.debug(f"  âš ï¸ Qdrant ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                except Exception as e:
                    logger.warning(f"  âš ï¸ ê¸°íƒ€ ì¸ë±ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # â­ STEP 2: ê° ì¸ë±ìŠ¤ë³„ RRF ê²°í•©
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“Š STEP 2: ê° ì¸ë±ìŠ¤ë³„ RRF ê²°í•©")
        logger.info(f"{'='*60}")
        
        # welcome_1st RRF ê²°í•©
        welcome_1st_rrf = []
        if welcome_1st_keyword_results or welcome_1st_vector_results:
            logger.info(f"ğŸ”„ [1/3] welcome_1st RRF ê²°í•© ì¤‘...")
            logger.info(f"  - í‚¤ì›Œë“œ: {len(welcome_1st_keyword_results)}ê±´, ë²¡í„°: {len(welcome_1st_vector_results)}ê±´")
            if welcome_1st_vector_results:
                welcome_1st_rrf = calculate_rrf_score(welcome_1st_keyword_results, welcome_1st_vector_results, k=60)
                logger.info(f"  âœ… welcome_1st RRF ì™„ë£Œ: {len(welcome_1st_rrf)}ê±´")
            else:
                welcome_1st_rrf = welcome_1st_keyword_results
                logger.info(f"  âœ… welcome_1st í‚¤ì›Œë“œë§Œ ì‚¬ìš©: {len(welcome_1st_rrf)}ê±´")
        
        # welcome_2nd RRF ê²°í•©
        welcome_2nd_rrf = []
        if welcome_2nd_keyword_results or welcome_2nd_vector_results:
            logger.info(f"ğŸ”„ [2/3] welcome_2nd RRF ê²°í•© ì¤‘...")
            logger.info(f"  - í‚¤ì›Œë“œ: {len(welcome_2nd_keyword_results)}ê±´, ë²¡í„°: {len(welcome_2nd_vector_results)}ê±´")
            if welcome_2nd_vector_results:
                welcome_2nd_rrf = calculate_rrf_score(welcome_2nd_keyword_results, welcome_2nd_vector_results, k=60)
                logger.info(f"  âœ… welcome_2nd RRF ì™„ë£Œ: {len(welcome_2nd_rrf)}ê±´")
            else:
                welcome_2nd_rrf = welcome_2nd_keyword_results
                logger.info(f"  âœ… welcome_2nd í‚¤ì›Œë“œë§Œ ì‚¬ìš©: {len(welcome_2nd_rrf)}ê±´")
        
        # ê¸°íƒ€ ì¸ë±ìŠ¤ RRF ê²°í•©
        other_rrf = []
        if other_keyword_results or other_vector_results:
            logger.info(f"ğŸ”„ [3/3] ê¸°íƒ€ ì¸ë±ìŠ¤ RRF ê²°í•© ì¤‘...")
            logger.info(f"  - í‚¤ì›Œë“œ: {len(other_keyword_results)}ê±´, ë²¡í„°: {len(other_vector_results)}ê±´")
            if other_vector_results:
                other_rrf = calculate_rrf_score(other_keyword_results, other_vector_results, k=60)
                logger.info(f"  âœ… ê¸°íƒ€ ì¸ë±ìŠ¤ RRF ì™„ë£Œ: {len(other_rrf)}ê±´")
            else:
                other_rrf = other_keyword_results
                logger.info(f"  âœ… ê¸°íƒ€ ì¸ë±ìŠ¤ í‚¤ì›Œë“œë§Œ ì‚¬ìš©: {len(other_rrf)}ê±´")
        
        # user_id ë° _id -> ì›ë³¸ ë¬¸ì„œ ë§¤í•‘ ìƒì„± (ëª¨ë“  ì¸ë±ìŠ¤ ê²°ê³¼ì—ì„œ)
        user_doc_map = {}
        id_doc_map = {}  # _id ê¸°ë°˜ ë§¤í•‘ë„ ì¶”ê°€
        
        # welcome_1st ë§¤í•‘
        for hit in welcome_1st_keyword_results:
            source = hit.get('_source', {})
            user_id = source.get('user_id')
            doc_id = hit.get('_id')
            
            doc_info = {
                'source': source,
                'inner_hits': hit.get('inner_hits', {}),
                'highlight': hit.get('highlight'),
                'index': 's_welcome_1st'
            }
            
            if user_id:
                user_doc_map[user_id] = doc_info
            if doc_id:
                id_doc_map[doc_id] = doc_info
        
        # welcome_2nd ë§¤í•‘
        for hit in welcome_2nd_keyword_results:
            source = hit.get('_source', {})
            user_id = source.get('user_id')
            doc_id = hit.get('_id')
            
            doc_info = {
                'source': source,
                'inner_hits': hit.get('inner_hits', {}),
                'highlight': hit.get('highlight'),
                'index': 's_welcome_2nd'
            }
            
            if user_id:
                user_doc_map[user_id] = doc_info
            if doc_id:
                id_doc_map[doc_id] = doc_info
        
        # ê¸°íƒ€ ì¸ë±ìŠ¤ ë§¤í•‘
        for hit in other_keyword_results:
            source = hit.get('_source', {})
            user_id = source.get('user_id')
            doc_id = hit.get('_id')
            
            doc_info = {
                'source': source,
                'inner_hits': hit.get('inner_hits', {}),
                'highlight': hit.get('highlight'),
                'index': hit.get('_index', 'unknown')
            }
            
            if user_id:
                user_doc_map[user_id] = doc_info
            if doc_id:
                id_doc_map[doc_id] = doc_info

        # â­ STEP 3: ì¸ë±ìŠ¤ ê°„ RRF ì¬ê²°í•©
        # welcome_1st, welcome_2nd, ê¸°íƒ€ ì¸ë±ìŠ¤ì˜ RRF ê²°ê³¼ë¥¼ user_id ê¸°ì¤€ìœ¼ë¡œ RRF ì¬ê²°í•©
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“Š STEP 3: ì¸ë±ìŠ¤ ê°„ RRF ì¬ê²°í•©")
        logger.info(f"{'='*60}")
        logger.info(f"  - welcome_1st RRF: {len(welcome_1st_rrf)}ê±´")
        logger.info(f"  - welcome_2nd RRF: {len(welcome_2nd_rrf)}ê±´")
        logger.info(f"  - ê¸°íƒ€ ì¸ë±ìŠ¤ RRF: {len(other_rrf)}ê±´")
        
        # user_id ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ì—¬ RRF ì¬ê²°í•©
        user_rrf_map = {}  # user_id -> [doc1, doc2, ...]
        
        # welcome_1st RRF ê²°ê³¼ ê·¸ë£¹í™”
        for doc in welcome_1st_rrf:
            source = doc.get('_source', {})
            if not source and 'doc' in doc:
                source = doc.get('doc', {}).get('_source', {})
            user_id = source.get('user_id') if isinstance(source, dict) else None
            if not user_id:
                user_id = doc.get('_id', '')
            
            if user_id:
                if user_id not in user_rrf_map:
                    user_rrf_map[user_id] = []
                # ì¸ë±ìŠ¤ ì •ë³´ ì¶”ê°€
                doc['_index'] = 's_welcome_1st'
                user_rrf_map[user_id].append(doc)
        
        # welcome_2nd RRF ê²°ê³¼ ê·¸ë£¹í™”
        for doc in welcome_2nd_rrf:
            source = doc.get('_source', {})
            if not source and 'doc' in doc:
                source = doc.get('doc', {}).get('_source', {})
            user_id = source.get('user_id') if isinstance(source, dict) else None
            if not user_id:
                user_id = doc.get('_id', '')
            
            if user_id:
                if user_id not in user_rrf_map:
                    user_rrf_map[user_id] = []
                # ì¸ë±ìŠ¤ ì •ë³´ ì¶”ê°€
                doc['_index'] = 's_welcome_2nd'
                user_rrf_map[user_id].append(doc)
        
        # ê¸°íƒ€ ì¸ë±ìŠ¤ RRF ê²°ê³¼ ê·¸ë£¹í™”
        for doc in other_rrf:
            source = doc.get('_source', {})
            if not source and 'doc' in doc:
                source = doc.get('doc', {}).get('_source', {})
            user_id = source.get('user_id') if isinstance(source, dict) else None
            if not user_id:
                user_id = doc.get('_id', '')
            
            if user_id:
                if user_id not in user_rrf_map:
                    user_rrf_map[user_id] = []
                # ì¸ë±ìŠ¤ ì •ë³´ ìœ ì§€ ë˜ëŠ” ì¶”ê°€
                if '_index' not in doc:
                    doc['_index'] = source.get('index', 'unknown') if isinstance(source, dict) else 'unknown'
                user_rrf_map[user_id].append(doc)
        
        # user_idë³„ë¡œ RRF ì¬ê²°í•©
        # ê°™ì€ user_idì˜ ì—¬ëŸ¬ ë¬¸ì„œê°€ ìˆìœ¼ë©´, ê°ê°ì„ ë…ë¦½ì ì¸ ê²°ê³¼ë¡œ ê°„ì£¼í•˜ê³  RRF ì ìˆ˜ë¥¼ í•©ì‚°
        final_rrf_results = []
        for user_id, user_docs in user_rrf_map.items():
            if len(user_docs) == 1:
                # ë‹¨ì¼ ë¬¸ì„œ: ê·¸ëŒ€ë¡œ ì‚¬ìš©
                final_rrf_results.append(user_docs[0])
            else:
                # ì—¬ëŸ¬ ë¬¸ì„œ: RRF ì ìˆ˜ë¥¼ í•©ì‚°í•˜ì—¬ ëŒ€í‘œ ë¬¸ì„œ ì„ íƒ
                # ê° ë¬¸ì„œì˜ RRF ì ìˆ˜ë¥¼ í•©ì‚°
                total_rrf_score = sum(
                    doc.get('_score', 0.0) or doc.get('rrf_score', 0.0)
                    for doc in user_docs
                )
                # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ë¬¸ì„œë¥¼ ëŒ€í‘œë¡œ ì„ íƒ
                best_doc = max(user_docs, key=lambda d: d.get('_score', 0.0) or d.get('rrf_score', 0.0))
                # í•©ì‚°ëœ RRF ì ìˆ˜ë¡œ ì—…ë°ì´íŠ¸
                best_doc['_score'] = total_rrf_score
                best_doc['_rrf_details'] = {
                    'combined_score': total_rrf_score,
                    'source_count': len(user_docs),
                    'sources': [d.get('_index', 'unknown') for d in user_docs]
                }
                final_rrf_results.append(best_doc)
        
        # RRF ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        final_rrf_results.sort(
            key=lambda d: d.get('_score', 0.0) or d.get('rrf_score', 0.0),
            reverse=True
        )
        
        rrf_results = final_rrf_results
        took_ms = 0  # ì—¬ëŸ¬ ê²€ìƒ‰ì˜ í•©ì´ë¯€ë¡œ ì •í™•í•œ ì‹œê°„ ì¸¡ì •ì€ ì–´ë ¤ì›€
        
        logger.info(f"  âœ… ì¸ë±ìŠ¤ ê°„ RRF ì¬ê²°í•© ì™„ë£Œ: {len(rrf_results)}ê±´ (ê³ ìœ  user_id: {len(user_rrf_map)}ê°œ)")
        
        # RRF ì ìˆ˜ ë””ë²„ê¹…: ìƒìœ„ 10ê°œ ì¶œë ¥
        if rrf_results:
            logger.info(f"  - RRF ì ìˆ˜ ìƒìœ„ 10ê°œ:")
            for i, doc in enumerate(rrf_results[:10], 1):
                rrf_score = doc.get('_score') or doc.get('rrf_score', 0.0)
                rrf_details = doc.get('_rrf_details', {})
                doc_index = doc.get('_index', 'unknown')
                logger.info(f"    {i}. doc_id={doc.get('_id', 'N/A')}, index={doc_index}, RRF={rrf_score:.6f}, "
                          f"keyword_rank={rrf_details.get('keyword_rank')}, vector_rank={rrf_details.get('vector_rank')}")
        
        # í•„í„°ê°€ ìˆëŠ” ê²½ìš°, í•„í„° ì¡°ê±´ì— ë§ëŠ” ê²°ê³¼ë§Œ ìœ ì§€
        final_hits = rrf_results
        # ë°°ì¹˜ ì¡°íšŒ ê²°ê³¼ë¥¼ ë£¨í”„ ë°–ì—ì„œ ì„ ì–¸ (í•„í„° ì¬ì ìš©ê³¼ ê²°ê³¼ í¬ë§·íŒ… ëª¨ë‘ì—ì„œ ì‚¬ìš©)
        welcome_1st_batch = {}
        welcome_2nd_batch = {}
        
        if filters:
            # ì„±ëŠ¥ ìµœì í™”: ë°°ì¹˜ ì¡°íšŒë¥¼ ìœ„í•´ ë¨¼ì € ëª¨ë“  user_id ìˆ˜ì§‘
            user_ids_to_fetch = set()
            doc_user_map = {}  # doc -> user_id ë§¤í•‘
            
            logger.info(f"ğŸ” user_id ìˆ˜ì§‘ ì¤‘: RRF ê²°ê³¼ {len(rrf_results)}ê±´...")
            for doc in rrf_results:
                # source ì¶”ì¶œ (ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„)
                source = doc.get('_source', {})
                if not source and 'doc' in doc:
                    source = doc.get('doc', {}).get('_source', {})
                
                # Qdrant ê²°ê³¼ì¸ ê²½ìš° payloadì—ì„œ ì¶”ì¶œ
                if not source or not isinstance(source, dict):
                    # Qdrant ê²°ê³¼ëŠ” payloadì— ìˆì„ ìˆ˜ ìˆìŒ
                    if 'payload' in doc:
                        payload = doc.get('payload', {})
                        if isinstance(payload, dict) and payload:
                            source = payload
                    elif isinstance(source, dict) and 'payload' in source:
                        payload = source.get('payload', {})
                        if isinstance(payload, dict) and payload:
                            source = payload
                
                # user_id ì¶”ì¶œ (ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„)
                user_id = None
                if isinstance(source, dict):
                    user_id = source.get('user_id')
                
                if not user_id:
                    user_id = doc.get('_id', '')
                
                if not user_id and 'payload' in doc:
                    payload = doc.get('payload', {})
                    if isinstance(payload, dict):
                        user_id = payload.get('user_id')
                
                if user_id:
                    user_ids_to_fetch.add(user_id)
                    doc_user_map[id(doc)] = user_id
            
            logger.info(f"  âœ… ìˆ˜ì§‘ëœ user_id: {len(user_ids_to_fetch)}ê±´")
            
            # â­ ë””ë²„ê¹…: user_id ìƒ˜í”Œ ë¡œê¹… (ì²˜ìŒ 10ê°œ)
            if user_ids_to_fetch:
                sample_user_ids = list(user_ids_to_fetch)[:10]
                logger.info(f"  ğŸ“‹ user_id ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ): {sample_user_ids}")
            
            # â­ ë°°ì¹˜ ì¡°íšŒ: welcome_1stì™€ welcome_2ndë¥¼ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì¡°íšŒ (íƒ€ì„ì•„ì›ƒ ë°©ì§€)
            if user_ids_to_fetch:
                user_ids_list = list(user_ids_to_fetch)
                batch_size = 200  # ë°°ì¹˜ í¬ê¸°: 200ê±´ì”© ë¶„í• 
                total_batches = (len(user_ids_list) + batch_size - 1) // batch_size
                logger.info(f"ğŸ” ë°°ì¹˜ ì¡°íšŒ: welcome_1st/welcome_2nd {len(user_ids_list)}ê±´ ì¡°íšŒ ì¤‘... (ë°°ì¹˜ í¬ê¸°: {batch_size}, ì´ {total_batches}ê°œ ë°°ì¹˜)")
                
                try:
                    # welcome_1st ë°°ì¹˜ ì¡°íšŒ (ë¶„í• )
                    if user_ids_list:
                        found_count = 0
                        for batch_idx in range(0, len(user_ids_list), batch_size):
                            batch_ids = user_ids_list[batch_idx:batch_idx + batch_size]
                            batch_num = (batch_idx // batch_size) + 1
                            try:
                                mget_body = [{"_index": "s_welcome_1st", "_id": uid} for uid in batch_ids]
                                mget_response = os_client.mget(body={"docs": mget_body}, ignore=[404], request_timeout=60)
                                for item in mget_response.get('docs', []):
                                    if item.get('found'):
                                        welcome_1st_batch[item['_id']] = item['_source']
                                        found_count += 1
                                logger.debug(f"  ğŸ“¦ welcome_1st ë°°ì¹˜ {batch_num}/{total_batches}: {len([d for d in mget_response.get('docs', []) if d.get('found')])}/{len(batch_ids)}ê±´")
                            except Exception as e:
                                logger.warning(f"  âš ï¸ welcome_1st ë°°ì¹˜ {batch_num}/{total_batches} ì‹¤íŒ¨: {e}")
                                continue
                        logger.info(f"  âœ… welcome_1st ë°°ì¹˜ ì¡°íšŒ: {found_count}/{len(user_ids_list)}ê±´ ì„±ê³µ")
                    
                    # welcome_2nd ë°°ì¹˜ ì¡°íšŒ (ë¶„í• )
                    if user_ids_list:
                        found_count = 0
                        for batch_idx in range(0, len(user_ids_list), batch_size):
                            batch_ids = user_ids_list[batch_idx:batch_idx + batch_size]
                            batch_num = (batch_idx // batch_size) + 1
                            try:
                                mget_body = [{"_index": "s_welcome_2nd", "_id": uid} for uid in batch_ids]
                                mget_response = os_client.mget(body={"docs": mget_body}, ignore=[404], request_timeout=60)
                                for item in mget_response.get('docs', []):
                                    if item.get('found'):
                                        welcome_2nd_batch[item['_id']] = item['_source']
                                        found_count += 1
                                logger.debug(f"  ğŸ“¦ welcome_2nd ë°°ì¹˜ {batch_num}/{total_batches}: {len([d for d in mget_response.get('docs', []) if d.get('found')])}/{len(batch_ids)}ê±´")
                            except Exception as e:
                                logger.warning(f"  âš ï¸ welcome_2nd ë°°ì¹˜ {batch_num}/{total_batches} ì‹¤íŒ¨: {e}")
                                continue
                        logger.info(f"  âœ… welcome_2nd ë°°ì¹˜ ì¡°íšŒ: {found_count}/{len(user_ids_list)}ê±´ ì„±ê³µ")
                    
                    logger.info(f"  âœ… ë°°ì¹˜ ì¡°íšŒ ì™„ë£Œ: welcome_1st={len(welcome_1st_batch)}ê±´, welcome_2nd={len(welcome_2nd_batch)}ê±´")
                    
                    # â­ ë°°ì¹˜ ì¡°íšŒì—ì„œ ì°¾ì§€ ëª»í•œ user_idì— ëŒ€í•´ ê°œë³„ ì¡°íšŒ ì‹œë„ (fallback) - ì œí•œì ìœ¼ë¡œë§Œ
                    missing_1st = user_ids_to_fetch - set(welcome_1st_batch.keys())
                    missing_2nd = user_ids_to_fetch - set(welcome_2nd_batch.keys())
                    
                    # ê°œë³„ ì¡°íšŒëŠ” ìµœëŒ€ 100ê±´ê¹Œì§€ë§Œ (ì„±ëŠ¥ ê³ ë ¤)
                    if missing_1st and len(missing_1st) <= 100:
                        logger.info(f"  ğŸ” welcome_1st ê°œë³„ ì¡°íšŒ ì‹œë„: {len(missing_1st)}ê±´...")
                        for uid in list(missing_1st)[:50]:  # ìµœëŒ€ 50ê±´ë§Œ
                            try:
                                os_doc = os_client.get(index="s_welcome_1st", id=uid, ignore=[404], request_timeout=60)
                                if os_doc.get('found'):
                                    welcome_1st_batch[uid] = os_doc['_source']
                            except Exception:
                                continue
                        logger.info(f"  âœ… welcome_1st ê°œë³„ ì¡°íšŒ: {len([k for k in missing_1st if k in welcome_1st_batch])}ê±´ ì¶”ê°€ ì„±ê³µ")
                    
                    if missing_2nd and len(missing_2nd) <= 100:
                        logger.info(f"  ğŸ” welcome_2nd ê°œë³„ ì¡°íšŒ ì‹œë„: {len(missing_2nd)}ê±´...")
                        for uid in list(missing_2nd)[:50]:  # ìµœëŒ€ 50ê±´ë§Œ
                            try:
                                os_doc = os_client.get(index="s_welcome_2nd", id=uid, ignore=[404], request_timeout=60)
                                if os_doc.get('found'):
                                    welcome_2nd_batch[uid] = os_doc['_source']
                            except Exception:
                                continue
                        logger.info(f"  âœ… welcome_2nd ê°œë³„ ì¡°íšŒ: {len([k for k in missing_2nd if k in welcome_2nd_batch])}ê±´ ì¶”ê°€ ì„±ê³µ")
                        
                except Exception as e:
                    logger.warning(f"  âš ï¸ ë°°ì¹˜ ì¡°íšŒ ì‹¤íŒ¨: {e}, ê°œë³„ ì¡°íšŒë¡œ fallback")
            
            # â­ ë””ë²„ê¹…: í•„í„°ë§ ì „ RRF ê²°ê³¼ ë¶„ì„
            logger.info(f"ğŸ“Š í•„í„°ë§ ì „ RRF ê²°ê³¼ ë¶„ì„:")
            logger.info(f"  - ì´ RRF ê²°ê³¼: {len(rrf_results)}ê±´")
            logger.info(f"  - welcome_1st ë°°ì¹˜ ì¡°íšŒ: {len(welcome_1st_batch)}ê±´")
            logger.info(f"  - welcome_2nd ë°°ì¹˜ ì¡°íšŒ: {len(welcome_2nd_batch)}ê±´")
            
            # ìƒ˜í”Œ 10ê°œ ë¶„ì„
            for i, doc in enumerate(rrf_results[:10]):
                source = doc.get('_source', {})
                if not source and 'doc' in doc:
                    source = doc.get('doc', {}).get('_source', {})
                user_id = source.get('user_id') if isinstance(source, dict) else doc.get('_id', '')
                logger.info(f"  ìƒ˜í”Œ {i+1}. user_id={user_id}, metadata={source.get('metadata', {}) if isinstance(source, dict) else 'N/A'}")
            
            # í•„í„° ì¬ì ìš©
            filtered_rrf_results = []
            source_not_found_count = 0
            low_score_count = 0
            opposite_count = 0
            
            # í•„í„°ë³„ ë¯¸ì¶©ì¡± í†µê³„
            age_filter_failed = 0
            gender_filter_failed = 0
            occupation_filter_failed = 0
            both_filters_failed = 0
            age_filter_failed_count = 0  # ë””ë²„ê¹…ìš© ì¹´ìš´í„°
            
            for doc in rrf_results:
                # source ì¶”ì¶œ (ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„)
                source = doc.get('_source', {})
                if not source and 'doc' in doc:
                    source = doc.get('doc', {}).get('_source', {})
                
                # Qdrant ê²°ê³¼ì¸ ê²½ìš° payloadì—ì„œ ì¶”ì¶œ
                if not source or not isinstance(source, dict):
                    payload = source.get('payload', {}) if isinstance(source, dict) else {}
                    if isinstance(payload, dict) and payload:
                        source = payload
                
                # user_idë¡œ OpenSearchì—ì„œ ì‹¤ì œ ë¬¸ì„œ ì¡°íšŒ (í•„í„° í™•ì¸ì„ ìœ„í•´)
                user_id = source.get('user_id') if isinstance(source, dict) else None
                if not user_id:
                    user_id = doc.get('_id', '')
                
                # OpenSearchì—ì„œ ì‹¤ì œ ë¬¸ì„œ ì¡°íšŒ (í•„í„° í™•ì¸ì„ ìœ„í•´)
                if user_id and user_id in user_doc_map:
                    source = user_doc_map[user_id]['source']
                elif user_id:
                    # ì§ì ‘ ì¡°íšŒ ì‹œë„
                    try:
                        for idx_name in [request.index_name] if request.index_name != '*' else ['s_welcome_2nd', 'survey_250106', 'survey_250107']:
                            try:
                                os_doc = os_client.get(index=idx_name, id=user_id, ignore=[404], request_timeout=60)
                                if os_doc.get('found'):
                                    source = os_doc['_source']
                                    break
                            except Exception:
                                continue
                    except Exception:
                        pass
                
                if not source or not isinstance(source, dict):
                    # sourceë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ í•„í„° í†µê³¼ ë¶ˆê°€
                    source_not_found_count += 1
                    continue
                
                # â­ í•„í„° ì¡°ê±´ í™•ì¸ (must: ëª¨ë“  í•„í„°ë¥¼ ë§Œì¡±í•´ì•¼ í•¨)
                # welcome_1st: ì—°ë ¹/ì„±ë³„ ì •ë³´, welcome_2nd: ì§ì—… ì •ë³´
                # user_idë¡œ ì¸ë±ìŠ¤ ê°„ ë°ì´í„°ë¥¼ ì—°ê²°í•˜ì—¬ í™•ì¸
                matches_all_filters = True
                
                # user_idë¡œ welcome_1stì™€ welcome_2ndì—ì„œ ê°ê° ì •ë³´ í™•ì¸
                user_id = source.get('user_id') if isinstance(source, dict) else None
                if not user_id:
                    user_id = doc.get('_id', '')
                    # doc_user_mapì—ì„œë„ í™•ì¸
                    if not user_id:
                        user_id = doc_user_map.get(id(doc))
                
                # â­ ë°°ì¹˜ ì¡°íšŒ ê²°ê³¼ì—ì„œ ê°€ì ¸ì˜¤ê¸° (ìºì‹œëœ ë°ì´í„° ì‚¬ìš©)
                welcome_1st_source = welcome_1st_batch.get(user_id) if user_id else None
                welcome_1st_found = welcome_1st_source is not None
                
                # â­ ë°°ì¹˜ ì¡°íšŒì—ì„œ ì°¾ì§€ ëª»í•œ ê²½ìš° ê°œë³„ ì¡°íšŒ ì‹œë„ (fallback)
                if not welcome_1st_found and user_id:
                    try:
                        os_doc = os_client.get(index="s_welcome_1st", id=user_id, ignore=[404], request_timeout=60)
                        if os_doc.get('found'):
                            welcome_1st_source = os_doc['_source']
                            welcome_1st_batch[user_id] = welcome_1st_source  # ìºì‹œì— ì¶”ê°€
                            welcome_1st_found = True
                    except Exception:
                        pass
                
                # welcome_2ndì—ì„œ ì§ì—… ì •ë³´ í™•ì¸ (í˜„ì¬ sourceê°€ welcome_2ndì¼ ìˆ˜ ìˆìŒ)
                welcome_2nd_source = source if source.get('metadata', {}).get('occupation') != 'ë¯¸ì •' or any('ì§ì—…' in str(qa.get('q_text', '')) for qa in source.get('qa_pairs', [])) else None
                welcome_2nd_found = bool(welcome_2nd_source)
                
                # ë°°ì¹˜ ì¡°íšŒ ê²°ê³¼ì—ì„œ ê°€ì ¸ì˜¤ê¸° (fallback)
                if not welcome_2nd_source and user_id:
                    welcome_2nd_source = welcome_2nd_batch.get(user_id)
                    welcome_2nd_found = welcome_2nd_source is not None
                
                # â­ ë°°ì¹˜ ì¡°íšŒì—ì„œ ì°¾ì§€ ëª»í•œ ê²½ìš° ê°œë³„ ì¡°íšŒ ì‹œë„ (fallback)
                if not welcome_2nd_found and user_id:
                    try:
                        os_doc = os_client.get(index="s_welcome_2nd", id=user_id, ignore=[404], request_timeout=60)
                        if os_doc.get('found'):
                            welcome_2nd_source = os_doc['_source']
                            welcome_2nd_batch[user_id] = welcome_2nd_source  # ìºì‹œì— ì¶”ê°€
                            welcome_2nd_found = True
                    except Exception:
                        pass
                
                # ë””ë²„ê¹…: welcome_1st/welcome_2nd ì¡°íšŒ ê²°ê³¼ ë¡œê¹… (ì²˜ìŒ 10ê°œë§Œ)
                # âš ï¸ ì—°ë ¹ í•„í„° ì‹¤íŒ¨ê°€ ë§ìœ¼ë¯€ë¡œ ë” ìì„¸íˆ ë¡œê¹…
                if opposite_count < 10 or (opposite_count < 20 and not welcome_1st_found):
                    logger.warning(f"ğŸ” user_id={user_id}: welcome_1st={welcome_1st_found}, welcome_2nd={welcome_2nd_found}, source_index={source.get('_index', 'unknown')}")
                    if not welcome_1st_found and user_id:
                        logger.warning(f"   âš ï¸ welcome_1st ì¡°íšŒ ì‹¤íŒ¨ (ë°°ì¹˜+ê°œë³„ ëª¨ë‘ ì‹œë„í–ˆì§€ë§Œ ì°¾ì§€ ëª»í•¨): user_id={user_id}")
                
                # ê° demographic í•„í„° í™•ì¸ (must: ëª¨ë“  í•„í„°ë¥¼ ë§Œì¡±í•´ì•¼ í•¨)
                filter_match_details = {}  # ë””ë²„ê¹…ìš©
                for demo in extracted_entities.demographics:
                    matches_this_filter = False
                    match_source = None  # ì–´ë””ì„œ ë§¤ì¹­ë˜ì—ˆëŠ”ì§€ ì¶”ì 
                    
                    if demo.demographic_type == DemographicType.AGE:
                        from datetime import datetime
                        # â­ 1ìˆœìœ„: welcome_1stì—ì„œ ì—°ë ¹ ì •ë³´ í™•ì¸
                        if welcome_1st_source:
                            age_group = welcome_1st_source.get('metadata', {}).get('age_group', '')
                            birth_year = welcome_1st_source.get('metadata', {}).get('birth_year', '')
                            
                            if age_group == demo.raw_value:
                                matches_this_filter = True
                                match_source = f"welcome_1st.metadata.age_group={age_group}"
                            elif birth_year and birth_year != 'ë¯¸ì •':
                                # ì¶œìƒë…„ë„ë¡œ ê³„ì‚°
                                current_year = datetime.now().year
                                try:
                                    birth_year_int = int(birth_year)
                                    age = current_year - birth_year_int
                                    
                                    if demo.raw_value == "30ëŒ€" and 30 <= age < 40:
                                        matches_this_filter = True
                                        match_source = f"welcome_1st.metadata.birth_year={birth_year} (age={age})"
                                    elif demo.raw_value == "20ëŒ€" and 20 <= age < 30:
                                        matches_this_filter = True
                                        match_source = f"welcome_1st.metadata.birth_year={birth_year} (age={age})"
                                    elif demo.raw_value == "40ëŒ€" and 40 <= age < 50:
                                        matches_this_filter = True
                                        match_source = f"welcome_1st.metadata.birth_year={birth_year} (age={age})"
                                except (ValueError, TypeError):
                                    pass
                        
                        # â­ ì—°ë ¹ ì •ë³´ëŠ” welcome_1stì—ë§Œ ìˆìœ¼ë¯€ë¡œ, welcome_1st_sourceê°€ ì—†ìœ¼ë©´ í•„í„° í†µê³¼ ë¶ˆê°€
                        # ë””ë²„ê¹…: ì—°ë ¹ í•„í„° ì‹¤íŒ¨ ì‹œ ìƒì„¸ ë¡œê¹… (ì²˜ìŒ 10ê°œë§Œ)
                        if not matches_this_filter and age_filter_failed_count < 10:
                            logger.warning(f"ğŸ” [ì—°ë ¹ í•„í„° ì‹¤íŒ¨] user_id={user_id}:")
                            logger.warning(f"   - ìš”ì²­ ì—°ë ¹: {demo.raw_value}")
                            if welcome_1st_source:
                                age_group = welcome_1st_source.get('metadata', {}).get('age_group', '')
                                birth_year = welcome_1st_source.get('metadata', {}).get('birth_year', '')
                                logger.warning(f"   - welcome_1st.age_group: '{age_group}'")
                                logger.warning(f"   - welcome_1st.birth_year: '{birth_year}'")
                                logger.warning(f"   - age_group ë§¤ì¹­: {age_group == demo.raw_value}")
                                if birth_year and birth_year != 'ë¯¸ì •':
                                    try:
                                        age = datetime.now().year - int(birth_year)
                                        logger.warning(f"   - ê³„ì‚°ëœ ë‚˜ì´: {age}ì„¸")
                                        logger.warning(f"   - 30ëŒ€ ë²”ìœ„ ì²´í¬: {30 <= age < 40}")
                                    except:
                                        pass
                            else:
                                logger.warning(f"   - welcome_1st: ì—†ìŒ (ì—°ë ¹ ì •ë³´ëŠ” welcome_1stì—ë§Œ ìˆìŒ)")
                        
                        # â­ í•„í„° ì‹¤íŒ¨ ì‹œ ì¹´ìš´í„° ì¦ê°€
                        if not matches_this_filter:
                            age_filter_failed_count += 1
                    
                    elif demo.demographic_type == DemographicType.GENDER:
                        # â­ ë™ì˜ì–´ í™•ì¥ê¸° ì‚¬ìš©
                        try:
                            from rag_query_analyzer.utils.synonym_expander import get_synonym_expander
                            expander = get_synonym_expander()
                            gender_synonyms = expander.expand(demo.raw_value)
                        except Exception:
                            # ë™ì˜ì–´ í™•ì¥ê¸° ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë™ì˜ì–´ ì‚¬ìš©
                            gender_synonyms = [demo.raw_value]
                            gender_synonyms.extend([syn for syn in demo.synonyms if syn])
                        
                        # welcome_1stì—ì„œ ì„±ë³„ ì •ë³´ í™•ì¸
                        if welcome_1st_source:
                            gender = welcome_1st_source.get('metadata', {}).get('gender', '')
                            # â­ ë™ì˜ì–´ í™•ì¥ëœ ê°’ë“¤ê³¼ ë§¤ì¹­
                            if gender in gender_synonyms:
                                matches_this_filter = True
                                match_source = f"welcome_1st.metadata.gender={gender}"
                        
                        # qa_pairsì—ì„œë„ í™•ì¸ (fallback)
                        if not matches_this_filter:
                            for src in [welcome_1st_source, source]:
                                if not src:
                                    continue
                                qa_pairs_list = src.get('qa_pairs', [])
                                if isinstance(qa_pairs_list, list):
                                    for qa in qa_pairs_list:
                                        if isinstance(qa, dict):
                                            q_text = qa.get('q_text', '')
                                            answer = qa.get('answer', qa.get('answer_text', ''))
                                            
                                            if 'ì„±ë³„' in q_text or 'gender' in q_text.lower():
                                                answer_str = str(answer).lower()
                                                # â­ ë™ì˜ì–´ í™•ì¥ëœ ê°’ë“¤ê³¼ ë§¤ì¹­
                                                if any(syn.lower() in answer_str or syn in str(answer) for syn in gender_synonyms):
                                                    matches_this_filter = True
                                                    match_source = f"qa_pairs.{q_text}={answer}"
                                                    break
                    
                    elif demo.demographic_type == DemographicType.OCCUPATION:
                        # â­ ë™ì˜ì–´ í™•ì¥ê¸° ì‚¬ìš©
                        try:
                            from rag_query_analyzer.utils.synonym_expander import get_synonym_expander
                            expander = get_synonym_expander()
                            occupation_synonyms = expander.expand(demo.raw_value)
                        except Exception:
                            # ë™ì˜ì–´ í™•ì¥ê¸° ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë™ì˜ì–´ ì‚¬ìš©
                            occupation_synonyms = [demo.raw_value]
                            occupation_synonyms.extend([syn for syn in demo.synonyms if syn])
                        
                        # â­ ì§ì—… ì •ë³´ëŠ” qa_pairsì—ì„œë§Œ í™•ì¸ (metadata.occupation í•„ë“œê°€ ì—†ê±°ë‚˜ "ë¯¸ì •"ì¸ ê²½ìš°ê°€ ë§ìŒ)
                        # welcome_2nd_source ìš°ì„  í™•ì¸
                        if welcome_2nd_source:
                            qa_pairs_list = welcome_2nd_source.get('qa_pairs', [])
                            if isinstance(qa_pairs_list, list):
                                for qa in qa_pairs_list:
                                    if isinstance(qa, dict):
                                        q_text = qa.get('q_text', '')
                                        answer = qa.get('answer', qa.get('answer_text', ''))
                                        
                                        # ì§ì—… ì§ˆë¬¸ í™•ì¸
                                        if 'ì§ì—…' in q_text or 'occupation' in q_text.lower() or 'ì§ë¬´' in q_text:
                                            answer_str = str(answer).lower()
                                            # â­ ë™ì˜ì–´ í™•ì¥ëœ ê°’ë“¤ê³¼ ë§¤ì¹­
                                            if any(syn.lower() in answer_str or syn in str(answer) for syn in occupation_synonyms):
                                                matches_this_filter = True
                                                match_source = f"welcome_2nd.qa_pairs.{q_text}={answer}"
                                                break
                        
                        # â­ welcome_2nd_sourceì—ì„œ ëª» ì°¾ìœ¼ë©´ í˜„ì¬ sourceì˜ qa_pairsì—ì„œ í™•ì¸ (fallback)
                        if not matches_this_filter:
                            for src in [source]:
                                if not src:
                                    continue
                                qa_pairs_list = src.get('qa_pairs', [])
                                if isinstance(qa_pairs_list, list):
                                    for qa in qa_pairs_list:
                                        if isinstance(qa, dict):
                                            q_text = qa.get('q_text', '')
                                            answer = qa.get('answer', qa.get('answer_text', ''))
                                            
                                            # ì§ì—… ì§ˆë¬¸ í™•ì¸
                                            if 'ì§ì—…' in q_text or 'occupation' in q_text.lower() or 'ì§ë¬´' in q_text:
                                                answer_str = str(answer).lower()
                                                # â­ ë™ì˜ì–´ í™•ì¥ëœ ê°’ë“¤ê³¼ ë§¤ì¹­
                                                if any(syn.lower() in answer_str or syn in str(answer) for syn in occupation_synonyms):
                                                    matches_this_filter = True
                                                    match_source = f"source.qa_pairs.{q_text}={answer}"
                                                    break
                    
                    # í•„í„° ë§¤ì¹­ ê²°ê³¼ ì €ì¥
                    filter_match_details[demo.demographic_type.value] = {
                        'matched': matches_this_filter,
                        'source': match_source,
                        'raw_value': demo.raw_value
                    }
                    
                    # í•˜ë‚˜ë¼ë„ í•„í„°ë¥¼ ë§Œì¡±í•˜ì§€ ì•Šìœ¼ë©´ ì œì™¸
                    if not matches_this_filter:
                        matches_all_filters = False
                        # í•„í„°ë³„ ë¯¸ì¶©ì¡± í†µê³„
                        if demo.demographic_type == DemographicType.AGE:
                            age_filter_failed += 1
                        elif demo.demographic_type == DemographicType.GENDER:
                            gender_filter_failed += 1
                        elif demo.demographic_type == DemographicType.OCCUPATION:
                            occupation_filter_failed += 1
                        logger.debug(f"âŒ user_id={user_id}: {demo.demographic_type.value} í•„í„° ë¯¸ì¶©ì¡± (ìš”êµ¬: {demo.raw_value})")
                        break
                    else:
                        logger.debug(f"âœ… user_id={user_id}: {demo.demographic_type.value} í•„í„° ì¶©ì¡± (ìš”êµ¬: {demo.raw_value}, ë§¤ì¹­: {match_source})")
                
                # ëª¨ë“  í•„í„°ë¥¼ ë§Œì¡±í•˜ëŠ” ë¬¸ì„œë§Œ í¬í•¨ (for ë£¨í”„ ë°–ì—ì„œ í™•ì¸)
                if matches_all_filters:
                    filtered_rrf_results.append(doc)
                    logger.debug(f"âœ… user_id={user_id}: ëª¨ë“  í•„í„° ì¶©ì¡± - í¬í•¨ë¨")
                else:
                    opposite_count += 1
                    # ë‘ í•„í„° ëª¨ë‘ ë¯¸ì¶©ì¡±ì¸ì§€ í™•ì¸
                    age_matched = filter_match_details.get('age', {}).get('matched', False)
                    occupation_matched = filter_match_details.get('occupation', {}).get('matched', False)
                    if not age_matched and not occupation_matched:
                        both_filters_failed += 1
                    # â­ ì œì™¸ëœ ë¬¸ì„œ ìƒ˜í”Œ ìƒì„¸ ë¡œê¹… (ì²˜ìŒ 10ê°œë§Œ)
                    if opposite_count <= 10:
                        logger.warning(f"âŒ ì œì™¸ëœ ë¬¸ì„œ ìƒ˜í”Œ {opposite_count}:")
                        logger.warning(f"   user_id: {user_id}")
                        logger.warning(f"   welcome_1st: {welcome_1st_source is not None}")
                        logger.warning(f"   welcome_2nd: {welcome_2nd_source is not None}")
                        if welcome_1st_source:
                            metadata_1st = welcome_1st_source.get('metadata', {})
                            logger.warning(f"   age_group: {metadata_1st.get('age_group', 'N/A')}")
                            logger.warning(f"   gender: {metadata_1st.get('gender', 'N/A')}")
                            logger.warning(f"   birth_year: {metadata_1st.get('birth_year', 'N/A')}")
                        if welcome_2nd_source:
                            metadata_2nd = welcome_2nd_source.get('metadata', {})
                            logger.warning(f"   occupation (metadata): {metadata_2nd.get('occupation', 'N/A')}")
                            qa_pairs = welcome_2nd_source.get('qa_pairs', [])
                            qa_texts = [qa.get('q_text', '') for qa in qa_pairs[:5] if isinstance(qa, dict)]
                            logger.warning(f"   qa_pairs (ì²˜ìŒ 5ê°œ): {qa_texts}")
                        logger.warning(f"   í•„í„° ë§¤ì¹­ ìƒì„¸: {filter_match_details}")
                    logger.debug(f"âŒ user_id={user_id}: í•„í„° ë¯¸ì¶©ì¡± - ì œì™¸ë¨ (ìƒì„¸: {filter_match_details})")
            
            final_hits = filtered_rrf_results[:size]
            logger.info(f"ğŸ” RRF í›„ í•„í„° ì¬ì ìš©: {len(rrf_results)}ê±´ â†’ {len(filtered_rrf_results)}ê±´")
            logger.info(f"  - sourceë¥¼ ì°¾ì§€ ëª»í•œ ë¬¸ì„œ: {source_not_found_count}ê±´")
            logger.info(f"  - RRF ì ìˆ˜ ë‚®ìŒ (0.001 ë¯¸ë§Œ): {low_score_count}ê±´")
            logger.info(f"  - í•„í„° ì¡°ê±´ ë¯¸ì¶©ì¡± ë¬¸ì„œ: {opposite_count}ê±´")
            logger.info(f"  - í•„í„° ì¡°ê±´ ì¶©ì¡± ë¬¸ì„œ: {len(filtered_rrf_results)}ê±´ (ìš”ì²­ size: {size})")
            logger.info(f"ğŸ“Š í•„í„°ë³„ ë¯¸ì¶©ì¡± í†µê³„:")
            logger.info(f"  - ì—°ë ¹ í•„í„° ë¯¸ì¶©ì¡±: {age_filter_failed}ê±´")
            logger.info(f"  - ì„±ë³„ í•„í„° ë¯¸ì¶©ì¡±: {gender_filter_failed}ê±´")
            logger.info(f"  - ì§ì—… í•„í„° ë¯¸ì¶©ì¡±: {occupation_filter_failed}ê±´")
            logger.info(f"  - ë‘ í•„í„° ëª¨ë‘ ë¯¸ì¶©ì¡±: {both_filters_failed}ê±´")
            
            # í•„í„° ì¡°ê±´ ë¯¸ì¶©ì¡± ë¬¸ì„œê°€ ë§ìœ¼ë©´ ê²½ê³ 
            if opposite_count > len(filtered_rrf_results) * 2:
                logger.warning(f"âš ï¸ í•„í„° ì¡°ê±´ ë¯¸ì¶©ì¡± ë¬¸ì„œê°€ ë§ìŠµë‹ˆë‹¤ ({opposite_count}ê±´). í•„í„° ë¡œì§ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        else:
            final_hits = rrf_results[:size]
            logger.info(f"ğŸ” RRF ê²°ê³¼ ì‚¬ìš© (í•„í„° ì—†ìŒ): {len(final_hits)}ê±´")
        
        logger.info(f"ğŸ” ìµœì¢… ê²°ê³¼: {len(final_hits)}ê±´")

        results = []
        for doc in final_hits:
            # RRF ê²°ê³¼ì—ì„œ user_id ì¶”ì¶œ (ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„)
            source = doc.get('_source', {})
            if not source and 'doc' in doc:
                # RRF ê²°ê³¼ êµ¬ì¡° í™•ì¸
                source = doc.get('doc', {}).get('_source', {})
            
            payload = source.get('payload', {}) if isinstance(source.get('payload'), dict) else {}
            user_id = (
                source.get('user_id') or 
                payload.get('user_id') or 
                doc.get('_id', '') or
                doc.get('doc', {}).get('_id', '')
            )
            
            # OpenSearchì—ì„œ ì‹¤ì œ ë¬¸ì„œ ì¡°íšŒ
            doc_id = doc.get('_id', '')
            welcome_1st_source = None  # ì—°ë ¹/ì„±ë³„ ì •ë³´ìš©
            welcome_2nd_source = None  # ì§ì—… ì •ë³´ìš©
            
            if user_id in user_doc_map:
                # user_idë¡œ ë§¤í•‘ëœ ê²½ìš°
                doc_data = user_doc_map[user_id]
                source = doc_data['source']
                inner_hits = doc_data['inner_hits']
                highlight = doc_data['highlight']
            elif doc_id in id_doc_map:
                # _idë¡œ ë§¤í•‘ëœ ê²½ìš°
                doc_data = id_doc_map[doc_id]
                source = doc_data['source']
                inner_hits = doc_data['inner_hits']
                highlight = doc_data['highlight']
            else:
                # Qdrant ê²°ê³¼ì¸ ê²½ìš°, OpenSearchì—ì„œ ì¡°íšŒ ì‹œë„
                source = {}
                inner_hits = {}
                highlight = None
                
                # Qdrant payloadì—ì„œ index ì •ë³´ í™•ì¸
                qdrant_index = payload.get('index')
                index_candidates = []
                if qdrant_index:
                    index_candidates.append(qdrant_index)
                
                # index_nameì—ì„œ ì‹¤ì œ ì¸ë±ìŠ¤ ëª©ë¡ ì¶”ì¶œ
                if request.index_name == '*':
                    # ëª¨ë“  ì¸ë±ìŠ¤ ì‹œë„ (ì¼ë°˜ì ì¸ ì¸ë±ìŠ¤ ì´ë¦„ë“¤)
                    index_candidates.extend(['s_welcome_2nd', 'survey_250106', 'survey_250107'])
                else:
                    index_candidates.extend([idx.strip() for idx in request.index_name.split(',')])
                
                # ê° ì¸ë±ìŠ¤ì—ì„œ ë¬¸ì„œ ì¡°íšŒ ì‹œë„
                for idx_name in index_candidates:
                    try:
                        os_doc = os_client.get(index=idx_name, id=user_id, ignore=[404], request_timeout=60)
                        if os_doc.get('found'):
                            source = os_doc['_source']
                            break
                    except Exception:
                        continue
            
            # â­ welcome_1stì™€ welcome_2ndì—ì„œ ì •ë³´ ì¡°íšŒ (ê²°ê³¼ì— í¬í•¨í•˜ê¸° ìœ„í•´)
            # ë°°ì¹˜ ì¡°íšŒ ê²°ê³¼ì—ì„œ ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ ì¡°íšŒí•œ ë°ì´í„° ì¬ì‚¬ìš©)
            welcome_1st_source = None
            welcome_2nd_source = None
            
            # â­ í•„í„°ê°€ ìˆëŠ” ê²½ìš°, ìµœì¢… ê²°ê³¼ í¬ë§¤íŒ… ë‹¨ê³„ì—ì„œ í•„í„° ì¡°ê±´ ì¬í™•ì¸
            if filters and extracted_entities:
                # í•„í„° ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸
                matches_all_filters = True
                
                if user_id:
                    # ë°°ì¹˜ ì¡°íšŒ ê²°ê³¼ì—ì„œ ê°€ì ¸ì˜¤ê¸°
                    welcome_1st_source = welcome_1st_batch.get(user_id) if user_id in welcome_1st_batch else None
                    welcome_2nd_source = welcome_2nd_batch.get(user_id) if user_id in welcome_2nd_batch else None
                    
                    # ê°œë³„ ì¡°íšŒ fallback
                    if not welcome_1st_source and user_id:
                        try:
                            os_doc = os_client.get(index="s_welcome_1st", id=user_id, ignore=[404], request_timeout=60)
                            if os_doc.get('found'):
                                welcome_1st_source = os_doc['_source']
                        except Exception:
                            pass
                    
                    if not welcome_2nd_source and user_id:
                        try:
                            os_doc = os_client.get(index="s_welcome_2nd", id=user_id, ignore=[404], request_timeout=60)
                            if os_doc.get('found'):
                                welcome_2nd_source = os_doc['_source']
                        except Exception:
                            pass
                
                # ê° í•„í„° ì¡°ê±´ í™•ì¸
                for demo in extracted_entities.demographics:
                    matches_this_filter = False
                    
                    if demo.demographic_type == DemographicType.AGE:
                        if welcome_1st_source:
                            age_group = welcome_1st_source.get('metadata', {}).get('age_group', '')
                            birth_year = welcome_1st_source.get('metadata', {}).get('birth_year', '')
                            
                            if age_group == demo.raw_value:
                                matches_this_filter = True
                            elif birth_year and birth_year != 'ë¯¸ì •':
                                from datetime import datetime
                                try:
                                    age = datetime.now().year - int(birth_year)
                                    if demo.raw_value == "30ëŒ€" and 30 <= age < 40:
                                        matches_this_filter = True
                                    elif demo.raw_value == "20ëŒ€" and 20 <= age < 30:
                                        matches_this_filter = True
                                    elif demo.raw_value == "40ëŒ€" and 40 <= age < 50:
                                        matches_this_filter = True
                                except (ValueError, TypeError):
                                    pass
                        
                        # qa_pairsì—ì„œë„ í™•ì¸ (fallback)
                        if not matches_this_filter:
                            for src in [welcome_1st_source, source]:
                                if not src:
                                    continue
                                qa_pairs_list = src.get('qa_pairs', [])
                                if isinstance(qa_pairs_list, list):
                                    for qa in qa_pairs_list:
                                        if isinstance(qa, dict):
                                            q_text = qa.get('q_text', '')
                                            answer = qa.get('answer', qa.get('answer_text', ''))
                                            if any(kw in q_text for kw in ['ì¶œìƒë…„ë„', 'ì¶œìƒ', 'ì—°ë ¹', 'ë‚˜ì´', 'ì—°ë ¹ëŒ€', 'age']):
                                                # ë™ì˜ì–´ í™•ì¥ ì‚¬ìš©
                                                try:
                                                    from rag_query_analyzer.utils.synonym_expander import get_synonym_expander
                                                    expander = get_synonym_expander()
                                                    age_synonyms = expander.expand(demo.raw_value)
                                                except Exception:
                                                    age_synonyms = [demo.raw_value]
                                                    age_synonyms.extend([syn for syn in demo.synonyms if syn])
                                                
                                                answer_str = str(answer).lower()
                                                if any(syn.lower() in answer_str or syn in str(answer) for syn in age_synonyms):
                                                    matches_this_filter = True
                                                    break
                    
                    elif demo.demographic_type == DemographicType.GENDER:
                        # â­ ì„±ë³„ í•„í„° í™•ì¸ ì¶”ê°€
                        # welcome_1stì—ì„œ ì„±ë³„ ì •ë³´ í™•ì¸
                        if welcome_1st_source:
                            gender = welcome_1st_source.get('metadata', {}).get('gender', '')
                            # ë™ì˜ì–´ í™•ì¥ ì‚¬ìš©
                            try:
                                from rag_query_analyzer.utils.synonym_expander import get_synonym_expander
                                expander = get_synonym_expander()
                                gender_synonyms = expander.expand(demo.raw_value)
                            except Exception:
                                gender_synonyms = [demo.raw_value]
                                gender_synonyms.extend([syn for syn in demo.synonyms if syn])
                            
                            if gender in gender_synonyms:
                                matches_this_filter = True
                        
                        # qa_pairsì—ì„œë„ í™•ì¸ (fallback)
                        if not matches_this_filter:
                            for src in [welcome_1st_source, source]:
                                if not src:
                                    continue
                                qa_pairs_list = src.get('qa_pairs', [])
                                if isinstance(qa_pairs_list, list):
                                    for qa in qa_pairs_list:
                                        if isinstance(qa, dict):
                                            q_text = qa.get('q_text', '')
                                            answer = qa.get('answer', qa.get('answer_text', ''))
                                            if 'ì„±ë³„' in q_text or 'gender' in q_text.lower():
                                                # ë™ì˜ì–´ í™•ì¥ ì‚¬ìš©
                                                try:
                                                    from rag_query_analyzer.utils.synonym_expander import get_synonym_expander
                                                    expander = get_synonym_expander()
                                                    gender_synonyms = expander.expand(demo.raw_value)
                                                except Exception:
                                                    gender_synonyms = [demo.raw_value]
                                                    gender_synonyms.extend([syn for syn in demo.synonyms if syn])
                                                
                                                answer_str = str(answer).lower()
                                                if any(syn.lower() in answer_str or syn in str(answer) for syn in gender_synonyms):
                                                    matches_this_filter = True
                                                    break
                    
                    elif demo.demographic_type == DemographicType.OCCUPATION:
                        # welcome_2nd_source ìš°ì„  í™•ì¸
                        if welcome_2nd_source:
                            qa_pairs_list = welcome_2nd_source.get('qa_pairs', [])
                            if isinstance(qa_pairs_list, list):
                                for qa in qa_pairs_list:
                                    if isinstance(qa, dict):
                                        q_text = qa.get('q_text', '')
                                        answer = qa.get('answer', qa.get('answer_text', ''))
                                        if 'ì§ì—…' in q_text or 'occupation' in q_text.lower() or 'ì§ë¬´' in q_text:
                                            # ë™ì˜ì–´ í™•ì¥ ì‚¬ìš©
                                            try:
                                                from rag_query_analyzer.utils.synonym_expander import get_synonym_expander
                                                expander = get_synonym_expander()
                                                occupation_synonyms = expander.expand(demo.raw_value)
                                            except Exception:
                                                occupation_synonyms = [demo.raw_value]
                                                occupation_synonyms.extend([syn for syn in demo.synonyms if syn])
                                            
                                            answer_str = str(answer).lower()
                                            if any(syn.lower() in answer_str or syn in str(answer) for syn in occupation_synonyms):
                                                matches_this_filter = True
                                                break
                        
                        # welcome_2nd_sourceì—ì„œ ëª» ì°¾ìœ¼ë©´ í˜„ì¬ sourceì˜ qa_pairsì—ì„œ í™•ì¸ (fallback)
                        if not matches_this_filter:
                            for src in [source]:
                                if not src:
                                    continue
                                qa_pairs_list = src.get('qa_pairs', [])
                                if isinstance(qa_pairs_list, list):
                                    for qa in qa_pairs_list:
                                        if isinstance(qa, dict):
                                            q_text = qa.get('q_text', '')
                                            answer = qa.get('answer', qa.get('answer_text', ''))
                                            if 'ì§ì—…' in q_text or 'occupation' in q_text.lower() or 'ì§ë¬´' in q_text:
                                                # ë™ì˜ì–´ í™•ì¥ ì‚¬ìš©
                                                try:
                                                    from rag_query_analyzer.utils.synonym_expander import get_synonym_expander
                                                    expander = get_synonym_expander()
                                                    occupation_synonyms = expander.expand(demo.raw_value)
                                                except Exception:
                                                    occupation_synonyms = [demo.raw_value]
                                                    occupation_synonyms.extend([syn for syn in demo.synonyms if syn])
                                                
                                                answer_str = str(answer).lower()
                                                if any(syn.lower() in answer_str or syn in str(answer) for syn in occupation_synonyms):
                                                    matches_this_filter = True
                                                    break
                    
                    if not matches_this_filter:
                        matches_all_filters = False
                        break
                
                # í•„í„° ì¡°ê±´ì„ ë§Œì¡±í•˜ì§€ ì•Šìœ¼ë©´ ì´ ë¬¸ì„œë¥¼ ê±´ë„ˆë›°ê¸°
                if not matches_all_filters:
                    logger.debug(f"âš ï¸ ìµœì¢… ê²°ê³¼ì—ì„œ ì œì™¸: user_id={user_id} (í•„í„° ì¡°ê±´ ë¯¸ì¶©ì¡±)")
                    continue
            else:
                # í•„í„°ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ welcome_1st/welcome_2nd ì¡°íšŒ
                if user_id:
                    # welcome_1st: ë°°ì¹˜ ì¡°íšŒ ê²°ê³¼ ì‚¬ìš©
                    if user_id in welcome_1st_batch:
                        welcome_1st_source = welcome_1st_batch[user_id]
                    else:
                        # ë°°ì¹˜ ì¡°íšŒì—ì„œ ëª» ì°¾ì€ ê²½ìš°ì—ë§Œ ê°œë³„ ì¡°íšŒ (fallback)
                        try:
                            os_doc = os_client.get(index='s_welcome_1st', id=user_id, ignore=[404], request_timeout=60)
                            if os_doc.get('found'):
                                welcome_1st_source = os_doc['_source']
                        except Exception:
                            pass
                    
                    # welcome_2nd: ë°°ì¹˜ ì¡°íšŒ ê²°ê³¼ ì‚¬ìš©
                    # ë¨¼ì € í˜„ì¬ sourceì—ì„œ ì§ì—… ì •ë³´ í™•ì¸
                    if source and isinstance(source, dict):
                        metadata = source.get('metadata', {})
                        occupation = metadata.get('occupation', '')
                        qa_pairs = source.get('qa_pairs', [])
                        # metadataì— occupationì´ ìˆê³  "ë¯¸ì •"ì´ ì•„ë‹ˆë©´ í˜„ì¬ source ì‚¬ìš©
                        if occupation and occupation != 'ë¯¸ì •':
                            welcome_2nd_source = source
                        # qa_pairsì— ì§ì—… ì •ë³´ê°€ ìˆìœ¼ë©´ í˜„ì¬ source ì‚¬ìš©
                        elif any('ì§ì—…' in str(qa.get('q_text', '')) for qa in qa_pairs if isinstance(qa, dict)):
                            welcome_2nd_source = source
                    
                    # ë°°ì¹˜ ì¡°íšŒ ê²°ê³¼ì—ì„œ ê°€ì ¸ì˜¤ê¸°
                    if not welcome_2nd_source and user_id in welcome_2nd_batch:
                        welcome_2nd_source = welcome_2nd_batch[user_id]
                    
                    # ë°°ì¹˜ ì¡°íšŒì—ì„œë„ ëª» ì°¾ì€ ê²½ìš°ì—ë§Œ ê°œë³„ ì¡°íšŒ (fallback)
                    if not welcome_2nd_source:
                        try:
                            os_doc = os_client.get(index='s_welcome_2nd', id=user_id, ignore=[404], request_timeout=60)
                            if os_doc.get('found'):
                                welcome_2nd_source = os_doc['_source']
                                # sourceê°€ ì—†ìœ¼ë©´ welcome_2ndë¥¼ sourceë¡œ ì‚¬ìš©
                                if not source:
                                    source = welcome_2nd_source
                        except Exception:
                            pass

            # matched_qa_pairs ì¶”ì¶œ (inner_hitsì—ì„œ)
            matched_qa = []
            
            # inner_hitsê°€ dictì¸ ê²½ìš°
            if isinstance(inner_hits, dict):
                # ëª¨ë“  nested path ìˆœíšŒ (qa_pairs, qa_pairs.answer ë“±)
                for path_name, nested_data in inner_hits.items():
                    if isinstance(nested_data, dict) and 'hits' in nested_data:
                        hits_list = nested_data['hits'].get('hits', [])
                        for inner_hit in hits_list:
                            source = inner_hit.get('_source', {})
                            if source:
                                qa_data = {
                                    'q_text': source.get('q_text', ''),
                                    'answer': source.get('answer', source.get('answer_text', '')),
                                    'answer_text': source.get('answer_text', source.get('answer', '')),
                                    'match_score': inner_hit.get('_score', 0.0)
                                }
                                if 'highlight' in inner_hit:
                                    qa_data['highlights'] = inner_hit['highlight']
                                matched_qa.append(qa_data)
            
            # RRF ê²°ê³¼ì—ì„œ ì§ì ‘ inner_hits í™•ì¸ (fallback)
            if not matched_qa and 'inner_hits' in doc:
                doc_inner_hits = doc.get('inner_hits', {})
                if isinstance(doc_inner_hits, dict):
                    for path_name, nested_data in doc_inner_hits.items():
                        if isinstance(nested_data, dict) and 'hits' in nested_data:
                            hits_list = nested_data['hits'].get('hits', [])
                            for inner_hit in hits_list:
                                source = inner_hit.get('_source', {})
                                if source:
                                    qa_data = {
                                        'q_text': source.get('q_text', ''),
                                        'answer': source.get('answer', source.get('answer_text', '')),
                                        'answer_text': source.get('answer_text', source.get('answer', '')),
                                        'match_score': inner_hit.get('_score', 0.0)
                                    }
                                    if 'highlight' in inner_hit:
                                        qa_data['highlights'] = inner_hit['highlight']
                                    matched_qa.append(qa_data)
            
            # â­ í•„í„° ë§¤ì¹­ ê²°ê³¼ë„ ì¶”ì¶œ (qa_pairsì—ì„œ ì§ì ‘ ì°¾ê¸°)
            if not matched_qa and source and 'qa_pairs' in source:
                qa_pairs_list = source.get('qa_pairs', [])
                if isinstance(qa_pairs_list, list):
                    # ì¶”ì¶œëœ ì—”í‹°í‹°ì™€ ë§¤ì¹­ë˜ëŠ” qa_pairs ì°¾ê¸°
                    for demo in extracted_entities.demographics:
                        demo_raw = demo.raw_value
                        demo_value = demo.value
                        
                        # qa_pairsì—ì„œ ë§¤ì¹­ë˜ëŠ” í•­ëª© ì°¾ê¸°
                        for qa in qa_pairs_list:
                            if isinstance(qa, dict):
                                q_text = qa.get('q_text', '')
                                answer = qa.get('answer', qa.get('answer_text', ''))
                                
                                # ì§ˆë¬¸ í‚¤ì›Œë“œ ë§¤ì¹­
                                is_demo_question = False
                                if demo.demographic_type == DemographicType.AGE:
                                    is_demo_question = any(kw in q_text for kw in ['ì—°ë ¹', 'ë‚˜ì´', 'ì—°ë ¹ëŒ€', 'age', 'ì¶œìƒ'])
                                elif demo.demographic_type == DemographicType.GENDER:
                                    is_demo_question = any(kw in q_text for kw in ['ì„±ë³„', 'gender'])
                                elif demo.demographic_type == DemographicType.OCCUPATION:
                                    is_demo_question = any(kw in q_text for kw in ['ì§ì—…', 'occupation', 'ì§ë¬´'])
                                
                                # ë‹µë³€ ë§¤ì¹­ (raw_value ë˜ëŠ” value í¬í•¨)
                                if is_demo_question and answer:
                                    answer_str = str(answer).lower()
                                    if (demo_raw.lower() in answer_str or 
                                        demo_value.lower() in answer_str or
                                        any(syn.lower() in answer_str for syn in demo.synonyms)):
                                        # ì¤‘ë³µ ì²´í¬
                                        if not any(m.get('q_text') == q_text and m.get('answer') == answer for m in matched_qa):
                                            matched_qa.append({
                                                'q_text': q_text,
                                                'answer': answer,
                                                'answer_text': answer,
                                                'match_score': 1.0,  # í•„í„° ë§¤ì¹­ì€ ë†’ì€ ì ìˆ˜
                                                'match_type': 'filter'
                                            })

            # â­ welcome_1stì™€ welcome_2nd ì •ë³´ë¥¼ ê²°ê³¼ì— í¬í•¨
            # ì—°ë ¹/ì„±ë³„ ì •ë³´ (welcome_1st)
            demographic_info = {}
            if welcome_1st_source:
                metadata_1st = welcome_1st_source.get('metadata', {})
                demographic_info['age_group'] = metadata_1st.get('age_group', 'ë¯¸ì •')
                demographic_info['gender'] = metadata_1st.get('gender', 'ë¯¸ì •')
                demographic_info['birth_year'] = metadata_1st.get('birth_year', 'ë¯¸ì •')
            
            # â­ ì§ì—… ì •ë³´ (qa_pairsì—ì„œë§Œ ì¶”ì¶œ - metadata.occupation í•„ë“œê°€ ì—†ê±°ë‚˜ "ë¯¸ì •"ì¸ ê²½ìš°ê°€ ë§ìŒ)
            occupation_value = 'ë¯¸ì •'
            
            # welcome_2nd_sourceì˜ qa_pairsì—ì„œ ì¶”ì¶œ
            if welcome_2nd_source:
                qa_pairs_list = welcome_2nd_source.get('qa_pairs', [])
                if isinstance(qa_pairs_list, list):
                    for qa in qa_pairs_list:
                        if isinstance(qa, dict):
                            q_text = qa.get('q_text', '')
                            answer = str(qa.get('answer', qa.get('answer_text', '')))
                            
                            # "ì§ì—…" ì§ˆë¬¸ì—ì„œ ë‹µë³€ ì¶”ì¶œ
                            if 'ì§ì—…' in q_text or 'occupation' in q_text.lower() or 'ì§ë¬´' in q_text:
                                if answer and answer != 'ë¯¸ì •':
                                    # ì§ì—… íƒ€ì… ë§¤í•‘
                                    answer_lower = answer.lower()
                                    if 'ì‚¬ë¬´ì§' in answer:
                                        occupation_value = 'office'
                                    elif 'ì „ë¬¸ì§' in answer:
                                        occupation_value = 'professional'
                                    elif 'ì„œë¹„ìŠ¤' in answer or 'ì„œë¹„ìŠ¤ì§' in answer:
                                        occupation_value = 'service'
                                    elif 'í•™ìƒ' in answer or 'ëŒ€í•™ìƒ' in answer or 'ëŒ€í•™ì›ìƒ' in answer:
                                        occupation_value = 'student'
                                    elif 'ì£¼ë¶€' in answer:
                                        occupation_value = 'housewife'
                                    elif 'ìì˜ì—…' in answer:
                                        occupation_value = 'self_employed'
                                    elif 'ë¬´ì§' in answer or 'ì—†ìŒ' in answer:
                                        occupation_value = 'unemployed'
                                    else:
                                        # ì›ë³¸ ê°’ ì‚¬ìš© (20ì ì œí•œ)
                                        occupation_value = answer[:20]
                                    break
            
            # welcome_2nd_sourceì—ì„œ ëª» ì°¾ì€ ê²½ìš°, í˜„ì¬ sourceì˜ qa_pairsì—ì„œ í™•ì¸
            if occupation_value == 'ë¯¸ì •' and source:
                qa_pairs_list = source.get('qa_pairs', [])
                if isinstance(qa_pairs_list, list):
                    for qa in qa_pairs_list:
                        if isinstance(qa, dict):
                            q_text = qa.get('q_text', '')
                            answer = str(qa.get('answer', qa.get('answer_text', '')))
                            
                            # "ì§ì—…" ì§ˆë¬¸ì—ì„œ ë‹µë³€ ì¶”ì¶œ
                            if 'ì§ì—…' in q_text or 'occupation' in q_text.lower() or 'ì§ë¬´' in q_text:
                                if answer and answer != 'ë¯¸ì •':
                                    # ì§ì—… íƒ€ì… ë§¤í•‘
                                    answer_lower = answer.lower()
                                    if 'ì‚¬ë¬´ì§' in answer:
                                        occupation_value = 'office'
                                    elif 'ì „ë¬¸ì§' in answer:
                                        occupation_value = 'professional'
                                    elif 'ì„œë¹„ìŠ¤' in answer or 'ì„œë¹„ìŠ¤ì§' in answer:
                                        occupation_value = 'service'
                                    elif 'í•™ìƒ' in answer or 'ëŒ€í•™ìƒ' in answer or 'ëŒ€í•™ì›ìƒ' in answer:
                                        occupation_value = 'student'
                                    elif 'ì£¼ë¶€' in answer:
                                        occupation_value = 'housewife'
                                    elif 'ìì˜ì—…' in answer:
                                        occupation_value = 'self_employed'
                                    elif 'ë¬´ì§' in answer or 'ì—†ìŒ' in answer:
                                        occupation_value = 'unemployed'
                                    else:
                                        # ì›ë³¸ ê°’ ì‚¬ìš© (20ì ì œí•œ)
                                        occupation_value = answer[:20]
                                    break
            
            demographic_info['occupation'] = occupation_value
            
            # user_idê°€ ì—†ìœ¼ë©´ doc_id ì‚¬ìš©
            final_user_id = user_id or doc_id or 'unknown'
            
            results.append(
                SearchResult(
                    user_id=final_user_id,
                    score=doc.get('_score', 0.0),
                    timestamp=source.get('timestamp'),
                    demographic_info=demographic_info if demographic_info else None,  # â­ ì¸êµ¬í†µê³„ ì •ë³´ ì¶”ê°€
                    qa_pairs=source.get('qa_pairs', [])[:5] if source else [],
                    matched_qa_pairs=matched_qa,
                    highlights=highlight,
                )
            )

        # â­ total_hits ìˆ˜ì •: ì‹¤ì œ ê²°ê³¼ ê°œìˆ˜ ì‚¬ìš© (RRF í›„ ê²°ê³¼ ê°œìˆ˜)
        actual_total_hits = len(results)
        
        return SearchResponse(
            query=request.query,
            total_hits=actual_total_hits,  # â­ ì‹¤ì œ ê²°ê³¼ ê°œìˆ˜
            max_score=final_hits[0].get('_score', 0.0) if final_hits else 0.0,
            results=results,
            query_analysis={
                "intent": analysis.intent,
                "must_terms": analysis.must_terms,
                "should_terms": analysis.should_terms,
                "alpha": analysis.alpha,
                "confidence": analysis.confidence,
                "extracted_entities": extracted_entities.to_dict(),
                "filters": filters,
                "size": size,
            },
            took_ms=took_ms,
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[ERROR] ìì—°ì–´ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# -----------------------------
# Qdrant ì§„ë‹¨/í—¬ìŠ¤ ì—”ë“œí¬ì¸íŠ¸ (ì½ê¸° ì „ìš©)
# -----------------------------

@router.get("/debug/welcome-1st", summary="welcome_1st ì¸ë±ìŠ¤ ìƒ˜í”Œ ë°ì´í„° í™•ì¸ (ë””ë²„ê¹…ìš©)")
async def get_welcome_1st_samples(
    user_id: str = None,
    age_group: str = None,
    size: int = 5,
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """
    welcome_1st ì¸ë±ìŠ¤ì˜ ìƒ˜í”Œ ë°ì´í„° í™•ì¸ (ë””ë²„ê¹…ìš©)
    
    - user_idë¡œ íŠ¹ì • ì‚¬ìš©ì ì¡°íšŒ
    - age_groupìœ¼ë¡œ í•„í„°ë§
    - metadata êµ¬ì¡° í™•ì¸
    """
    try:
        if not os_client or not os_client.ping():
            raise HTTPException(status_code=503, detail="OpenSearch ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        query = {"match_all": {}}
        
        if user_id:
            # íŠ¹ì • user_id ì¡°íšŒ
            query = {"term": {"_id": user_id}}
        elif age_group:
            # age_groupìœ¼ë¡œ í•„í„°ë§
            query = {
                "term": {
                    "metadata.age_group.keyword": age_group
                }
            }
        
        response = os_client.search(
            index="s_welcome_1st",
            body={
                "query": query,
                "size": size,
                "_source": {
                    "includes": ["user_id", "metadata", "qa_pairs"]
                }
            }
        )
        
        results = []
        for hit in response['hits']['hits']:
            source = hit.get('_source', {})
            results.append({
                "_id": hit.get('_id'),
                "user_id": source.get('user_id'),
                "metadata": source.get('metadata', {}),
                "qa_pairs_sample": source.get('qa_pairs', [])[:10] if source.get('qa_pairs') else []
            })
        
        return {
            "index_name": "s_welcome_1st",
            "query": {
                "user_id": user_id,
                "age_group": age_group
            },
            "total_hits": response['hits']['total']['value'],
            "samples": results
        }
    
    except Exception as e:
        logger.error(f"[ERROR] welcome_1st ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/debug/welcome-2nd", summary="welcome_2nd ì¸ë±ìŠ¤ ìƒ˜í”Œ ë°ì´í„° í™•ì¸ (ë””ë²„ê¹…ìš©)")
async def get_welcome_2nd_samples(
    user_id: str = None,
    occupation: str = None,
    size: int = 5,
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """
    welcome_2nd ì¸ë±ìŠ¤ì˜ ìƒ˜í”Œ ë°ì´í„° í™•ì¸ (ë””ë²„ê¹…ìš©)
    
    - user_idë¡œ íŠ¹ì • ì‚¬ìš©ì ì¡°íšŒ
    - occupationìœ¼ë¡œ í•„í„°ë§ (qa_pairsì—ì„œ)
    - metadata êµ¬ì¡° í™•ì¸
    """
    try:
        if not os_client or not os_client.ping():
            raise HTTPException(status_code=503, detail="OpenSearch ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        query = {"match_all": {}}
        
        if user_id:
            # íŠ¹ì • user_id ì¡°íšŒ
            query = {"term": {"_id": user_id}}
        elif occupation:
            # qa_pairsì—ì„œ ì§ì—…ìœ¼ë¡œ í•„í„°ë§
            query = {
                "nested": {
                    "path": "qa_pairs",
                    "query": {
                        "bool": {
                            "must": [
                                {"match": {"qa_pairs.q_text": "ì§ì—…"}},
                                {"match": {"qa_pairs.answer_text": occupation}}
                            ]
                        }
                    }
                }
            }
        
        response = os_client.search(
            index="s_welcome_2nd",
            body={
                "query": query,
                "size": size,
                "_source": {
                    "includes": ["user_id", "metadata", "qa_pairs"]
                }
            }
        )
        
        results = []
        for hit in response['hits']['hits']:
            source = hit.get('_source', {})
            results.append({
                "_id": hit.get('_id'),
                "user_id": source.get('user_id'),
                "metadata": source.get('metadata', {}),
                "qa_pairs_sample": source.get('qa_pairs', [])[:10] if source.get('qa_pairs') else []
            })
        
        return {
            "index_name": "s_welcome_2nd",
            "query": {
                "user_id": user_id,
                "occupation": occupation
            },
            "total_hits": response['hits']['total']['value'],
            "samples": results
        }
    
    except Exception as e:
        logger.error(f"[ERROR] welcome_2nd ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/debug/sample-data", summary="ì¸ë±ìŠ¤ë³„ ìƒ˜í”Œ ë°ì´í„° í™•ì¸ (ë””ë²„ê¹…ìš©)")
async def get_sample_data(
    index_name: str = "*",
    question_keyword: str = None,
    answer_keyword: str = None,
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """
    ì¸ë±ìŠ¤ë³„ ìƒ˜í”Œ ë°ì´í„° í™•ì¸ (ë””ë²„ê¹…ìš©)
    
    - íŠ¹ì • ì§ˆë¬¸ í‚¤ì›Œë“œë¡œ ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ
    - íŠ¹ì • ë‹µë³€ í‚¤ì›Œë“œë¡œ ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ
    - ì‹¤ì œ ë‹µë³€ í˜•ì‹ í™•ì¸
    """
    try:
        if not os_client or not os_client.ping():
            raise HTTPException(status_code=503, detail="OpenSearch ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        query = {"match_all": {}}
        if question_keyword or answer_keyword:
            nested_query = {}
            if question_keyword and answer_keyword:
                nested_query = {
                    "bool": {
                        "must": [
                            {"match": {"qa_pairs.q_text": question_keyword}},
                            {
                                "bool": {
                                    "should": [
                                        {"match_phrase": {"qa_pairs.answer_text": answer_keyword}},
                                        {"match_phrase": {"qa_pairs.answer": answer_keyword}},
                                        {"match": {"qa_pairs.answer_text": {"query": answer_keyword, "operator": "or"}}},
                                        {"match": {"qa_pairs.answer": {"query": answer_keyword, "operator": "or"}}}
                                    ],
                                    "minimum_should_match": 1
                                }
                            }
                        ]
                    }
                }
            elif question_keyword:
                nested_query = {"match": {"qa_pairs.q_text": question_keyword}}
            elif answer_keyword:
                nested_query = {
                    "bool": {
                        "should": [
                            {"match_phrase": {"qa_pairs.answer_text": answer_keyword}},
                            {"match_phrase": {"qa_pairs.answer": answer_keyword}},
                            {"match": {"qa_pairs.answer_text": {"query": answer_keyword, "operator": "or"}}},
                            {"match": {"qa_pairs.answer": {"query": answer_keyword, "operator": "or"}}}
                        ],
                        "minimum_should_match": 1
                    }
                }
            
            query = {
                "nested": {
                    "path": "qa_pairs",
                    "query": nested_query,
                    "inner_hits": {
                        "size": 5,
                        "_source": {"includes": ["qa_pairs.q_text", "qa_pairs.answer_text", "qa_pairs.answer"]}
                    }
                }
            }
        
        response = os_client.search(
            index=index_name,
            body={
                "query": query,
                "size": 5,
                "_source": {"includes": ["user_id", "metadata", "qa_pairs"]}
            }
        )
        
        results = []
        for hit in response['hits']['hits']:
            source = hit.get('_source', {})
            result = {
                "index": hit.get('_index'),
                "user_id": source.get('user_id'),
                "metadata": source.get('metadata', {}),
                "qa_pairs_sample": source.get('qa_pairs', [])[:5]
            }
            
            if (question_keyword or answer_keyword) and 'inner_hits' in hit:
                result['matched_qa_pairs'] = []
                for inner_hit in hit['inner_hits']['qa_pairs']['hits']['hits']:
                    result['matched_qa_pairs'].append(inner_hit.get('_source', {}))
            
            results.append(result)
        
        return {
            "index_name": index_name,
            "question_keyword": question_keyword,
            "answer_keyword": answer_keyword,
            "total_hits": response['hits']['total']['value'],
            "samples": results
        }
    
    except Exception as e:
        logger.error(f"[ERROR] ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


class TestFiltersRequest(BaseModel):
    """í•„í„° í…ŒìŠ¤íŠ¸ ìš”ì²­"""
    filters: List[Dict[str, Any]] = Field(..., description="í…ŒìŠ¤íŠ¸í•  í•„í„° ë¦¬ìŠ¤íŠ¸")
    index_name: str = Field(default="*", description="ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ì´ë¦„")


@router.post("/debug/test-filters", summary="í•„í„° ê°œë³„ í…ŒìŠ¤íŠ¸ (ë””ë²„ê¹…ìš©)")
async def test_filters(
    request: TestFiltersRequest,
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """
    í•„í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ì—¬ ì–´ë–¤ ì¸ë±ìŠ¤ì—ì„œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
    
    - ê° í•„í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì‹¤í–‰
    - ì¸ë±ìŠ¤ë³„ ê²°ê³¼ ê°œìˆ˜ í™•ì¸
    
    ì‚¬ìš© ì˜ˆì‹œ:
    ```json
    {
      "filters": [
        {
          "bool": {
            "should": [
              {"term": {"metadata.age_group.keyword": "30ëŒ€"}}
            ],
            "minimum_should_match": 1
          }
        }
      ],
      "index_name": "*"
    }
    ```
    """
    try:
        if not os_client or not os_client.ping():
            raise HTTPException(status_code=503, detail="OpenSearch ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        filters = request.filters
        index_name = request.index_name
        
        results = []
        for i, filter_dict in enumerate(filters):
            # ê° í•„í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            query = {
                "query": {
                    "bool": {
                        "must": [filter_dict]
                    }
                },
                "size": 0,  # ê°œìˆ˜ë§Œ í™•ì¸
                "aggs": {
                    "by_index": {
                        "terms": {
                            "field": "_index",
                            "size": 20
                        }
                    }
                }
            }
            
            response = os_client.search(
                index=request.index_name,
                body=query
            )
            
            # ì¸ë±ìŠ¤ë³„ ê²°ê³¼ ê°œìˆ˜
            index_counts = {}
            if 'aggregations' in response and 'by_index' in response['aggregations']:
                for bucket in response['aggregations']['by_index']['buckets']:
                    index_counts[bucket['key']] = bucket['doc_count']
            
            results.append({
                "filter_index": i,
                "filter": filter_dict,
                "total_hits": response['hits']['total']['value'],
                "index_counts": index_counts
            })
        
        # ëª¨ë“  í•„í„°ë¥¼ ANDë¡œ ê²°í•©í•œ ê²°ê³¼ë„ í…ŒìŠ¤íŠ¸
        if len(filters) > 1:
            combined_query = {
                "query": {
                    "bool": {
                        "must": filters
                    }
                },
                "size": 0,
                "aggs": {
                    "by_index": {
                        "terms": {
                            "field": "_index",
                            "size": 20
                        }
                    }
                }
            }
            
            combined_response = os_client.search(
                index=request.index_name,
                body=combined_query
            )
            
            combined_index_counts = {}
            if 'aggregations' in combined_response and 'by_index' in combined_response['aggregations']:
                for bucket in combined_response['aggregations']['by_index']['buckets']:
                    combined_index_counts[bucket['key']] = bucket['doc_count']
            
            results.append({
                "filter_index": "combined",
                "filter": "ALL FILTERS (AND)",
                "total_hits": combined_response['hits']['total']['value'],
                "index_counts": combined_index_counts
            })
        
        return {
            "index_name": request.index_name,
            "results": results
        }
    
    except Exception as e:
        logger.error(f"[ERROR] í•„í„° í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/qdrant/collections", summary="Qdrant ì»¬ë ‰ì…˜ ëª©ë¡ ë° í†µê³„")
async def list_qdrant_collections():
    qdrant_client = getattr(router, 'qdrant_client', None)
    if not qdrant_client:
        raise HTTPException(status_code=503, detail="Qdrant í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    try:
        cols = qdrant_client.get_collections()
        items = []
        for c in cols.collections:
            try:
                info = qdrant_client.get_collection(c.name)
                items.append({
                    "name": c.name,
                    "vectors_count": info.vectors_count if hasattr(info, 'vectors_count') else None,
                    "points_count": getattr(info, 'points_count', None),
                    "config": getattr(info, 'config', None).__dict__ if hasattr(info, 'config') else None,
                })
            except Exception:
                items.append({"name": c.name})
        return {"collections": items}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


class QdrantTestSearchRequest(BaseModel):
    query: str = Field(..., description="ì„ë² ë”©ìœ¼ë¡œ ê²€ìƒ‰í•  í…ìŠ¤íŠ¸")
    limit: int = Field(5, ge=1, le=100)


@router.post("/qdrant/test-search", summary="Qdrant ì „ ì»¬ë ‰ì…˜ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ (ì½ê¸° ì „ìš©)")
async def qdrant_test_search(req: QdrantTestSearchRequest):
    qdrant_client = getattr(router, 'qdrant_client', None)
    embedding_model = getattr(router, 'embedding_model', None)
    if not qdrant_client:
        raise HTTPException(status_code=503, detail="Qdrant í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if not embedding_model:
        raise HTTPException(status_code=503, detail="ì„ë² ë”© ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    try:
        qvec = embedding_model.encode(req.query).tolist()
        cols = qdrant_client.get_collections()
        results = []
        for c in cols.collections:
            try:
                r = qdrant_client.search(
                    collection_name=c.name,
                    query_vector=qvec,
                    limit=req.limit,
                )
                results.append({
                    "collection": c.name,
                    "hits": [
                        {
                            "id": str(h.id),
                            "score": h.score,
                            "payload": getattr(h, 'payload', None)
                        } for h in r
                    ]
                })
            except Exception as e:
                results.append({"collection": c.name, "error": str(e)})
        return {"query": req.query, "results": results}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/similar", summary="ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (í”Œë ˆì´ìŠ¤í™€ë”)")
async def search_similar(
    user_id: str,
    index_name: str = "s_welcome_2nd",
    size: int = 10
):
    """
    íŠ¹ì • ì‚¬ìš©ìì™€ ìœ ì‚¬í•œ ì‘ë‹µì„ ê°€ì§„ ì‚¬ìš©ì ê²€ìƒ‰ (í–¥í›„ êµ¬í˜„)
    """
    raise HTTPException(
        status_code=501,
        detail="ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ê¸°ëŠ¥ì€ í–¥í›„ êµ¬í˜„ ì˜ˆì •ì…ë‹ˆë‹¤."
    )


@router.get("/stats/{index_name}", summary="ê²€ìƒ‰ í†µê³„")
async def get_search_stats(
    index_name: str,
    os_client: OpenSearch = Depends(lambda: router.os_client),
):
    """ì¸ë±ìŠ¤ ê²€ìƒ‰ í†µê³„ ì¡°íšŒ"""
    try:
        if not os_client.indices.exists(index=index_name):
            raise HTTPException(
                status_code=404,
                detail=f"ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {index_name}"
            )

        stats = os_client.indices.stats(index=index_name)
        count = os_client.count(index=index_name)

        return {
            "index_name": index_name,
            "doc_count": count['count'],
            "size_mb": round(stats['_all']['total']['store']['size_in_bytes'] / 1024 / 1024, 2),
            "search_total": stats['_all']['total']['search']['query_total'],
            "search_time_ms": stats['_all']['total']['search']['query_time_in_millis']
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[ERROR] í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))
