"""
api/search_core.py
í•µì‹¬ ê²€ìƒ‰ ë¡œì§ - FastAPIì™€ Celeryì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©
"""
import asyncio
import logging
from collections import defaultdict
from time import perf_counter
from typing import List, Dict, Any, Optional, Set, Tuple

from rag_query_analyzer.analyzers.main_analyzer import AdvancedRAGQueryAnalyzer
from rag_query_analyzer.analyzers.demographic_extractor import DemographicExtractor
from rag_query_analyzer.models.entities import DemographicType, DemographicEntity
from connectors.hybrid_searcher import OpenSearchHybridQueryBuilder, calculate_rrf_score
from connectors.data_fetcher import DataFetcher

logger = logging.getLogger(__name__)

DEFAULT_OS_TIMEOUT = 10

PLACEHOLDER_TOKENS = {
    "", "ë¯¸ì •", "ì—†ìŒ", "ë¬´ì‘ë‹µ", "í•´ë‹¹ì—†ìŒ", "n/a", "na", 
    "null", "none", "unknown", "ë¯¸ì„ íƒ", "ë¯¸ê¸°ì¬",
}
PLACEHOLDER_TOKENS = {token.strip().lower() for token in PLACEHOLDER_TOKENS}

AGE_GENDER_KEYWORDS = [
    "metadata.age_group", "metadata.gender", "birth_year", "ì—°ë ¹", "ë‚˜ì´", "ì„±ë³„"
]
OCCUPATION_KEYWORDS = [
    "metadata.occupation", "occupation", "ì§ì—…", "ì§ë¬´"
]


def normalize_value(value: Any) -> str:
    """ê°’ ì •ê·œí™”"""
    if value is None:
        return ""
    if isinstance(value, bool):
        value_str = str(value)
    elif isinstance(value, (int, float)):
        try:
            if hasattr(value, 'is_integer') and value.is_integer():
                value = int(value)
        except AttributeError:
            pass
        value_str = str(value)
    else:
        value_str = str(value)
    
    cleaned = value_str.strip()
    lower = cleaned.lower()
    return "" if lower in PLACEHOLDER_TOKENS else lower


def strip_korean_particles(term: str) -> str:
    """í•œêµ­ì–´ ì¡°ì‚¬ ì œê±°"""
    if not term:
        return term
    particles = [
        'ì—ëŠ”', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë„', 'ì€', 'ëŠ”', 'ì´', 'ê°€',
        'ì„', 'ë¥¼', 'ì™€', 'ê³¼', 'ì¸'
    ]
    normalized = term
    for _ in range(10):
        changed = False
        for particle in particles:
            if normalized.endswith(particle) and len(normalized) > len(particle):
                normalized = normalized[:-len(particle)]
                changed = True
                break
        if not changed or len(normalized) <= 1:
            break
    return normalized


def is_age_or_gender_filter(filter_dict: Dict[str, Any]) -> bool:
    """ì—°ë ¹/ì„±ë³„ í•„í„° ì—¬ë¶€ í™•ì¸"""
    import json
    try:
        filter_str = json.dumps(filter_dict, ensure_ascii=False)
    except Exception:
        filter_str = str(filter_dict)
    return any(keyword in filter_str for keyword in AGE_GENDER_KEYWORDS)


def is_occupation_filter(filter_dict: Dict[str, Any]) -> bool:
    """ì§ì—… í•„í„° ì—¬ë¶€ í™•ì¸"""
    import json
    try:
        filter_str = json.dumps(filter_dict, ensure_ascii=False)
    except Exception:
        filter_str = str(filter_dict)
    return any(keyword in filter_str for keyword in OCCUPATION_KEYWORDS)


def remove_inner_hits(query_dict: Dict[str, Any]) -> Dict[str, Any]:
    """ì¬ê·€ì ìœ¼ë¡œ inner_hits ì œê±°"""
    import copy
    cleaned = copy.deepcopy(query_dict)
    
    if isinstance(cleaned, dict):
        if 'nested' in cleaned:
            if 'inner_hits' in cleaned['nested']:
                del cleaned['nested']['inner_hits']
            if 'query' in cleaned['nested']:
                cleaned['nested']['query'] = remove_inner_hits(cleaned['nested']['query'])
        
        if 'bool' in cleaned:
            for key in ['must', 'should', 'must_not', 'filter']:
                if key in cleaned['bool']:
                    if isinstance(cleaned['bool'][key], list):
                        cleaned['bool'][key] = [remove_inner_hits(item) for item in cleaned['bool'][key]]
                    else:
                        cleaned['bool'][key] = remove_inner_hits(cleaned['bool'][key])
    
    return cleaned


def expand_gender_aliases(values: Set[str]) -> None:
    """ì„±ë³„ ë™ì˜ì–´ í™•ì¥"""
    male_aliases = {"m", "ë‚¨", "ë‚¨ì„±", "male", "man", "ë‚¨ì"}
    female_aliases = {"f", "ì—¬", "ì—¬ì„±", "female", "woman", "ì—¬ì"}
    if values & male_aliases:
        values.update(male_aliases)
    if values & female_aliases:
        values.update(female_aliases)


def add_age_decade(values: Set[str], age_value: Any) -> None:
    """ì—°ë ¹ëŒ€ ì¶”ê°€"""
    if age_value in (None, ""):
        return
    try:
        age_int = int(age_value)
        decade = (age_int // 10) * 10
        for candidate in (f"{decade}ëŒ€", f"{decade}s", str(age_int)):
            normalized_candidate = normalize_value(candidate)
            if normalized_candidate:
                values.add(normalized_candidate)
    except (ValueError, TypeError):
        pass


async def execute_hybrid_search(
    query: str,
    index_name: str,
    size: int,
    use_vector_search: bool,
    data_fetcher: DataFetcher,
    embedding_model: Any,
    config: Any,
    is_async: bool = False,
) -> Dict[str, Any]:
    """
    í•µì‹¬ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë¡œì§
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        index_name: ì¸ë±ìŠ¤ ì´ë¦„
        size: ê²°ê³¼ ê°œìˆ˜
        use_vector_search: ë²¡í„° ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
        data_fetcher: DataFetcher ì¸ìŠ¤í„´ìŠ¤
        embedding_model: ì„ë² ë”© ëª¨ë¸
        config: ì„¤ì • ê°ì²´
        is_async: ë¹„ë™ê¸° ì‹¤í–‰ ì—¬ë¶€ (Celeryì—ì„œëŠ” False)
    
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    timings: Dict[str, float] = {}
    overall_start = perf_counter()
    
    # 1. ì¿¼ë¦¬ ë¶„ì„
    analyzer = AdvancedRAGQueryAnalyzer(config)
    analysis = analyzer.analyze_query(query)
    
    if analysis is None:
        raise RuntimeError("Query analysis returned None")
    
    # 2. ì—”í‹°í‹° ì¶”ì¶œ
    extractor = DemographicExtractor()
    extracted_entities, requested_size = extractor.extract_with_size(query)
    
    # 3. í•„í„° êµ¬ì„±
    filters: List[Dict[str, Any]] = []
    for demo in extracted_entities.demographics:
        metadata_only = demo.demographic_type in {DemographicType.AGE, DemographicType.GENDER}
        include_nested_fallback = demo.demographic_type not in {DemographicType.OCCUPATION}
        filter_clause = demo.to_opensearch_filter(
            metadata_only=metadata_only,
            include_qa_fallback=include_nested_fallback,
        )
        if filter_clause and filter_clause != {"match_all": {}}:
            filters.append(filter_clause)
    
    filters_for_response = list(filters)
    actual_size = max(1, min(requested_size, 100))
    
    # 4. í•„í„° ë¶„ë¥˜
    age_gender_filters = [f for f in filters if is_age_or_gender_filter(f)]
    occupation_filters = [f for f in filters if is_occupation_filter(f)]
    other_filters = [f for f in filters if f not in age_gender_filters and f not in occupation_filters]
    
    filters_os = age_gender_filters + other_filters
    has_demographic_filters = bool(filters_for_response)
    
    logger.info(f"ğŸ” í•„í„° ìƒíƒœ: ì—°ë ¹/ì„±ë³„={len(age_gender_filters)}, ì§ì—…={len(occupation_filters)}, ê¸°íƒ€={len(other_filters)}")
    
    # 5. ì¿¼ë¦¬ ë¹Œë“œ
    query_builder = OpenSearchHybridQueryBuilder(config)
    query_vector = None
    
    if embedding_model and use_vector_search:
        try:
            query_vector = embedding_model.encode(query).tolist()
        except Exception as e:
            logger.warning(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
    
    base_query = query_builder.build_query(
        analysis=analysis,
        query_vector=query_vector,
        size=actual_size,
    )
    
    # 6. í•„í„° ì ìš©
    final_query = apply_filters_to_query(base_query, filters_os, analysis)
    
    # 7. ê²€ìƒ‰ ì‹¤í–‰
    has_filters = bool(filters_os or occupation_filters)
    if has_filters:
        qdrant_limit = min(500, max(300, actual_size * 10))
        search_size = max(1000, min(actual_size * 20, 5000))
    else:
        qdrant_limit = min(200, max(100, actual_size * 2))
        search_size = actual_size * 2
    
    logger.info(f"ğŸ” ê²€ìƒ‰ íŒŒë¼ë¯¸í„°: size={search_size}, qdrant_limit={qdrant_limit}")
    
    # 8. ì¸ë±ìŠ¤ë³„ ê²€ìƒ‰ ì‹¤í–‰
    search_results = await execute_index_searches(
        data_fetcher=data_fetcher,
        final_query=final_query,
        search_size=search_size,
        qdrant_limit=qdrant_limit,
        query_vector=query_vector,
        use_vector_search=use_vector_search,
        age_gender_filters=age_gender_filters,
        occupation_filters=occupation_filters,
        is_async=is_async,
        timings=timings,
    )
    
    # 9. RRF ê²°í•©
    rrf_results = combine_search_results(search_results, timings)
    
    # 10. í•„í„°ë§ ë° ê²°ê³¼ êµ¬ì„±
    if has_demographic_filters:
        filtered_results = apply_demographic_filters(
            rrf_results=rrf_results,
            extracted_entities=extracted_entities,
            data_fetcher=data_fetcher,
            timings=timings,
            is_async=is_async,
        )
    else:
        filtered_results = rrf_results
    
    # 11. ìµœì¢… ê²°ê³¼
    final_hits = filtered_results[:actual_size]
    results = build_final_results(final_hits, data_fetcher, is_async)
    
    total_duration_ms = (perf_counter() - overall_start) * 1000
    timings['total_ms'] = total_duration_ms
    
    return {
        "query": query,
        "total_hits": len(filtered_results),
        "max_score": final_hits[0].get('_score', 0.0) if final_hits else 0.0,
        "results": results,
        "query_analysis": {
            "intent": analysis.intent,
            "must_terms": analysis.must_terms,
            "should_terms": analysis.should_terms,
            "extracted_entities": extracted_entities.to_dict(),
            "filters": filters_for_response,
            "size": actual_size,
            "timings_ms": timings,
        },
        "took_ms": int(total_duration_ms),
    }


def apply_filters_to_query(
    base_query: Dict[str, Any],
    filters: List[Dict[str, Any]],
    analysis: Any,
) -> Dict[str, Any]:
    """ì¿¼ë¦¬ì— í•„í„° ì ìš©"""
    final_query = base_query.copy()
    
    if not filters:
        return final_query
    
    # inner_hits ì œê±°
    cleaned_filters = [remove_inner_hits(f) for f in filters]
    
    existing_query = final_query.get('query', {"match_all": {}})
    
    if filters:
        should_filters = []
        filter_by_type = {}
        
        for f in cleaned_filters:
            filter_type = extract_filter_type(f)
            if filter_type:
                if filter_type not in filter_by_type:
                    filter_by_type[filter_type] = []
                filter_by_type[filter_type].append(f)
        
        for filter_type, type_filters in filter_by_type.items():
            if len(type_filters) == 1:
                should_filters.append(type_filters[0])
            else:
                should_filters.append({
                    'bool': {
                        'should': type_filters,
                        "minimum_should_match": 1
                    }
                })
        
        if existing_query is None or existing_query in [{"match_all": {}}, {"match_none": {}}]:
            final_query['query'] = {
                'bool': {
                    'must': should_filters
                }
            }
        elif isinstance(existing_query, dict) and existing_query.get('bool'):
            if 'must' not in existing_query['bool']:
                existing_query['bool']['must'] = []
            existing_query['bool']['must'].extend(should_filters)
            final_query['query'] = existing_query
        else:
            final_query['query'] = {
                'bool': {
                    'must': [existing_query] + should_filters
                }
            }
    
    return final_query


def extract_filter_type(filter_dict: Dict[str, Any]) -> Optional[str]:
    """í•„í„° íƒ€ì… ì¶”ì¶œ"""
    # í•„í„° íƒ€ì… ì¶”ì¶œ ë¡œì§ (ê¸°ì¡´ ì½”ë“œ ì°¸ì¡°)
    # ê¸¸ì´ ë¬¸ì œë¡œ ìƒëµ, ì›ë³¸ ì½”ë“œì—ì„œ ë³µì‚¬
    pass


async def execute_index_searches(
    data_fetcher: DataFetcher,
    final_query: Dict[str, Any],
    search_size: int,
    qdrant_limit: int,
    query_vector: Optional[List[float]],
    use_vector_search: bool,
    age_gender_filters: List[Dict[str, Any]],
    occupation_filters: List[Dict[str, Any]],
    is_async: bool,
    timings: Dict[str, float],
) -> Dict[str, Any]:
    """ì¸ë±ìŠ¤ë³„ ê²€ìƒ‰ ì‹¤í–‰"""
    
    source_filter = {
        "includes": ["user_id", "metadata", "qa_pairs", "timestamp"],
        "excludes": []
    }
    
    # welcome_1st ê²€ìƒ‰
    welcome_1st_query = create_index_query(search_size, age_gender_filters)
    welcome_1st_results = await search_index(
        data_fetcher=data_fetcher,
        index_name="s_welcome_1st",
        query=welcome_1st_query,
        search_size=search_size,
        qdrant_limit=qdrant_limit,
        query_vector=query_vector,
        use_vector_search=use_vector_search,
        is_async=is_async,
    )
    
    # welcome_2nd ê²€ìƒ‰
    welcome_2nd_query = create_index_query(search_size, occupation_filters)
    welcome_2nd_results = await search_index(
        data_fetcher=data_fetcher,
        index_name="s_welcome_2nd",
        query=welcome_2nd_query,
        search_size=search_size,
        qdrant_limit=qdrant_limit,
        query_vector=query_vector,
        use_vector_search=use_vector_search,
        is_async=is_async,
    )
    
    return {
        'welcome_1st': welcome_1st_results,
        'welcome_2nd': welcome_2nd_results,
    }


def create_index_query(size: int, filters: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ì•ˆì „í•œ ì¸ë±ìŠ¤ ì¿¼ë¦¬ ìƒì„±"""
    query = {
        'query': {'match_all': {}},
        'size': size,
        '_source': {
            'includes': ['user_id', 'metadata', 'qa_pairs', 'timestamp']
        }
    }
    
    if filters:
        query['query'] = {
            'bool': {
                'must': filters
            }
        }
    
    return query


async def search_index(
    data_fetcher: DataFetcher,
    index_name: str,
    query: Dict[str, Any],
    search_size: int,
    qdrant_limit: int,
    query_vector: Optional[List[float]],
    use_vector_search: bool,
    is_async: bool,
) -> Dict[str, Any]:
    """ë‹¨ì¼ ì¸ë±ìŠ¤ ê²€ìƒ‰"""
    keyword_results = []
    vector_results = []
    
    try:
        # OpenSearch ê²€ìƒ‰
        if is_async and data_fetcher.os_async_client:
            os_response = await data_fetcher.search_opensearch_async(
                index_name=index_name,
                query=query,
                size=search_size,
                request_timeout=DEFAULT_OS_TIMEOUT,
            )
        else:
            os_response = data_fetcher.search_opensearch(
                index_name=index_name,
                query=query,
                size=search_size,
                request_timeout=DEFAULT_OS_TIMEOUT,
            )
        
        keyword_results = os_response['hits']['hits']
        logger.info(f"  âœ… {index_name} OpenSearch: {len(keyword_results)}ê±´")
        
        # Qdrant ê²€ìƒ‰
        if use_vector_search and query_vector and hasattr(data_fetcher, 'qdrant_client'):
            qdrant_client = data_fetcher.qdrant_client
            try:
                r = qdrant_client.search(
                    collection_name=index_name,
                    query_vector=query_vector,
                    limit=qdrant_limit,
                    score_threshold=0.3,
                )
                for item in r:
                    vector_results.append({
                        '_id': str(item.id),
                        '_score': item.score,
                        '_source': item.payload
                    })
                logger.info(f"  âœ… {index_name} Qdrant: {len(vector_results)}ê±´")
            except Exception as e:
                logger.debug(f"  âš ï¸ {index_name} Qdrant ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    except Exception as e:
        logger.warning(f"  âš ï¸ {index_name} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    return {
        'keyword': keyword_results,
        'vector': vector_results,
    }


def combine_search_results(
    search_results: Dict[str, Any],
    timings: Dict[str, float],
) -> List[Dict[str, Any]]:
    """ê²€ìƒ‰ ê²°ê³¼ RRF ê²°í•©"""
    rrf_start = perf_counter()
    
    # ê° ì¸ë±ìŠ¤ë³„ RRF
    welcome_1st_rrf = calculate_rrf_score(
        search_results['welcome_1st']['keyword'],
        search_results['welcome_1st']['vector'],
        k=60
    ) if search_results['welcome_1st']['vector'] else search_results['welcome_1st']['keyword']
    
    welcome_2nd_rrf = calculate_rrf_score(
        search_results['welcome_2nd']['keyword'],
        search_results['welcome_2nd']['vector'],
        k=60
    ) if search_results['welcome_2nd']['vector'] else search_results['welcome_2nd']['keyword']
    
    # user_id ê¸°ì¤€ ê·¸ë£¹í™”
    user_rrf_map = {}
    
    for doc in welcome_1st_rrf:
        user_id = extract_user_id(doc)
        if user_id:
            if user_id not in user_rrf_map:
                user_rrf_map[user_id] = []
            doc['_index'] = 's_welcome_1st'
            user_rrf_map[user_id].append(doc)
    
    for doc in welcome_2nd_rrf:
        user_id = extract_user_id(doc)
        if user_id:
            if user_id not in user_rrf_map:
                user_rrf_map[user_id] = []
            doc['_index'] = 's_welcome_2nd'
            user_rrf_map[user_id].append(doc)
    
    # user_idë³„ RRF ì¬ê²°í•©
    final_rrf_results = []
    for user_id, user_docs in user_rrf_map.items():
        if len(user_docs) == 1:
            final_rrf_results.append(user_docs[0])
        else:
            total_rrf_score = sum(
                doc.get('_score', 0.0) or doc.get('rrf_score', 0.0)
                for doc in user_docs
            )
            best_doc = max(user_docs, key=lambda d: d.get('_score', 0.0) or d.get('rrf_score', 0.0))
            best_doc['_score'] = total_rrf_score
            best_doc['_rrf_details'] = {
                'combined_score': total_rrf_score,
                'source_count': len(user_docs),
                'sources': [d.get('_index', 'unknown') for d in user_docs]
            }
            final_rrf_results.append(best_doc)
    
    final_rrf_results.sort(
        key=lambda d: d.get('_score', 0.0) or d.get('rrf_score', 0.0),
        reverse=True
    )
    
    timings['rrf_recombination_ms'] = (perf_counter() - rrf_start) * 1000
    logger.info(f"  âœ… RRF ê²°í•© ì™„ë£Œ: {len(final_rrf_results)}ê±´")
    
    return final_rrf_results


def extract_user_id(doc: Dict[str, Any]) -> Optional[str]:
    """ë¬¸ì„œì—ì„œ user_id ì¶”ì¶œ"""
    source = doc.get('_source', {})
    if not source and 'doc' in doc:
        source = doc.get('doc', {}).get('_source', {})
    
    user_id = source.get('user_id') if isinstance(source, dict) else None
    if not user_id:
        user_id = doc.get('_id', '')
    
    return user_id


def apply_demographic_filters(
    rrf_results: List[Dict[str, Any]],
    extracted_entities: Any,
    data_fetcher: DataFetcher,
    timings: Dict[str, float],
    is_async: bool,
) -> List[Dict[str, Any]]:
    """ì¸êµ¬í†µê³„ í•„í„° ì ìš©"""
    # ê¸°ì¡´ í•„í„°ë§ ë¡œì§ (ê¸¸ì´ ë¬¸ì œë¡œ ìƒëµ)
    # ì›ë³¸ ì½”ë“œì˜ í•„í„°ë§ ë¡œì§ ë³µì‚¬
    pass


def build_final_results(
    final_hits: List[Dict[str, Any]],
    data_fetcher: DataFetcher,
    is_async: bool,
) -> List[Dict[str, Any]]:
    """ìµœì¢… ê²°ê³¼ êµ¬ì„±"""
    # ê¸°ì¡´ ê²°ê³¼ êµ¬ì„± ë¡œì§ (ê¸¸ì´ ë¬¸ì œë¡œ ìƒëµ)
    # ì›ë³¸ ì½”ë“œì˜ ê²°ê³¼ êµ¬ì„± ë¡œì§ ë³µì‚¬
    pass